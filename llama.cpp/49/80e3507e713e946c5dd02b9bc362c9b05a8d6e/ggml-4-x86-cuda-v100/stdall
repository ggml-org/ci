Requirement already satisfied: numpy~=1.24.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.9.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.2.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)
Requirement already satisfied: triton==2.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)
Requirement already satisfied: typing-extensions in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.9.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.9.0-py3-none-any.whl size=3230 sha256=ac872c337f5fc9f901211761316c23d2141cf11c8e7104c6575f94e9556df216
  Stored in directory: /tmp/pip-ephem-wheel-cache-k1axgc3p/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.9.0
    Uninstalling gguf-0.9.0:
      Successfully uninstalled gguf-0.9.0
Successfully installed gguf-0.9.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUDA=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- CUDA found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.1s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m3.390s
user	0m2.563s
sys	0m0.822s
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/ctest_debug-make.log
+ make -j
[  0%] Generating build details from Git
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o
[  2%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o
[  3%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o
[  6%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o
[  7%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o
[  8%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o
[ 11%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[ 11%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o
[ 13%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o
[ 13%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o
[ 16%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o
[ 16%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[ 16%] Built target build_info
[ 16%] Built target ggml
[ 17%] Linking CUDA static library libggml_static.a
[ 17%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[ 19%] Linking CXX executable ../../bin/gguf
[ 19%] Built target ggml_static
[ 19%] Built target gguf
[ 20%] Linking CXX static library libllama.a
[ 20%] Built target llama
[ 20%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 21%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 21%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 23%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 27%] Linking CXX executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/benchmark
[ 28%] Built target llava
[ 28%] Built target test-c
[ 28%] Linking CXX executable ../../bin/quantize
[ 28%] Linking CXX static library libllava_static.a
[ 28%] Linking CXX executable ../../bin/quantize-stats
[ 28%] Built target benchmark
[ 28%] Built target llava_static
[ 29%] Linking CXX static library libcommon.a
[ 29%] Built target common
[ 29%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-quantize-fns
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-chat-template
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 55%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 55%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 55%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 55%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 55%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 55%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 56%] Linking CXX executable ../bin/test-sampling
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-grad0
[ 57%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 58%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 58%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 58%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 58%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 58%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 58%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 58%] Linking CXX executable ../bin/test-grammar-parser
[ 58%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 58%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 59%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 60%] Linking CXX executable ../../bin/baby-llama
[ 60%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 61%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 61%] Built target quantize
[ 61%] Linking CXX executable ../bin/test-rope
[ 62%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 63%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 64%] Linking CXX executable ../bin/test-autorelease
[ 65%] Linking CXX executable ../bin/test-grammar-integration
[ 66%] Linking CXX executable ../bin/test-quantize-perf
[ 67%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 68%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 69%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 70%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 70%] Built target test-quantize-fns
[ 71%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 71%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 72%] Built target test-grad0
[ 73%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 74%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 75%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 76%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 77%] Linking CXX executable ../bin/test-backend-ops
[ 77%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 77%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 78%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 79%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 79%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 79%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 80%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 81%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 81%] Built target quantize-stats
[ 81%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 82%] Linking CXX executable ../../bin/batched-bench
[ 82%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 82%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 83%] Linking CXX executable ../../bin/eval-callback
[ 84%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 85%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 86%] Linking CXX executable ../../bin/gritlm
[ 87%] Linking CXX executable ../../bin/embedding
[ 88%] Linking CXX executable ../../bin/batched
[ 88%] Built target test-rope
[ 89%] Linking CXX executable ../../bin/beam-search
[ 90%] Linking CXX executable ../../bin/finetune
[ 91%] Linking CXX executable ../../bin/infill
[ 92%] Linking CXX executable ../../bin/tokenize
[ 92%] Built target test-grammar-parser
[ 93%] Linking CXX executable ../../bin/llama-bench
[ 93%] Linking CXX executable ../../bin/main
[ 94%] Linking CXX executable ../../bin/gguf-split
[ 95%] Linking CXX executable ../../bin/llava-cli
[ 95%] Linking CXX executable ../../bin/save-load-state
[ 96%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 96%] Linking CXX executable ../../bin/parallel
[ 96%] Built target test-quantize-perf
[ 96%] Linking CXX executable ../../bin/retrieval
[ 96%] Linking CXX executable ../../bin/perplexity
[ 96%] Linking CXX executable ../../bin/passkey
[ 96%] Linking CXX executable ../../bin/simple
[ 96%] Linking CXX executable ../../bin/q8dot
[ 96%] Linking CXX executable ../../bin/speculative
[ 96%] Linking CXX executable ../../bin/lookup-stats
[ 96%] Linking CXX executable ../../bin/lookahead
[ 97%] Linking CXX executable ../../bin/vdot
[ 98%] Linking CXX executable ../../bin/imatrix
[ 98%] Linking CXX executable ../../bin/lookup
[ 98%] Linking CXX executable ../../bin/lookup-merge
[ 98%] Linking CXX executable ../../bin/lookup-create
[ 99%] Linking CXX executable ../../bin/export-lora
[100%] Linking CXX executable ../../bin/train-text-from-scratch
[100%] Built target convert-llama2c-to-ggml
[100%] Built target q8dot
[100%] Built target vdot
[100%] Built target lookup-merge
[100%] Linking CXX executable ../../bin/server
[100%] Built target test-backend-ops
[100%] Built target export-lora
[100%] Built target test-chat-template
[100%] Built target test-sampling
[100%] Built target test-autorelease
[100%] Built target test-model-load-cancel
[100%] Built target test-json-schema-to-grammar
[100%] Built target test-grammar-integration
[100%] Built target gguf-split
[100%] Built target test-tokenizer-1-refact
[100%] Built target test-tokenizer-1-baichuan
[100%] Built target main
[100%] Built target test-tokenizer-0-llama
[100%] Built target gritlm
[100%] Built target test-tokenizer-1-stablelm-3b-4e1t
[100%] Built target baby-llama
[100%] Built target batched-bench
[100%] Built target test-tokenizer-0-falcon
[100%] Built target llama-bench
[100%] Built target llava-cli
[100%] Built target eval-callback
[100%] Built target test-tokenizer-1-falcon
[100%] Built target test-tokenizer-1-gpt2
[100%] Built target beam-search
[100%] Built target test-tokenizer-1-mpt
[100%] Built target test-tokenizer-1-llama
[100%] Built target save-load-state
[100%] Built target embedding
[100%] Built target test-tokenizer-1-starcoder
[100%] Built target test-tokenizer-1-gpt-neox
[100%] Built target batched
[100%] Built target parallel
[100%] Built target test-tokenizer-1-aquila
[100%] Built target tokenize
[100%] Built target train-text-from-scratch
[100%] Built target finetune
[100%] Built target retrieval
[100%] Built target lookup-stats
[100%] Built target simple
[100%] Built target passkey
[100%] Built target imatrix
[100%] Built target lookup-create
[100%] Built target speculative
[100%] Built target lookup
[100%] Built target infill
[100%] Built target lookahead
[100%] Built target perplexity
[100%] Built target server
[100%] Linking CXX executable ../bin/test-llama-grammar
[100%] Built target test-llama-grammar

real	0m40.107s
user	1m21.326s
sys	0m12.840s
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-quantize-fns
 1/23 Test  #1: test-quantize-fns ...................   Passed   36.15 sec
      Start  2: test-quantize-perf
 2/23 Test  #2: test-quantize-perf ..................   Passed   10.60 sec
      Start  3: test-sampling
 3/23 Test  #3: test-sampling .......................   Passed    0.07 sec
      Start  4: test-chat-template
 4/23 Test  #4: test-chat-template ..................   Passed    0.04 sec
      Start  5: test-tokenizer-0-llama
 5/23 Test  #5: test-tokenizer-0-llama ..............   Passed    0.30 sec
      Start  6: test-tokenizer-0-falcon
 6/23 Test  #6: test-tokenizer-0-falcon .............   Passed    1.76 sec
      Start  7: test-tokenizer-1-llama
 7/23 Test  #7: test-tokenizer-1-llama ..............   Passed    4.19 sec
      Start  8: test-tokenizer-1-baichuan
 8/23 Test  #8: test-tokenizer-1-baichuan ...........   Passed    4.15 sec
      Start  9: test-tokenizer-1-falcon
 9/23 Test  #9: test-tokenizer-1-falcon .............   Passed    7.82 sec
      Start 10: test-tokenizer-1-aquila
10/23 Test #10: test-tokenizer-1-aquila .............   Passed   12.66 sec
      Start 11: test-tokenizer-1-mpt
11/23 Test #11: test-tokenizer-1-mpt ................   Passed    6.42 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/23 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    6.44 sec
      Start 13: test-tokenizer-1-gpt-neox
13/23 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    6.77 sec
      Start 14: test-tokenizer-1-refact
14/23 Test #14: test-tokenizer-1-refact .............   Passed    6.80 sec
      Start 15: test-tokenizer-1-starcoder
15/23 Test #15: test-tokenizer-1-starcoder ..........   Passed    6.19 sec
      Start 16: test-tokenizer-1-gpt2
16/23 Test #16: test-tokenizer-1-gpt2 ...............   Passed    6.53 sec
      Start 17: test-grammar-parser
17/23 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/23 Test #18: test-llama-grammar ..................   Passed    0.04 sec
      Start 19: test-grammar-integration
19/23 Test #19: test-grammar-integration ............   Passed    0.05 sec
      Start 20: test-grad0
20/23 Test #20: test-grad0 ..........................   Passed    4.46 sec
      Start 21: test-backend-ops
21/23 Test #21: test-backend-ops ....................   Passed  534.55 sec
      Start 22: test-rope
22/23 Test #22: test-rope ...........................   Passed    0.08 sec
      Start 25: test-json-schema-to-grammar
23/23 Test #25: test-json-schema-to-grammar .........   Passed    1.64 sec

100% tests passed, 0 tests failed out of 23

Label Time Summary:
main    = 657.71 sec*proc (23 tests)

Total Test time (real) = 657.72 sec

real	10m57.764s
user	14m37.379s
sys	0m7.919s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUDA=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- CUDA found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.1s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.377s
user	0m2.549s
sys	0m0.825s
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/ctest_release-make.log
+ make -j
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  3%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o
[  6%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o
[  6%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o
[  7%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o
[  7%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o
[  7%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o
[  8%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o
[  8%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o
[ 11%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o
[ 12%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o
[ 13%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o
[ 13%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o
[ 14%] Built target build_info
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o
[ 16%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o
[ 16%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[ 16%] Built target ggml
[ 17%] Linking CUDA static library libggml_static.a
[ 17%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[ 17%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Built target ggml_static
[ 19%] Linking CXX executable ../../bin/gguf
[ 19%] Built target gguf
[ 20%] Linking CXX static library libllama.a
[ 20%] Built target llama
[ 20%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 20%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 21%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 23%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 25%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 27%] Linking CXX executable ../bin/test-c
[ 27%] Built target test-c
[ 28%] Linking CXX executable ../../bin/benchmark
[ 28%] Built target benchmark
[ 28%] Linking CXX executable ../../bin/quantize
[ 28%] Built target quantize
[ 28%] Linking CXX executable ../../bin/quantize-stats
[ 28%] Built target quantize-stats
[ 28%] Built target llava
[ 28%] Linking CXX static library libllava_static.a
[ 28%] Built target llava_static
[ 29%] Linking CXX static library libcommon.a
[ 29%] Built target common
[ 29%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 54%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 54%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 54%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 54%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 54%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 54%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 54%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 54%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 54%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 54%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 54%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 54%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 54%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 55%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 55%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 56%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 57%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 58%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 59%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 60%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 61%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 62%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 63%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 64%] Linking CXX executable ../bin/test-model-load-cancel
[ 65%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 66%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 67%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 67%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 68%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 69%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 69%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 70%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 70%] Built target test-model-load-cancel
[ 71%] Linking CXX executable ../bin/test-chat-template
[ 71%] Linking CXX executable ../bin/test-rope
[ 72%] Linking CXX executable ../bin/test-autorelease
[ 72%] Linking CXX executable ../bin/test-quantize-fns
[ 72%] Built target test-rope
[ 72%] Built target test-quantize-fns
[ 72%] Built target test-chat-template
[ 72%] Built target test-autorelease
[ 72%] Linking CXX executable ../bin/test-grammar-parser
[ 72%] Built target test-grammar-parser
[ 73%] Linking CXX executable ../bin/test-sampling
[ 74%] Linking CXX executable ../../bin/tokenize
[ 75%] Linking CXX executable ../bin/test-grammar-integration
[ 75%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 76%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 76%] Linking CXX executable ../../bin/lookup-merge
[ 76%] Linking CXX executable ../../bin/q8dot
[ 77%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 77%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 78%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 79%] Linking CXX executable ../../bin/baby-llama
[ 79%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 80%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 80%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 80%] Built target q8dot
[ 81%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 81%] Built target lookup-merge
[ 82%] Linking CXX executable ../../bin/vdot
[ 82%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 82%] Built target test-sampling
[ 82%] Linking CXX executable ../../bin/save-load-state
[ 82%] Built target test-grammar-integration
[ 82%] Built target vdot
[ 82%] Built target test-tokenizer-1-baichuan
[ 82%] Built target test-tokenizer-1-llama
[ 82%] Built target tokenize
[ 82%] Built target test-tokenizer-1-gpt2
[ 83%] Linking CXX executable ../../bin/eval-callback
[ 83%] Built target test-tokenizer-1-mpt
[ 83%] Built target test-tokenizer-1-falcon
[ 83%] Built target test-tokenizer-1-gpt-neox
[ 83%] Linking CXX executable ../bin/test-grad0
[ 83%] Built target test-tokenizer-1-refact
[ 83%] Built target baby-llama
[ 83%] Linking CXX executable ../../bin/lookup-create
[ 83%] Built target test-tokenizer-1-starcoder
[ 83%] Built target test-tokenizer-1-aquila
[ 84%] Linking CXX executable ../../bin/beam-search
[ 84%] Built target test-grad0
[ 84%] Built target save-load-state
[ 84%] Built target test-tokenizer-1-stablelm-3b-4e1t
[ 85%] Linking CXX executable ../../bin/gguf-split
[ 86%] Linking CXX executable ../../bin/llava-cli
[ 86%] Built target eval-callback
[ 86%] Linking CXX executable ../../bin/simple
[ 87%] Linking CXX executable ../../bin/batched
[ 87%] Built target lookup-create
[ 88%] Linking CXX executable ../../bin/gritlm
[ 89%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 90%] Linking CXX executable ../../bin/batched-bench
[ 91%] Linking CXX executable ../../bin/embedding
[ 91%] Linking CXX executable ../../bin/passkey
[ 91%] Linking CXX executable ../../bin/parallel
[ 92%] Linking CXX executable ../bin/test-quantize-perf
[ 92%] Built target beam-search
[ 92%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 92%] Linking CXX executable ../../bin/lookup-stats
[ 92%] Built target gguf-split
[ 92%] Built target llava-cli
[ 92%] Built target test-quantize-perf
[ 93%] Linking CXX executable ../../bin/export-lora
[ 93%] Built target simple
[ 93%] Built target parallel
[ 93%] Linking CXX executable ../../bin/retrieval
[ 93%] Built target test-tokenizer-0-llama
[ 93%] Built target batched-bench
[ 93%] Built target gritlm
[ 93%] Built target embedding
[ 93%] Built target batched
[ 93%] Built target passkey
[ 93%] Built target export-lora
[ 93%] Built target lookup-stats
[ 93%] Built target test-tokenizer-0-falcon
[ 94%] Linking CXX executable ../../bin/finetune
[ 94%] Linking CXX executable ../../bin/lookup
[ 94%] Built target retrieval
[ 94%] Linking CXX executable ../../bin/lookahead
[ 95%] Linking CXX executable ../../bin/train-text-from-scratch
[ 96%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 96%] Built target lookup
[ 96%] Built target finetune
[ 96%] Built target convert-llama2c-to-ggml
[ 96%] Built target train-text-from-scratch
[ 96%] Built target lookahead
[ 97%] Linking CXX executable ../../bin/infill
[ 98%] Linking CXX executable ../../bin/imatrix
[ 98%] Linking CXX executable ../../bin/speculative
[ 98%] Built target infill
[ 98%] Built target imatrix
[ 98%] Built target speculative
[ 98%] Linking CXX executable ../../bin/main
[ 98%] Built target main
[ 98%] Linking CXX executable ../bin/test-backend-ops
[ 98%] Linking CXX executable ../../bin/perplexity
[ 98%] Built target test-backend-ops
[ 98%] Built target perplexity
[ 99%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 99%] Built target test-json-schema-to-grammar
[100%] Linking CXX executable ../../bin/llama-bench
[100%] Built target llama-bench
[100%] Linking CXX executable ../bin/test-llama-grammar
[100%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	3m5.995s
user	7m31.925s
sys	0m34.979s
+ '[' -z ']'
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /home/ggml/work/llama.cpp/build-ci-release
      Start  1: test-quantize-fns
 1/23 Test  #1: test-quantize-fns ...................   Passed   20.05 sec
      Start  2: test-quantize-perf
 2/23 Test  #2: test-quantize-perf ..................   Passed    6.07 sec
      Start  3: test-sampling
 3/23 Test  #3: test-sampling .......................   Passed    0.05 sec
      Start  4: test-chat-template
 4/23 Test  #4: test-chat-template ..................   Passed    0.04 sec
      Start  5: test-tokenizer-0-llama
 5/23 Test  #5: test-tokenizer-0-llama ..............   Passed    0.09 sec
      Start  6: test-tokenizer-0-falcon
 6/23 Test  #6: test-tokenizer-0-falcon .............   Passed    0.35 sec
      Start  7: test-tokenizer-1-llama
 7/23 Test  #7: test-tokenizer-1-llama ..............   Passed    0.54 sec
      Start  8: test-tokenizer-1-baichuan
 8/23 Test  #8: test-tokenizer-1-baichuan ...........   Passed    0.60 sec
      Start  9: test-tokenizer-1-falcon
 9/23 Test  #9: test-tokenizer-1-falcon .............   Passed    1.11 sec
      Start 10: test-tokenizer-1-aquila
10/23 Test #10: test-tokenizer-1-aquila .............   Passed    1.73 sec
      Start 11: test-tokenizer-1-mpt
11/23 Test #11: test-tokenizer-1-mpt ................   Passed    0.91 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/23 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    0.91 sec
      Start 13: test-tokenizer-1-gpt-neox
13/23 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    0.90 sec
      Start 14: test-tokenizer-1-refact
14/23 Test #14: test-tokenizer-1-refact .............   Passed    0.87 sec
      Start 15: test-tokenizer-1-starcoder
15/23 Test #15: test-tokenizer-1-starcoder ..........   Passed    0.87 sec
      Start 16: test-tokenizer-1-gpt2
16/23 Test #16: test-tokenizer-1-gpt2 ...............   Passed    0.91 sec
      Start 17: test-grammar-parser
17/23 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/23 Test #18: test-llama-grammar ..................   Passed    0.04 sec
      Start 19: test-grammar-integration
19/23 Test #19: test-grammar-integration ............   Passed    0.04 sec
      Start 20: test-grad0
20/23 Test #20: test-grad0 ..........................   Passed    4.00 sec
      Start 21: test-backend-ops
21/23 Test #21: test-backend-ops ....................   Passed  137.29 sec
      Start 22: test-rope
22/23 Test #22: test-rope ...........................   Passed    0.06 sec
      Start 25: test-json-schema-to-grammar
23/23 Test #25: test-json-schema-to-grammar .........   Passed    1.57 sec

100% tests passed, 0 tests failed out of 23

Label Time Summary:
main    = 179.00 sec*proc (23 tests)

Total Test time (real) = 179.02 sec

real	2m59.053s
user	3m2.318s
sys	0m5.989s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/embd_bge_small.log
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:06 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:06 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json [711396/711396] -> "tokenizer.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:06 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:06 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:07 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:07 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:07 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:07 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:36:07 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUDA=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- CUDA found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.1s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.405s
user	0m2.562s
sys	0m0.828s
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/embd_bge_small-make.log
+ make -j
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  3%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o
[  3%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o
[  4%] Built target build_info
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o
[  6%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o
[  6%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o
[  7%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o
[  8%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o
[  8%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o
[  8%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o
[ 11%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o
[ 11%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o
[ 12%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o
[ 12%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o
[ 13%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[ 16%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o
[ 16%] Built target ggml
[ 17%] Linking CUDA static library libggml_static.a
[ 17%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[ 18%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Linking CXX executable ../../bin/gguf
[ 20%] Linking CXX static library libllama.a
[ 20%] Built target ggml_static
[ 20%] Built target gguf
[ 20%] Built target llama
[ 21%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 21%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 21%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 21%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 22%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 27%] Linking CXX executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/benchmark
[ 28%] Linking CXX executable ../../bin/quantize-stats
[ 28%] Linking CXX executable ../../bin/quantize
[ 28%] Built target llava
[ 29%] Linking CXX static library libcommon.a
[ 29%] Linking CXX static library libllava_static.a
[ 29%] Built target test-c
[ 29%] Built target benchmark
[ 29%] Built target common
[ 30%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 36%] Linking CXX executable ../bin/test-chat-template
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Linking CXX executable ../bin/test-quantize-fns
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 39%] Linking CXX executable ../bin/test-sampling
[ 40%] Linking CXX executable ../bin/test-quantize-perf
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 42%] Built target llava_static
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 47%] Built target quantize-stats
[ 48%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 49%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 50%] Built target quantize
[ 51%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 51%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 52%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 53%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 54%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 57%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-grammar-parser
[ 58%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 58%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-llama-grammar
[ 61%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 61%] Built target test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-grad0
[ 62%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../bin/test-grammar-integration
[ 64%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 64%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Linking CXX executable ../bin/test-model-load-cancel
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Linking CXX executable ../bin/test-backend-ops
[ 65%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 66%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 66%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 67%] Linking CXX executable ../bin/test-autorelease
[ 67%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 67%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 67%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 68%] Linking CXX executable ../../bin/baby-llama
[ 68%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 69%] Linking CXX executable ../../bin/batched-bench
[ 70%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 70%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 71%] Linking CXX executable ../../bin/batched
[ 71%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 72%] Linking CXX executable ../../bin/embedding
[ 72%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 73%] Linking CXX executable ../../bin/beam-search
[ 73%] Built target test-grad0
[ 73%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 73%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 74%] Linking CXX executable ../../bin/eval-callback
[ 75%] Linking CXX executable ../../bin/finetune
[ 76%] Linking CXX executable ../../bin/gguf-split
[ 77%] Linking CXX executable ../../bin/gritlm
[ 78%] Linking CXX executable ../../bin/llama-bench
[ 80%] Linking CXX executable ../../bin/llava-cli
[ 80%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 81%] Linking CXX executable ../../bin/infill
[ 81%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 82%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 82%] Built target test-chat-template
[ 83%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 84%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 85%] Linking CXX executable ../../bin/tokenize
[ 85%] Linking CXX executable ../../bin/main
[ 85%] Linking CXX executable ../../bin/parallel
[ 86%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 86%] Linking CXX executable ../../bin/perplexity
[ 87%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 87%] Built target test-grammar-parser
[ 88%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 89%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 89%] Linking CXX executable ../../bin/retrieval
[ 91%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 91%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 91%] Linking CXX executable ../../bin/simple
[ 91%] Linking CXX executable ../../bin/passkey
[ 91%] Linking CXX executable ../../bin/save-load-state
[ 91%] Built target test-llama-grammar
[ 91%] Linking CXX executable ../../bin/speculative
[ 92%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 92%] Linking CXX executable ../../bin/lookahead
[ 93%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 93%] Built target test-rope
[ 93%] Linking CXX executable ../../bin/lookup
[ 94%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 94%] Linking CXX executable ../../bin/lookup-merge
[ 94%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 94%] Built target test-sampling
[ 94%] Built target test-tokenizer-1-mpt
[ 94%] Linking CXX executable ../../bin/lookup-create
[ 94%] Linking CXX executable ../../bin/lookup-stats
[ 94%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 95%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 96%] Linking CXX executable ../../bin/train-text-from-scratch
[ 96%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 96%] Built target test-json-schema-to-grammar
[ 97%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/imatrix
[ 99%] Linking CXX executable ../../bin/vdot
[100%] Linking CXX executable ../../bin/export-lora
[100%] Built target test-model-load-cancel
[100%] Linking CXX executable ../../bin/server
[100%] Built target test-backend-ops
[100%] Linking CXX executable ../../bin/q8dot
[100%] Built target test-tokenizer-1-aquila
[100%] Built target convert-llama2c-to-ggml
[100%] Built target llama-bench
[100%] Built target test-tokenizer-0-llama
[100%] Built target test-tokenizer-1-refact
[100%] Built target test-autorelease
[100%] Built target test-tokenizer-1-gpt-neox
[100%] Built target lookup-merge
[100%] Built target test-tokenizer-0-falcon
[100%] Built target test-tokenizer-1-falcon
[100%] Built target test-tokenizer-1-baichuan
[100%] Built target test-tokenizer-1-llama
[100%] Built target test-grammar-integration
[100%] Built target test-tokenizer-1-stablelm-3b-4e1t
[100%] Built target test-tokenizer-1-gpt2
[100%] Built target test-tokenizer-1-starcoder
[100%] Built target q8dot
[100%] Built target batched-bench
[100%] Built target gritlm
[100%] Built target embedding
[100%] Built target vdot
[100%] Built target speculative
[100%] Built target tokenize
[100%] Built target beam-search
[100%] Built target gguf-split
[100%] Built target parallel
[100%] Built target llava-cli
[100%] Built target baby-llama
[100%] Built target eval-callback
[100%] Built target batched
[100%] Built target infill
[100%] Built target main
[100%] Built target finetune
[100%] Built target retrieval
[100%] Built target perplexity
[100%] Built target save-load-state
[100%] Built target lookahead
[100%] Built target passkey
[100%] Built target export-lora
[100%] Built target lookup-create
[100%] Built target train-text-from-scratch
[100%] Built target lookup
[100%] Built target simple
[100%] Built target lookup-stats
[100%] Built target imatrix
[100%] Built target server

real	0m3.190s
user	0m12.517s
sys	0m4.502s
+ python3 ../convert-hf-to-gguf.py ../models-mnt/bge-small
/mnt/llama.cpp/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading model: bge-small
gguf: This GGUF file is for Little Endian only
Set model parameters
gguf: context length = 512
gguf: embedding length = 384
gguf: feed forward length = 1536
gguf: head count = 12
gguf: layer norm epsilon = 1e-12
gguf: file type = 1
Set model tokenizer
gguf: Setting special token type unk to 100
gguf: Setting special token type sep to 102
gguf: Setting special token type pad to 0
gguf: Setting special token type cls to 101
gguf: Setting special token type mask to 103
Exporting model to '../models-mnt/bge-small/ggml-model-f16.gguf'
gguf: loading model part 'pytorch_model.bin'
token_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
position_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
token_types.weight, n_dims = 2, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
Model successfully exported to '../models-mnt/bge-small/ggml-model-f16.gguf'
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 20 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llama_model_quantize_internal: meta size = 638752 bytes
[   1/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   2/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f16, size =    0.375 MB
[   3/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   6/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   7/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   8/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   9/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  10/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  11/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  13/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  14/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  16/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  17/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  18/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  19/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  20/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  23/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  24/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  25/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  26/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  27/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  29/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  30/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  32/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  33/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  34/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  35/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  36/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  39/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  40/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  41/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  42/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  43/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  45/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  46/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  48/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  49/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  50/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  51/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  52/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  55/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  56/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  57/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  58/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  59/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  61/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  62/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  64/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  65/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  66/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  67/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  68/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  71/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  72/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  73/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  74/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  75/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  77/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  78/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  80/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  81/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  82/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  83/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  84/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  87/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  88/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  89/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  90/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  91/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  93/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  94/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  96/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  97/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  98/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  99/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 100/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 103/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 104/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 105/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 106/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 107/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 109/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 110/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 112/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 113/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 114/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 115/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 116/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 119/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 120/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 121/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 122/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 123/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 125/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 126/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 128/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 129/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 130/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 131/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 132/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 135/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 136/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 137/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 138/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 139/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 141/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 142/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 144/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 145/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 146/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 147/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 148/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 151/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 152/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 153/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 154/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 155/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 157/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 158/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 160/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 161/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 162/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 163/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 164/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 167/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 168/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 169/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 170/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 171/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 173/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 174/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 176/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 177/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 178/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 179/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 180/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 183/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 184/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 185/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 186/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 187/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 189/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 190/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 192/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 193/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 194/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 195/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 196/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_internal: model size  =    63.46 MB
llama_model_quantize_internal: quant size  =    34.00 MB

main: quantize time =   152.69 ms
main:    total time =   152.69 ms
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/embd_bge_small-tg-f16.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is'
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1713440177
llama_model_loader: loaded meta data with 20 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 63.46 MiB (16.03 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/13 layers to GPU
llm_load_tensors:        CPU buffer size =    63.46 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    16.28 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     2.51 MiB
llama_new_context_with_model: graph nodes  = 429
llama_new_context_with_model: graph splits = 196

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1

llama_print_timings:        load time =     415.34 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =      10.94 ms /     9 tokens (    1.22 ms per token,   823.05 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =     110.22 ms /    10 tokens

embedding 0: -0.043980 -0.019912  0.007672 -0.000847  0.001357 -0.037027  0.109449  0.042568  0.092071 -0.015919  0.006763 -0.035700 -0.017880  0.015042  0.018139  0.015873 -0.011285  0.010411 -0.085250 -0.008474  0.091356 -0.017079 -0.060333 -0.024499  0.027517  0.076074  0.027980 -0.014591  0.017626 -0.033282 -0.037869 -0.018992  0.068690 -0.009838 -0.025042  0.072337 -0.046593  0.011029 -0.050264  0.047713  0.032379 -0.011764  0.022071  0.049636  0.010484  0.005801 -0.028863  0.008952 -0.018501 -0.051440 -0.046068  0.030500 -0.035428  0.054216 -0.069662  0.044206  0.029797  0.046287  0.073412 -0.042574  0.076089  0.038884 -0.181177  0.082531  0.042260 -0.064546 -0.060114 -0.017828  0.006472  0.005864  0.017187 -0.026625  0.064595  0.112581  0.035168 -0.067422  0.027059 -0.067307 -0.033451 -0.033223  0.033243  0.013542 -0.003368 -0.037499 -0.052080  0.055146 -0.001978 -0.038265  0.064486  0.028840 -0.043353 -0.029268 -0.039464  0.036323  0.008383 -0.015491 -0.036587  0.018145  0.028600  0.342800 -0.044476  0.056111  0.017655 -0.020872 -0.066827  0.000160 -0.037881 -0.030081 -0.008533 -0.021565  0.000552 -0.003196  0.004042  0.018897 -0.008551  0.025819  0.049436  0.000078  0.050953 -0.042478 -0.031875  0.023616  0.030699 -0.023155 -0.046279 -0.079284  0.115171  0.046716  0.027822 -0.040720  0.067771 -0.022930  0.010317 -0.032928 -0.018292  0.043866  0.024255  0.052401  0.007463  0.008894  0.011232 -0.074659 -0.065567 -0.026750 -0.041192 -0.023865  0.026715  0.006917  0.027764  0.052897 -0.036688  0.057705 -0.000202  0.031743 -0.019776 -0.022092  0.041055 -0.058905  0.019599  0.043131  0.043616  0.041597 -0.022521  0.027044 -0.021815  0.005445 -0.041320 -0.001241  0.024429  0.002087  0.044353 -0.022751  0.043664  0.064753  0.055475  0.037085 -0.000899  0.046110  0.045818 -0.008478  0.063057 -0.073266 -0.011961  0.032133  0.023960  0.014734 -0.033697  0.001105 -0.015820 -0.019001  0.047878  0.110849  0.028416  0.031340 -0.013306 -0.057499  0.006644  0.005149 -0.012237 -0.051460 -0.000969 -0.017693 -0.019416 -0.040905  0.009214 -0.057975  0.050946  0.052343 -0.009575 -0.040239 -0.014082 -0.024889 -0.017240  0.006319  0.006573 -0.026899  0.015624  0.030750  0.002538  0.023197 -0.022215 -0.098542 -0.051112 -0.278000 -0.014962 -0.061544 -0.027231  0.017668 -0.010942 -0.017094  0.035077  0.046984 -0.015432  0.015203 -0.025466  0.047837 -0.005920 -0.000713 -0.060997 -0.068917 -0.060335 -0.035942  0.043342 -0.055060  0.015081  0.000529 -0.058183 -0.010451  0.012614  0.151479  0.127119 -0.013602  0.042012 -0.025663  0.014014 -0.001043 -0.150475  0.044860  0.005318 -0.036279 -0.029824 -0.020180 -0.034897  0.010248  0.033528 -0.048167 -0.051813 -0.017450 -0.023504  0.047350  0.052051 -0.016789 -0.055461  0.025830 -0.005702  0.010714  0.038694  0.008197 -0.009728 -0.105782 -0.027424 -0.096107  0.025068 -0.011257  0.092328  0.056092  0.003724  0.027822  0.002057 -0.051083 -0.039902 -0.013536 -0.044918 -0.015356  0.002925 -0.043535 -0.077952  0.065224 -0.006834 -0.001631 -0.014626  0.071550  0.023699 -0.037172  0.009176  0.001566 -0.032266  0.015456  0.037870  0.000353 -0.053218  0.021342 -0.039816  0.000043  0.013418  0.019822 -0.057899  0.006468 -0.049556 -0.267871  0.039179 -0.067976  0.038277 -0.012316  0.041499 -0.016146  0.052380 -0.071357  0.011356  0.024716 -0.007242  0.082073  0.028525 -0.021525  0.040495 -0.004553 -0.074583 -0.014756  0.020065  0.002288  0.023130  0.197208 -0.043240 -0.026012 -0.004964 -0.019272  0.074255  0.001725 -0.031951 -0.036587 -0.045069  0.000554 -0.011543  0.018139 -0.029457 -0.008457  0.006393  0.050811 -0.014942  0.006183  0.026101 -0.030813  0.048039  0.114068 -0.040829 -0.011454  0.005410 -0.003583  0.025136 -0.059151  0.013749 -0.010404  0.038711  0.051439  0.035423  0.035030 -0.017037  0.026372 -0.014510 -0.050039  0.003235  0.054098  0.039754 -0.039142 

real	0m0.737s
user	0m0.237s
sys	0m0.505s
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/embd_bge_small-tg-q8_0.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is'
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1713440178
llama_model_loader: loaded meta data with 21 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 7
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q8_0:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 34.00 MiB (8.59 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/13 layers to GPU
llm_load_tensors:        CPU buffer size =    34.00 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    16.90 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     2.51 MiB
llama_new_context_with_model: graph nodes  = 429
llama_new_context_with_model: graph splits = 196

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1

llama_print_timings:        load time =     385.60 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       9.22 ms /     9 tokens (    1.02 ms per token,   976.35 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =     107.51 ms /    10 tokens

embedding 0: -0.044454 -0.019806  0.008425 -0.001658  0.001779 -0.036759  0.109019  0.043373  0.091165 -0.015181  0.006789 -0.035570 -0.019173  0.013641  0.017719  0.015106 -0.013268  0.011068 -0.083926 -0.007082  0.093548 -0.017052 -0.061701 -0.024357  0.028294  0.076383  0.026934 -0.014491  0.018342 -0.034793 -0.037537 -0.017987  0.068542 -0.010658 -0.023986  0.071772 -0.046301  0.010584 -0.050453  0.050388  0.033141 -0.012715  0.021456  0.050307  0.010165  0.005712 -0.028839  0.008599 -0.018906 -0.052632 -0.045646  0.029770 -0.035216  0.051594 -0.068929  0.043866  0.029205  0.045984  0.073591 -0.042048  0.075155  0.038411 -0.182611  0.082708  0.044309 -0.065399 -0.059829 -0.016909  0.005649  0.005957  0.017086 -0.027434  0.064515  0.111774  0.035061 -0.068578  0.027102 -0.067448 -0.034405 -0.034707  0.032971  0.014231 -0.004415 -0.037086 -0.052460  0.053751 -0.003201 -0.036324  0.063400  0.028793 -0.041630 -0.029495 -0.039325  0.036156  0.007287 -0.015820 -0.035803  0.017638  0.030548  0.346732 -0.044102  0.056144  0.016571 -0.021412 -0.063607 -0.000904 -0.037150 -0.029707 -0.009419 -0.020827  0.000708 -0.002746  0.004237  0.018166 -0.010456  0.024902  0.049130 -0.001884  0.051348 -0.041483 -0.030454  0.023481  0.030748 -0.022423 -0.044380 -0.079705  0.114010  0.047061  0.026691 -0.041594  0.067796 -0.022886  0.011182 -0.033732 -0.017395  0.043946  0.022508  0.051437  0.007903  0.007249  0.009941 -0.076188 -0.064218 -0.027048 -0.040565 -0.023684  0.027036  0.006132  0.026607  0.051503 -0.037247  0.058885  0.001360  0.031979 -0.020218 -0.022019  0.041428 -0.059317  0.018868  0.043380  0.042847  0.040733 -0.021502  0.028539 -0.023809  0.006640 -0.041287  0.001707  0.024675  0.000569  0.043791 -0.024013  0.043722  0.064947  0.055666  0.038235 -0.000724  0.047338  0.045326 -0.007942  0.061108 -0.072899 -0.011986  0.033168  0.023641  0.014780 -0.033558  0.000947 -0.015551 -0.018370  0.048707  0.110872  0.028607  0.030930 -0.011407 -0.055840  0.006044  0.005040 -0.011321 -0.051681 -0.002646 -0.017572 -0.020664 -0.040170  0.009202 -0.057681  0.050109  0.051904 -0.009844 -0.041472 -0.015863 -0.025166 -0.015681  0.004928  0.007403 -0.027497  0.016514  0.029875  0.001132  0.022691 -0.021976 -0.097322 -0.050516 -0.277596 -0.013000 -0.060898 -0.026051  0.017103 -0.009631 -0.016511  0.035074  0.048198 -0.016130  0.014956 -0.024193  0.049728 -0.006102  0.000276 -0.059688 -0.069145 -0.060383 -0.035510  0.044582 -0.054949  0.015589  0.000059 -0.058474 -0.011353  0.011102  0.150165  0.126349 -0.012129  0.042156 -0.025802  0.015186 -0.000960 -0.150463  0.043391  0.005722 -0.036747 -0.029880 -0.019002 -0.033883  0.010825  0.035010 -0.048887 -0.052897 -0.016389 -0.024895  0.048129  0.050769 -0.017301 -0.056308  0.023259 -0.006064  0.011419  0.039715  0.007303 -0.008586 -0.105995 -0.027139 -0.097807  0.023942 -0.010873  0.092480  0.056145  0.004337  0.027369  0.000976 -0.051116 -0.039264 -0.012851 -0.045764 -0.014922  0.003409 -0.043754 -0.077136  0.065538 -0.006276 -0.001407 -0.015571  0.069890  0.024463 -0.035971  0.007809  0.002233 -0.033726  0.016579  0.037806 -0.000158 -0.051953  0.021956 -0.039035  0.000098  0.012115  0.021044 -0.057901  0.005945 -0.049439 -0.269367  0.038631 -0.066569  0.037664 -0.012043  0.043172 -0.016206  0.051795 -0.072338  0.012282  0.024592 -0.007345  0.082538  0.030200 -0.021015  0.042242 -0.003036 -0.074485 -0.015457  0.019792  0.001831  0.023724  0.194983 -0.043327 -0.025375 -0.005301 -0.018949  0.074287  0.000796 -0.031119 -0.037353 -0.044217  0.000275 -0.010344  0.019217 -0.027160 -0.009570  0.006057  0.050269 -0.013845  0.006808  0.026766 -0.031702  0.048504  0.111706 -0.040231 -0.011500  0.004156 -0.003576  0.024720 -0.060324  0.014739 -0.010465  0.038206  0.049283  0.035079  0.037173 -0.016362  0.025752 -0.015522 -0.050739  0.004487  0.054170  0.040495 -0.039931 

real	0m0.704s
user	0m0.199s
sys	0m0.523s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_test_scripts_debug
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/test_scripts_debug.log
+ set -e
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/test_scripts_debug-scripts.log
+ cd ./examples/gguf-split
+ bash tests.sh /home/ggml/work/llama.cpp/build-ci-debug/bin /mnt/llama.cpp/models
+ SPLIT=/home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split
+ MAIN=/home/ggml/work/llama.cpp/build-ci-debug/bin/main
+ WORK_PATH=/mnt/llama.cpp/models/gguf-split
++ pwd
+ CUR_DIR=/home/ggml/work/llama.cpp/examples/gguf-split
+ mkdir -p /mnt/llama.cpp/models/gguf-split
+ rm -f '/mnt/llama.cpp/models/gguf-split/ggml-model-split*.gguf' '/mnt/llama.cpp/models/gguf-split/ggml-model-merge*.gguf'
+ cd /mnt/llama.cpp/models/gguf-split
+ /home/ggml/work/llama.cpp/examples/gguf-split/../../scripts/hf.sh --repo ggml-org/gemma-1.1-2b-it-Q8_0-GGUF --file gemma-1.1-2b-it.Q8_0.gguf
[+] attempting to download gemma-1.1-2b-it.Q8_0.gguf
[+] wget -q --show-progress -c -O ./gemma-1.1-2b-it.Q8_0.gguf https://huggingface.co/ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/resolve/main/gemma-1.1-2b-it.Q8_0.gguf
./gemma-1.1-2b-it.Q8_0.gguf
+ echo PASS
PASS
+ /home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split --split-max-tensors 28 /mnt/llama.cpp/models/gguf-split/gemma-1.1-2b-it.Q8_0.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split
n_split: 6
split 00001: n_tensors = 28, total_size = 871M
split 00002: n_tensors = 28, total_size = 334M
split 00003: n_tensors = 28, total_size = 402M
split 00004: n_tensors = 28, total_size = 335M
split 00005: n_tensors = 28, total_size = 338M
split 00006: n_tensors = 24, total_size = 261M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ... gguf_split: 6 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf --random-prompt --n-predict 32
Log start
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1713440182
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.06 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/19 layers to GPU
llm_load_tensors:        CPU buffer size =   865.98 MiB
llm_load_tensors:        CPU buffer size =   334.74 MiB
llm_load_tensors:        CPU buffer size =   402.73 MiB
llm_load_tensors:        CPU buffer size =   335.27 MiB
llm_load_tensors:        CPU buffer size =   338.98 MiB
llm_load_tensors:        CPU buffer size =   261.96 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CUDA0 buffer from size 0.00 MiB to 1035.25 MiB
ggml_gallocr_reserve_n: reallocating CUDA_Host buffer from size 0.00 MiB to 5.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1035.25 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 202
ggml_gallocr_needs_realloc: graph has different number of nodes
ggml_gallocr_alloc_graph: cannot reallocate multi buffer graph automatically, call reserve
ggml_backend_sched_alloc_splits: failed to allocate graph, reserving

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


 Shekera was a powerful warrior queen who defended her kingdom from the clutches of invaders. She was also a skilled administrator and diplomat, earning the respect and admiration of
llama_print_timings:        load time =    3470.03 ms
llama_print_timings:      sample time =      51.50 ms /    32 runs   (    1.61 ms per token,   621.35 tokens per second)
llama_print_timings: prompt eval time =    2602.81 ms /     2 tokens ( 1301.40 ms per token,     0.77 tokens per second)
llama_print_timings:        eval time =   49953.37 ms /    31 runs   ( 1611.40 ms per token,     0.62 tokens per second)
llama_print_timings:       total time =   54431.99 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split --merge /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf
gguf_merge: /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf -> /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ...[3Ddone
gguf_merge: /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf merged from 6 split with 164 tensors.
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf --random-prompt --n-predict 32
Log start
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1713440246
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 0
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.06 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/19 layers to GPU
llm_load_tensors:        CPU buffer size =  2539.66 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CUDA0 buffer from size 0.00 MiB to 1035.25 MiB
ggml_gallocr_reserve_n: reallocating CUDA_Host buffer from size 0.00 MiB to 5.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1035.25 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 202
ggml_gallocr_needs_realloc: graph has different number of nodes
ggml_gallocr_alloc_graph: cannot reallocate multi buffer graph automatically, call reserve
ggml_backend_sched_alloc_splits: failed to allocate graph, reserving

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


 Soar!

**Verse 1:**
In the realm of skies so vast,
Where dreams take flight with grace.
With wings outstretched, soaring high
llama_print_timings:        load time =    3162.74 ms
llama_print_timings:      sample time =      50.77 ms /    32 runs   (    1.59 ms per token,   630.32 tokens per second)
llama_print_timings: prompt eval time =    2622.14 ms /     2 tokens ( 1311.07 ms per token,     0.76 tokens per second)
llama_print_timings:        eval time =   48767.78 ms /    31 runs   ( 1573.15 ms per token,     0.64 tokens per second)
llama_print_timings:       total time =   51623.05 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split --split-max-size 2G /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G
n_split: 2
split 00001: n_tensors = 118, total_size = 2021M
split 00002: n_tensors = 46, total_size = 523M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00002-of-00002.gguf ... gguf_split: 2 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf --random-prompt --n-predict 32
Log start
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1713440305
llama_model_loader: additional 1 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 2
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.06 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/19 layers to GPU
llm_load_tensors:        CPU buffer size =  2015.76 MiB
llm_load_tensors:        CPU buffer size =   523.91 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CUDA0 buffer from size 0.00 MiB to 1035.25 MiB
ggml_gallocr_reserve_n: reallocating CUDA_Host buffer from size 0.00 MiB to 5.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1035.25 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 202
ggml_gallocr_needs_realloc: graph has different number of nodes
ggml_gallocr_alloc_graph: cannot reallocate multi buffer graph automatically, call reserve
ggml_backend_sched_alloc_splits: failed to allocate graph, reserving

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


 When to Use the Subjunctive Mood

The subjunctive mood is used to express a hypothetical situation, a wish, a suggestion, or a possibility.

**Usage
llama_print_timings:        load time =    3724.36 ms
llama_print_timings:      sample time =      50.90 ms /    32 runs   (    1.59 ms per token,   628.67 tokens per second)
llama_print_timings: prompt eval time =    2817.33 ms /     2 tokens ( 1408.66 ms per token,     0.71 tokens per second)
llama_print_timings:        eval time =   47955.43 ms /    31 runs   ( 1546.95 ms per token,     0.65 tokens per second)
llama_print_timings:       total time =   51008.03 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ rm -f /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00002-of-00002.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf

real	3m4.366s
user	8m11.936s
sys	0m12.329s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_test_scripts_release
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/test_scripts_release.log
+ set -e
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/test_scripts_release-scripts.log
+ cd ./examples/gguf-split
+ bash tests.sh /home/ggml/work/llama.cpp/build-ci-release/bin /mnt/llama.cpp/models
+ SPLIT=/home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split
+ MAIN=/home/ggml/work/llama.cpp/build-ci-release/bin/main
+ WORK_PATH=/mnt/llama.cpp/models/gguf-split
++ pwd
+ CUR_DIR=/home/ggml/work/llama.cpp/examples/gguf-split
+ mkdir -p /mnt/llama.cpp/models/gguf-split
+ rm -f '/mnt/llama.cpp/models/gguf-split/ggml-model-split*.gguf' '/mnt/llama.cpp/models/gguf-split/ggml-model-merge*.gguf'
+ cd /mnt/llama.cpp/models/gguf-split
+ /home/ggml/work/llama.cpp/examples/gguf-split/../../scripts/hf.sh --repo ggml-org/gemma-1.1-2b-it-Q8_0-GGUF --file gemma-1.1-2b-it.Q8_0.gguf
[+] attempting to download gemma-1.1-2b-it.Q8_0.gguf
[+] wget -q --show-progress -c -O ./gemma-1.1-2b-it.Q8_0.gguf https://huggingface.co/ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/resolve/main/gemma-1.1-2b-it.Q8_0.gguf
./gemma-1.1-2b-it.Q8_0.gguf
+ echo PASS
PASS
+ /home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split --split-max-tensors 28 /mnt/llama.cpp/models/gguf-split/gemma-1.1-2b-it.Q8_0.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split
n_split: 6
split 00001: n_tensors = 28, total_size = 871M
split 00002: n_tensors = 28, total_size = 334M
split 00003: n_tensors = 28, total_size = 402M
split 00004: n_tensors = 28, total_size = 335M
split 00005: n_tensors = 28, total_size = 338M
split 00006: n_tensors = 24, total_size = 261M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ... gguf_split: 6 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf --random-prompt --n-predict 32
Log start
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1713440366
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.06 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/19 layers to GPU
llm_load_tensors:        CPU buffer size =   865.98 MiB
llm_load_tensors:        CPU buffer size =   334.74 MiB
llm_load_tensors:        CPU buffer size =   402.73 MiB
llm_load_tensors:        CPU buffer size =   335.27 MiB
llm_load_tensors:        CPU buffer size =   338.98 MiB
llm_load_tensors:        CPU buffer size =   261.96 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1035.25 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 202

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


 They call me the "Forgotten One." A name that stings with a sense of isolation and longing.

For years, I wandered lost, searching for my place in
llama_print_timings:        load time =     698.50 ms
llama_print_timings:      sample time =       5.29 ms /    32 runs   (    0.17 ms per token,  6048.01 tokens per second)
llama_print_timings: prompt eval time =     165.98 ms /     2 tokens (   82.99 ms per token,    12.05 tokens per second)
llama_print_timings:        eval time =    3440.67 ms /    31 runs   (  110.99 ms per token,     9.01 tokens per second)
llama_print_timings:       total time =    3696.04 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split --merge /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf
gguf_merge: /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf -> /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ...[3Ddone
gguf_merge: /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf merged from 6 split with 164 tensors.
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf --random-prompt --n-predict 32
Log start
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1713440375
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 0
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.06 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/19 layers to GPU
llm_load_tensors:        CPU buffer size =  2539.66 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1035.25 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 202

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


 import java.util.*;

public class WordSearch {

    public static void main(String[] args) {
        String[][] board = {
                {"
llama_print_timings:        load time =     662.29 ms
llama_print_timings:      sample time =       5.05 ms /    32 runs   (    0.16 ms per token,  6331.62 tokens per second)
llama_print_timings: prompt eval time =     158.29 ms /     2 tokens (   79.15 ms per token,    12.63 tokens per second)
llama_print_timings:        eval time =    3443.89 ms /    31 runs   (  111.09 ms per token,     9.00 tokens per second)
llama_print_timings:       total time =    3688.86 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split --split-max-size 2G /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G
n_split: 2
split 00001: n_tensors = 118, total_size = 2021M
split 00002: n_tensors = 46, total_size = 523M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00002-of-00002.gguf ... gguf_split: 2 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf --random-prompt --n-predict 32
Log start
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1713440383
llama_model_loader: additional 1 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 2
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.06 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/19 layers to GPU
llm_load_tensors:        CPU buffer size =  2015.76 MiB
llm_load_tensors:        CPU buffer size =   523.91 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1035.25 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 202

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


 Shew, a leading online marketplace for curated personal care products, is offering a 10% discount on all orders placed between [date] and [date].
llama_print_timings:        load time =     723.98 ms
llama_print_timings:      sample time =       6.38 ms /    32 runs   (    0.20 ms per token,  5018.82 tokens per second)
llama_print_timings: prompt eval time =     191.10 ms /     2 tokens (   95.55 ms per token,    10.47 tokens per second)
llama_print_timings:        eval time =    3929.71 ms /    31 runs   (  126.76 ms per token,     7.89 tokens per second)
llama_print_timings:       total time =    4215.63 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ rm -f /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00002-of-00002.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf

real	0m26.533s
user	0m37.164s
sys	0m12.058s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_open_llama_7b_v2
+ tee /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/open_llama_7b_v2.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:39:50 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json [502/502] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:39:50 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json [593/593] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:39:50 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json [330/330] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:39:50 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json [26788/26788] -> "pytorch_model.bin.index.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
Last-modified header missing -- time-stamps turned off.
2024-04-18 11:39:51 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json [132/132] -> "generation_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /home/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ path_models=../models-mnt/open-llama/7B-v2
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/open_llama_7b_v2-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUDA=1 -DLLAMA_CUDA=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- CUDA found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.1s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.383s
user	0m2.523s
sys	0m0.862s
+ tee -a /home/ggml/results/llama.cpp/49/80e3507e713e946c5dd02b9bc362c9b05a8d6e/ggml-4-x86-cuda-v100/open_llama_7b_v2-make.log
+ make -j
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  3%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o
[  5%] Built target build_info
[  6%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o
[  6%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o
[  7%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o
[  8%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o
[  8%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o
[  9%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o
[ 10%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o
[ 11%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o
[ 11%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o
[ 12%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o
[ 12%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o
[ 13%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o
[ 13%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o
[ 14%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o
[ 15%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[ 16%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o
[ 16%] Built target ggml
[ 17%] Linking CUDA static library libggml_static.a
[ 17%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[ 18%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Linking CXX executable ../../bin/gguf
[ 20%] Linking CXX static library libllama.a
[ 20%] Built target gguf
[ 20%] Built target ggml_static
[ 20%] Built target llama
[ 21%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 21%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 21%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 21%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 21%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 23%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 27%] Linking CXX executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/benchmark
[ 28%] Built target llava
[ 28%] Linking CXX executable ../../bin/quantize
[ 28%] Linking CXX executable ../../bin/quantize-stats
[ 28%] Linking CXX static library libllava_static.a
[ 29%] Linking CXX static library libcommon.a
[ 29%] Built target test-c
[ 29%] Built target benchmark
[ 29%] Built target llava_static
[ 29%] Built target common
[ 29%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-quantize-fns
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-sampling
[ 38%] Linking CXX executable ../bin/test-quantize-perf
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 39%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Linking CXX executable ../bin/test-chat-template
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 47%] Built target quantize
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 48%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 48%] Built target quantize-stats
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 49%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 49%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 51%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 53%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 54%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 56%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 56%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 57%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Linking CXX executable ../bin/test-grammar-parser
[ 58%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 58%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 59%] Linking CXX executable ../bin/test-grammar-integration
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-llama-grammar
[ 62%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 62%] Linking CXX executable ../bin/test-grad0
[ 62%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 64%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 64%] Linking CXX executable ../bin/test-backend-ops
[ 64%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 64%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 65%] Linking CXX executable ../bin/test-model-load-cancel
[ 66%] Linking CXX executable ../bin/test-autorelease
[ 67%] Linking CXX executable ../../bin/batched
[ 67%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 67%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 68%] Linking CXX executable ../../bin/baby-llama
[ 68%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 69%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 69%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 69%] Built target test-quantize-perf
[ 69%] Built target test-quantize-fns
[ 70%] Linking CXX executable ../../bin/batched-bench
[ 71%] Linking CXX executable ../../bin/beam-search
[ 72%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 72%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 72%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 73%] Linking CXX executable ../../bin/embedding
[ 73%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 73%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 73%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 74%] Linking CXX executable ../../bin/finetune
[ 75%] Linking CXX executable ../../bin/eval-callback
[ 75%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 75%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 76%] Built target test-grad0
[ 77%] Linking CXX executable ../../bin/gritlm
[ 78%] Linking CXX executable ../../bin/gguf-split
[ 78%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 79%] Linking CXX executable ../../bin/infill
[ 80%] Linking CXX executable ../../bin/llava-cli
[ 80%] Linking CXX executable ../../bin/main
[ 81%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 81%] Built target test-chat-template
[ 82%] Linking CXX executable ../../bin/llama-bench
[ 82%] Built target test-grammar-parser
[ 83%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 84%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 85%] Linking CXX executable ../../bin/tokenize
[ 85%] Linking CXX executable ../../bin/parallel
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 87%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 88%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 89%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 90%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 90%] Linking CXX executable ../../bin/perplexity
[ 90%] Linking CXX executable ../../bin/retrieval
[ 91%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 91%] Linking CXX executable ../../bin/save-load-state
[ 91%] Linking CXX executable ../../bin/passkey
[ 91%] Linking CXX executable ../../bin/simple
[ 92%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 93%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 93%] Linking CXX executable ../../bin/lookahead
[ 93%] Linking CXX executable ../../bin/speculative
[ 94%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 94%] Built target test-sampling
[ 94%] Linking CXX executable ../../bin/lookup
[ 94%] Linking CXX executable ../../bin/lookup-merge
[ 94%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 94%] Linking CXX executable ../../bin/lookup-create
[ 94%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 94%] Built target test-rope
[ 95%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 95%] Built target convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/train-text-from-scratch
[ 96%] Linking CXX executable ../../bin/lookup-stats
[ 96%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 97%] Linking CXX executable ../../bin/imatrix
[ 97%] Built target test-json-schema-to-grammar
[ 97%] Built target test-model-load-cancel
[ 97%] Built target test-tokenizer-1-baichuan
[ 97%] Built target test-grammar-integration
[ 98%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 99%] Linking CXX executable ../../bin/export-lora
[ 99%] Linking CXX executable ../../bin/server
[ 99%] Linking CXX executable ../../bin/q8dot
[100%] Linking CXX executable ../../bin/vdot
[100%] Built target test-tokenizer-1-falcon
[100%] Built target baby-llama
[100%] Built target test-tokenizer-1-stablelm-3b-4e1t
[100%] Built target test-llama-grammar
[100%] Built target test-tokenizer-1-llama
[100%] Built target test-autorelease
[100%] Built target test-tokenizer-0-falcon
[100%] Built target test-tokenizer-1-mpt
[100%] Built target test-tokenizer-1-aquila
[100%] Built target test-tokenizer-1-refact
[100%] Built target test-tokenizer-0-llama
[100%] Built target test-backend-ops
[100%] Built target test-tokenizer-1-gpt-neox
[100%] Built target lookup-merge
[100%] Built target main
[100%] Built target test-tokenizer-1-starcoder
[100%] Built target test-tokenizer-1-gpt2
[100%] Built target q8dot
[100%] Built target batched
[100%] Built target finetune
[100%] Built target beam-search
[100%] Built target embedding
[100%] Built target gguf-split
[100%] Built target vdot
[100%] Built target gritlm
[100%] Built target batched-bench
[100%] Built target llama-bench
[100%] Built target infill
[100%] Built target eval-callback
[100%] Built target retrieval
[100%] Built target lookahead
[100%] Built target save-load-state
[100%] Built target train-text-from-scratch
[100%] Built target tokenize
[100%] Built target export-lora
[100%] Built target imatrix
[100%] Built target lookup-stats
[100%] Built target speculative
[100%] Built target llava-cli
[100%] Built target parallel
[100%] Built target perplexity
[100%] Built target server
[100%] Built target lookup-create
[100%] Built target simple
[100%] Built target passkey
[100%] Built target lookup

real	0m3.180s
user	0m12.340s
sys	0m4.704s
+ python3 ../convert.py ../models-mnt/open-llama/7B-v2
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00002-of-00002.bin
params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/7B-v2'))
Loaded vocab file PosixPath('../models-mnt/open-llama/7B-v2/tokenizer.model'), type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
Permuting layer 26
Permuting layer 27
Permuting layer 28
Permuting layer 29
Permuting layer 30
Permuting layer 31
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.0.attn_rot_embd
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.1.attn_rot_embd
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.2.attn_rot_embd
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.3.attn_rot_embd
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.4.attn_rot_embd
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.5.attn_rot_embd
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.6.attn_rot_embd
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.7.attn_rot_embd
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.8.attn_rot_embd
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.9.attn_rot_embd
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.10.attn_rot_embd
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.11.attn_rot_embd
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.12.attn_rot_embd
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.13.attn_rot_embd
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.14.attn_rot_embd
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.15.attn_rot_embd
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.16.attn_rot_embd
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.17.attn_rot_embd
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.18.attn_rot_embd
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.19.attn_rot_embd
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.20.attn_rot_embd
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.21.attn_rot_embd
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.22.attn_rot_embd
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.23.attn_rot_embd
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.24.attn_rot_embd
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.25.attn_rot_embd
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]
model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.26.attn_rot_embd
model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]
model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]
model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.27.attn_rot_embd
model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]
model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]
model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.28.attn_rot_embd
model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]
model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]
model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.29.attn_rot_embd
model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]
model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]
model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.30.attn_rot_embd
model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]
model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]
model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.31.attn_rot_embd
model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]
model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]
model.norm.weight                                -> output_norm.weight                       | F16    | [4096]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]
Writing ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf, format 1
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0
[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0
[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0
[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0
[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0
[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0
[  7/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0
[  8/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0
[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0
[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0
[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0
[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 16/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 17/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 25/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 26/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 34/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 35/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 43/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 44/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 52/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 53/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 61/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 62/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[ 70/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 71/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[ 79/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 80/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[ 88/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 89/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[ 97/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[ 98/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   4
[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[106/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[107/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   4
[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[115/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[116/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   4
[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[124/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[125/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   5
[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5
[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[133/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[134/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   5
[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5
[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[142/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[143/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   5
[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[151/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[152/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   6
[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6
[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[160/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[161/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   6
[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6
[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[169/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[170/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   7
[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[178/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[179/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7
[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   7
[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[187/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[188/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7
[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   7
[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[196/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[197/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   8
[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[205/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[206/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   8
[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[214/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[215/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   8
[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[223/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[224/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   9
[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9
[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9
[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9
[232/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9
[233/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9
[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   9
[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9
[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9
[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9
[241/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10
[242/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10
[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  10
[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10
[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10
[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10
[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10
[250/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10
[251/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10
[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  10
[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10
[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10
[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11
[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  11
[259/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  11
[260/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  11
[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  11
[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  11
[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11
[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  11
[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  11
[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11
[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  11
[268/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  11
[269/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  11
[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  11
[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  11
[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11
[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  11
[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  11
[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11
[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  11
[277/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  11
[278/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  11
[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  11
[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  11
[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11
[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  11
[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  11
[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11
[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  12
[286/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  12
[287/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  12
[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  12
[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  12
[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  12
[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+  12
Wrote ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_f16=../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf
+ wiki_test=../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf q8_0
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   250.00 MiB ->   132.81 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   250.00 MiB ->   132.81 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  6828.64 MB

main: quantize time = 48503.85 ms
main:    total time = 48503.85 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf q4_0
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_0 .. size =   250.00 MiB ->    70.31 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3647.87 MB

main: quantize time = 26040.38 ms
main:    total time = 26040.38 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf q4_1
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_1 .. size =   250.00 MiB ->    78.12 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4041.68 MB

main: quantize time = 28240.44 ms
main:    total time = 28240.44 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf q5_0
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q5_0 .. size =   250.00 MiB ->    85.94 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4435.49 MB

main: quantize time = 31336.09 ms
main:    total time = 31336.09 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf q5_1
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q5_1 .. size =   250.00 MiB ->    93.75 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4829.30 MB

main: quantize time = 32985.54 ms
main:    total time = 32985.54 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf q2_k
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q2_K .. size =   250.00 MiB ->    41.02 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  2414.82 MB

main: quantize time = 74633.80 ms
main:    total time = 74633.80 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf q3_k
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q3_K .. size =   250.00 MiB ->    53.71 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3144.52 MB

main: quantize time = 72056.81 ms
main:    total time = 72056.81 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf q4_k
main: build = 2710 (4980e350)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_K .. size =   250.00 MiB ->    70.31 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. 