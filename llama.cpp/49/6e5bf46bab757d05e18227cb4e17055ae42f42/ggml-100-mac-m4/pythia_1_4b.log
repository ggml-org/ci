Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.595s
user	0m0.911s
sys	0m1.236s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Linking C executable ../bin/test-c
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-sampling
[ 49%] Built target test-log
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Built target test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-chat-template
[ 63%] Built target test-gguf
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-backend-ops
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target llama-batched-bench
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Built target test-quantize-fns
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target test-rope
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-batched
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-infill
[ 75%] Built target llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-imatrix
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-parallel
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-quantize
[ 84%] Built target llama-perplexity
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Built target llama-passkey
[ 87%] Built target llama-retrieval
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-speculative
[ 93%] Built target llama-run
[ 93%] Built target llama-tts
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.114s
user	0m6.230s
sys	0m9.756s

main: quantize time =  2870.10 ms
main:    total time =  2870.10 ms

main: quantize time =  4011.11 ms
main:    total time =  4011.11 ms

main: quantize time =  2714.18 ms
main:    total time =  2714.18 ms

main: quantize time =  2648.24 ms
main:    total time =  2648.24 ms

main: quantize time =  3264.52 ms
main:    total time =  3264.52 ms

main: quantize time =  5375.22 ms
main:    total time =  5375.22 ms

main: quantize time =  6034.08 ms
main:    total time =  6034.08 ms

main: quantize time =  6881.87 ms
main:    total time =  6881.87 ms

main: quantize time =  6273.47 ms
main:    total time =  6273.47 ms

main: quantize time =  4702.02 ms
main:    total time =  4702.02 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.168 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.382 I main: llama backend init
0.00.000.393 I main: load the model and apply lora adapter, if any
0.00.046.390 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.059.515 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.059.543 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.059.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.059.548 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.059.549 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.059.549 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.059.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.059.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.059.554 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.059.554 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.059.555 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.059.555 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.059.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.059.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.059.561 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.059.561 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.059.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.066.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.068.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.077.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.077.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.077.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.077.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.077.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.077.594 I llama_model_loader: - type  f32:  194 tensors
0.00.077.595 I llama_model_loader: - type  f16:   98 tensors
0.00.077.599 I print_info: file format = GGUF V3 (latest)
0.00.077.601 I print_info: file type   = all F32 (guessed)
0.00.077.602 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.093.965 I load: special tokens cache size = 25
0.00.103.484 I load: token to piece cache size = 0.2984 MB
0.00.103.490 I print_info: arch             = gptneox
0.00.103.490 I print_info: vocab_only       = 0
0.00.103.490 I print_info: n_ctx_train      = 2048
0.00.103.491 I print_info: n_embd           = 2048
0.00.103.491 I print_info: n_layer          = 24
0.00.103.496 I print_info: n_head           = 16
0.00.103.497 I print_info: n_head_kv        = 16
0.00.103.497 I print_info: n_rot            = 32
0.00.103.500 I print_info: n_swa            = 0
0.00.103.500 I print_info: n_embd_head_k    = 128
0.00.103.500 I print_info: n_embd_head_v    = 128
0.00.103.501 I print_info: n_gqa            = 1
0.00.103.502 I print_info: n_embd_k_gqa     = 2048
0.00.103.503 I print_info: n_embd_v_gqa     = 2048
0.00.103.506 I print_info: f_norm_eps       = 1.0e-05
0.00.103.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.506 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.506 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.507 I print_info: f_logit_scale    = 0.0e+00
0.00.103.508 I print_info: n_ff             = 8192
0.00.103.508 I print_info: n_expert         = 0
0.00.103.508 I print_info: n_expert_used    = 0
0.00.103.508 I print_info: causal attn      = 1
0.00.103.508 I print_info: pooling type     = 0
0.00.103.511 I print_info: rope type        = 2
0.00.103.511 I print_info: rope scaling     = linear
0.00.103.512 I print_info: freq_base_train  = 10000.0
0.00.103.512 I print_info: freq_scale_train = 1
0.00.103.512 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.512 I print_info: rope_finetuned   = unknown
0.00.103.513 I print_info: ssm_d_conv       = 0
0.00.103.513 I print_info: ssm_d_inner      = 0
0.00.103.513 I print_info: ssm_d_state      = 0
0.00.103.513 I print_info: ssm_dt_rank      = 0
0.00.103.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.514 I print_info: model type       = 1.4B
0.00.103.514 I print_info: model params     = 1.41 B
0.00.103.515 I print_info: general.name     = 1.4B
0.00.103.515 I print_info: vocab type       = BPE
0.00.103.515 I print_info: n_vocab          = 50304
0.00.103.516 I print_info: n_merges         = 50009
0.00.103.516 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.517 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.517 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.517 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.103.518 I print_info: LF token         = 128 'Ä'
0.00.103.518 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.103.518 I print_info: max token length = 1024
0.00.142.977 I load_tensors: offloading 24 repeating layers to GPU
0.00.142.981 I load_tensors: offloading output layer to GPU
0.00.142.981 I load_tensors: offloaded 25/25 layers to GPU
0.00.142.997 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.142.999 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.143.456 I llama_init_from_model: n_seq_max     = 1
0.00.143.456 I llama_init_from_model: n_ctx         = 2048
0.00.143.457 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.143.457 I llama_init_from_model: n_batch       = 2048
0.00.143.457 I llama_init_from_model: n_ubatch      = 512
0.00.143.457 I llama_init_from_model: flash_attn    = 0
0.00.143.457 I llama_init_from_model: freq_base     = 10000.0
0.00.143.458 I llama_init_from_model: freq_scale    = 1
0.00.143.458 I ggml_metal_init: allocating
0.00.143.482 I ggml_metal_init: found device: Apple M4
0.00.143.489 I ggml_metal_init: picking default device: Apple M4
0.00.144.085 I ggml_metal_init: using embedded metal library
0.00.156.257 I ggml_metal_init: GPU name:   Apple M4
0.00.156.258 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.156.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.156.259 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.156.259 I ggml_metal_init: simdgroup reduction   = true
0.00.156.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.156.260 I ggml_metal_init: has residency sets    = true
0.00.156.260 I ggml_metal_init: has bfloat            = true
0.00.156.260 I ggml_metal_init: use bfloat            = true
0.00.156.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.156.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.218.484 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.248.874 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.248.881 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.248.903 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.253.028 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.253.030 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.253.031 I llama_init_from_model: graph nodes  = 967
0.00.253.031 I llama_init_from_model: graph splits = 2
0.00.253.037 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.253.170 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.253.170 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.318.843 I main: llama threadpool init, n_threads = 4
0.00.318.885 I 
0.00.318.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.318.918 I 
0.00.319.096 I sampler seed: 1234
0.00.319.101 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.319.126 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.319.127 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.319.127 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.144.532 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.02.144.532 I llama_perf_context_print:        load time =     271.41 ms
0.02.144.533 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.52 tokens per second)
0.02.144.534 I llama_perf_context_print:        eval time =    1778.87 ms /    63 runs   (   28.24 ms per token,    35.42 tokens per second)
0.02.144.535 I llama_perf_context_print:       total time =    1826.72 ms /    70 tokens
0.02.144.807 I ggml_metal_free: deallocating

real	0m2.518s
user	0m0.136s
sys	0m0.141s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.623 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.914 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.922 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.926 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.927 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.927 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.927 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.929 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.076 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.027 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.028 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.028 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.029 I llama_model_loader: - type  f32:  194 tensors
0.00.038.029 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.030 I print_info: file format = GGUF V3 (latest)
0.00.038.031 I print_info: file type   = Q8_0
0.00.038.033 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.307 I load: special tokens cache size = 25
0.00.054.264 I load: token to piece cache size = 0.2984 MB
0.00.054.268 I print_info: arch             = gptneox
0.00.054.268 I print_info: vocab_only       = 0
0.00.054.268 I print_info: n_ctx_train      = 2048
0.00.054.269 I print_info: n_embd           = 2048
0.00.054.269 I print_info: n_layer          = 24
0.00.054.282 I print_info: n_head           = 16
0.00.054.286 I print_info: n_head_kv        = 16
0.00.054.286 I print_info: n_rot            = 32
0.00.054.286 I print_info: n_swa            = 0
0.00.054.287 I print_info: n_embd_head_k    = 128
0.00.054.287 I print_info: n_embd_head_v    = 128
0.00.054.288 I print_info: n_gqa            = 1
0.00.054.289 I print_info: n_embd_k_gqa     = 2048
0.00.054.290 I print_info: n_embd_v_gqa     = 2048
0.00.054.291 I print_info: f_norm_eps       = 1.0e-05
0.00.054.291 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.292 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.292 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.292 I print_info: f_logit_scale    = 0.0e+00
0.00.054.293 I print_info: n_ff             = 8192
0.00.054.293 I print_info: n_expert         = 0
0.00.054.296 I print_info: n_expert_used    = 0
0.00.054.297 I print_info: causal attn      = 1
0.00.054.297 I print_info: pooling type     = 0
0.00.054.297 I print_info: rope type        = 2
0.00.054.297 I print_info: rope scaling     = linear
0.00.054.298 I print_info: freq_base_train  = 10000.0
0.00.054.298 I print_info: freq_scale_train = 1
0.00.054.298 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.298 I print_info: rope_finetuned   = unknown
0.00.054.299 I print_info: ssm_d_conv       = 0
0.00.054.299 I print_info: ssm_d_inner      = 0
0.00.054.299 I print_info: ssm_d_state      = 0
0.00.054.299 I print_info: ssm_dt_rank      = 0
0.00.054.299 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.300 I print_info: model type       = 1.4B
0.00.054.300 I print_info: model params     = 1.41 B
0.00.054.300 I print_info: general.name     = 1.4B
0.00.054.301 I print_info: vocab type       = BPE
0.00.054.301 I print_info: n_vocab          = 50304
0.00.054.301 I print_info: n_merges         = 50009
0.00.054.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.302 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.304 I print_info: LF token         = 128 'Ä'
0.00.054.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.305 I print_info: max token length = 1024
0.01.159.380 I load_tensors: offloading 24 repeating layers to GPU
0.01.159.383 I load_tensors: offloading output layer to GPU
0.01.159.383 I load_tensors: offloaded 25/25 layers to GPU
0.01.159.403 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.159.404 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.160.663 I llama_init_from_model: n_seq_max     = 1
0.01.160.665 I llama_init_from_model: n_ctx         = 2048
0.01.160.666 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.160.666 I llama_init_from_model: n_batch       = 2048
0.01.160.666 I llama_init_from_model: n_ubatch      = 512
0.01.160.667 I llama_init_from_model: flash_attn    = 0
0.01.160.667 I llama_init_from_model: freq_base     = 10000.0
0.01.160.668 I llama_init_from_model: freq_scale    = 1
0.01.160.669 I ggml_metal_init: allocating
0.01.160.699 I ggml_metal_init: found device: Apple M4
0.01.160.711 I ggml_metal_init: picking default device: Apple M4
0.01.161.976 I ggml_metal_init: using embedded metal library
0.01.167.495 I ggml_metal_init: GPU name:   Apple M4
0.01.167.498 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.167.498 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.167.499 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.167.500 I ggml_metal_init: simdgroup reduction   = true
0.01.167.500 I ggml_metal_init: simdgroup matrix mul. = true
0.01.167.500 I ggml_metal_init: has residency sets    = true
0.01.167.500 I ggml_metal_init: has bfloat            = true
0.01.167.501 I ggml_metal_init: use bfloat            = true
0.01.167.501 I ggml_metal_init: hasUnifiedMemory      = true
0.01.167.502 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.183.417 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.235.043 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.235.049 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.235.070 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.239.648 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.239.650 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.239.650 I llama_init_from_model: graph nodes  = 967
0.01.239.650 I llama_init_from_model: graph splits = 2
0.01.239.657 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.239.786 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.239.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.296.742 I main: llama threadpool init, n_threads = 4
0.01.296.784 I 
0.01.296.808 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.296.812 I 
0.01.296.988 I sampler seed: 1234
0.01.296.992 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.297.003 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.297.004 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.297.004 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.376.200 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.02.376.201 I llama_perf_context_print:        load time =    1286.23 ms
0.02.376.202 I llama_perf_context_print: prompt eval time =      48.98 ms /     7 tokens (    7.00 ms per token,   142.90 tokens per second)
0.02.376.203 I llama_perf_context_print:        eval time =    1027.36 ms /    63 runs   (   16.31 ms per token,    61.32 tokens per second)
0.02.376.204 I llama_perf_context_print:       total time =    1080.34 ms /    70 tokens
0.02.376.456 I ggml_metal_free: deallocating

real	0m2.401s
user	0m0.110s
sys	0m0.291s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.011.452 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.410 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.415 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.417 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.418 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.418 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.419 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.420 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.421 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.421 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.422 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.422 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.425 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.428 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.363 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.211 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.213 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.213 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.214 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.214 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.215 I llama_model_loader: - type  f32:  194 tensors
0.00.028.215 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.216 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.216 I print_info: file format = GGUF V3 (latest)
0.00.028.217 I print_info: file type   = Q4_0
0.00.028.218 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.274 I load: special tokens cache size = 25
0.00.042.237 I load: token to piece cache size = 0.2984 MB
0.00.042.241 I print_info: arch             = gptneox
0.00.042.241 I print_info: vocab_only       = 0
0.00.042.241 I print_info: n_ctx_train      = 2048
0.00.042.241 I print_info: n_embd           = 2048
0.00.042.241 I print_info: n_layer          = 24
0.00.042.246 I print_info: n_head           = 16
0.00.042.248 I print_info: n_head_kv        = 16
0.00.042.248 I print_info: n_rot            = 32
0.00.042.249 I print_info: n_swa            = 0
0.00.042.249 I print_info: n_embd_head_k    = 128
0.00.042.249 I print_info: n_embd_head_v    = 128
0.00.042.250 I print_info: n_gqa            = 1
0.00.042.251 I print_info: n_embd_k_gqa     = 2048
0.00.042.251 I print_info: n_embd_v_gqa     = 2048
0.00.042.252 I print_info: f_norm_eps       = 1.0e-05
0.00.042.252 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.252 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.253 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.253 I print_info: f_logit_scale    = 0.0e+00
0.00.042.254 I print_info: n_ff             = 8192
0.00.042.254 I print_info: n_expert         = 0
0.00.042.254 I print_info: n_expert_used    = 0
0.00.042.254 I print_info: causal attn      = 1
0.00.042.254 I print_info: pooling type     = 0
0.00.042.255 I print_info: rope type        = 2
0.00.042.255 I print_info: rope scaling     = linear
0.00.042.255 I print_info: freq_base_train  = 10000.0
0.00.042.257 I print_info: freq_scale_train = 1
0.00.042.257 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.257 I print_info: rope_finetuned   = unknown
0.00.042.258 I print_info: ssm_d_conv       = 0
0.00.042.258 I print_info: ssm_d_inner      = 0
0.00.042.258 I print_info: ssm_d_state      = 0
0.00.042.258 I print_info: ssm_dt_rank      = 0
0.00.042.258 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.258 I print_info: model type       = 1.4B
0.00.042.259 I print_info: model params     = 1.41 B
0.00.042.259 I print_info: general.name     = 1.4B
0.00.042.260 I print_info: vocab type       = BPE
0.00.042.260 I print_info: n_vocab          = 50304
0.00.042.260 I print_info: n_merges         = 50009
0.00.042.261 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.265 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.265 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.265 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.266 I print_info: LF token         = 128 'Ä'
0.00.042.266 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.266 I print_info: max token length = 1024
0.00.614.619 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.635 I load_tensors: offloading output layer to GPU
0.00.614.635 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.672 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.614.673 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.615.998 I llama_init_from_model: n_seq_max     = 1
0.00.616.004 I llama_init_from_model: n_ctx         = 2048
0.00.616.004 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.616.005 I llama_init_from_model: n_batch       = 2048
0.00.616.005 I llama_init_from_model: n_ubatch      = 512
0.00.616.006 I llama_init_from_model: flash_attn    = 0
0.00.616.007 I llama_init_from_model: freq_base     = 10000.0
0.00.616.008 I llama_init_from_model: freq_scale    = 1
0.00.616.010 I ggml_metal_init: allocating
0.00.616.088 I ggml_metal_init: found device: Apple M4
0.00.616.102 I ggml_metal_init: picking default device: Apple M4
0.00.617.902 I ggml_metal_init: using embedded metal library
0.00.623.520 I ggml_metal_init: GPU name:   Apple M4
0.00.623.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.527 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.528 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.528 I ggml_metal_init: simdgroup reduction   = true
0.00.623.529 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.529 I ggml_metal_init: has residency sets    = true
0.00.623.529 I ggml_metal_init: has bfloat            = true
0.00.623.530 I ggml_metal_init: use bfloat            = true
0.00.623.531 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.714 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.701.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.701.306 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.701.334 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.705.842 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.705.845 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.705.845 I llama_init_from_model: graph nodes  = 967
0.00.705.845 I llama_init_from_model: graph splits = 2
0.00.705.852 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.705.984 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.705.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.085 I main: llama threadpool init, n_threads = 4
0.00.764.129 I 
0.00.764.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.157 I 
0.00.764.332 I sampler seed: 1234
0.00.764.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.375 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.376 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.377 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.441.830 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.441.831 I llama_perf_context_print:        load time =     751.74 ms
0.01.441.832 I llama_perf_context_print: prompt eval time =      48.86 ms /     7 tokens (    6.98 ms per token,   143.27 tokens per second)
0.01.441.833 I llama_perf_context_print:        eval time =     625.82 ms /    63 runs   (    9.93 ms per token,   100.67 tokens per second)
0.01.441.834 I llama_perf_context_print:       total time =     678.63 ms /    70 tokens
0.01.442.067 I ggml_metal_free: deallocating

real	0m1.461s
user	0m0.110s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.685 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.689 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.691 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.692 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.692 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.698 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.702 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.702 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.532 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.587 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.390 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.392 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.392 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.392 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.393 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.393 I llama_model_loader: - type  f32:  194 tensors
0.00.025.394 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.394 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.394 I print_info: file format = GGUF V3 (latest)
0.00.025.395 I print_info: file type   = Q4_1
0.00.025.396 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.503 I load: special tokens cache size = 25
0.00.039.422 I load: token to piece cache size = 0.2984 MB
0.00.039.425 I print_info: arch             = gptneox
0.00.039.425 I print_info: vocab_only       = 0
0.00.039.425 I print_info: n_ctx_train      = 2048
0.00.039.425 I print_info: n_embd           = 2048
0.00.039.426 I print_info: n_layer          = 24
0.00.039.428 I print_info: n_head           = 16
0.00.039.429 I print_info: n_head_kv        = 16
0.00.039.431 I print_info: n_rot            = 32
0.00.039.431 I print_info: n_swa            = 0
0.00.039.432 I print_info: n_embd_head_k    = 128
0.00.039.432 I print_info: n_embd_head_v    = 128
0.00.039.432 I print_info: n_gqa            = 1
0.00.039.433 I print_info: n_embd_k_gqa     = 2048
0.00.039.440 I print_info: n_embd_v_gqa     = 2048
0.00.039.442 I print_info: f_norm_eps       = 1.0e-05
0.00.039.443 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.443 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.443 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.443 I print_info: f_logit_scale    = 0.0e+00
0.00.039.445 I print_info: n_ff             = 8192
0.00.039.446 I print_info: n_expert         = 0
0.00.039.446 I print_info: n_expert_used    = 0
0.00.039.446 I print_info: causal attn      = 1
0.00.039.446 I print_info: pooling type     = 0
0.00.039.448 I print_info: rope type        = 2
0.00.039.449 I print_info: rope scaling     = linear
0.00.039.450 I print_info: freq_base_train  = 10000.0
0.00.039.450 I print_info: freq_scale_train = 1
0.00.039.450 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.450 I print_info: rope_finetuned   = unknown
0.00.039.450 I print_info: ssm_d_conv       = 0
0.00.039.451 I print_info: ssm_d_inner      = 0
0.00.039.451 I print_info: ssm_d_state      = 0
0.00.039.451 I print_info: ssm_dt_rank      = 0
0.00.039.451 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.451 I print_info: model type       = 1.4B
0.00.039.452 I print_info: model params     = 1.41 B
0.00.039.452 I print_info: general.name     = 1.4B
0.00.039.452 I print_info: vocab type       = BPE
0.00.039.453 I print_info: n_vocab          = 50304
0.00.039.453 I print_info: n_merges         = 50009
0.00.039.453 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.453 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.453 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.453 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.454 I print_info: LF token         = 128 'Ä'
0.00.039.455 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.455 I print_info: max token length = 1024
0.00.651.126 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.142 I load_tensors: offloading output layer to GPU
0.00.651.143 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.180 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.651.181 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.652.744 I llama_init_from_model: n_seq_max     = 1
0.00.652.749 I llama_init_from_model: n_ctx         = 2048
0.00.652.750 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.652.750 I llama_init_from_model: n_batch       = 2048
0.00.652.751 I llama_init_from_model: n_ubatch      = 512
0.00.652.751 I llama_init_from_model: flash_attn    = 0
0.00.652.753 I llama_init_from_model: freq_base     = 10000.0
0.00.652.753 I llama_init_from_model: freq_scale    = 1
0.00.652.760 I ggml_metal_init: allocating
0.00.652.846 I ggml_metal_init: found device: Apple M4
0.00.652.861 I ggml_metal_init: picking default device: Apple M4
0.00.654.684 I ggml_metal_init: using embedded metal library
0.00.661.411 I ggml_metal_init: GPU name:   Apple M4
0.00.661.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.419 I ggml_metal_init: simdgroup reduction   = true
0.00.661.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.419 I ggml_metal_init: has residency sets    = true
0.00.661.420 I ggml_metal_init: has bfloat            = true
0.00.661.420 I ggml_metal_init: use bfloat            = true
0.00.661.421 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.679.402 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.254 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.735.261 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.735.283 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.618 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.740.620 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.740.620 I llama_init_from_model: graph nodes  = 967
0.00.740.620 I llama_init_from_model: graph splits = 2
0.00.740.627 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.761 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.560 I main: llama threadpool init, n_threads = 4
0.00.795.603 I 
0.00.795.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.627 I 
0.00.795.796 I sampler seed: 1234
0.00.795.801 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.812 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.813 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.813 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.521.499 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.521.500 I llama_perf_context_print:        load time =     785.79 ms
0.01.521.500 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.12 tokens per second)
0.01.521.501 I llama_perf_context_print:        eval time =     673.95 ms /    63 runs   (   10.70 ms per token,    93.48 tokens per second)
0.01.521.501 I llama_perf_context_print:       total time =     726.80 ms /    70 tokens
0.01.521.745 I ggml_metal_free: deallocating

real	0m1.538s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.935 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.175 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.180 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.182 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.182 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.192 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.567 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.569 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.569 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.569 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.570 I llama_model_loader: - type  f32:  194 tensors
0.00.025.570 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.570 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.571 I print_info: file format = GGUF V3 (latest)
0.00.025.572 I print_info: file type   = Q5_0
0.00.025.572 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.335 I load: special tokens cache size = 25
0.00.039.319 I load: token to piece cache size = 0.2984 MB
0.00.039.322 I print_info: arch             = gptneox
0.00.039.322 I print_info: vocab_only       = 0
0.00.039.323 I print_info: n_ctx_train      = 2048
0.00.039.323 I print_info: n_embd           = 2048
0.00.039.323 I print_info: n_layer          = 24
0.00.039.326 I print_info: n_head           = 16
0.00.039.327 I print_info: n_head_kv        = 16
0.00.039.327 I print_info: n_rot            = 32
0.00.039.327 I print_info: n_swa            = 0
0.00.039.327 I print_info: n_embd_head_k    = 128
0.00.039.327 I print_info: n_embd_head_v    = 128
0.00.039.328 I print_info: n_gqa            = 1
0.00.039.329 I print_info: n_embd_k_gqa     = 2048
0.00.039.330 I print_info: n_embd_v_gqa     = 2048
0.00.039.330 I print_info: f_norm_eps       = 1.0e-05
0.00.039.331 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.331 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.331 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.331 I print_info: f_logit_scale    = 0.0e+00
0.00.039.332 I print_info: n_ff             = 8192
0.00.039.332 I print_info: n_expert         = 0
0.00.039.332 I print_info: n_expert_used    = 0
0.00.039.335 I print_info: causal attn      = 1
0.00.039.335 I print_info: pooling type     = 0
0.00.039.337 I print_info: rope type        = 2
0.00.039.337 I print_info: rope scaling     = linear
0.00.039.337 I print_info: freq_base_train  = 10000.0
0.00.039.338 I print_info: freq_scale_train = 1
0.00.039.338 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.338 I print_info: rope_finetuned   = unknown
0.00.039.340 I print_info: ssm_d_conv       = 0
0.00.039.340 I print_info: ssm_d_inner      = 0
0.00.039.340 I print_info: ssm_d_state      = 0
0.00.039.340 I print_info: ssm_dt_rank      = 0
0.00.039.340 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.341 I print_info: model type       = 1.4B
0.00.039.341 I print_info: model params     = 1.41 B
0.00.039.341 I print_info: general.name     = 1.4B
0.00.039.342 I print_info: vocab type       = BPE
0.00.039.343 I print_info: n_vocab          = 50304
0.00.039.343 I print_info: n_merges         = 50009
0.00.039.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.343 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.343 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: LF token         = 128 'Ä'
0.00.039.345 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.345 I print_info: max token length = 1024
0.00.685.333 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.344 I load_tensors: offloading output layer to GPU
0.00.685.344 I load_tensors: offloaded 25/25 layers to GPU
0.00.685.379 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.685.382 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.686.935 I llama_init_from_model: n_seq_max     = 1
0.00.686.940 I llama_init_from_model: n_ctx         = 2048
0.00.686.941 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.686.941 I llama_init_from_model: n_batch       = 2048
0.00.686.942 I llama_init_from_model: n_ubatch      = 512
0.00.686.942 I llama_init_from_model: flash_attn    = 0
0.00.686.945 I llama_init_from_model: freq_base     = 10000.0
0.00.686.945 I llama_init_from_model: freq_scale    = 1
0.00.686.951 I ggml_metal_init: allocating
0.00.687.006 I ggml_metal_init: found device: Apple M4
0.00.687.020 I ggml_metal_init: picking default device: Apple M4
0.00.689.038 I ggml_metal_init: using embedded metal library
0.00.695.816 I ggml_metal_init: GPU name:   Apple M4
0.00.695.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.824 I ggml_metal_init: simdgroup reduction   = true
0.00.695.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.825 I ggml_metal_init: has residency sets    = true
0.00.695.825 I ggml_metal_init: has bfloat            = true
0.00.695.825 I ggml_metal_init: use bfloat            = true
0.00.695.826 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.828 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.713.914 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.771.220 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.771.226 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.771.249 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.775.751 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.775.753 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.775.754 I llama_init_from_model: graph nodes  = 967
0.00.775.754 I llama_init_from_model: graph splits = 2
0.00.775.760 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.775.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.775.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.744 I main: llama threadpool init, n_threads = 4
0.00.836.786 I 
0.00.836.805 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.806 I 
0.00.836.965 I sampler seed: 1234
0.00.836.970 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.837.008 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.837.013 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.837.013 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.625.060 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47811.45 tokens per second)
0.01.625.061 I llama_perf_context_print:        load time =     826.91 ms
0.01.625.062 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.61 tokens per second)
0.01.625.062 I llama_perf_context_print:        eval time =     733.72 ms /    63 runs   (   11.65 ms per token,    85.86 tokens per second)
0.01.625.063 I llama_perf_context_print:       total time =     789.21 ms /    70 tokens
0.01.625.310 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.111s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.319 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.681 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.684 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.571 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.360 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.361 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.362 I llama_model_loader: - type  f32:  194 tensors
0.00.026.362 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.362 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.363 I print_info: file format = GGUF V3 (latest)
0.00.026.363 I print_info: file type   = Q5_1
0.00.026.364 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.561 I load: special tokens cache size = 25
0.00.040.484 I load: token to piece cache size = 0.2984 MB
0.00.040.487 I print_info: arch             = gptneox
0.00.040.487 I print_info: vocab_only       = 0
0.00.040.487 I print_info: n_ctx_train      = 2048
0.00.040.488 I print_info: n_embd           = 2048
0.00.040.488 I print_info: n_layer          = 24
0.00.040.491 I print_info: n_head           = 16
0.00.040.491 I print_info: n_head_kv        = 16
0.00.040.491 I print_info: n_rot            = 32
0.00.040.492 I print_info: n_swa            = 0
0.00.040.492 I print_info: n_embd_head_k    = 128
0.00.040.492 I print_info: n_embd_head_v    = 128
0.00.040.493 I print_info: n_gqa            = 1
0.00.040.494 I print_info: n_embd_k_gqa     = 2048
0.00.040.494 I print_info: n_embd_v_gqa     = 2048
0.00.040.495 I print_info: f_norm_eps       = 1.0e-05
0.00.040.495 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.495 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.496 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.496 I print_info: f_logit_scale    = 0.0e+00
0.00.040.496 I print_info: n_ff             = 8192
0.00.040.496 I print_info: n_expert         = 0
0.00.040.497 I print_info: n_expert_used    = 0
0.00.040.497 I print_info: causal attn      = 1
0.00.040.497 I print_info: pooling type     = 0
0.00.040.497 I print_info: rope type        = 2
0.00.040.497 I print_info: rope scaling     = linear
0.00.040.498 I print_info: freq_base_train  = 10000.0
0.00.040.498 I print_info: freq_scale_train = 1
0.00.040.499 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.500 I print_info: rope_finetuned   = unknown
0.00.040.500 I print_info: ssm_d_conv       = 0
0.00.040.500 I print_info: ssm_d_inner      = 0
0.00.040.500 I print_info: ssm_d_state      = 0
0.00.040.500 I print_info: ssm_dt_rank      = 0
0.00.040.500 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.501 I print_info: model type       = 1.4B
0.00.040.501 I print_info: model params     = 1.41 B
0.00.040.501 I print_info: general.name     = 1.4B
0.00.040.502 I print_info: vocab type       = BPE
0.00.040.502 I print_info: n_vocab          = 50304
0.00.040.502 I print_info: n_merges         = 50009
0.00.040.502 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.503 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.503 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.503 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.503 I print_info: LF token         = 128 'Ä'
0.00.040.504 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.504 I print_info: max token length = 1024
0.00.621.766 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.775 I load_tensors: offloading output layer to GPU
0.00.621.775 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.804 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.621.805 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.623.022 I llama_init_from_model: n_seq_max     = 1
0.00.623.031 I llama_init_from_model: n_ctx         = 2048
0.00.623.031 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.623.032 I llama_init_from_model: n_batch       = 2048
0.00.623.032 I llama_init_from_model: n_ubatch      = 512
0.00.623.032 I llama_init_from_model: flash_attn    = 0
0.00.623.034 I llama_init_from_model: freq_base     = 10000.0
0.00.623.035 I llama_init_from_model: freq_scale    = 1
0.00.623.037 I ggml_metal_init: allocating
0.00.623.094 I ggml_metal_init: found device: Apple M4
0.00.623.123 I ggml_metal_init: picking default device: Apple M4
0.00.624.942 I ggml_metal_init: using embedded metal library
0.00.631.834 I ggml_metal_init: GPU name:   Apple M4
0.00.631.839 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.840 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.842 I ggml_metal_init: simdgroup reduction   = true
0.00.631.842 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.842 I ggml_metal_init: has residency sets    = true
0.00.631.843 I ggml_metal_init: has bfloat            = true
0.00.631.843 I ggml_metal_init: use bfloat            = true
0.00.631.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.083 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.739 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.712.746 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.712.769 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.000 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.717.002 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.717.003 I llama_init_from_model: graph nodes  = 967
0.00.717.003 I llama_init_from_model: graph splits = 2
0.00.717.009 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.717.138 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.139 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.230 I main: llama threadpool init, n_threads = 4
0.00.777.275 I 
0.00.777.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.300 I 
0.00.777.450 I sampler seed: 1234
0.00.777.455 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.466 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.468 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.468 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.611.441 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.611.441 I llama_perf_context_print:        load time =     766.03 ms
0.01.611.442 I llama_perf_context_print: prompt eval time =      46.75 ms /     7 tokens (    6.68 ms per token,   149.75 tokens per second)
0.01.611.443 I llama_perf_context_print:        eval time =     784.41 ms /    63 runs   (   12.45 ms per token,    80.31 tokens per second)
0.01.611.444 I llama_perf_context_print:       total time =     835.09 ms /    70 tokens
0.01.611.680 I ggml_metal_free: deallocating

real	0m1.630s
user	0m0.111s
sys	0m0.232s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.443 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.449 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.450 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.451 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.451 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.452 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.452 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.453 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.454 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.454 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.454 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.455 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.455 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.457 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.457 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.458 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.355 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.235 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.235 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.236 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.236 I llama_model_loader: - type  f32:  194 tensors
0.00.024.237 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.237 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.237 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.238 I print_info: file format = GGUF V3 (latest)
0.00.024.238 I print_info: file type   = Q2_K - Medium
0.00.024.239 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.342 I load: special tokens cache size = 25
0.00.038.278 I load: token to piece cache size = 0.2984 MB
0.00.038.280 I print_info: arch             = gptneox
0.00.038.281 I print_info: vocab_only       = 0
0.00.038.281 I print_info: n_ctx_train      = 2048
0.00.038.281 I print_info: n_embd           = 2048
0.00.038.281 I print_info: n_layer          = 24
0.00.038.284 I print_info: n_head           = 16
0.00.038.285 I print_info: n_head_kv        = 16
0.00.038.285 I print_info: n_rot            = 32
0.00.038.286 I print_info: n_swa            = 0
0.00.038.286 I print_info: n_embd_head_k    = 128
0.00.038.286 I print_info: n_embd_head_v    = 128
0.00.038.287 I print_info: n_gqa            = 1
0.00.038.288 I print_info: n_embd_k_gqa     = 2048
0.00.038.288 I print_info: n_embd_v_gqa     = 2048
0.00.038.289 I print_info: f_norm_eps       = 1.0e-05
0.00.038.289 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.290 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.290 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.290 I print_info: f_logit_scale    = 0.0e+00
0.00.038.291 I print_info: n_ff             = 8192
0.00.038.291 I print_info: n_expert         = 0
0.00.038.291 I print_info: n_expert_used    = 0
0.00.038.291 I print_info: causal attn      = 1
0.00.038.291 I print_info: pooling type     = 0
0.00.038.291 I print_info: rope type        = 2
0.00.038.292 I print_info: rope scaling     = linear
0.00.038.292 I print_info: freq_base_train  = 10000.0
0.00.038.292 I print_info: freq_scale_train = 1
0.00.038.293 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.293 I print_info: rope_finetuned   = unknown
0.00.038.293 I print_info: ssm_d_conv       = 0
0.00.038.293 I print_info: ssm_d_inner      = 0
0.00.038.293 I print_info: ssm_d_state      = 0
0.00.038.293 I print_info: ssm_dt_rank      = 0
0.00.038.293 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.294 I print_info: model type       = 1.4B
0.00.038.294 I print_info: model params     = 1.41 B
0.00.038.294 I print_info: general.name     = 1.4B
0.00.038.295 I print_info: vocab type       = BPE
0.00.038.295 I print_info: n_vocab          = 50304
0.00.038.295 I print_info: n_merges         = 50009
0.00.038.296 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.296 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.296 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.296 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.297 I print_info: LF token         = 128 'Ä'
0.00.038.297 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.297 I print_info: max token length = 1024
0.00.353.757 I load_tensors: offloading 24 repeating layers to GPU
0.00.353.770 I load_tensors: offloading output layer to GPU
0.00.353.771 I load_tensors: offloaded 25/25 layers to GPU
0.00.353.802 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.353.804 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.355.270 I llama_init_from_model: n_seq_max     = 1
0.00.355.281 I llama_init_from_model: n_ctx         = 2048
0.00.355.282 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.355.282 I llama_init_from_model: n_batch       = 2048
0.00.355.283 I llama_init_from_model: n_ubatch      = 512
0.00.355.283 I llama_init_from_model: flash_attn    = 0
0.00.355.284 I llama_init_from_model: freq_base     = 10000.0
0.00.355.284 I llama_init_from_model: freq_scale    = 1
0.00.355.289 I ggml_metal_init: allocating
0.00.355.376 I ggml_metal_init: found device: Apple M4
0.00.355.390 I ggml_metal_init: picking default device: Apple M4
0.00.357.129 I ggml_metal_init: using embedded metal library
0.00.362.846 I ggml_metal_init: GPU name:   Apple M4
0.00.362.858 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.362.858 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.362.859 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.362.860 I ggml_metal_init: simdgroup reduction   = true
0.00.362.860 I ggml_metal_init: simdgroup matrix mul. = true
0.00.362.860 I ggml_metal_init: has residency sets    = true
0.00.362.860 I ggml_metal_init: has bfloat            = true
0.00.362.861 I ggml_metal_init: use bfloat            = true
0.00.362.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.362.867 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.384.342 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.447.859 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.447.866 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.447.897 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.452.044 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.452.046 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.452.046 I llama_init_from_model: graph nodes  = 967
0.00.452.047 I llama_init_from_model: graph splits = 2
0.00.452.053 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.452.184 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.452.184 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.514.311 I main: llama threadpool init, n_threads = 4
0.00.514.367 I 
0.00.514.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.514.391 I 
0.00.514.570 I sampler seed: 1234
0.00.514.574 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.514.585 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.514.585 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.514.585 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.193.351 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.193.352 I llama_perf_context_print:        load time =     504.66 ms
0.01.193.353 I llama_perf_context_print: prompt eval time =      42.87 ms /     7 tokens (    6.12 ms per token,   163.30 tokens per second)
0.01.193.354 I llama_perf_context_print:        eval time =     633.07 ms /    63 runs   (   10.05 ms per token,    99.52 tokens per second)
0.01.193.355 I llama_perf_context_print:       total time =     679.92 ms /    70 tokens
0.01.193.570 I ggml_metal_free: deallocating

real	0m1.212s
user	0m0.113s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.747 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.290 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.297 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.297 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.298 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.298 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.298 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.299 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.300 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.300 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.852 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.853 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.853 I llama_model_loader: - type  f32:  194 tensors
0.00.023.854 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.854 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.854 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.854 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.855 I print_info: file format = GGUF V3 (latest)
0.00.023.856 I print_info: file type   = Q3_K - Medium
0.00.023.856 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.686 I load: special tokens cache size = 25
0.00.037.596 I load: token to piece cache size = 0.2984 MB
0.00.037.599 I print_info: arch             = gptneox
0.00.037.599 I print_info: vocab_only       = 0
0.00.037.599 I print_info: n_ctx_train      = 2048
0.00.037.599 I print_info: n_embd           = 2048
0.00.037.600 I print_info: n_layer          = 24
0.00.037.603 I print_info: n_head           = 16
0.00.037.603 I print_info: n_head_kv        = 16
0.00.037.604 I print_info: n_rot            = 32
0.00.037.604 I print_info: n_swa            = 0
0.00.037.604 I print_info: n_embd_head_k    = 128
0.00.037.604 I print_info: n_embd_head_v    = 128
0.00.037.605 I print_info: n_gqa            = 1
0.00.037.606 I print_info: n_embd_k_gqa     = 2048
0.00.037.606 I print_info: n_embd_v_gqa     = 2048
0.00.037.607 I print_info: f_norm_eps       = 1.0e-05
0.00.037.607 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.608 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.608 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.608 I print_info: f_logit_scale    = 0.0e+00
0.00.037.610 I print_info: n_ff             = 8192
0.00.037.610 I print_info: n_expert         = 0
0.00.037.610 I print_info: n_expert_used    = 0
0.00.037.610 I print_info: causal attn      = 1
0.00.037.610 I print_info: pooling type     = 0
0.00.037.611 I print_info: rope type        = 2
0.00.037.611 I print_info: rope scaling     = linear
0.00.037.611 I print_info: freq_base_train  = 10000.0
0.00.037.612 I print_info: freq_scale_train = 1
0.00.037.612 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.612 I print_info: rope_finetuned   = unknown
0.00.037.612 I print_info: ssm_d_conv       = 0
0.00.037.612 I print_info: ssm_d_inner      = 0
0.00.037.613 I print_info: ssm_d_state      = 0
0.00.037.613 I print_info: ssm_dt_rank      = 0
0.00.037.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.613 I print_info: model type       = 1.4B
0.00.037.613 I print_info: model params     = 1.41 B
0.00.037.614 I print_info: general.name     = 1.4B
0.00.037.614 I print_info: vocab type       = BPE
0.00.037.614 I print_info: n_vocab          = 50304
0.00.037.615 I print_info: n_merges         = 50009
0.00.037.615 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.615 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.615 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.616 I print_info: LF token         = 128 'Ä'
0.00.037.616 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.616 I print_info: max token length = 1024
0.00.447.228 I load_tensors: offloading 24 repeating layers to GPU
0.00.447.244 I load_tensors: offloading output layer to GPU
0.00.447.244 I load_tensors: offloaded 25/25 layers to GPU
0.00.447.280 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.447.281 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.448.739 I llama_init_from_model: n_seq_max     = 1
0.00.448.743 I llama_init_from_model: n_ctx         = 2048
0.00.448.744 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.448.744 I llama_init_from_model: n_batch       = 2048
0.00.448.745 I llama_init_from_model: n_ubatch      = 512
0.00.448.745 I llama_init_from_model: flash_attn    = 0
0.00.448.748 I llama_init_from_model: freq_base     = 10000.0
0.00.448.748 I llama_init_from_model: freq_scale    = 1
0.00.448.751 I ggml_metal_init: allocating
0.00.448.857 I ggml_metal_init: found device: Apple M4
0.00.448.870 I ggml_metal_init: picking default device: Apple M4
0.00.450.720 I ggml_metal_init: using embedded metal library
0.00.456.260 I ggml_metal_init: GPU name:   Apple M4
0.00.456.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.277 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.278 I ggml_metal_init: simdgroup reduction   = true
0.00.456.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.279 I ggml_metal_init: has residency sets    = true
0.00.456.279 I ggml_metal_init: has bfloat            = true
0.00.456.279 I ggml_metal_init: use bfloat            = true
0.00.456.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.477.032 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.535.399 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.535.410 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.535.447 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.539.496 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.539.499 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.539.499 I llama_init_from_model: graph nodes  = 967
0.00.539.499 I llama_init_from_model: graph splits = 2
0.00.539.504 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.539.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.539.629 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.100 I main: llama threadpool init, n_threads = 4
0.00.591.142 I 
0.00.591.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.166 I 
0.00.591.345 I sampler seed: 1234
0.00.591.349 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.591.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.591.360 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.591.360 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.345.030 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.345.030 I llama_perf_context_print:        load time =     581.47 ms
0.01.345.031 I llama_perf_context_print: prompt eval time =      50.52 ms /     7 tokens (    7.22 ms per token,   138.56 tokens per second)
0.01.345.032 I llama_perf_context_print:        eval time =     700.26 ms /    63 runs   (   11.12 ms per token,    89.97 tokens per second)
0.01.345.033 I llama_perf_context_print:       total time =     754.82 ms /    70 tokens
0.01.345.271 I ggml_metal_free: deallocating

real	0m1.361s
user	0m0.111s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.158 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.593 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.598 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.599 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.603 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.380 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.383 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.109 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.112 I llama_model_loader: - type  f32:  194 tensors
0.00.024.112 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.113 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.113 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.113 I print_info: file format = GGUF V3 (latest)
0.00.024.114 I print_info: file type   = Q4_K - Medium
0.00.024.114 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.031.991 I load: special tokens cache size = 25
0.00.037.955 I load: token to piece cache size = 0.2984 MB
0.00.037.958 I print_info: arch             = gptneox
0.00.037.958 I print_info: vocab_only       = 0
0.00.037.958 I print_info: n_ctx_train      = 2048
0.00.037.958 I print_info: n_embd           = 2048
0.00.037.959 I print_info: n_layer          = 24
0.00.037.961 I print_info: n_head           = 16
0.00.037.962 I print_info: n_head_kv        = 16
0.00.037.962 I print_info: n_rot            = 32
0.00.037.962 I print_info: n_swa            = 0
0.00.037.962 I print_info: n_embd_head_k    = 128
0.00.037.963 I print_info: n_embd_head_v    = 128
0.00.037.963 I print_info: n_gqa            = 1
0.00.037.964 I print_info: n_embd_k_gqa     = 2048
0.00.037.965 I print_info: n_embd_v_gqa     = 2048
0.00.037.966 I print_info: f_norm_eps       = 1.0e-05
0.00.037.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.968 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.968 I print_info: f_logit_scale    = 0.0e+00
0.00.037.969 I print_info: n_ff             = 8192
0.00.037.969 I print_info: n_expert         = 0
0.00.037.969 I print_info: n_expert_used    = 0
0.00.037.970 I print_info: causal attn      = 1
0.00.037.970 I print_info: pooling type     = 0
0.00.037.970 I print_info: rope type        = 2
0.00.037.970 I print_info: rope scaling     = linear
0.00.037.971 I print_info: freq_base_train  = 10000.0
0.00.037.971 I print_info: freq_scale_train = 1
0.00.037.972 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.973 I print_info: rope_finetuned   = unknown
0.00.037.973 I print_info: ssm_d_conv       = 0
0.00.037.973 I print_info: ssm_d_inner      = 0
0.00.037.973 I print_info: ssm_d_state      = 0
0.00.037.973 I print_info: ssm_dt_rank      = 0
0.00.037.974 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.974 I print_info: model type       = 1.4B
0.00.037.975 I print_info: model params     = 1.41 B
0.00.037.975 I print_info: general.name     = 1.4B
0.00.037.975 I print_info: vocab type       = BPE
0.00.037.976 I print_info: n_vocab          = 50304
0.00.037.976 I print_info: n_merges         = 50009
0.00.037.976 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.976 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.977 I print_info: LF token         = 128 'Ä'
0.00.037.979 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.979 I print_info: max token length = 1024
0.00.509.380 I load_tensors: offloading 24 repeating layers to GPU
0.00.509.396 I load_tensors: offloading output layer to GPU
0.00.509.396 I load_tensors: offloaded 25/25 layers to GPU
0.00.509.434 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.509.435 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.510.852 I llama_init_from_model: n_seq_max     = 1
0.00.510.858 I llama_init_from_model: n_ctx         = 2048
0.00.510.859 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.510.859 I llama_init_from_model: n_batch       = 2048
0.00.510.859 I llama_init_from_model: n_ubatch      = 512
0.00.510.860 I llama_init_from_model: flash_attn    = 0
0.00.510.862 I llama_init_from_model: freq_base     = 10000.0
0.00.510.866 I llama_init_from_model: freq_scale    = 1
0.00.510.872 I ggml_metal_init: allocating
0.00.510.973 I ggml_metal_init: found device: Apple M4
0.00.510.986 I ggml_metal_init: picking default device: Apple M4
0.00.512.810 I ggml_metal_init: using embedded metal library
0.00.519.312 I ggml_metal_init: GPU name:   Apple M4
0.00.519.316 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.519.317 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.519.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.519.319 I ggml_metal_init: simdgroup reduction   = true
0.00.519.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.519.319 I ggml_metal_init: has residency sets    = true
0.00.519.320 I ggml_metal_init: has bfloat            = true
0.00.519.320 I ggml_metal_init: use bfloat            = true
0.00.519.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.519.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.537.345 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.592.460 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.592.467 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.592.489 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.596.747 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.596.749 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.596.750 I llama_init_from_model: graph nodes  = 967
0.00.596.750 I llama_init_from_model: graph splits = 2
0.00.596.756 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.596.880 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.596.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.348 I main: llama threadpool init, n_threads = 4
0.00.654.392 I 
0.00.654.416 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.416 I 
0.00.654.590 I sampler seed: 1234
0.00.654.595 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.654.606 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.654.608 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.654.609 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.424.088 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.424.089 I llama_perf_context_print:        load time =     644.33 ms
0.01.424.090 I llama_perf_context_print: prompt eval time =      55.97 ms /     7 tokens (    8.00 ms per token,   125.07 tokens per second)
0.01.424.091 I llama_perf_context_print:        eval time =     710.62 ms /    63 runs   (   11.28 ms per token,    88.65 tokens per second)
0.01.424.091 I llama_perf_context_print:       total time =     770.60 ms /    70 tokens
0.01.424.339 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.109s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.826 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.502 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.504 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.504 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.505 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.506 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.511 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.323 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.104 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.107 I llama_model_loader: - type  f32:  194 tensors
0.00.024.108 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.108 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.109 I print_info: file format = GGUF V3 (latest)
0.00.024.109 I print_info: file type   = Q5_K - Medium
0.00.024.110 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.258 I load: special tokens cache size = 25
0.00.038.061 I load: token to piece cache size = 0.2984 MB
0.00.038.064 I print_info: arch             = gptneox
0.00.038.065 I print_info: vocab_only       = 0
0.00.038.065 I print_info: n_ctx_train      = 2048
0.00.038.065 I print_info: n_embd           = 2048
0.00.038.065 I print_info: n_layer          = 24
0.00.038.068 I print_info: n_head           = 16
0.00.038.069 I print_info: n_head_kv        = 16
0.00.038.071 I print_info: n_rot            = 32
0.00.038.071 I print_info: n_swa            = 0
0.00.038.071 I print_info: n_embd_head_k    = 128
0.00.038.071 I print_info: n_embd_head_v    = 128
0.00.038.072 I print_info: n_gqa            = 1
0.00.038.073 I print_info: n_embd_k_gqa     = 2048
0.00.038.074 I print_info: n_embd_v_gqa     = 2048
0.00.038.074 I print_info: f_norm_eps       = 1.0e-05
0.00.038.075 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.075 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.075 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.075 I print_info: f_logit_scale    = 0.0e+00
0.00.038.076 I print_info: n_ff             = 8192
0.00.038.076 I print_info: n_expert         = 0
0.00.038.076 I print_info: n_expert_used    = 0
0.00.038.076 I print_info: causal attn      = 1
0.00.038.077 I print_info: pooling type     = 0
0.00.038.077 I print_info: rope type        = 2
0.00.038.077 I print_info: rope scaling     = linear
0.00.038.077 I print_info: freq_base_train  = 10000.0
0.00.038.078 I print_info: freq_scale_train = 1
0.00.038.078 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.078 I print_info: rope_finetuned   = unknown
0.00.038.078 I print_info: ssm_d_conv       = 0
0.00.038.082 I print_info: ssm_d_inner      = 0
0.00.038.082 I print_info: ssm_d_state      = 0
0.00.038.083 I print_info: ssm_dt_rank      = 0
0.00.038.083 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.083 I print_info: model type       = 1.4B
0.00.038.083 I print_info: model params     = 1.41 B
0.00.038.084 I print_info: general.name     = 1.4B
0.00.038.084 I print_info: vocab type       = BPE
0.00.038.084 I print_info: n_vocab          = 50304
0.00.038.085 I print_info: n_merges         = 50009
0.00.038.085 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.085 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.085 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.086 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.086 I print_info: LF token         = 128 'Ä'
0.00.038.086 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.086 I print_info: max token length = 1024
0.00.600.109 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.123 I load_tensors: offloading output layer to GPU
0.00.600.124 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.160 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.600.161 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.601.641 I llama_init_from_model: n_seq_max     = 1
0.00.601.646 I llama_init_from_model: n_ctx         = 2048
0.00.601.647 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.647 I llama_init_from_model: n_batch       = 2048
0.00.601.648 I llama_init_from_model: n_ubatch      = 512
0.00.601.648 I llama_init_from_model: flash_attn    = 0
0.00.601.650 I llama_init_from_model: freq_base     = 10000.0
0.00.601.650 I llama_init_from_model: freq_scale    = 1
0.00.601.654 I ggml_metal_init: allocating
0.00.601.753 I ggml_metal_init: found device: Apple M4
0.00.601.768 I ggml_metal_init: picking default device: Apple M4
0.00.603.689 I ggml_metal_init: using embedded metal library
0.00.610.296 I ggml_metal_init: GPU name:   Apple M4
0.00.610.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.302 I ggml_metal_init: simdgroup reduction   = true
0.00.610.302 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.302 I ggml_metal_init: has residency sets    = true
0.00.610.303 I ggml_metal_init: has bfloat            = true
0.00.610.303 I ggml_metal_init: use bfloat            = true
0.00.610.304 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.305 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.062 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.239 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.247 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.282 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.202 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.204 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.204 I llama_init_from_model: graph nodes  = 967
0.00.688.204 I llama_init_from_model: graph splits = 2
0.00.688.212 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.241 I main: llama threadpool init, n_threads = 4
0.00.752.285 I 
0.00.752.309 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.309 I 
0.00.752.486 I sampler seed: 1234
0.00.752.491 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.504 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.504 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.813 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55124.22 tokens per second)
0.01.601.813 I llama_perf_context_print:        load time =     742.53 ms
0.01.601.814 I llama_perf_context_print: prompt eval time =      51.26 ms /     7 tokens (    7.32 ms per token,   136.56 tokens per second)
0.01.601.815 I llama_perf_context_print:        eval time =     795.17 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.601.815 I llama_perf_context_print:       total time =     850.46 ms /    70 tokens
0.01.602.038 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.109s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.702 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.223 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.227 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.229 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.230 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.230 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.232 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.232 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.234 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.236 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.236 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.917 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.537 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.538 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.539 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.540 I llama_model_loader: - type  f32:  194 tensors
0.00.023.540 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.541 I print_info: file format = GGUF V3 (latest)
0.00.023.541 I print_info: file type   = Q6_K
0.00.023.542 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.364 I load: special tokens cache size = 25
0.00.037.136 I load: token to piece cache size = 0.2984 MB
0.00.037.139 I print_info: arch             = gptneox
0.00.037.139 I print_info: vocab_only       = 0
0.00.037.139 I print_info: n_ctx_train      = 2048
0.00.037.140 I print_info: n_embd           = 2048
0.00.037.140 I print_info: n_layer          = 24
0.00.037.143 I print_info: n_head           = 16
0.00.037.143 I print_info: n_head_kv        = 16
0.00.037.146 I print_info: n_rot            = 32
0.00.037.146 I print_info: n_swa            = 0
0.00.037.147 I print_info: n_embd_head_k    = 128
0.00.037.147 I print_info: n_embd_head_v    = 128
0.00.037.148 I print_info: n_gqa            = 1
0.00.037.148 I print_info: n_embd_k_gqa     = 2048
0.00.037.149 I print_info: n_embd_v_gqa     = 2048
0.00.037.150 I print_info: f_norm_eps       = 1.0e-05
0.00.037.150 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.150 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.151 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.151 I print_info: f_logit_scale    = 0.0e+00
0.00.037.152 I print_info: n_ff             = 8192
0.00.037.152 I print_info: n_expert         = 0
0.00.037.152 I print_info: n_expert_used    = 0
0.00.037.152 I print_info: causal attn      = 1
0.00.037.153 I print_info: pooling type     = 0
0.00.037.153 I print_info: rope type        = 2
0.00.037.153 I print_info: rope scaling     = linear
0.00.037.154 I print_info: freq_base_train  = 10000.0
0.00.037.154 I print_info: freq_scale_train = 1
0.00.037.154 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.155 I print_info: rope_finetuned   = unknown
0.00.037.155 I print_info: ssm_d_conv       = 0
0.00.037.155 I print_info: ssm_d_inner      = 0
0.00.037.155 I print_info: ssm_d_state      = 0
0.00.037.155 I print_info: ssm_dt_rank      = 0
0.00.037.155 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.157 I print_info: model type       = 1.4B
0.00.037.157 I print_info: model params     = 1.41 B
0.00.037.157 I print_info: general.name     = 1.4B
0.00.037.158 I print_info: vocab type       = BPE
0.00.037.158 I print_info: n_vocab          = 50304
0.00.037.158 I print_info: n_merges         = 50009
0.00.037.159 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.159 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.159 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.159 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.159 I print_info: LF token         = 128 'Ä'
0.00.037.160 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.160 I print_info: max token length = 1024
0.00.643.659 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.662 I load_tensors: offloading output layer to GPU
0.00.643.663 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.686 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.643.689 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.645.162 I llama_init_from_model: n_seq_max     = 1
0.00.645.164 I llama_init_from_model: n_ctx         = 2048
0.00.645.164 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.645.165 I llama_init_from_model: n_batch       = 2048
0.00.645.165 I llama_init_from_model: n_ubatch      = 512
0.00.645.165 I llama_init_from_model: flash_attn    = 0
0.00.645.166 I llama_init_from_model: freq_base     = 10000.0
0.00.645.167 I llama_init_from_model: freq_scale    = 1
0.00.645.168 I ggml_metal_init: allocating
0.00.645.217 I ggml_metal_init: found device: Apple M4
0.00.645.234 I ggml_metal_init: picking default device: Apple M4
0.00.646.671 I ggml_metal_init: using embedded metal library
0.00.652.796 I ggml_metal_init: GPU name:   Apple M4
0.00.652.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.800 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.801 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.801 I ggml_metal_init: simdgroup reduction   = true
0.00.652.801 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.802 I ggml_metal_init: has residency sets    = true
0.00.652.802 I ggml_metal_init: has bfloat            = true
0.00.652.802 I ggml_metal_init: use bfloat            = true
0.00.652.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.111 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.392 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.398 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.721.420 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.298 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.726.300 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.726.301 I llama_init_from_model: graph nodes  = 967
0.00.726.301 I llama_init_from_model: graph splits = 2
0.00.726.306 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.726.447 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.227 I main: llama threadpool init, n_threads = 4
0.00.789.271 I 
0.00.789.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.297 I 
0.00.789.466 I sampler seed: 1234
0.00.789.470 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.508 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.512 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.512 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.673.820 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.01.673.820 I llama_perf_context_print:        load time =     779.65 ms
0.01.673.821 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.78 tokens per second)
0.01.673.822 I llama_perf_context_print:        eval time =     827.34 ms /    63 runs   (   13.13 ms per token,    76.15 tokens per second)
0.01.673.822 I llama_perf_context_print:       total time =     885.47 ms /    70 tokens
0.01.674.059 I ggml_metal_free: deallocating

real	0m1.689s
user	0m0.106s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.605 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.350 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.867 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.878 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.881 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.886 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.887 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.888 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.888 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.893 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.179 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.199 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.200 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.200 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.201 I llama_model_loader: - type  f32:  194 tensors
0.00.054.202 I llama_model_loader: - type  f16:   98 tensors
0.00.054.203 I print_info: file format = GGUF V3 (latest)
0.00.054.204 I print_info: file type   = all F32 (guessed)
0.00.054.206 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.012 I load: special tokens cache size = 25
0.00.072.983 I load: token to piece cache size = 0.2984 MB
0.00.072.987 I print_info: arch             = gptneox
0.00.072.988 I print_info: vocab_only       = 0
0.00.072.988 I print_info: n_ctx_train      = 2048
0.00.072.988 I print_info: n_embd           = 2048
0.00.072.988 I print_info: n_layer          = 24
0.00.072.992 I print_info: n_head           = 16
0.00.072.993 I print_info: n_head_kv        = 16
0.00.072.993 I print_info: n_rot            = 32
0.00.072.993 I print_info: n_swa            = 0
0.00.072.993 I print_info: n_embd_head_k    = 128
0.00.072.993 I print_info: n_embd_head_v    = 128
0.00.072.994 I print_info: n_gqa            = 1
0.00.072.995 I print_info: n_embd_k_gqa     = 2048
0.00.072.996 I print_info: n_embd_v_gqa     = 2048
0.00.072.996 I print_info: f_norm_eps       = 1.0e-05
0.00.072.997 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.997 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.997 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.997 I print_info: f_logit_scale    = 0.0e+00
0.00.072.998 I print_info: n_ff             = 8192
0.00.072.998 I print_info: n_expert         = 0
0.00.072.998 I print_info: n_expert_used    = 0
0.00.072.998 I print_info: causal attn      = 1
0.00.072.998 I print_info: pooling type     = 0
0.00.072.999 I print_info: rope type        = 2
0.00.073.001 I print_info: rope scaling     = linear
0.00.073.001 I print_info: freq_base_train  = 10000.0
0.00.073.001 I print_info: freq_scale_train = 1
0.00.073.002 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.002 I print_info: rope_finetuned   = unknown
0.00.073.002 I print_info: ssm_d_conv       = 0
0.00.073.002 I print_info: ssm_d_inner      = 0
0.00.073.002 I print_info: ssm_d_state      = 0
0.00.073.002 I print_info: ssm_dt_rank      = 0
0.00.073.002 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.003 I print_info: model type       = 1.4B
0.00.073.003 I print_info: model params     = 1.41 B
0.00.073.003 I print_info: general.name     = 1.4B
0.00.073.003 I print_info: vocab type       = BPE
0.00.073.004 I print_info: n_vocab          = 50304
0.00.073.004 I print_info: n_merges         = 50009
0.00.073.004 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.004 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.004 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.004 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.005 I print_info: LF token         = 128 'Ä'
0.00.073.005 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.005 I print_info: max token length = 1024
0.01.707.440 I load_tensors: offloading 24 repeating layers to GPU
0.01.707.448 I load_tensors: offloading output layer to GPU
0.01.707.448 I load_tensors: offloaded 25/25 layers to GPU
0.01.707.482 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.707.484 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.708.558 I llama_init_from_model: n_seq_max     = 1
0.01.708.561 I llama_init_from_model: n_ctx         = 128
0.01.708.561 I llama_init_from_model: n_ctx_per_seq = 128
0.01.708.561 I llama_init_from_model: n_batch       = 128
0.01.708.562 I llama_init_from_model: n_ubatch      = 128
0.01.708.562 I llama_init_from_model: flash_attn    = 0
0.01.708.562 I llama_init_from_model: freq_base     = 10000.0
0.01.708.563 I llama_init_from_model: freq_scale    = 1
0.01.708.563 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.708.564 I ggml_metal_init: allocating
0.01.708.658 I ggml_metal_init: found device: Apple M4
0.01.708.664 I ggml_metal_init: picking default device: Apple M4
0.01.709.828 I ggml_metal_init: using embedded metal library
0.01.713.878 I ggml_metal_init: GPU name:   Apple M4
0.01.713.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.713.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.713.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.713.883 I ggml_metal_init: simdgroup reduction   = true
0.01.713.883 I ggml_metal_init: simdgroup matrix mul. = true
0.01.713.883 I ggml_metal_init: has residency sets    = true
0.01.713.883 I ggml_metal_init: has bfloat            = true
0.01.713.883 I ggml_metal_init: use bfloat            = true
0.01.713.884 I ggml_metal_init: hasUnifiedMemory      = true
0.01.713.885 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.724.467 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.726.217 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.726.221 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.726.236 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.727.925 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.727.926 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.727.926 I llama_init_from_model: graph nodes  = 967
0.01.727.927 I llama_init_from_model: graph splits = 2
0.01.727.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.727.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.762.252 I 
0.01.762.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.762.319 I perplexity: tokenizing the input ..
0.01.767.477 I perplexity: tokenization took 5.156 ms
0.01.767.499 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.886.098 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.887.418 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.887.434 I llama_perf_context_print:        load time =    1737.89 ms
0.01.887.435 I llama_perf_context_print: prompt eval time =     118.33 ms /   128 tokens (    0.92 ms per token,  1081.68 tokens per second)
0.01.887.435 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.887.435 I llama_perf_context_print:       total time =     125.18 ms /   129 tokens
0.01.887.841 I ggml_metal_free: deallocating

real	0m2.128s
user	0m0.091s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.935 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.933 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.949 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.950 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.950 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.950 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.878 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.673 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.674 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.674 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.675 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.675 I llama_model_loader: - type  f32:  194 tensors
0.00.028.676 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.676 I print_info: file format = GGUF V3 (latest)
0.00.028.677 I print_info: file type   = Q8_0
0.00.028.678 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.684 I load: special tokens cache size = 25
0.00.042.678 I load: token to piece cache size = 0.2984 MB
0.00.042.682 I print_info: arch             = gptneox
0.00.042.682 I print_info: vocab_only       = 0
0.00.042.683 I print_info: n_ctx_train      = 2048
0.00.042.683 I print_info: n_embd           = 2048
0.00.042.683 I print_info: n_layer          = 24
0.00.042.687 I print_info: n_head           = 16
0.00.042.688 I print_info: n_head_kv        = 16
0.00.042.688 I print_info: n_rot            = 32
0.00.042.688 I print_info: n_swa            = 0
0.00.042.689 I print_info: n_embd_head_k    = 128
0.00.042.689 I print_info: n_embd_head_v    = 128
0.00.042.689 I print_info: n_gqa            = 1
0.00.042.690 I print_info: n_embd_k_gqa     = 2048
0.00.042.691 I print_info: n_embd_v_gqa     = 2048
0.00.042.692 I print_info: f_norm_eps       = 1.0e-05
0.00.042.692 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.692 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.692 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.693 I print_info: f_logit_scale    = 0.0e+00
0.00.042.693 I print_info: n_ff             = 8192
0.00.042.693 I print_info: n_expert         = 0
0.00.042.693 I print_info: n_expert_used    = 0
0.00.042.694 I print_info: causal attn      = 1
0.00.042.694 I print_info: pooling type     = 0
0.00.042.694 I print_info: rope type        = 2
0.00.042.694 I print_info: rope scaling     = linear
0.00.042.695 I print_info: freq_base_train  = 10000.0
0.00.042.695 I print_info: freq_scale_train = 1
0.00.042.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.695 I print_info: rope_finetuned   = unknown
0.00.042.695 I print_info: ssm_d_conv       = 0
0.00.042.696 I print_info: ssm_d_inner      = 0
0.00.042.696 I print_info: ssm_d_state      = 0
0.00.042.696 I print_info: ssm_dt_rank      = 0
0.00.042.696 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.696 I print_info: model type       = 1.4B
0.00.042.697 I print_info: model params     = 1.41 B
0.00.042.697 I print_info: general.name     = 1.4B
0.00.042.698 I print_info: vocab type       = BPE
0.00.042.698 I print_info: n_vocab          = 50304
0.00.042.698 I print_info: n_merges         = 50009
0.00.042.698 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.698 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.699 I print_info: LF token         = 128 'Ä'
0.00.042.700 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.700 I print_info: max token length = 1024
0.01.073.641 I load_tensors: offloading 24 repeating layers to GPU
0.01.073.645 I load_tensors: offloading output layer to GPU
0.01.073.645 I load_tensors: offloaded 25/25 layers to GPU
0.01.073.671 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.073.674 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.074.992 I llama_init_from_model: n_seq_max     = 1
0.01.074.995 I llama_init_from_model: n_ctx         = 128
0.01.074.995 I llama_init_from_model: n_ctx_per_seq = 128
0.01.074.996 I llama_init_from_model: n_batch       = 128
0.01.074.999 I llama_init_from_model: n_ubatch      = 128
0.01.075.000 I llama_init_from_model: flash_attn    = 0
0.01.075.001 I llama_init_from_model: freq_base     = 10000.0
0.01.075.001 I llama_init_from_model: freq_scale    = 1
0.01.075.007 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.075.008 I ggml_metal_init: allocating
0.01.075.065 I ggml_metal_init: found device: Apple M4
0.01.075.076 I ggml_metal_init: picking default device: Apple M4
0.01.076.438 I ggml_metal_init: using embedded metal library
0.01.082.030 I ggml_metal_init: GPU name:   Apple M4
0.01.082.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.082.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.082.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.082.035 I ggml_metal_init: simdgroup reduction   = true
0.01.082.035 I ggml_metal_init: simdgroup matrix mul. = true
0.01.082.036 I ggml_metal_init: has residency sets    = true
0.01.082.036 I ggml_metal_init: has bfloat            = true
0.01.082.036 I ggml_metal_init: use bfloat            = true
0.01.082.037 I ggml_metal_init: hasUnifiedMemory      = true
0.01.082.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.097.388 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.100.830 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.100.834 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.100.881 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.104.048 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.104.050 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.104.050 I llama_init_from_model: graph nodes  = 967
0.01.104.050 I llama_init_from_model: graph splits = 2
0.01.104.053 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.104.053 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.128.110 I 
0.01.128.157 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.128.169 I perplexity: tokenizing the input ..
0.01.134.065 I perplexity: tokenization took 5.894 ms
0.01.134.080 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.257.695 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.259.046 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.259.063 I llama_perf_context_print:        load time =    1119.17 ms
0.01.259.064 I llama_perf_context_print: prompt eval time =     123.32 ms /   128 tokens (    0.96 ms per token,  1037.93 tokens per second)
0.01.259.065 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.259.065 I llama_perf_context_print:       total time =     130.96 ms /   129 tokens
0.01.259.455 I ggml_metal_free: deallocating

real	0m1.275s
user	0m0.075s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.866 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.688 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.688 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.690 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.690 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.691 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.692 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.694 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.695 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.401 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.411 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.413 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.413 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.414 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.414 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.414 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.415 I llama_model_loader: - type  f32:  194 tensors
0.00.033.415 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.415 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.416 I print_info: file format = GGUF V3 (latest)
0.00.033.416 I print_info: file type   = Q4_0
0.00.033.417 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.042.091 I load: special tokens cache size = 25
0.00.048.607 I load: token to piece cache size = 0.2984 MB
0.00.048.610 I print_info: arch             = gptneox
0.00.048.610 I print_info: vocab_only       = 0
0.00.048.610 I print_info: n_ctx_train      = 2048
0.00.048.610 I print_info: n_embd           = 2048
0.00.048.610 I print_info: n_layer          = 24
0.00.048.613 I print_info: n_head           = 16
0.00.048.614 I print_info: n_head_kv        = 16
0.00.048.614 I print_info: n_rot            = 32
0.00.048.615 I print_info: n_swa            = 0
0.00.048.615 I print_info: n_embd_head_k    = 128
0.00.048.615 I print_info: n_embd_head_v    = 128
0.00.048.616 I print_info: n_gqa            = 1
0.00.048.616 I print_info: n_embd_k_gqa     = 2048
0.00.048.617 I print_info: n_embd_v_gqa     = 2048
0.00.048.617 I print_info: f_norm_eps       = 1.0e-05
0.00.048.618 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.618 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.618 I print_info: f_logit_scale    = 0.0e+00
0.00.048.619 I print_info: n_ff             = 8192
0.00.048.619 I print_info: n_expert         = 0
0.00.048.619 I print_info: n_expert_used    = 0
0.00.048.619 I print_info: causal attn      = 1
0.00.048.620 I print_info: pooling type     = 0
0.00.048.620 I print_info: rope type        = 2
0.00.048.620 I print_info: rope scaling     = linear
0.00.048.620 I print_info: freq_base_train  = 10000.0
0.00.048.620 I print_info: freq_scale_train = 1
0.00.048.621 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.621 I print_info: rope_finetuned   = unknown
0.00.048.621 I print_info: ssm_d_conv       = 0
0.00.048.621 I print_info: ssm_d_inner      = 0
0.00.048.621 I print_info: ssm_d_state      = 0
0.00.048.621 I print_info: ssm_dt_rank      = 0
0.00.048.621 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.621 I print_info: model type       = 1.4B
0.00.048.622 I print_info: model params     = 1.41 B
0.00.048.622 I print_info: general.name     = 1.4B
0.00.048.622 I print_info: vocab type       = BPE
0.00.048.622 I print_info: n_vocab          = 50304
0.00.048.623 I print_info: n_merges         = 50009
0.00.048.623 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.623 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.623 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.623 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.623 I print_info: LF token         = 128 'Ä'
0.00.048.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.624 I print_info: max token length = 1024
0.00.613.448 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.463 I load_tensors: offloading output layer to GPU
0.00.613.464 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.503 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.613.504 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.615.027 I llama_init_from_model: n_seq_max     = 1
0.00.615.032 I llama_init_from_model: n_ctx         = 128
0.00.615.033 I llama_init_from_model: n_ctx_per_seq = 128
0.00.615.037 I llama_init_from_model: n_batch       = 128
0.00.615.037 I llama_init_from_model: n_ubatch      = 128
0.00.615.038 I llama_init_from_model: flash_attn    = 0
0.00.615.046 I llama_init_from_model: freq_base     = 10000.0
0.00.615.046 I llama_init_from_model: freq_scale    = 1
0.00.615.047 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.615.052 I ggml_metal_init: allocating
0.00.615.202 I ggml_metal_init: found device: Apple M4
0.00.615.216 I ggml_metal_init: picking default device: Apple M4
0.00.617.146 I ggml_metal_init: using embedded metal library
0.00.623.824 I ggml_metal_init: GPU name:   Apple M4
0.00.623.832 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.833 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.834 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.835 I ggml_metal_init: simdgroup reduction   = true
0.00.623.835 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.835 I ggml_metal_init: has residency sets    = true
0.00.623.835 I ggml_metal_init: has bfloat            = true
0.00.623.836 I ggml_metal_init: use bfloat            = true
0.00.623.837 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.838 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.633 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.645.127 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.645.131 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.645.156 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.648.656 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.648.657 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.648.658 I llama_init_from_model: graph nodes  = 967
0.00.648.658 I llama_init_from_model: graph splits = 2
0.00.648.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.648.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.924 I 
0.00.677.005 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.026 I perplexity: tokenizing the input ..
0.00.684.705 I perplexity: tokenization took 7.675 ms
0.00.684.726 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.313 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.823.649 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.823.664 I llama_perf_context_print:        load time =     666.05 ms
0.00.823.665 I llama_perf_context_print: prompt eval time =     136.69 ms /   128 tokens (    1.07 ms per token,   936.41 tokens per second)
0.00.823.666 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.666 I llama_perf_context_print:       total time =     146.74 ms /   129 tokens
0.00.824.045 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.081s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.906 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.107 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.114 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.118 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.120 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.120 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.120 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.121 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.121 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.123 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.942 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.984 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.802 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.804 I llama_model_loader: - type  f32:  194 tensors
0.00.026.804 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.805 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.805 I print_info: file format = GGUF V3 (latest)
0.00.026.806 I print_info: file type   = Q4_1
0.00.026.807 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.980 I load: special tokens cache size = 25
0.00.040.686 I load: token to piece cache size = 0.2984 MB
0.00.040.689 I print_info: arch             = gptneox
0.00.040.689 I print_info: vocab_only       = 0
0.00.040.689 I print_info: n_ctx_train      = 2048
0.00.040.690 I print_info: n_embd           = 2048
0.00.040.690 I print_info: n_layer          = 24
0.00.040.693 I print_info: n_head           = 16
0.00.040.694 I print_info: n_head_kv        = 16
0.00.040.695 I print_info: n_rot            = 32
0.00.040.695 I print_info: n_swa            = 0
0.00.040.695 I print_info: n_embd_head_k    = 128
0.00.040.695 I print_info: n_embd_head_v    = 128
0.00.040.696 I print_info: n_gqa            = 1
0.00.040.697 I print_info: n_embd_k_gqa     = 2048
0.00.040.698 I print_info: n_embd_v_gqa     = 2048
0.00.040.698 I print_info: f_norm_eps       = 1.0e-05
0.00.040.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.699 I print_info: f_logit_scale    = 0.0e+00
0.00.040.700 I print_info: n_ff             = 8192
0.00.040.700 I print_info: n_expert         = 0
0.00.040.700 I print_info: n_expert_used    = 0
0.00.040.700 I print_info: causal attn      = 1
0.00.040.701 I print_info: pooling type     = 0
0.00.040.701 I print_info: rope type        = 2
0.00.040.701 I print_info: rope scaling     = linear
0.00.040.701 I print_info: freq_base_train  = 10000.0
0.00.040.702 I print_info: freq_scale_train = 1
0.00.040.702 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.702 I print_info: rope_finetuned   = unknown
0.00.040.702 I print_info: ssm_d_conv       = 0
0.00.040.702 I print_info: ssm_d_inner      = 0
0.00.040.703 I print_info: ssm_d_state      = 0
0.00.040.703 I print_info: ssm_dt_rank      = 0
0.00.040.703 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.703 I print_info: model type       = 1.4B
0.00.040.703 I print_info: model params     = 1.41 B
0.00.040.704 I print_info: general.name     = 1.4B
0.00.040.704 I print_info: vocab type       = BPE
0.00.040.704 I print_info: n_vocab          = 50304
0.00.040.705 I print_info: n_merges         = 50009
0.00.040.705 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.705 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.705 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.705 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.708 I print_info: LF token         = 128 'Ä'
0.00.040.708 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.708 I print_info: max token length = 1024
0.00.662.038 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.053 I load_tensors: offloading output layer to GPU
0.00.662.054 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.085 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.662.086 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.663.277 I llama_init_from_model: n_seq_max     = 1
0.00.663.284 I llama_init_from_model: n_ctx         = 128
0.00.663.285 I llama_init_from_model: n_ctx_per_seq = 128
0.00.663.285 I llama_init_from_model: n_batch       = 128
0.00.663.286 I llama_init_from_model: n_ubatch      = 128
0.00.663.286 I llama_init_from_model: flash_attn    = 0
0.00.663.288 I llama_init_from_model: freq_base     = 10000.0
0.00.663.288 I llama_init_from_model: freq_scale    = 1
0.00.663.289 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.663.291 I ggml_metal_init: allocating
0.00.663.336 I ggml_metal_init: found device: Apple M4
0.00.663.349 I ggml_metal_init: picking default device: Apple M4
0.00.665.081 I ggml_metal_init: using embedded metal library
0.00.671.121 I ggml_metal_init: GPU name:   Apple M4
0.00.671.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.128 I ggml_metal_init: simdgroup reduction   = true
0.00.671.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.129 I ggml_metal_init: has residency sets    = true
0.00.671.129 I ggml_metal_init: has bfloat            = true
0.00.671.130 I ggml_metal_init: use bfloat            = true
0.00.671.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.132 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.985 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.548 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.693.552 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.693.584 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.916 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.696.918 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.696.919 I llama_init_from_model: graph nodes  = 967
0.00.696.919 I llama_init_from_model: graph splits = 2
0.00.696.922 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.696.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.385 I 
0.00.724.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.502 I perplexity: tokenizing the input ..
0.00.731.725 I perplexity: tokenization took 7.219 ms
0.00.731.749 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.869.467 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.870.872 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.870.885 I llama_perf_context_print:        load time =     715.47 ms
0.00.870.887 I llama_perf_context_print: prompt eval time =     136.75 ms /   128 tokens (    1.07 ms per token,   936.03 tokens per second)
0.00.870.887 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.870.888 I llama_perf_context_print:       total time =     146.51 ms /   129 tokens
0.00.871.276 I ggml_metal_free: deallocating

real	0m0.885s
user	0m0.079s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.788 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.145 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.023.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.154 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.155 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.156 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.157 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.158 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.158 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.159 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.161 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.161 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.957 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.064 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.845 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.847 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.847 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.847 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.847 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.848 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.031.848 I llama_model_loader: - type  f32:  194 tensors
0.00.031.848 I llama_model_loader: - type q5_0:   97 tensors
0.00.031.849 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.849 I print_info: file format = GGUF V3 (latest)
0.00.031.850 I print_info: file type   = Q5_0
0.00.031.850 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.040.193 I load: special tokens cache size = 25
0.00.046.099 I load: token to piece cache size = 0.2984 MB
0.00.046.101 I print_info: arch             = gptneox
0.00.046.102 I print_info: vocab_only       = 0
0.00.046.102 I print_info: n_ctx_train      = 2048
0.00.046.102 I print_info: n_embd           = 2048
0.00.046.102 I print_info: n_layer          = 24
0.00.046.105 I print_info: n_head           = 16
0.00.046.106 I print_info: n_head_kv        = 16
0.00.046.106 I print_info: n_rot            = 32
0.00.046.107 I print_info: n_swa            = 0
0.00.046.107 I print_info: n_embd_head_k    = 128
0.00.046.107 I print_info: n_embd_head_v    = 128
0.00.046.108 I print_info: n_gqa            = 1
0.00.046.108 I print_info: n_embd_k_gqa     = 2048
0.00.046.109 I print_info: n_embd_v_gqa     = 2048
0.00.046.110 I print_info: f_norm_eps       = 1.0e-05
0.00.046.110 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.110 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.112 I print_info: f_logit_scale    = 0.0e+00
0.00.046.112 I print_info: n_ff             = 8192
0.00.046.113 I print_info: n_expert         = 0
0.00.046.113 I print_info: n_expert_used    = 0
0.00.046.113 I print_info: causal attn      = 1
0.00.046.113 I print_info: pooling type     = 0
0.00.046.113 I print_info: rope type        = 2
0.00.046.113 I print_info: rope scaling     = linear
0.00.046.114 I print_info: freq_base_train  = 10000.0
0.00.046.114 I print_info: freq_scale_train = 1
0.00.046.114 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.114 I print_info: rope_finetuned   = unknown
0.00.046.114 I print_info: ssm_d_conv       = 0
0.00.046.115 I print_info: ssm_d_inner      = 0
0.00.046.115 I print_info: ssm_d_state      = 0
0.00.046.115 I print_info: ssm_dt_rank      = 0
0.00.046.115 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.115 I print_info: model type       = 1.4B
0.00.046.116 I print_info: model params     = 1.41 B
0.00.046.116 I print_info: general.name     = 1.4B
0.00.046.116 I print_info: vocab type       = BPE
0.00.046.116 I print_info: n_vocab          = 50304
0.00.046.117 I print_info: n_merges         = 50009
0.00.046.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.119 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.119 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.119 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.120 I print_info: LF token         = 128 'Ä'
0.00.046.120 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.120 I print_info: max token length = 1024
0.00.821.669 I load_tensors: offloading 24 repeating layers to GPU
0.00.821.684 I load_tensors: offloading output layer to GPU
0.00.821.684 I load_tensors: offloaded 25/25 layers to GPU
0.00.821.715 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.821.717 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.823.069 I llama_init_from_model: n_seq_max     = 1
0.00.823.073 I llama_init_from_model: n_ctx         = 128
0.00.823.074 I llama_init_from_model: n_ctx_per_seq = 128
0.00.823.078 I llama_init_from_model: n_batch       = 128
0.00.823.079 I llama_init_from_model: n_ubatch      = 128
0.00.823.079 I llama_init_from_model: flash_attn    = 0
0.00.823.082 I llama_init_from_model: freq_base     = 10000.0
0.00.823.095 I llama_init_from_model: freq_scale    = 1
0.00.823.095 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.823.097 I ggml_metal_init: allocating
0.00.823.182 I ggml_metal_init: found device: Apple M4
0.00.823.196 I ggml_metal_init: picking default device: Apple M4
0.00.825.032 I ggml_metal_init: using embedded metal library
0.00.831.787 I ggml_metal_init: GPU name:   Apple M4
0.00.831.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.831.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.831.794 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.831.794 I ggml_metal_init: simdgroup reduction   = true
0.00.831.795 I ggml_metal_init: simdgroup matrix mul. = true
0.00.831.795 I ggml_metal_init: has residency sets    = true
0.00.831.795 I ggml_metal_init: has bfloat            = true
0.00.831.796 I ggml_metal_init: use bfloat            = true
0.00.831.797 I ggml_metal_init: hasUnifiedMemory      = true
0.00.831.801 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.849.855 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.853.358 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.853.362 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.853.387 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.856.761 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.856.763 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.856.763 I llama_init_from_model: graph nodes  = 967
0.00.856.764 I llama_init_from_model: graph splits = 2
0.00.856.767 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.856.767 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.889.876 I 
0.00.889.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.889.988 I perplexity: tokenizing the input ..
0.00.897.031 I perplexity: tokenization took 7.039 ms
0.00.897.051 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.046.292 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.01.047.632 I Final estimate: PPL = 10.0972 +/- 3.20136

0.01.047.646 I llama_perf_context_print:        load time =     881.08 ms
0.01.047.647 I llama_perf_context_print: prompt eval time =     148.37 ms /   128 tokens (    1.16 ms per token,   862.71 tokens per second)
0.01.047.647 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.047.648 I llama_perf_context_print:       total time =     157.78 ms /   129 tokens
0.01.048.079 I ggml_metal_free: deallocating

real	0m1.062s
user	0m0.080s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.513 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.020.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.284 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.285 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.286 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.287 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.288 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.289 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.290 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.044 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.876 I llama_model_loader: - type  f32:  194 tensors
0.00.028.877 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.877 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.877 I print_info: file format = GGUF V3 (latest)
0.00.028.878 I print_info: file type   = Q5_1
0.00.028.879 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.036.674 I load: special tokens cache size = 25
0.00.042.380 I load: token to piece cache size = 0.2984 MB
0.00.042.383 I print_info: arch             = gptneox
0.00.042.383 I print_info: vocab_only       = 0
0.00.042.383 I print_info: n_ctx_train      = 2048
0.00.042.384 I print_info: n_embd           = 2048
0.00.042.384 I print_info: n_layer          = 24
0.00.042.387 I print_info: n_head           = 16
0.00.042.387 I print_info: n_head_kv        = 16
0.00.042.388 I print_info: n_rot            = 32
0.00.042.388 I print_info: n_swa            = 0
0.00.042.391 I print_info: n_embd_head_k    = 128
0.00.042.391 I print_info: n_embd_head_v    = 128
0.00.042.391 I print_info: n_gqa            = 1
0.00.042.392 I print_info: n_embd_k_gqa     = 2048
0.00.042.393 I print_info: n_embd_v_gqa     = 2048
0.00.042.394 I print_info: f_norm_eps       = 1.0e-05
0.00.042.394 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.394 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.394 I print_info: f_logit_scale    = 0.0e+00
0.00.042.395 I print_info: n_ff             = 8192
0.00.042.395 I print_info: n_expert         = 0
0.00.042.396 I print_info: n_expert_used    = 0
0.00.042.396 I print_info: causal attn      = 1
0.00.042.396 I print_info: pooling type     = 0
0.00.042.396 I print_info: rope type        = 2
0.00.042.396 I print_info: rope scaling     = linear
0.00.042.398 I print_info: freq_base_train  = 10000.0
0.00.042.398 I print_info: freq_scale_train = 1
0.00.042.398 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.399 I print_info: rope_finetuned   = unknown
0.00.042.399 I print_info: ssm_d_conv       = 0
0.00.042.399 I print_info: ssm_d_inner      = 0
0.00.042.399 I print_info: ssm_d_state      = 0
0.00.042.399 I print_info: ssm_dt_rank      = 0
0.00.042.399 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.399 I print_info: model type       = 1.4B
0.00.042.400 I print_info: model params     = 1.41 B
0.00.042.400 I print_info: general.name     = 1.4B
0.00.042.400 I print_info: vocab type       = BPE
0.00.042.401 I print_info: n_vocab          = 50304
0.00.042.402 I print_info: n_merges         = 50009
0.00.042.405 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.406 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.406 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.406 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.406 I print_info: LF token         = 128 'Ä'
0.00.042.407 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.407 I print_info: max token length = 1024
0.00.633.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.249 I load_tensors: offloading output layer to GPU
0.00.633.250 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.270 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.633.273 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.634.717 I llama_init_from_model: n_seq_max     = 1
0.00.634.719 I llama_init_from_model: n_ctx         = 128
0.00.634.719 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.720 I llama_init_from_model: n_batch       = 128
0.00.634.720 I llama_init_from_model: n_ubatch      = 128
0.00.634.720 I llama_init_from_model: flash_attn    = 0
0.00.634.721 I llama_init_from_model: freq_base     = 10000.0
0.00.634.722 I llama_init_from_model: freq_scale    = 1
0.00.634.723 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.724 I ggml_metal_init: allocating
0.00.634.757 I ggml_metal_init: found device: Apple M4
0.00.634.767 I ggml_metal_init: picking default device: Apple M4
0.00.636.195 I ggml_metal_init: using embedded metal library
0.00.642.210 I ggml_metal_init: GPU name:   Apple M4
0.00.642.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.215 I ggml_metal_init: simdgroup reduction   = true
0.00.642.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.216 I ggml_metal_init: has residency sets    = true
0.00.642.216 I ggml_metal_init: has bfloat            = true
0.00.642.216 I ggml_metal_init: use bfloat            = true
0.00.642.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.218 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.669 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.104 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.106 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.136 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.665.318 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.665.319 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.665.320 I llama_init_from_model: graph nodes  = 967
0.00.665.320 I llama_init_from_model: graph splits = 2
0.00.665.323 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.323 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.216 I 
0.00.693.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.316 I perplexity: tokenizing the input ..
0.00.700.651 I perplexity: tokenization took 7.331 ms
0.00.700.674 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.866 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.838.290 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.838.302 I llama_perf_context_print:        load time =     681.70 ms
0.00.838.303 I llama_perf_context_print: prompt eval time =     135.24 ms /   128 tokens (    1.06 ms per token,   946.49 tokens per second)
0.00.838.304 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.304 I llama_perf_context_print:       total time =     145.09 ms /   129 tokens
0.00.838.634 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.077s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.087 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.916 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.923 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.929 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.929 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.930 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.930 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.930 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.931 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.932 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.932 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.932 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.934 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.935 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.935 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.937 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.937 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.729 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.538 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.539 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.540 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.541 I llama_model_loader: - type  f32:  194 tensors
0.00.024.541 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.542 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.542 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.542 I print_info: file format = GGUF V3 (latest)
0.00.024.543 I print_info: file type   = Q2_K - Medium
0.00.024.544 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.644 I load: special tokens cache size = 25
0.00.038.516 I load: token to piece cache size = 0.2984 MB
0.00.038.518 I print_info: arch             = gptneox
0.00.038.518 I print_info: vocab_only       = 0
0.00.038.519 I print_info: n_ctx_train      = 2048
0.00.038.519 I print_info: n_embd           = 2048
0.00.038.519 I print_info: n_layer          = 24
0.00.038.522 I print_info: n_head           = 16
0.00.038.523 I print_info: n_head_kv        = 16
0.00.038.523 I print_info: n_rot            = 32
0.00.038.523 I print_info: n_swa            = 0
0.00.038.523 I print_info: n_embd_head_k    = 128
0.00.038.524 I print_info: n_embd_head_v    = 128
0.00.038.524 I print_info: n_gqa            = 1
0.00.038.525 I print_info: n_embd_k_gqa     = 2048
0.00.038.526 I print_info: n_embd_v_gqa     = 2048
0.00.038.526 I print_info: f_norm_eps       = 1.0e-05
0.00.038.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.527 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.527 I print_info: f_logit_scale    = 0.0e+00
0.00.038.528 I print_info: n_ff             = 8192
0.00.038.528 I print_info: n_expert         = 0
0.00.038.528 I print_info: n_expert_used    = 0
0.00.038.528 I print_info: causal attn      = 1
0.00.038.530 I print_info: pooling type     = 0
0.00.038.530 I print_info: rope type        = 2
0.00.038.530 I print_info: rope scaling     = linear
0.00.038.531 I print_info: freq_base_train  = 10000.0
0.00.038.531 I print_info: freq_scale_train = 1
0.00.038.531 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.531 I print_info: rope_finetuned   = unknown
0.00.038.532 I print_info: ssm_d_conv       = 0
0.00.038.532 I print_info: ssm_d_inner      = 0
0.00.038.532 I print_info: ssm_d_state      = 0
0.00.038.532 I print_info: ssm_dt_rank      = 0
0.00.038.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.534 I print_info: model type       = 1.4B
0.00.038.535 I print_info: model params     = 1.41 B
0.00.038.535 I print_info: general.name     = 1.4B
0.00.038.535 I print_info: vocab type       = BPE
0.00.038.535 I print_info: n_vocab          = 50304
0.00.038.535 I print_info: n_merges         = 50009
0.00.038.536 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: LF token         = 128 'Ä'
0.00.038.537 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.540 I print_info: max token length = 1024
0.00.364.301 I load_tensors: offloading 24 repeating layers to GPU
0.00.364.315 I load_tensors: offloading output layer to GPU
0.00.364.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.364.344 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.364.346 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.365.741 I llama_init_from_model: n_seq_max     = 1
0.00.365.747 I llama_init_from_model: n_ctx         = 128
0.00.365.747 I llama_init_from_model: n_ctx_per_seq = 128
0.00.365.747 I llama_init_from_model: n_batch       = 128
0.00.365.748 I llama_init_from_model: n_ubatch      = 128
0.00.365.748 I llama_init_from_model: flash_attn    = 0
0.00.365.749 I llama_init_from_model: freq_base     = 10000.0
0.00.365.749 I llama_init_from_model: freq_scale    = 1
0.00.365.750 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.365.752 I ggml_metal_init: allocating
0.00.365.795 I ggml_metal_init: found device: Apple M4
0.00.365.808 I ggml_metal_init: picking default device: Apple M4
0.00.367.484 I ggml_metal_init: using embedded metal library
0.00.373.435 I ggml_metal_init: GPU name:   Apple M4
0.00.373.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.373.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.373.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.373.444 I ggml_metal_init: simdgroup reduction   = true
0.00.373.444 I ggml_metal_init: simdgroup matrix mul. = true
0.00.373.445 I ggml_metal_init: has residency sets    = true
0.00.373.445 I ggml_metal_init: has bfloat            = true
0.00.373.445 I ggml_metal_init: use bfloat            = true
0.00.373.450 I ggml_metal_init: hasUnifiedMemory      = true
0.00.373.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.395.950 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.399.631 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.399.638 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.399.682 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.403.134 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.403.136 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.403.137 I llama_init_from_model: graph nodes  = 967
0.00.403.137 I llama_init_from_model: graph splits = 2
0.00.403.141 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.403.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.435.697 I 
0.00.435.778 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.435.797 I perplexity: tokenizing the input ..
0.00.442.613 I perplexity: tokenization took 6.813 ms
0.00.442.630 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.582.274 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.583.617 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.583.633 I llama_perf_context_print:        load time =     426.60 ms
0.00.583.634 I llama_perf_context_print: prompt eval time =     138.69 ms /   128 tokens (    1.08 ms per token,   922.89 tokens per second)
0.00.583.635 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.583.635 I llama_perf_context_print:       total time =     147.94 ms /   129 tokens
0.00.584.034 I ggml_metal_free: deallocating

real	0m0.598s
user	0m0.081s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.065 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.844 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.849 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.851 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.851 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.851 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.852 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.852 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.853 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.859 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.724 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.585 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.585 I llama_model_loader: - type  f32:  194 tensors
0.00.024.585 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.586 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.586 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.586 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.587 I print_info: file format = GGUF V3 (latest)
0.00.024.587 I print_info: file type   = Q3_K - Medium
0.00.024.589 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.688 I load: special tokens cache size = 25
0.00.038.336 I load: token to piece cache size = 0.2984 MB
0.00.038.338 I print_info: arch             = gptneox
0.00.038.339 I print_info: vocab_only       = 0
0.00.038.339 I print_info: n_ctx_train      = 2048
0.00.038.339 I print_info: n_embd           = 2048
0.00.038.339 I print_info: n_layer          = 24
0.00.038.342 I print_info: n_head           = 16
0.00.038.343 I print_info: n_head_kv        = 16
0.00.038.343 I print_info: n_rot            = 32
0.00.038.343 I print_info: n_swa            = 0
0.00.038.345 I print_info: n_embd_head_k    = 128
0.00.038.345 I print_info: n_embd_head_v    = 128
0.00.038.346 I print_info: n_gqa            = 1
0.00.038.347 I print_info: n_embd_k_gqa     = 2048
0.00.038.348 I print_info: n_embd_v_gqa     = 2048
0.00.038.348 I print_info: f_norm_eps       = 1.0e-05
0.00.038.349 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.349 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.349 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.349 I print_info: f_logit_scale    = 0.0e+00
0.00.038.350 I print_info: n_ff             = 8192
0.00.038.350 I print_info: n_expert         = 0
0.00.038.350 I print_info: n_expert_used    = 0
0.00.038.350 I print_info: causal attn      = 1
0.00.038.351 I print_info: pooling type     = 0
0.00.038.351 I print_info: rope type        = 2
0.00.038.351 I print_info: rope scaling     = linear
0.00.038.351 I print_info: freq_base_train  = 10000.0
0.00.038.352 I print_info: freq_scale_train = 1
0.00.038.352 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.352 I print_info: rope_finetuned   = unknown
0.00.038.352 I print_info: ssm_d_conv       = 0
0.00.038.353 I print_info: ssm_d_inner      = 0
0.00.038.353 I print_info: ssm_d_state      = 0
0.00.038.353 I print_info: ssm_dt_rank      = 0
0.00.038.355 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.355 I print_info: model type       = 1.4B
0.00.038.355 I print_info: model params     = 1.41 B
0.00.038.356 I print_info: general.name     = 1.4B
0.00.038.356 I print_info: vocab type       = BPE
0.00.038.356 I print_info: n_vocab          = 50304
0.00.038.356 I print_info: n_merges         = 50009
0.00.038.357 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.357 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.357 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.358 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.358 I print_info: LF token         = 128 'Ä'
0.00.038.359 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.359 I print_info: max token length = 1024
0.00.447.241 I load_tensors: offloading 24 repeating layers to GPU
0.00.447.254 I load_tensors: offloading output layer to GPU
0.00.447.255 I load_tensors: offloaded 25/25 layers to GPU
0.00.447.290 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.447.291 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.448.865 I llama_init_from_model: n_seq_max     = 1
0.00.448.870 I llama_init_from_model: n_ctx         = 128
0.00.448.871 I llama_init_from_model: n_ctx_per_seq = 128
0.00.448.871 I llama_init_from_model: n_batch       = 128
0.00.448.872 I llama_init_from_model: n_ubatch      = 128
0.00.448.872 I llama_init_from_model: flash_attn    = 0
0.00.448.874 I llama_init_from_model: freq_base     = 10000.0
0.00.448.875 I llama_init_from_model: freq_scale    = 1
0.00.448.875 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.448.878 I ggml_metal_init: allocating
0.00.448.937 I ggml_metal_init: found device: Apple M4
0.00.448.950 I ggml_metal_init: picking default device: Apple M4
0.00.450.643 I ggml_metal_init: using embedded metal library
0.00.456.556 I ggml_metal_init: GPU name:   Apple M4
0.00.456.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.563 I ggml_metal_init: simdgroup reduction   = true
0.00.456.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.564 I ggml_metal_init: has residency sets    = true
0.00.456.564 I ggml_metal_init: has bfloat            = true
0.00.456.564 I ggml_metal_init: use bfloat            = true
0.00.456.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.843 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.479.408 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.479.415 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.479.459 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.482.717 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.482.719 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.482.720 I llama_init_from_model: graph nodes  = 967
0.00.482.720 I llama_init_from_model: graph splits = 2
0.00.482.723 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.482.723 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.548 I 
0.00.508.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.664 I perplexity: tokenizing the input ..
0.00.515.254 I perplexity: tokenization took 6.588 ms
0.00.515.271 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.647.267 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.648.611 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.648.626 I llama_perf_context_print:        load time =     499.47 ms
0.00.648.627 I llama_perf_context_print: prompt eval time =     131.61 ms /   128 tokens (    1.03 ms per token,   972.55 tokens per second)
0.00.648.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.648.628 I llama_perf_context_print:       total time =     140.09 ms /   129 tokens
0.00.649.023 I ggml_metal_free: deallocating

real	0m0.664s
user	0m0.078s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.865 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.823 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.829 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.836 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.837 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.837 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.839 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.839 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.839 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.841 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.420 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.422 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.422 I llama_model_loader: - type  f32:  194 tensors
0.00.024.423 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.423 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.423 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.424 I print_info: file format = GGUF V3 (latest)
0.00.024.424 I print_info: file type   = Q4_K - Medium
0.00.024.425 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.524 I load: special tokens cache size = 25
0.00.038.302 I load: token to piece cache size = 0.2984 MB
0.00.038.304 I print_info: arch             = gptneox
0.00.038.304 I print_info: vocab_only       = 0
0.00.038.305 I print_info: n_ctx_train      = 2048
0.00.038.305 I print_info: n_embd           = 2048
0.00.038.305 I print_info: n_layer          = 24
0.00.038.308 I print_info: n_head           = 16
0.00.038.309 I print_info: n_head_kv        = 16
0.00.038.309 I print_info: n_rot            = 32
0.00.038.310 I print_info: n_swa            = 0
0.00.038.310 I print_info: n_embd_head_k    = 128
0.00.038.310 I print_info: n_embd_head_v    = 128
0.00.038.311 I print_info: n_gqa            = 1
0.00.038.311 I print_info: n_embd_k_gqa     = 2048
0.00.038.312 I print_info: n_embd_v_gqa     = 2048
0.00.038.313 I print_info: f_norm_eps       = 1.0e-05
0.00.038.313 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.313 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.313 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.313 I print_info: f_logit_scale    = 0.0e+00
0.00.038.315 I print_info: n_ff             = 8192
0.00.038.315 I print_info: n_expert         = 0
0.00.038.316 I print_info: n_expert_used    = 0
0.00.038.316 I print_info: causal attn      = 1
0.00.038.316 I print_info: pooling type     = 0
0.00.038.316 I print_info: rope type        = 2
0.00.038.316 I print_info: rope scaling     = linear
0.00.038.317 I print_info: freq_base_train  = 10000.0
0.00.038.317 I print_info: freq_scale_train = 1
0.00.038.317 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.317 I print_info: rope_finetuned   = unknown
0.00.038.318 I print_info: ssm_d_conv       = 0
0.00.038.318 I print_info: ssm_d_inner      = 0
0.00.038.318 I print_info: ssm_d_state      = 0
0.00.038.320 I print_info: ssm_dt_rank      = 0
0.00.038.320 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.320 I print_info: model type       = 1.4B
0.00.038.321 I print_info: model params     = 1.41 B
0.00.038.321 I print_info: general.name     = 1.4B
0.00.038.321 I print_info: vocab type       = BPE
0.00.038.322 I print_info: n_vocab          = 50304
0.00.038.322 I print_info: n_merges         = 50009
0.00.038.322 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.322 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.322 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.326 I print_info: LF token         = 128 'Ä'
0.00.038.327 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.327 I print_info: max token length = 1024
0.00.507.745 I load_tensors: offloading 24 repeating layers to GPU
0.00.507.759 I load_tensors: offloading output layer to GPU
0.00.507.760 I load_tensors: offloaded 25/25 layers to GPU
0.00.507.794 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.507.795 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.509.334 I llama_init_from_model: n_seq_max     = 1
0.00.509.339 I llama_init_from_model: n_ctx         = 128
0.00.509.340 I llama_init_from_model: n_ctx_per_seq = 128
0.00.509.340 I llama_init_from_model: n_batch       = 128
0.00.509.341 I llama_init_from_model: n_ubatch      = 128
0.00.509.341 I llama_init_from_model: flash_attn    = 0
0.00.509.343 I llama_init_from_model: freq_base     = 10000.0
0.00.509.344 I llama_init_from_model: freq_scale    = 1
0.00.509.344 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.509.352 I ggml_metal_init: allocating
0.00.509.427 I ggml_metal_init: found device: Apple M4
0.00.509.441 I ggml_metal_init: picking default device: Apple M4
0.00.511.197 I ggml_metal_init: using embedded metal library
0.00.517.916 I ggml_metal_init: GPU name:   Apple M4
0.00.517.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.517.922 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.517.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.517.924 I ggml_metal_init: simdgroup reduction   = true
0.00.517.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.517.925 I ggml_metal_init: has residency sets    = true
0.00.517.925 I ggml_metal_init: has bfloat            = true
0.00.517.925 I ggml_metal_init: use bfloat            = true
0.00.517.926 I ggml_metal_init: hasUnifiedMemory      = true
0.00.517.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.937 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.539.407 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.539.414 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.539.447 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.542.610 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.542.612 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.542.612 I llama_init_from_model: graph nodes  = 967
0.00.542.613 I llama_init_from_model: graph splits = 2
0.00.542.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.542.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.108 I 
0.00.572.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.572.214 I perplexity: tokenizing the input ..
0.00.579.385 I perplexity: tokenization took 7.168 ms
0.00.579.405 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.724.147 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.725.572 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.725.595 I llama_perf_context_print:        load time =     563.23 ms
0.00.725.595 I llama_perf_context_print: prompt eval time =     144.34 ms /   128 tokens (    1.13 ms per token,   886.78 tokens per second)
0.00.725.596 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.725.596 I llama_perf_context_print:       total time =     153.49 ms /   129 tokens
0.00.725.969 I ggml_metal_free: deallocating

real	0m0.740s
user	0m0.078s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.211 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.079 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.080 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.080 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.086 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.897 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.713 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.715 I llama_model_loader: - type  f32:  194 tensors
0.00.025.716 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.716 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.716 I print_info: file format = GGUF V3 (latest)
0.00.025.717 I print_info: file type   = Q5_K - Medium
0.00.025.718 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.907 I load: special tokens cache size = 25
0.00.039.839 I load: token to piece cache size = 0.2984 MB
0.00.039.842 I print_info: arch             = gptneox
0.00.039.842 I print_info: vocab_only       = 0
0.00.039.843 I print_info: n_ctx_train      = 2048
0.00.039.843 I print_info: n_embd           = 2048
0.00.039.843 I print_info: n_layer          = 24
0.00.039.846 I print_info: n_head           = 16
0.00.039.846 I print_info: n_head_kv        = 16
0.00.039.849 I print_info: n_rot            = 32
0.00.039.849 I print_info: n_swa            = 0
0.00.039.849 I print_info: n_embd_head_k    = 128
0.00.039.849 I print_info: n_embd_head_v    = 128
0.00.039.850 I print_info: n_gqa            = 1
0.00.039.851 I print_info: n_embd_k_gqa     = 2048
0.00.039.856 I print_info: n_embd_v_gqa     = 2048
0.00.039.856 I print_info: f_norm_eps       = 1.0e-05
0.00.039.857 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.857 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.857 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.857 I print_info: f_logit_scale    = 0.0e+00
0.00.039.858 I print_info: n_ff             = 8192
0.00.039.858 I print_info: n_expert         = 0
0.00.039.858 I print_info: n_expert_used    = 0
0.00.039.858 I print_info: causal attn      = 1
0.00.039.858 I print_info: pooling type     = 0
0.00.039.859 I print_info: rope type        = 2
0.00.039.859 I print_info: rope scaling     = linear
0.00.039.859 I print_info: freq_base_train  = 10000.0
0.00.039.860 I print_info: freq_scale_train = 1
0.00.039.860 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.863 I print_info: rope_finetuned   = unknown
0.00.039.863 I print_info: ssm_d_conv       = 0
0.00.039.863 I print_info: ssm_d_inner      = 0
0.00.039.864 I print_info: ssm_d_state      = 0
0.00.039.864 I print_info: ssm_dt_rank      = 0
0.00.039.864 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.864 I print_info: model type       = 1.4B
0.00.039.865 I print_info: model params     = 1.41 B
0.00.039.865 I print_info: general.name     = 1.4B
0.00.039.865 I print_info: vocab type       = BPE
0.00.039.865 I print_info: n_vocab          = 50304
0.00.039.866 I print_info: n_merges         = 50009
0.00.039.866 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.866 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.866 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.866 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.868 I print_info: LF token         = 128 'Ä'
0.00.039.868 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.868 I print_info: max token length = 1024
0.00.607.989 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.993 I load_tensors: offloading output layer to GPU
0.00.607.994 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.016 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.608.019 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.609.433 I llama_init_from_model: n_seq_max     = 1
0.00.609.435 I llama_init_from_model: n_ctx         = 128
0.00.609.435 I llama_init_from_model: n_ctx_per_seq = 128
0.00.609.436 I llama_init_from_model: n_batch       = 128
0.00.609.436 I llama_init_from_model: n_ubatch      = 128
0.00.609.436 I llama_init_from_model: flash_attn    = 0
0.00.609.437 I llama_init_from_model: freq_base     = 10000.0
0.00.609.438 I llama_init_from_model: freq_scale    = 1
0.00.609.439 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.440 I ggml_metal_init: allocating
0.00.609.457 I ggml_metal_init: found device: Apple M4
0.00.609.466 I ggml_metal_init: picking default device: Apple M4
0.00.610.839 I ggml_metal_init: using embedded metal library
0.00.616.910 I ggml_metal_init: GPU name:   Apple M4
0.00.616.913 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.914 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.915 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.915 I ggml_metal_init: simdgroup reduction   = true
0.00.616.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.916 I ggml_metal_init: has residency sets    = true
0.00.616.916 I ggml_metal_init: has bfloat            = true
0.00.616.916 I ggml_metal_init: use bfloat            = true
0.00.616.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.181 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.636.613 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.636.616 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.636.663 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.871 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.639.873 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.639.874 I llama_init_from_model: graph nodes  = 967
0.00.639.874 I llama_init_from_model: graph splits = 2
0.00.639.877 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.639.878 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.362 I 
0.00.671.444 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.464 I perplexity: tokenizing the input ..
0.00.678.569 I perplexity: tokenization took 7.103 ms
0.00.678.581 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.645 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.819.978 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.819.992 I llama_perf_context_print:        load time =     661.14 ms
0.00.819.993 I llama_perf_context_print: prompt eval time =     139.84 ms /   128 tokens (    1.09 ms per token,   915.36 tokens per second)
0.00.819.994 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.994 I llama_perf_context_print:       total time =     148.64 ms /   129 tokens
0.00.820.400 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.077s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.871 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.376 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.431 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.160 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.161 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.162 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.162 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.162 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.163 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.163 I llama_model_loader: - type  f32:  194 tensors
0.00.024.163 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.164 I print_info: file format = GGUF V3 (latest)
0.00.024.164 I print_info: file type   = Q6_K
0.00.024.165 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.202 I load: special tokens cache size = 25
0.00.038.142 I load: token to piece cache size = 0.2984 MB
0.00.038.144 I print_info: arch             = gptneox
0.00.038.145 I print_info: vocab_only       = 0
0.00.038.145 I print_info: n_ctx_train      = 2048
0.00.038.145 I print_info: n_embd           = 2048
0.00.038.145 I print_info: n_layer          = 24
0.00.038.148 I print_info: n_head           = 16
0.00.038.149 I print_info: n_head_kv        = 16
0.00.038.151 I print_info: n_rot            = 32
0.00.038.152 I print_info: n_swa            = 0
0.00.038.152 I print_info: n_embd_head_k    = 128
0.00.038.152 I print_info: n_embd_head_v    = 128
0.00.038.153 I print_info: n_gqa            = 1
0.00.038.153 I print_info: n_embd_k_gqa     = 2048
0.00.038.154 I print_info: n_embd_v_gqa     = 2048
0.00.038.155 I print_info: f_norm_eps       = 1.0e-05
0.00.038.155 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.155 I print_info: f_logit_scale    = 0.0e+00
0.00.038.156 I print_info: n_ff             = 8192
0.00.038.156 I print_info: n_expert         = 0
0.00.038.156 I print_info: n_expert_used    = 0
0.00.038.157 I print_info: causal attn      = 1
0.00.038.157 I print_info: pooling type     = 0
0.00.038.157 I print_info: rope type        = 2
0.00.038.158 I print_info: rope scaling     = linear
0.00.038.162 I print_info: freq_base_train  = 10000.0
0.00.038.162 I print_info: freq_scale_train = 1
0.00.038.163 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.163 I print_info: rope_finetuned   = unknown
0.00.038.164 I print_info: ssm_d_conv       = 0
0.00.038.164 I print_info: ssm_d_inner      = 0
0.00.038.165 I print_info: ssm_d_state      = 0
0.00.038.165 I print_info: ssm_dt_rank      = 0
0.00.038.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.165 I print_info: model type       = 1.4B
0.00.038.165 I print_info: model params     = 1.41 B
0.00.038.165 I print_info: general.name     = 1.4B
0.00.038.166 I print_info: vocab type       = BPE
0.00.038.166 I print_info: n_vocab          = 50304
0.00.038.166 I print_info: n_merges         = 50009
0.00.038.166 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.167 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.167 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.169 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.169 I print_info: LF token         = 128 'Ä'
0.00.038.170 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.170 I print_info: max token length = 1024
0.00.608.813 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.816 I load_tensors: offloading output layer to GPU
0.00.608.817 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.841 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.608.844 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.610.253 I llama_init_from_model: n_seq_max     = 1
0.00.610.255 I llama_init_from_model: n_ctx         = 128
0.00.610.256 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.256 I llama_init_from_model: n_batch       = 128
0.00.610.260 I llama_init_from_model: n_ubatch      = 128
0.00.610.261 I llama_init_from_model: flash_attn    = 0
0.00.610.262 I llama_init_from_model: freq_base     = 10000.0
0.00.610.262 I llama_init_from_model: freq_scale    = 1
0.00.610.271 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.272 I ggml_metal_init: allocating
0.00.610.334 I ggml_metal_init: found device: Apple M4
0.00.610.346 I ggml_metal_init: picking default device: Apple M4
0.00.611.781 I ggml_metal_init: using embedded metal library
0.00.617.725 I ggml_metal_init: GPU name:   Apple M4
0.00.617.728 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.729 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.730 I ggml_metal_init: simdgroup reduction   = true
0.00.617.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.730 I ggml_metal_init: has residency sets    = true
0.00.617.730 I ggml_metal_init: has bfloat            = true
0.00.617.731 I ggml_metal_init: use bfloat            = true
0.00.617.731 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.732 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.990 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.637.364 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.637.367 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.637.393 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.640.724 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.640.726 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.640.726 I llama_init_from_model: graph nodes  = 967
0.00.640.727 I llama_init_from_model: graph splits = 2
0.00.640.729 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.640.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.434 I 
0.00.671.531 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.550 I perplexity: tokenizing the input ..
0.00.678.763 I perplexity: tokenization took 7.209 ms
0.00.678.785 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.877 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.821.237 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.821.256 I llama_perf_context_print:        load time =     662.56 ms
0.00.821.258 I llama_perf_context_print: prompt eval time =     140.22 ms /   128 tokens (    1.10 ms per token,   912.82 tokens per second)
0.00.821.258 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.259 I llama_perf_context_print:       total time =     149.82 ms /   129 tokens
0.00.821.637 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.078s
sys	0m0.141s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.394 I build: 4592 (496e5bf4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.353 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.046 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.791 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.572 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.573 I llama_model_loader: - type  f32:  194 tensors
0.00.050.573 I llama_model_loader: - type  f16:   98 tensors
0.00.050.574 I print_info: file format = GGUF V3 (latest)
0.00.050.575 I print_info: file type   = all F32 (guessed)
0.00.050.576 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.495 I load: special tokens cache size = 25
0.00.070.428 I load: token to piece cache size = 0.2984 MB
0.00.070.431 I print_info: arch             = gptneox
0.00.070.431 I print_info: vocab_only       = 0
0.00.070.431 I print_info: n_ctx_train      = 2048
0.00.070.432 I print_info: n_embd           = 2048
0.00.070.432 I print_info: n_layer          = 24
0.00.070.435 I print_info: n_head           = 16
0.00.070.436 I print_info: n_head_kv        = 16
0.00.070.438 I print_info: n_rot            = 32
0.00.070.438 I print_info: n_swa            = 0
0.00.070.438 I print_info: n_embd_head_k    = 128
0.00.070.438 I print_info: n_embd_head_v    = 128
0.00.070.439 I print_info: n_gqa            = 1
0.00.070.440 I print_info: n_embd_k_gqa     = 2048
0.00.070.440 I print_info: n_embd_v_gqa     = 2048
0.00.070.441 I print_info: f_norm_eps       = 1.0e-05
0.00.070.441 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.441 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.442 I print_info: f_logit_scale    = 0.0e+00
0.00.070.442 I print_info: n_ff             = 8192
0.00.070.443 I print_info: n_expert         = 0
0.00.070.443 I print_info: n_expert_used    = 0
0.00.070.443 I print_info: causal attn      = 1
0.00.070.443 I print_info: pooling type     = 0
0.00.070.443 I print_info: rope type        = 2
0.00.070.444 I print_info: rope scaling     = linear
0.00.070.444 I print_info: freq_base_train  = 10000.0
0.00.070.446 I print_info: freq_scale_train = 1
0.00.070.446 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.446 I print_info: rope_finetuned   = unknown
0.00.070.446 I print_info: ssm_d_conv       = 0
0.00.070.446 I print_info: ssm_d_inner      = 0
0.00.070.446 I print_info: ssm_d_state      = 0
0.00.070.446 I print_info: ssm_dt_rank      = 0
0.00.070.447 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.447 I print_info: model type       = 1.4B
0.00.070.447 I print_info: model params     = 1.41 B
0.00.070.447 I print_info: general.name     = 1.4B
0.00.070.448 I print_info: vocab type       = BPE
0.00.070.448 I print_info: n_vocab          = 50304
0.00.070.452 I print_info: n_merges         = 50009
0.00.070.452 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.453 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.453 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.453 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.453 I print_info: LF token         = 128 'Ä'
0.00.070.454 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.454 I print_info: max token length = 1024
0.01.360.527 I load_tensors: offloading 24 repeating layers to GPU
0.01.360.535 I load_tensors: offloading output layer to GPU
0.01.360.536 I load_tensors: offloaded 25/25 layers to GPU
0.01.360.560 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.360.562 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.361.562 I llama_init_from_model: n_seq_max     = 1
0.01.361.563 I llama_init_from_model: n_ctx         = 128
0.01.361.564 I llama_init_from_model: n_ctx_per_seq = 128
0.01.361.564 I llama_init_from_model: n_batch       = 128
0.01.361.564 I llama_init_from_model: n_ubatch      = 128
0.01.361.564 I llama_init_from_model: flash_attn    = 0
0.01.361.565 I llama_init_from_model: freq_base     = 10000.0
0.01.361.565 I llama_init_from_model: freq_scale    = 1
0.01.361.565 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.361.568 I ggml_metal_init: allocating
0.01.361.607 I ggml_metal_init: found device: Apple M4
0.01.361.612 I ggml_metal_init: picking default device: Apple M4
0.01.362.612 I ggml_metal_init: using embedded metal library
0.01.366.520 I ggml_metal_init: GPU name:   Apple M4
0.01.366.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.366.522 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.366.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.366.523 I ggml_metal_init: simdgroup reduction   = true
0.01.366.523 I ggml_metal_init: simdgroup matrix mul. = true
0.01.366.523 I ggml_metal_init: has residency sets    = true
0.01.366.523 I ggml_metal_init: has bfloat            = true
0.01.366.524 I ggml_metal_init: use bfloat            = true
0.01.366.524 I ggml_metal_init: hasUnifiedMemory      = true
0.01.366.526 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.377.486 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.379.248 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.379.251 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.379.272 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.380.982 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.380.983 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.380.984 I llama_init_from_model: graph nodes  = 967
0.01.380.984 I llama_init_from_model: graph splits = 2
0.01.380.985 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.380.985 I 
0.01.381.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.381.023 I compute_imatrix: tokenizing the input ..
0.01.385.110 I compute_imatrix: tokenization took 4.086 ms
0.01.385.113 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.653.263 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.656.425 I llama_perf_context_print:        load time =    1630.91 ms
0.01.656.426 I llama_perf_context_print: prompt eval time =     266.33 ms /   128 tokens (    2.08 ms per token,   480.61 tokens per second)
0.01.656.427 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.656.428 I llama_perf_context_print:       total time =    1634.07 ms /   129 tokens
0.01.657.055 I ggml_metal_free: deallocating

real	0m1.872s
user	0m0.123s
sys	0m0.248s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4592 (496e5bf4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11b604ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11b608560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11b6089d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11b608e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11b6092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11b609720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11b609b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11b60a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11b60a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11b60a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11b60ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11b60b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11b60bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11b60c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11b60ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11b60d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11b60dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11b60e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11b60eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11b60f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11b60fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11b610160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11b610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11b611120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11b611840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11b611b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11b611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11b612230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11b612950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11b612dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11b613380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11b613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11b613d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11b613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11b614430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11b6148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11b614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11b615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11b6155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11b615a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11b615ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11b616340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11b6167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11b616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11b617090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11b617500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11b617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11b617de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11b618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11b6189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11b618e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11b6192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11b619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11b619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11b61a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11b61a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11b61abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11b61ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11b61b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11b61b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11b61bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11b61c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11b61c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11b61ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11b61cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11b61d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11b61d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11b61de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11b61e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11b61e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11b61ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11b61f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11b61f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11b61fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11b620240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11b6207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11b620da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11b621350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11b621900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11b621eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11b622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11b622a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11b622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11b623570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11b623b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11b6240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11b624680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11b624c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11b6251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11b625790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11b625d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11b6262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11b6268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11b626e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11b627400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11b6279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11b627f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11b6180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11b6286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11b628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11b628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11b629550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11b629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11b62a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11b62a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11b62ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11b62b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11b62b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11b62bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11b62c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11b62c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11b62ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11b62d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11b62d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11b62de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11b62e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11b62e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11b62ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11b62f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11b62f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11b62fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11b630190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11b630690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11b630b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11b631090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11b631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11b631a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11b631f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11b632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11b632990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11b632e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11b633390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11b633890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11b633d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11b634290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11b634790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11b634c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11b635190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11b635690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11b635b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11b636090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11b636590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11b636a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11b636f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11b637490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11b637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11b637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11b638390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11b638890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11b638d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11b639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11b639790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11b639c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11b63a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11b63a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11b63ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11b63b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11b63b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11b63ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11b63bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11b63c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11b63c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11b63ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11b63d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11b63d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11b63dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11b63e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11b63e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11b63ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11b63f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11b63f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11b63fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11b640090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11b640590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11b640a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11b640f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11b641490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11b641990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11b641e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11b642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11b642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11b642d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11b643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11b643790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11b643c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11b644190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11b644690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11b644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11b645090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11b645590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11b645a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11b645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11b646490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11b646990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11b646f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11b6474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11b647aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11b648050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11b648660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11b648c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11b649280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11b649a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11b649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11b64a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11b64a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11b64adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11b64b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11b64ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11b64bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11b64c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11b64cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11b64d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11b64d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11b64db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11b64e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11b64e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11b64eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11b64f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11b64f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11b64fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11b650090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11b6505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11b650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11b651080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11b6515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11b651b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11b652070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11b6525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11b652b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11b653060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11b6535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11b653b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11b654050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11b6545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11b654af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11b655040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11b655590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11b655ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11b656030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11b656580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11b656ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11b657020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11b657570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11b657ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11b658010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11b658560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11b658ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11b659000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11b659550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11b659aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11b659ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11b65a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11b65aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11b65afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11b65b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11b65ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11b65bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11b65c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11b65ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11b65cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11b65d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11b65da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11b65dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11b65e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11b65ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11b65efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11b65f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11b65f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11b65fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11b6602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11b660770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11b660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11b6610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11b661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11b6619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11b661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11b662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11b6627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11b662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11b663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11b6635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11b663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11b663fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11b6646c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11b664de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11b665500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11b665c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11b665ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11b6666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11b666990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11b666fa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.749.877 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.882 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10a604dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10a605240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10a6056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10a605b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10a605f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10a606400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10a606870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10a606ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10a607150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10a6075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10a607a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10a608120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10a608c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10a6093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10a609c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10a60a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10a60aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10a60b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10a60b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10a60bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10a60c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10a60cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10a60d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10a60dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10a60e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10a60e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10a60e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10a60ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10a60f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10a60f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10a60fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10a60ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10a610430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10a6106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10a610b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10a610fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10a611440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10a6118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10a611d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10a612190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10a612600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10a612a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10a612ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10a613350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10a6137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10a613c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10a6140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10a614510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10a614980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10a614df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10a615260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10a6156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10a615b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10a615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10a616420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10a616890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10a616e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10a617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10a617770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10a617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10a618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10a6184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10a618930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10a618da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10a619210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10a619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10a619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10a619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10a61a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10a61a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10a61acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10a61b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10a61b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10a61ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10a61be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10a61c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10a61c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10a61cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10a61d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10a61d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10a61d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10a61dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10a61e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10a61e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10a61ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10a61ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10a61f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10a61f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10a61fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10a620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10a620570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10a6209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10a620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10a6212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10a621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10a621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10a622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10a622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10a6228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10a622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10a6231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10a623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10a623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10a623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10a624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10a624800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10a624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10a6250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10a625550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10a6259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10a625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10a6262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10a626710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10a626b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10a626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10a627460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10a6278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10a627d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10a6281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10a628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10a628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10a628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10a629370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10a6297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10a629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10a62a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10a62a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10a62a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10a62ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10a62b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10a62b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10a62bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10a62bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10a62c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10a62c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10a62cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10a62d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10a62d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10a62da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10a62dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10a62e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10a62e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10a62ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10a62f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10a62f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10a62f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10a62fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10a630260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10a6306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10a630b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10a630fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10a631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10a631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10a631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10a632170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10a6325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10a632a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10a632ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10a633330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10a6337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10a633c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10a634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10a6344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10a634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10a634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10a635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10a635e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10a636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10a6363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10a636860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10a636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10a637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10a6375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10a637a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10a637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10a638300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10a638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10a638be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10a639050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10a6394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10a639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10a639da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10a63a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10a63a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10a63aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10a63af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10a63b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10a63b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10a63bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10a63c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10a63c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10a63ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10a63ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10a63d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10a63d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10a63dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10a63e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10a63e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10a63e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10a63ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10a63f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10a63f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10a63fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10a6400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10a640540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10a6409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10a640e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10a641290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10a6417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10a641cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10a642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10a642af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10a6430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10a643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10a643c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10a6441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10a6447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10a644d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10a645330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10a6458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10a645eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10a646470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10a646a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10a646ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10a6475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10a647b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10a648130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10a6486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10a648cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10a649270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10a649830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10a649df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10a64a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10a64a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10a64af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10a64b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10a64bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10a64c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10a64c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10a64cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10a64d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10a64d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10a64dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10a64e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10a64e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10a64ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10a64f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10a64f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10a64ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10a650570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10a650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10a6510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10a6516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10a651c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10a652230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10a6527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10a652db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10a653370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10a653930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10a653ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10a6544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10a654a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10a655030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10a6555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10a655bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10a656170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10a656730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10a656cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10a6571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10a6576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10a657bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10a6580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10a6585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10a658af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10a658ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10a6594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10a6599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10a659ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10a65a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10a65a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10a65adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10a65b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10a65b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10a65c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10a65c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10a65d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10a65d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10a65da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10a65e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10a65e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10a65eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10a65bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10a64c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10a64b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10a6483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10a645bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10a6552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10a652ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10a650830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10a64e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10a646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10a643ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10a648f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10a64a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10a64f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10a64c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10a6541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10a647e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10a6513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10a64ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10a64ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10a647870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10a6558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10a644a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10a643370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10a6455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10a655e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10a64b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10a653630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10a649530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10a64bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10a64fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10a6472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10a650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10a651970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10a646170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10a654770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10a651f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10a64da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10a6569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10a645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10a656430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10a6444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10a654d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10a64eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10a650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10a653bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10a6524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10a64a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10a641f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10a604880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10a65dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10a60bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10a65ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10a65f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10a65f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10a65f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10a65fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10a65fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10a65ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10a660280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10a660540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10a660800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10a660ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10a660d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10a661040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10a661300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10a6615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10a661880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10a661b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10a661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10a6620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10a662380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10a662640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10a662900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10a662bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10a662e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10a663140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10a663400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10a6636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10a663980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10a663c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10a663f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10a6641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10a664480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10a664740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10a664a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10a664cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10a664f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10a665240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10a665500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10a6657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10a665a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10a665d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10a666000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10a6662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10a666580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10a666840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10a666b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10a666dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10a667080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10a667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10a667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10a6678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10a667b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10a667e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10a668100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10a6683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10a668680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10a668940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10a668c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10a668ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10a669180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10a669440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10a669700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10a6699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10a669c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10a669f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10a66a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10a66a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10a66a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10a66aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10a66ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10a66afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10a66b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10a66b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10a66b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10a66bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10a66bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10a66c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10a66c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10a66c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10a66c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10a66cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10a66ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10a66d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10a66d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10a66d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10a66d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10a66dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10a66de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10a66e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10a66e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10a66e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10a66e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10a66ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10a66ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10a66f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10a66f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10a66f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10a66fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10a66fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10a66ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10a670240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10a670500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10a6707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10a670a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10a670d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10a671000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10a6712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10a671580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10a671840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10a671b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10a671dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10a672080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10a672340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10a672600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10a6728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10a672b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10a672e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10a673100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10a6733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10a673680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10a673940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10a673c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10a673ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10a674180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10a674440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10a674700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10a6749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10a674c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10a674f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10a675200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10a6754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10a675780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10a675a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10a675d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10a675fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10a676280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10a676540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10a676800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10a676ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10a676d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10a677040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10a677300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10a6775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10a677880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10a677b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10a677e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10a6780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10a678380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10a678640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10a678900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10a678bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10a678e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10a679140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10a679400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10a6796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10a679980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10a679c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10a679f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10a67a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10a67a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10a67aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10a67ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10a67b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10a67b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10a67bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10a67c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10a67c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10a67ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10a67d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10a67d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10a67dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10a67e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10a67e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10a67ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10a67f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10a67f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10a67fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10a680210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10a680760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10a680cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10a681200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10a681750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10a681ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10a6821f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10a682740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10a682c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10a6831e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10a683730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10a683c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10a6841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10a684720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10a684c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10a6851c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10a685710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10a685c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10a6861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10a686700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10a686c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10a6871a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10a6876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10a687c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10a688190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10a6886e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10a688c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10a689180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10a6896d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10a689c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10a68a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10a68a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10a68ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10a68b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10a68b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10a68bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10a68c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10a68c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10a68c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10a68cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10a68cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10a68d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10a68d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10a68dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10a68e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10a68e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10a68e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10a68edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10a68f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10a68f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10a68fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10a68ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10a690420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10a690890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10a690d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10a6919f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10a692110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10a692830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10a692af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10a692f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10a693560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10a693b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.835s
user	0m0.278s
sys	0m0.336s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4592 (496e5bf4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c807710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c8083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c808980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c808f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c8094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c809a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c80a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c80a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c80aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c80aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c80b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c80c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c80c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c80cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c80d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c80de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c80e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c80ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c80f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c80fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c810260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c810980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c811220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c811940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c811c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c812210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c812e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c8133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c813680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c813b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c813de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c814670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c814bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c814e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c815310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c8157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c815c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c8160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c816a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c816ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c817810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c817ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c8180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c8186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c819010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c819620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c819c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c81a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c81a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c81ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c81b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c81bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c81c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c81c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c81c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c81ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c81d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c81d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c81ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c81e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c81e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c81eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c81f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c81f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c81f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c81fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c8202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c820760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c820c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c8210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c8215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c821b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c822090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c8225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c822b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c823080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c8235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c823b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c824070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c8245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c824b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c825060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c8255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c825b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c826050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c8265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c826af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c827040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c827590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c828030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c828580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c828ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c829020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c818d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c829490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c829c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c82a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c82a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c82ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c82b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c82b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c82bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c82c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c82c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c82cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c82d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c82d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c82dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c82e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c82e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c82ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c82ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c82f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c82f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c82fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c8301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c830650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c830af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c830f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c831430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c8318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c831d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c832210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c8326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c832b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c832ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c833490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c833930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c833dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c834270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c834710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c834bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c835050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c8354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c835e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c8362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c836770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c836c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c8370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c837550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c8379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c837e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c838330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c8387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c838c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c839110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c8395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c839a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c839ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c83a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c83a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c83acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c83b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c83b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c83bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c83bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c83c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c83c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c83cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c83d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c83d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c83db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c83dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c83e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c83e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c83ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c83f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c83f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c83fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c840010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c8404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c840950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c840df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c841290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c841730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c841bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c842070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c842510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c8429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c8432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c843790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c843c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c8440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c844570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c844a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c844eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c845350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c8458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c845df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c846340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c846890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c846b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c847160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c847770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c847d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c848570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c848a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c848cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c8492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c8498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c84a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c84a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c84aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c84aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c84b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c84bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c84c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c84c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c84cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c84d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c84d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c84dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c84e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c84e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c84eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c84f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c84f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c84fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c8500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c850620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c850b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c8510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c851610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c851b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c8520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c852600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c8530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c8535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c853b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c854090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c8545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c854b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c855080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c8555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c855b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c856070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c8565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c856b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c857060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c8575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c857b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c858050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c8585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c858af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c859040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c859590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c859ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c85a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c85a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c85aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c85b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c85b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c85bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c85c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c85c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c85cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c85d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c85d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c85daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c85dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c85e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c85e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c85edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c85f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c85f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c85fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c860050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c8604f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c860990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c860e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c8612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c861770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c861c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c8620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c862550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c862aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c8631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c8638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c864000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c864720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c8649e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c8651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c865490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c865aa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.026 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b607cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b608340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b6087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b60b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b60b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b60bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b60bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b60c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b60c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b60cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b60d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b60d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b60e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b60eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b60f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b60fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b610160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b610fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b611770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b611e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b6125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b612cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b6133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b613b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b613dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b614090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b614500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b614970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b615250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b615780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b615bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b615eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b616320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b616790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b617070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b6174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b617950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b617dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b618230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b6186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b618b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b618f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b6193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b619860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b619cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b61a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b61a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b61aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b61ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b61b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b61b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b61bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b61c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b61c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b61cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b61cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b61d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b61d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b61dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b61e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b61e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b61e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b61ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b61f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b61f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b61fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b620000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b620470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b6208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b620d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b6211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b621630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b621aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b622380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b6227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b622c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b6230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b623540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b6239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b623e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b624700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b624b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b624fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b625450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b6258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b625d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b6261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b626610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b626a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b626ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b627360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b6277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b627c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b6280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b628520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b628990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b629270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b6296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b629b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b629fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b62a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b62a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b62ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b62b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b62b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b62ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b62bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b62c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b62c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b62cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b62d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b62d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b62d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b62dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b62e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b62e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b62eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b62efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b62f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b62f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b62fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b630160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b6305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b630a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b630eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b631320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b631790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b631c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b632070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b6324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b632950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b632dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b633230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b6336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b633b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b633f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b6343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b634860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b634cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b635140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b6355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b635a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b635e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b636300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b636770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b636be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b637050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b6374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b637930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b637da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b638210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b638680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b638af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b638f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b6393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b639840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b639cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b63a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b63a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b63aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b63b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b63b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b63bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b63c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b63c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b63c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b63cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b63d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b63d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b63dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b63df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b63e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b63e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b63ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b63f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b63f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b63f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b63fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b6402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b640720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b640b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b641000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b641470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b6418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b6421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b642630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b642aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b642f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b643380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b6437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b643c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b6440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b644540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b6449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14b644e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14b645380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b645890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b645d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14b646170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b6465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b646a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b646f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b647480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b647ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b6482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b648870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b648e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b6493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b6499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b649f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b64a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b64aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b64b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b64b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b64bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b64c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b64c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b64cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b64d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b64d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b64deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b64e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b64ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b64eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b64f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b64f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b64fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b650000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b650880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b650dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b651320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b651870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b651dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b652310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b652860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b652db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b653300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b653850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b653da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b6542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b654840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b654d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b6552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b655830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b655d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b6562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b656820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b656d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b6572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b657810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b657d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b6582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b658800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b658d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b6592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b6597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b659d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b65a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b65a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b65ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14b65b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14b65b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b65bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b65bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b65c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b65c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b65cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b65d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b65d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b65db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b65e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b65e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b65e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b65edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b65f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b65f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b65ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b660620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b660d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b661460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b661720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14b661f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b6621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b6627e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b708ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b709110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b709580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b7099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b709e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b70a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b70a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b70abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b70b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b70b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b70b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b70c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b70cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b70d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b70dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b70e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b70e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b70f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b70f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b70fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b7105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b710cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b7113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b711b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b712230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b7124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b7127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b712c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b713090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b713500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b713970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b713ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b714310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b7145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b714a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b714eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b715320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b715790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b715c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b716070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b7164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b716950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b716dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b717230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b7176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b717b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b7183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b718860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b718cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b719140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b7195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b719a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b719e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b71a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b71a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b71ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b71b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b71b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b71bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b71bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b71c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b71c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b71cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b71d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b71d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b71d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b71de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b71e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b71e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b71eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b71f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b71f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b71f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b71fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b7201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b720630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b720aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b720f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b721380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b7217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b721c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b7220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b722540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b7229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b722e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b723290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b723700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b723b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b723fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b724450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b7248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b724d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b7251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b725610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b725a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b725ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b726360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b7267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b726c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b7270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b727520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b727990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b728220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b7284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b728dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b729230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b7296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b729b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b729f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b72a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b72a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b72acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b72b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b72b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b72ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b72be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b72c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b72c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b72cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b72d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b72d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b72d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b72dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b72e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b72e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b72eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b72ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b72f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b72f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b72fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b730120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b730590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b730a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b730e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b7312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b731750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b731bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b732030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b7324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b732910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b732d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b7331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b733660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b733ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b733f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b7343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b734820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b734c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b735100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b735570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b7359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b735e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b7362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b736730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b736ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b737010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b737480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b7378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b737d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b7381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b738640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b738ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b738f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b739390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b739800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b739c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b73a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b73a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b73a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b73ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b73b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b73b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b73bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b73bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b73c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b73c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b73cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b73d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b73d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b73da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b73df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b73e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b73e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b73ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b73f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b73f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b73f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b73fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b740280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b7406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b740b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b740fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b741440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b7418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b741d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b742190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b742600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b742a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b742ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14b743350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14b7437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b743c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b7440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14b744510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b744980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b744df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b745260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b7456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b746250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b746510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b7467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b746c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b7470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b747520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b747990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b747e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b748270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b7486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b748b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b748fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b749430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b7498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b749d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b74a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b74a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b74aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b74aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b74b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b74b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b74bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b74c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b74c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b74c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b74cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b74d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b74d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b74db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b74dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b74e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b74e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b74ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b74f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b74f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b74fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b74feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b750320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b750790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b750c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b751070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b7514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b751950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b751dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b752230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b7526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b752b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b752f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b7533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b753860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b753cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b754140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b7545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b754a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b754e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b755300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b755770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14b755be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14b756050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b7564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b756930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b756da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b757210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b757680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b757af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b757f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b7583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b758840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b758cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b759120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b759590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b759a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b75a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b75b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b75b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b75be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b75c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14b75c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b75cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b75d180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.953s
user	0m0.235s
sys	0m0.183s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
