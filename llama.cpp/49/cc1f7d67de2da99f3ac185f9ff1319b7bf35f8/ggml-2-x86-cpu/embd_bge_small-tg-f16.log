+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is'
main: build = 2137 (49cc1f7d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707822339
llama_model_loader: loaded meta data with 18 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                        bert.context_length u32              = 512
llama_model_loader: - kv   3:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   4:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                      bert.attention.causal bool             = false
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  12:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 63.46 MiB (16.03 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =    63.46 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.76 MiB
llama_new_context_with_model:        CPU compute buffer size =    13.50 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 

llama_print_timings:        load time =      12.74 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       5.08 ms /     9 tokens (    0.56 ms per token,  1770.26 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =       4.64 ms /    10 tokens
-0.013164 -0.015418 0.011469 -0.017305 0.016743 0.010647 0.134299 0.041921 0.091975 -0.004363 -0.014776 -0.115600 0.007242 0.050950 0.048672 0.038940 0.019763 -0.004929 -0.071155 0.020471 0.076589 -0.058686 -0.044171 -0.013310 -0.000999 0.055371 0.008309 -0.019493 -0.017303 -0.095211 -0.001754 -0.021735 0.106772 -0.034389 -0.031189 0.000549 -0.055412 0.054285 -0.019119 0.026201 0.043847 -0.019214 -0.011519 0.033494 -0.018786 -0.035830 -0.011884 -0.003580 -0.010124 -0.022879 0.013955 -0.010415 -0.033402 0.075486 -0.083964 0.033648 0.017927 0.013933 0.067975 0.012810 0.056441 0.065254 -0.245377 0.098725 0.026454 -0.001489 -0.030105 0.014593 0.036605 0.009772 0.012681 -0.027010 0.006543 0.097448 0.040485 -0.058524 0.032067 -0.071673 -0.005434 0.017761 0.015763 -0.012738 -0.021906 -0.027585 -0.027237 -0.002892 -0.000835 -0.046046 0.062179 0.003138 -0.054321 -0.006951 -0.057790 0.060346 0.017298 0.011111 -0.050561 0.008828 0.001157 0.131424 -0.066698 0.009691 0.053189 -0.028300 -0.046926 -0.017490 -0.025175 -0.066472 -0.029062 -0.051532 -0.019966 -0.010660 -0.020282 0.025588 -0.003456 0.056441 0.059018 0.015545 0.058536 -0.060330 -0.053599 0.026977 0.046034 -0.013651 -0.062791 -0.081235 0.083658 0.051213 0.036913 0.018571 0.067281 -0.061339 -0.069295 -0.027648 -0.018005 0.016910 0.037749 0.020445 0.003394 -0.020354 -0.038377 -0.101380 -0.065022 -0.076654 -0.020437 0.037674 -0.037358 -0.024588 0.009444 0.036358 0.021555 0.031775 0.024636 0.065331 0.020740 -0.022283 -0.012627 -0.014411 -0.000295 0.056815 0.000857 0.003198 -0.049900 0.056961 -0.029012 0.037425 -0.059836 -0.005976 0.034395 0.006435 0.077844 0.029883 0.048084 0.064611 0.063978 0.028006 -0.004598 0.027534 0.054411 -0.010328 0.087399 -0.038421 -0.020006 0.059150 0.037125 0.011718 -0.042692 -0.010216 -0.016768 -0.003076 0.052343 0.103919 0.016632 0.032916 -0.030677 -0.054765 -0.002802 0.002716 -0.000881 -0.056308 0.037012 -0.015805 -0.005675 0.036409 -0.005854 -0.038127 0.008692 0.045086 0.022273 -0.044828 -0.003206 -0.042175 0.088086 0.001780 0.000149 -0.025987 0.063973 0.044929 0.018792 0.018408 0.011303 -0.053874 -0.041352 -0.256515 0.004716 0.005836 -0.022569 -0.036217 -0.021114 0.001864 -0.006141 0.044024 0.034647 0.075487 -0.008480 -0.001607 0.019282 -0.021847 -0.011970 -0.016960 -0.041230 -0.023629 0.070871 -0.030732 0.017566 -0.031282 -0.099413 0.016482 0.042641 0.195253 0.124388 -0.033226 0.059441 -0.042774 0.014779 -0.011436 -0.144748 0.034967 0.021988 -0.021334 -0.066025 -0.017777 -0.048418 0.010341 0.029592 -0.057243 -0.085372 -0.005038 -0.037683 -0.012033 0.042877 -0.004920 0.013277 0.069917 0.024094 0.024822 0.044179 0.010562 -0.029641 -0.102649 -0.019772 -0.054299 0.031513 0.024182 0.025765 0.017258 -0.010333 0.062990 0.001261 -0.051911 -0.038467 0.016594 -0.025142 -0.003484 -0.000955 -0.004456 -0.078932 0.105934 -0.001925 -0.020433 -0.028246 0.030762 -0.004286 -0.032173 -0.003059 0.019660 -0.009533 0.024548 0.054759 0.010502 -0.003701 0.064240 -0.039675 -0.028596 0.001299 0.004627 -0.099405 -0.030465 -0.063812 -0.258523 0.060175 -0.029128 0.046499 -0.009751 0.064722 0.001166 0.068118 -0.085025 -0.046574 -0.006970 0.030168 0.035245 0.041778 -0.039828 0.025267 0.020608 -0.074039 0.017687 -0.038391 -0.017581 0.044057 0.240786 -0.033064 0.007888 -0.001886 -0.036539 0.022679 -0.021721 -0.014501 -0.002230 -0.032337 0.034407 -0.010393 0.016457 -0.027998 -0.035643 -0.011797 0.012485 -0.040000 -0.019934 0.010414 -0.069980 0.009704 0.151730 -0.022292 -0.021588 -0.014447 -0.025486 -0.001277 -0.019448 0.034188 -0.032809 -0.005860 0.059637 0.025898 0.033749 -0.038242 0.007680 -0.032986 -0.034322 0.035463 0.028560 0.040086 -0.027537 

real	0m0.058s
user	0m0.067s
sys	0m0.014s
