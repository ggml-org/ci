+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is'
main: build = 2137 (49cc1f7d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1707822530
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                        bert.context_length u32              = 512
llama_model_loader: - kv   3:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   4:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                      bert.attention.causal bool             = false
llama_model_loader: - kv   9:                          general.file_type u32              = 7
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  12:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q8_0:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 34.00 MiB (8.59 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/13 layers to GPU
llm_load_tensors:        CPU buffer size =    34.00 MiB
................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     1.76 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    13.50 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 

llama_print_timings:        load time =      24.18 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       7.75 ms /     9 tokens (    0.86 ms per token,  1160.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =       7.94 ms /    10 tokens
-0.014465 -0.016323 0.012125 -0.016798 0.018036 0.010881 0.134957 0.043784 0.092635 -0.004208 -0.014901 -0.115535 0.006740 0.050229 0.047962 0.037930 0.016996 -0.003108 -0.072002 0.018970 0.079151 -0.059161 -0.045086 -0.013151 -0.001387 0.056557 0.008076 -0.019554 -0.017869 -0.093923 -0.000698 -0.021537 0.107879 -0.035518 -0.031529 0.000812 -0.054218 0.055086 -0.018419 0.026112 0.044660 -0.019706 -0.011252 0.034523 -0.020128 -0.037724 -0.010732 -0.003728 -0.011965 -0.024403 0.013516 -0.011368 -0.033305 0.074283 -0.083576 0.032350 0.018697 0.013478 0.069236 0.012362 0.056028 0.063805 -0.245510 0.098130 0.027087 -0.003864 -0.029091 0.014989 0.036749 0.007079 0.012702 -0.027952 0.006142 0.096022 0.040633 -0.057484 0.032166 -0.071157 -0.007332 0.017071 0.016182 -0.012213 -0.023035 -0.028619 -0.028321 -0.004093 -0.001004 -0.045886 0.062521 0.003964 -0.053868 -0.006410 -0.058234 0.061481 0.017173 0.011450 -0.050040 0.009129 0.002741 0.133379 -0.066238 0.010383 0.054113 -0.029649 -0.045631 -0.017423 -0.023988 -0.067963 -0.028836 -0.050102 -0.019965 -0.010003 -0.020204 0.025807 -0.004576 0.057152 0.059022 0.013751 0.057645 -0.060954 -0.052825 0.026529 0.046262 -0.013110 -0.063382 -0.080280 0.082513 0.051268 0.037597 0.017421 0.067588 -0.060431 -0.069627 -0.027138 -0.017089 0.015754 0.037615 0.018791 0.004543 -0.022224 -0.038184 -0.102185 -0.064103 -0.075058 -0.021360 0.038780 -0.036504 -0.026622 0.010173 0.035262 0.021963 0.032881 0.024983 0.066327 0.019011 -0.021815 -0.012802 -0.014720 -0.000774 0.058232 0.001747 0.002574 -0.050000 0.056918 -0.030644 0.037229 -0.060040 -0.005134 0.033487 0.007398 0.077808 0.029112 0.048016 0.062918 0.063686 0.028781 -0.005241 0.027326 0.052702 -0.010763 0.087357 -0.035809 -0.018704 0.060586 0.035831 0.010731 -0.042630 -0.011026 -0.016152 -0.003617 0.051545 0.103651 0.017795 0.033141 -0.028840 -0.052463 -0.005175 0.002576 0.000186 -0.057028 0.037752 -0.017570 -0.007017 0.036269 -0.005180 -0.037389 0.009388 0.045909 0.021969 -0.045393 -0.004829 -0.042878 0.089467 0.000872 0.001414 -0.024392 0.064711 0.045508 0.018889 0.018390 0.012818 -0.052244 -0.040931 -0.254974 0.004574 0.006584 -0.023348 -0.035370 -0.019782 0.003398 -0.005494 0.045018 0.033517 0.074862 -0.006549 0.000013 0.019759 -0.020578 -0.011892 -0.017809 -0.040198 -0.025116 0.071451 -0.031464 0.017638 -0.032191 -0.099767 0.016324 0.041427 0.194522 0.126292 -0.032977 0.059527 -0.041811 0.015260 -0.012146 -0.145096 0.034627 0.021812 -0.021438 -0.067438 -0.016315 -0.048095 0.009727 0.029303 -0.055459 -0.086657 -0.005012 -0.039645 -0.012381 0.042083 -0.004301 0.012508 0.068560 0.025122 0.026348 0.044087 0.011221 -0.028019 -0.102041 -0.019157 -0.053740 0.031253 0.022969 0.024866 0.017854 -0.009245 0.064059 0.001499 -0.050115 -0.040311 0.016739 -0.025633 -0.003237 0.000085 -0.006016 -0.080270 0.105102 -0.001871 -0.020024 -0.030283 0.030711 -0.003917 -0.032492 -0.003856 0.020982 -0.010074 0.026138 0.053199 0.009439 -0.001560 0.066541 -0.041515 -0.030040 0.001266 0.004063 -0.101035 -0.030029 -0.064782 -0.258424 0.060167 -0.027758 0.045432 -0.009045 0.065108 0.001751 0.068054 -0.084989 -0.045106 -0.007682 0.029077 0.035844 0.042269 -0.040674 0.026430 0.019796 -0.074108 0.016924 -0.038251 -0.018314 0.043918 0.239511 -0.032581 0.008360 0.000217 -0.037146 0.022353 -0.020792 -0.015122 -0.002076 -0.030863 0.032721 -0.010439 0.016325 -0.027807 -0.034116 -0.013683 0.013031 -0.039801 -0.019186 0.011084 -0.072900 0.009994 0.151243 -0.021645 -0.022163 -0.015065 -0.024383 -0.000583 -0.020878 0.033826 -0.032994 -0.006009 0.058926 0.027851 0.033757 -0.039402 0.008240 -0.034533 -0.032497 0.034876 0.027750 0.040134 -0.027977 

real	0m0.648s
user	0m0.153s
sys	0m0.513s
