+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is'
main: build = 2137 (49cc1f7d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1707822480
llama_model_loader: loaded meta data with 18 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                        bert.context_length u32              = 512
llama_model_loader: - kv   3:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   4:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                      bert.attention.causal bool             = false
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  12:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 63.46 MiB (16.03 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =    63.46 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.76 MiB
llama_new_context_with_model:        CPU compute buffer size =    13.50 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 

llama_print_timings:        load time =      40.62 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =      19.23 ms /     9 tokens (    2.14 ms per token,   468.07 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =      18.62 ms /    10 tokens
-0.013162 -0.015403 0.011415 -0.017325 0.016752 0.010680 0.134299 0.041934 0.091964 -0.004331 -0.014790 -0.115575 0.007213 0.050965 0.048700 0.038900 0.019751 -0.004889 -0.071157 0.020471 0.076562 -0.058723 -0.044200 -0.013307 -0.000974 0.055340 0.008331 -0.019500 -0.017276 -0.095217 -0.001740 -0.021691 0.106738 -0.034378 -0.031212 0.000535 -0.055389 0.054293 -0.019144 0.026163 0.043878 -0.019208 -0.011539 0.033512 -0.018793 -0.035794 -0.011859 -0.003599 -0.010143 -0.022891 0.014002 -0.010418 -0.033374 0.075473 -0.083993 0.033629 0.017897 0.013915 0.067971 0.012850 0.056427 0.065308 -0.245348 0.098748 0.026456 -0.001465 -0.030083 0.014632 0.036604 0.009742 0.012694 -0.027039 0.006536 0.097445 0.040490 -0.058547 0.032060 -0.071665 -0.005418 0.017835 0.015723 -0.012743 -0.021908 -0.027562 -0.027232 -0.002883 -0.000845 -0.046055 0.062215 0.003142 -0.054301 -0.006939 -0.057819 0.060358 0.017297 0.011093 -0.050535 0.008803 0.001163 0.131425 -0.066705 0.009697 0.053217 -0.028270 -0.046920 -0.017456 -0.025146 -0.066474 -0.029058 -0.051540 -0.019998 -0.010644 -0.020306 0.025549 -0.003497 0.056394 0.059023 0.015574 0.058514 -0.060329 -0.053595 0.026963 0.046062 -0.013655 -0.062806 -0.081275 0.083660 0.051207 0.036913 0.018575 0.067244 -0.061349 -0.069332 -0.027661 -0.018011 0.016931 0.037765 0.020450 0.003402 -0.020332 -0.038353 -0.101365 -0.064992 -0.076646 -0.020464 0.037635 -0.037333 -0.024562 0.009414 0.036370 0.021546 0.031753 0.024626 0.065321 0.020744 -0.022309 -0.012613 -0.014413 -0.000315 0.056776 0.000847 0.003185 -0.049909 0.056955 -0.029011 0.037442 -0.059856 -0.005995 0.034400 0.006418 0.077891 0.029886 0.048118 0.064571 0.063995 0.027943 -0.004576 0.027498 0.054451 -0.010323 0.087435 -0.038445 -0.020014 0.059198 0.037105 0.011738 -0.042714 -0.010213 -0.016764 -0.003079 0.052372 0.103968 0.016685 0.032891 -0.030694 -0.054798 -0.002772 0.002675 -0.000884 -0.056308 0.037003 -0.015789 -0.005680 0.036434 -0.005900 -0.038131 0.008662 0.045042 0.022300 -0.044867 -0.003212 -0.042164 0.088077 0.001816 0.000150 -0.026000 0.063975 0.044964 0.018811 0.018379 0.011311 -0.053864 -0.041318 -0.256522 0.004652 0.005812 -0.022556 -0.036219 -0.021141 0.001874 -0.006189 0.044022 0.034643 0.075502 -0.008423 -0.001609 0.019276 -0.021830 -0.011958 -0.016956 -0.041221 -0.023627 0.070879 -0.030713 0.017570 -0.031261 -0.099434 0.016499 0.042649 0.195279 0.124388 -0.033187 0.059396 -0.042784 0.014749 -0.011420 -0.144709 0.034921 0.021941 -0.021307 -0.065990 -0.017786 -0.048431 0.010334 0.029622 -0.057294 -0.085312 -0.005076 -0.037669 -0.012042 0.042907 -0.004900 0.013301 0.069888 0.024109 0.024838 0.044162 0.010574 -0.029656 -0.102687 -0.019725 -0.054297 0.031489 0.024207 0.025712 0.017266 -0.010348 0.063014 0.001296 -0.051952 -0.038469 0.016601 -0.025139 -0.003496 -0.000994 -0.004448 -0.078984 0.105928 -0.001884 -0.020408 -0.028216 0.030762 -0.004289 -0.032164 -0.003086 0.019647 -0.009521 0.024558 0.054745 0.010511 -0.003679 0.064255 -0.039652 -0.028626 0.001285 0.004636 -0.099413 -0.030466 -0.063760 -0.258537 0.060152 -0.029113 0.046473 -0.009733 0.064666 0.001156 0.068111 -0.085026 -0.046559 -0.006955 0.030173 0.035287 0.041742 -0.039825 0.025286 0.020587 -0.074011 0.017673 -0.038350 -0.017546 0.044025 0.240781 -0.033093 0.007857 -0.001886 -0.036538 0.022703 -0.021739 -0.014522 -0.002213 -0.032307 0.034407 -0.010436 0.016442 -0.027986 -0.035644 -0.011797 0.012494 -0.040011 -0.019964 0.010440 -0.069957 0.009725 0.151770 -0.022293 -0.021579 -0.014423 -0.025477 -0.001277 -0.019464 0.034159 -0.032809 -0.005869 0.059671 0.025869 0.033755 -0.038237 0.007701 -0.033006 -0.034350 0.035477 0.028516 0.040059 -0.027524 

real	0m0.123s
user	0m0.146s
sys	0m0.049s
