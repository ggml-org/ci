### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.42 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.93 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.03 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.03 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.97 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.96 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.36 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.24 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 166.81 sec*proc (29 tests)

Total Test time (real) = 166.82 sec

real	2m46.835s
user	4m42.506s
sys	0m5.984s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.84 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.39 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.39 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.02 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.22 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.37 sec*proc (29 tests)

Total Test time (real) =  48.38 sec

real	0m48.391s
user	0m54.392s
sys	0m5.229s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.135 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.471 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.478 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.481 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.481 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.482 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.483 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.484 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.485 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.486 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.486 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.487 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.488 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.491 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.491 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.492 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.493 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.493 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.494 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.495 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.206 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.208 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.209 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.209 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.210 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.210 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.211 I llama_model_loader: - type  f32:  124 tensors
0.00.027.211 I llama_model_loader: - type  f16:   73 tensors
0.00.027.212 I print_info: file format = GGUF V3 (latest)
0.00.027.213 I print_info: file type   = F16
0.00.027.214 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.655 I load: special tokens cache size = 5
0.00.033.876 I load: token to piece cache size = 0.2032 MB
0.00.033.903 I print_info: arch             = bert
0.00.033.905 I print_info: vocab_only       = 0
0.00.033.905 I print_info: n_ctx_train      = 512
0.00.033.905 I print_info: n_embd           = 384
0.00.033.905 I print_info: n_layer          = 12
0.00.033.908 I print_info: n_head           = 12
0.00.033.909 I print_info: n_head_kv        = 12
0.00.033.909 I print_info: n_rot            = 32
0.00.033.910 I print_info: n_swa            = 0
0.00.033.911 I print_info: n_embd_head_k    = 32
0.00.033.911 I print_info: n_embd_head_v    = 32
0.00.033.915 I print_info: n_gqa            = 1
0.00.033.915 I print_info: n_embd_k_gqa     = 384
0.00.033.916 I print_info: n_embd_v_gqa     = 384
0.00.033.917 I print_info: f_norm_eps       = 1.0e-12
0.00.033.918 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.918 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.918 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.920 I print_info: f_logit_scale    = 0.0e+00
0.00.033.921 I print_info: n_ff             = 1536
0.00.033.921 I print_info: n_expert         = 0
0.00.033.922 I print_info: n_expert_used    = 0
0.00.033.922 I print_info: causal attn      = 0
0.00.033.922 I print_info: pooling type     = 2
0.00.033.922 I print_info: rope type        = 2
0.00.033.923 I print_info: rope scaling     = linear
0.00.033.923 I print_info: freq_base_train  = 10000.0
0.00.033.924 I print_info: freq_scale_train = 1
0.00.033.924 I print_info: n_ctx_orig_yarn  = 512
0.00.033.930 I print_info: rope_finetuned   = unknown
0.00.033.930 I print_info: ssm_d_conv       = 0
0.00.033.931 I print_info: ssm_d_inner      = 0
0.00.033.931 I print_info: ssm_d_state      = 0
0.00.033.931 I print_info: ssm_dt_rank      = 0
0.00.033.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.931 I print_info: model type       = 33M
0.00.033.932 I print_info: model params     = 33.21 M
0.00.033.932 I print_info: general.name     = Bge Small
0.00.033.933 I print_info: vocab type       = WPM
0.00.033.933 I print_info: n_vocab          = 30522
0.00.033.933 I print_info: n_merges         = 0
0.00.033.934 I print_info: BOS token        = 101 '[CLS]'
0.00.033.934 I print_info: UNK token        = 100 '[UNK]'
0.00.033.934 I print_info: SEP token        = 102 '[SEP]'
0.00.033.935 I print_info: PAD token        = 0 '[PAD]'
0.00.033.935 I print_info: MASK token       = 103 '[MASK]'
0.00.033.935 I print_info: LF token         = 0 '[PAD]'
0.00.033.935 I print_info: max token length = 21
0.00.033.936 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.037.122 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.124 I load_tensors: offloading output layer to GPU
0.00.037.125 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.147 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.148 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.533 I llama_init_from_model: n_seq_max     = 1
0.00.037.534 I llama_init_from_model: n_ctx         = 512
0.00.037.534 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.535 I llama_init_from_model: n_batch       = 2048
0.00.037.535 I llama_init_from_model: n_ubatch      = 2048
0.00.037.535 I llama_init_from_model: flash_attn    = 0
0.00.037.536 I llama_init_from_model: freq_base     = 10000.0
0.00.037.536 I llama_init_from_model: freq_scale    = 1
0.00.037.536 I ggml_metal_init: allocating
0.00.037.548 I ggml_metal_init: found device: Apple M4
0.00.037.553 I ggml_metal_init: picking default device: Apple M4
0.00.038.188 I ggml_metal_init: using embedded metal library
0.00.042.120 I ggml_metal_init: GPU name:   Apple M4
0.00.042.122 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.122 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.123 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.123 I ggml_metal_init: simdgroup reduction   = true
0.00.042.123 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.124 I ggml_metal_init: has residency sets    = true
0.00.042.124 I ggml_metal_init: has bfloat            = true
0.00.042.124 I ggml_metal_init: use bfloat            = true
0.00.042.124 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.125 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.391 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.073 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.076 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.077 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.256 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.258 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.258 I llama_init_from_model: graph nodes  = 429
0.00.056.258 I llama_init_from_model: graph splits = 2
0.00.056.260 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.260 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.662 I 
0.00.061.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.370 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.066.356 I llama_perf_context_print:        load time =      45.81 ms
0.00.066.357 I llama_perf_context_print: prompt eval time =       3.84 ms /     9 tokens (    0.43 ms per token,  2344.36 tokens per second)
0.00.066.358 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.066.359 I llama_perf_context_print:       total time =       4.69 ms /    10 tokens
0.00.066.513 I ggml_metal_free: deallocating

real	0m0.275s
user	0m0.050s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.414 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.120 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.124 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.125 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.127 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.128 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.128 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.129 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.129 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.130 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.130 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.130 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.133 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.133 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.133 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.134 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.134 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.134 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.490 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.137 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.138 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.139 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.139 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.139 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.139 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.140 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.140 I llama_model_loader: - type  f32:  124 tensors
0.00.015.140 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.141 I print_info: file format = GGUF V3 (latest)
0.00.015.141 I print_info: file type   = Q8_0
0.00.015.142 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.649 I load: special tokens cache size = 5
0.00.018.843 I load: token to piece cache size = 0.2032 MB
0.00.018.851 I print_info: arch             = bert
0.00.018.851 I print_info: vocab_only       = 0
0.00.018.852 I print_info: n_ctx_train      = 512
0.00.018.852 I print_info: n_embd           = 384
0.00.018.852 I print_info: n_layer          = 12
0.00.018.856 I print_info: n_head           = 12
0.00.018.856 I print_info: n_head_kv        = 12
0.00.018.857 I print_info: n_rot            = 32
0.00.018.857 I print_info: n_swa            = 0
0.00.018.858 I print_info: n_embd_head_k    = 32
0.00.018.859 I print_info: n_embd_head_v    = 32
0.00.018.859 I print_info: n_gqa            = 1
0.00.018.862 I print_info: n_embd_k_gqa     = 384
0.00.018.862 I print_info: n_embd_v_gqa     = 384
0.00.018.863 I print_info: f_norm_eps       = 1.0e-12
0.00.018.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.865 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.865 I print_info: f_logit_scale    = 0.0e+00
0.00.018.865 I print_info: n_ff             = 1536
0.00.018.865 I print_info: n_expert         = 0
0.00.018.865 I print_info: n_expert_used    = 0
0.00.018.866 I print_info: causal attn      = 0
0.00.018.866 I print_info: pooling type     = 2
0.00.018.867 I print_info: rope type        = 2
0.00.018.867 I print_info: rope scaling     = linear
0.00.018.867 I print_info: freq_base_train  = 10000.0
0.00.018.867 I print_info: freq_scale_train = 1
0.00.018.868 I print_info: n_ctx_orig_yarn  = 512
0.00.018.868 I print_info: rope_finetuned   = unknown
0.00.018.868 I print_info: ssm_d_conv       = 0
0.00.018.868 I print_info: ssm_d_inner      = 0
0.00.018.868 I print_info: ssm_d_state      = 0
0.00.018.871 I print_info: ssm_dt_rank      = 0
0.00.018.871 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.871 I print_info: model type       = 33M
0.00.018.872 I print_info: model params     = 33.21 M
0.00.018.872 I print_info: general.name     = Bge Small
0.00.018.872 I print_info: vocab type       = WPM
0.00.018.872 I print_info: n_vocab          = 30522
0.00.018.872 I print_info: n_merges         = 0
0.00.018.873 I print_info: BOS token        = 101 '[CLS]'
0.00.018.874 I print_info: UNK token        = 100 '[UNK]'
0.00.018.874 I print_info: SEP token        = 102 '[SEP]'
0.00.018.875 I print_info: PAD token        = 0 '[PAD]'
0.00.018.875 I print_info: MASK token       = 103 '[MASK]'
0.00.018.875 I print_info: LF token         = 0 '[PAD]'
0.00.018.875 I print_info: max token length = 21
0.00.018.876 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.844 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.845 I load_tensors: offloading output layer to GPU
0.00.020.846 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.854 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.854 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.111 I llama_init_from_model: n_seq_max     = 1
0.00.021.112 I llama_init_from_model: n_ctx         = 512
0.00.021.112 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.112 I llama_init_from_model: n_batch       = 2048
0.00.021.112 I llama_init_from_model: n_ubatch      = 2048
0.00.021.113 I llama_init_from_model: flash_attn    = 0
0.00.021.113 I llama_init_from_model: freq_base     = 10000.0
0.00.021.113 I llama_init_from_model: freq_scale    = 1
0.00.021.114 I ggml_metal_init: allocating
0.00.021.131 I ggml_metal_init: found device: Apple M4
0.00.021.134 I ggml_metal_init: picking default device: Apple M4
0.00.021.619 I ggml_metal_init: using embedded metal library
0.00.024.219 I ggml_metal_init: GPU name:   Apple M4
0.00.024.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.223 I ggml_metal_init: simdgroup reduction   = true
0.00.024.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.223 I ggml_metal_init: has residency sets    = true
0.00.024.223 I ggml_metal_init: has bfloat            = true
0.00.024.223 I ggml_metal_init: use bfloat            = true
0.00.024.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.841 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.447 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.449 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.451 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.446 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.447 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.447 I llama_init_from_model: graph nodes  = 429
0.00.036.448 I llama_init_from_model: graph splits = 2
0.00.036.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.937 I 
0.00.039.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.504 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.664 I llama_perf_context_print:        load time =      30.52 ms
0.00.043.665 I llama_perf_context_print: prompt eval time =       3.02 ms /     9 tokens (    0.34 ms per token,  2980.13 tokens per second)
0.00.043.666 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.666 I llama_perf_context_print:       total time =       3.73 ms /    10 tokens
0.00.043.847 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.364 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.816 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.823 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.824 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.827 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.828 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.828 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.830 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.831 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.831 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.832 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.832 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.835 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.836 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.836 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.838 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.141 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.724 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.724 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.724 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.725 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.725 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.726 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.726 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.726 I llama_model_loader: - type  f32:   40 tensors
0.00.049.727 I llama_model_loader: - type  f16:   30 tensors
0.00.049.727 I print_info: file format = GGUF V3 (latest)
0.00.049.728 I print_info: file type   = F16
0.00.049.729 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.073 W load: empty token at index 5
0.00.059.155 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.603 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.060.637 I load: special tokens cache size = 5
0.00.322.875 I load: token to piece cache size = 1.5060 MB
0.00.322.906 I print_info: arch             = jina-bert-v2
0.00.322.907 I print_info: vocab_only       = 0
0.00.322.907 I print_info: n_ctx_train      = 8192
0.00.322.907 I print_info: n_embd           = 384
0.00.322.907 I print_info: n_layer          = 4
0.00.322.912 I print_info: n_head           = 12
0.00.322.913 I print_info: n_head_kv        = 12
0.00.322.913 I print_info: n_rot            = 32
0.00.322.913 I print_info: n_swa            = 0
0.00.322.915 I print_info: n_embd_head_k    = 32
0.00.322.915 I print_info: n_embd_head_v    = 32
0.00.322.915 I print_info: n_gqa            = 1
0.00.322.917 I print_info: n_embd_k_gqa     = 384
0.00.322.919 I print_info: n_embd_v_gqa     = 384
0.00.322.919 I print_info: f_norm_eps       = 1.0e-12
0.00.322.920 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.322.920 I print_info: f_clamp_kqv      = 0.0e+00
0.00.322.920 I print_info: f_max_alibi_bias = 8.0e+00
0.00.322.920 I print_info: f_logit_scale    = 0.0e+00
0.00.322.921 I print_info: n_ff             = 1536
0.00.322.922 I print_info: n_expert         = 0
0.00.322.922 I print_info: n_expert_used    = 0
0.00.322.922 I print_info: causal attn      = 0
0.00.322.922 I print_info: pooling type     = -1
0.00.322.923 I print_info: rope type        = -1
0.00.322.923 I print_info: rope scaling     = linear
0.00.322.923 I print_info: freq_base_train  = 10000.0
0.00.322.923 I print_info: freq_scale_train = 1
0.00.322.923 I print_info: n_ctx_orig_yarn  = 8192
0.00.322.924 I print_info: rope_finetuned   = unknown
0.00.322.924 I print_info: ssm_d_conv       = 0
0.00.322.924 I print_info: ssm_d_inner      = 0
0.00.322.924 I print_info: ssm_d_state      = 0
0.00.322.924 I print_info: ssm_dt_rank      = 0
0.00.322.924 I print_info: ssm_dt_b_c_rms   = 0
0.00.322.924 I print_info: model type       = 33M
0.00.322.925 I print_info: model params     = 32.90 M
0.00.322.925 I print_info: general.name     = Jina Bert Implementation
0.00.322.925 I print_info: vocab type       = BPE
0.00.322.926 I print_info: n_vocab          = 61056
0.00.322.926 I print_info: n_merges         = 39382
0.00.322.926 I print_info: BOS token        = 0 '<s>'
0.00.322.926 I print_info: EOS token        = 2 '</s>'
0.00.322.926 I print_info: UNK token        = 3 '<unk>'
0.00.322.926 I print_info: SEP token        = 2 '</s>'
0.00.322.927 I print_info: PAD token        = 1 '<pad>'
0.00.322.927 I print_info: MASK token       = 4 '<mask>'
0.00.322.927 I print_info: EOG token        = 2 '</s>'
0.00.322.927 I print_info: max token length = 45
0.00.322.928 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.202 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.203 I load_tensors: offloading output layer to GPU
0.00.324.203 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.223 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.225 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.513 I llama_init_from_model: n_seq_max     = 1
0.00.324.514 I llama_init_from_model: n_ctx         = 8192
0.00.324.514 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.514 I llama_init_from_model: n_batch       = 2048
0.00.324.515 I llama_init_from_model: n_ubatch      = 2048
0.00.324.515 I llama_init_from_model: flash_attn    = 0
0.00.324.516 I llama_init_from_model: freq_base     = 10000.0
0.00.324.516 I llama_init_from_model: freq_scale    = 1
0.00.324.516 I ggml_metal_init: allocating
0.00.324.526 I ggml_metal_init: found device: Apple M4
0.00.324.531 I ggml_metal_init: picking default device: Apple M4
0.00.325.041 I ggml_metal_init: using embedded metal library
0.00.327.685 I ggml_metal_init: GPU name:   Apple M4
0.00.327.687 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.327.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.327.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.327.688 I ggml_metal_init: simdgroup reduction   = true
0.00.327.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.327.688 I ggml_metal_init: has residency sets    = true
0.00.327.688 I ggml_metal_init: has bfloat            = true
0.00.327.688 I ggml_metal_init: use bfloat            = true
0.00.327.689 I ggml_metal_init: hasUnifiedMemory      = true
0.00.327.690 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.612 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.340.898 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.340.902 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.340.904 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.796 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.798 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.799 I llama_init_from_model: graph nodes  = 154
0.00.347.799 I llama_init_from_model: graph splits = 2
0.00.347.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.355.302 I 
0.00.355.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.355.572 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.355.573 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.355.576 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.355.576 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.355.581 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.355.581 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.356.100 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.359.683 I llama_perf_context_print:        load time =     332.40 ms
0.00.359.684 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17337.81 tokens per second)
0.00.359.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.359.686 I llama_perf_context_print:       total time =       4.38 ms /    63 tokens
0.00.359.965 I ggml_metal_free: deallocating

real	0m1.158s
user	0m0.329s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.186 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.387 I main: llama backend init
0.00.000.401 I main: load the model and apply lora adapter, if any
0.00.099.371 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.112.518 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.112.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.112.544 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.112.545 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.112.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.112.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.112.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.112.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.112.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.112.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.112.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.112.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.112.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.112.555 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.112.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.112.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.112.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.119.728 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.121.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.128.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.128.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.128.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.128.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.128.978 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.128.979 I llama_model_loader: - type  f32:  194 tensors
0.00.128.980 I llama_model_loader: - type  f16:   98 tensors
0.00.128.982 I print_info: file format = GGUF V3 (latest)
0.00.128.985 I print_info: file type   = all F32 (guessed)
0.00.128.986 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.147.379 I load: special tokens cache size = 25
0.00.158.597 I load: token to piece cache size = 0.2984 MB
0.00.158.627 I print_info: arch             = gptneox
0.00.158.629 I print_info: vocab_only       = 0
0.00.158.629 I print_info: n_ctx_train      = 2048
0.00.158.629 I print_info: n_embd           = 2048
0.00.158.629 I print_info: n_layer          = 24
0.00.158.636 I print_info: n_head           = 16
0.00.158.637 I print_info: n_head_kv        = 16
0.00.158.637 I print_info: n_rot            = 32
0.00.158.637 I print_info: n_swa            = 0
0.00.158.638 I print_info: n_embd_head_k    = 128
0.00.158.638 I print_info: n_embd_head_v    = 128
0.00.158.639 I print_info: n_gqa            = 1
0.00.158.640 I print_info: n_embd_k_gqa     = 2048
0.00.158.641 I print_info: n_embd_v_gqa     = 2048
0.00.158.642 I print_info: f_norm_eps       = 1.0e-05
0.00.158.642 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.158.642 I print_info: f_clamp_kqv      = 0.0e+00
0.00.158.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.158.642 I print_info: f_logit_scale    = 0.0e+00
0.00.158.644 I print_info: n_ff             = 8192
0.00.158.644 I print_info: n_expert         = 0
0.00.158.644 I print_info: n_expert_used    = 0
0.00.158.644 I print_info: causal attn      = 1
0.00.158.644 I print_info: pooling type     = 0
0.00.158.645 I print_info: rope type        = 2
0.00.158.647 I print_info: rope scaling     = linear
0.00.158.649 I print_info: freq_base_train  = 10000.0
0.00.158.649 I print_info: freq_scale_train = 1
0.00.158.649 I print_info: n_ctx_orig_yarn  = 2048
0.00.158.650 I print_info: rope_finetuned   = unknown
0.00.158.650 I print_info: ssm_d_conv       = 0
0.00.158.650 I print_info: ssm_d_inner      = 0
0.00.158.650 I print_info: ssm_d_state      = 0
0.00.158.652 I print_info: ssm_dt_rank      = 0
0.00.158.652 I print_info: ssm_dt_b_c_rms   = 0
0.00.158.652 I print_info: model type       = 1.4B
0.00.158.653 I print_info: model params     = 1.41 B
0.00.158.653 I print_info: general.name     = 1.4B
0.00.158.654 I print_info: vocab type       = BPE
0.00.158.654 I print_info: n_vocab          = 50304
0.00.158.654 I print_info: n_merges         = 50009
0.00.158.655 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.158.655 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.158.655 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.158.655 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.158.656 I print_info: LF token         = 187 ''
0.00.158.656 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.158.656 I print_info: max token length = 1024
0.00.158.657 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.227.281 I load_tensors: offloading 24 repeating layers to GPU
0.00.227.285 I load_tensors: offloading output layer to GPU
0.00.227.285 I load_tensors: offloaded 25/25 layers to GPU
0.00.227.310 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.227.311 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.227.902 I llama_init_from_model: n_seq_max     = 1
0.00.227.903 I llama_init_from_model: n_ctx         = 2048
0.00.227.903 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.227.903 I llama_init_from_model: n_batch       = 2048
0.00.227.904 I llama_init_from_model: n_ubatch      = 512
0.00.227.904 I llama_init_from_model: flash_attn    = 0
0.00.227.904 I llama_init_from_model: freq_base     = 10000.0
0.00.227.905 I llama_init_from_model: freq_scale    = 1
0.00.227.905 I ggml_metal_init: allocating
0.00.227.946 I ggml_metal_init: found device: Apple M4
0.00.227.952 I ggml_metal_init: picking default device: Apple M4
0.00.228.493 I ggml_metal_init: using embedded metal library
0.00.243.494 I ggml_metal_init: GPU name:   Apple M4
0.00.243.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.243.496 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.243.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.243.497 I ggml_metal_init: simdgroup reduction   = true
0.00.243.497 I ggml_metal_init: simdgroup matrix mul. = true
0.00.243.497 I ggml_metal_init: has residency sets    = true
0.00.243.497 I ggml_metal_init: has bfloat            = true
0.00.243.497 I ggml_metal_init: use bfloat            = true
0.00.243.498 I ggml_metal_init: hasUnifiedMemory      = true
0.00.243.498 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.306.565 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.337.286 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.337.294 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.337.316 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.341.321 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.341.323 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.341.324 I llama_init_from_model: graph nodes  = 967
0.00.341.324 I llama_init_from_model: graph splits = 2
0.00.341.329 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.341.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.341.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.407.850 I main: llama threadpool init, n_threads = 4
0.00.407.906 I 
0.00.407.934 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.407.935 I 
0.00.408.108 I sampler seed: 1234
0.00.408.113 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.408.147 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.408.149 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.408.149 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.240.522 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.02.240.522 I llama_perf_context_print:        load time =     307.53 ms
0.02.240.524 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.09 tokens per second)
0.02.240.525 I llama_perf_context_print:        eval time =    1785.81 ms /    63 runs   (   28.35 ms per token,    35.28 tokens per second)
0.02.240.525 I llama_perf_context_print:       total time =    1833.61 ms /    70 tokens
0.02.240.774 I ggml_metal_free: deallocating

real	0m2.608s
user	0m0.139s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.674 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.970 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.986 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.987 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.988 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.988 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.989 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.992 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.004 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.005 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.006 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.923 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.159 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.282 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.283 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.283 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.284 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.285 I llama_model_loader: - type  f32:  194 tensors
0.00.056.285 I llama_model_loader: - type  f16:   98 tensors
0.00.056.286 I print_info: file format = GGUF V3 (latest)
0.00.056.288 I print_info: file type   = all F32 (guessed)
0.00.056.290 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.725 I load: special tokens cache size = 25
0.00.080.180 I load: token to piece cache size = 0.2984 MB
0.00.080.196 I print_info: arch             = gptneox
0.00.080.197 I print_info: vocab_only       = 0
0.00.080.197 I print_info: n_ctx_train      = 2048
0.00.080.198 I print_info: n_embd           = 2048
0.00.080.198 I print_info: n_layer          = 24
0.00.080.202 I print_info: n_head           = 16
0.00.080.203 I print_info: n_head_kv        = 16
0.00.080.203 I print_info: n_rot            = 32
0.00.080.204 I print_info: n_swa            = 0
0.00.080.205 I print_info: n_embd_head_k    = 128
0.00.080.206 I print_info: n_embd_head_v    = 128
0.00.080.207 I print_info: n_gqa            = 1
0.00.080.208 I print_info: n_embd_k_gqa     = 2048
0.00.080.208 I print_info: n_embd_v_gqa     = 2048
0.00.080.209 I print_info: f_norm_eps       = 1.0e-05
0.00.080.210 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.080.210 I print_info: f_clamp_kqv      = 0.0e+00
0.00.080.210 I print_info: f_max_alibi_bias = 0.0e+00
0.00.080.210 I print_info: f_logit_scale    = 0.0e+00
0.00.080.211 I print_info: n_ff             = 8192
0.00.080.211 I print_info: n_expert         = 0
0.00.080.212 I print_info: n_expert_used    = 0
0.00.080.213 I print_info: causal attn      = 1
0.00.080.215 I print_info: pooling type     = 0
0.00.080.215 I print_info: rope type        = 2
0.00.080.215 I print_info: rope scaling     = linear
0.00.080.216 I print_info: freq_base_train  = 10000.0
0.00.080.216 I print_info: freq_scale_train = 1
0.00.080.216 I print_info: n_ctx_orig_yarn  = 2048
0.00.080.216 I print_info: rope_finetuned   = unknown
0.00.080.217 I print_info: ssm_d_conv       = 0
0.00.080.217 I print_info: ssm_d_inner      = 0
0.00.080.217 I print_info: ssm_d_state      = 0
0.00.080.217 I print_info: ssm_dt_rank      = 0
0.00.080.217 I print_info: ssm_dt_b_c_rms   = 0
0.00.080.218 I print_info: model type       = 1.4B
0.00.080.219 I print_info: model params     = 1.41 B
0.00.080.219 I print_info: general.name     = 1.4B
0.00.080.220 I print_info: vocab type       = BPE
0.00.080.220 I print_info: n_vocab          = 50304
0.00.080.220 I print_info: n_merges         = 50009
0.00.080.220 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.080.221 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.080.221 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.080.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.080.222 I print_info: LF token         = 187 ''
0.00.080.223 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.080.223 I print_info: max token length = 1024
0.00.080.223 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.420.949 I load_tensors: offloading 24 repeating layers to GPU
0.01.420.953 I load_tensors: offloading output layer to GPU
0.01.420.953 I load_tensors: offloaded 25/25 layers to GPU
0.01.420.978 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.420.980 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.422.289 I llama_init_from_model: n_seq_max     = 1
0.01.422.290 I llama_init_from_model: n_ctx         = 128
0.01.422.290 I llama_init_from_model: n_ctx_per_seq = 128
0.01.422.290 I llama_init_from_model: n_batch       = 128
0.01.422.291 I llama_init_from_model: n_ubatch      = 128
0.01.422.291 I llama_init_from_model: flash_attn    = 0
0.01.422.291 I llama_init_from_model: freq_base     = 10000.0
0.01.422.292 I llama_init_from_model: freq_scale    = 1
0.01.422.292 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.422.293 I ggml_metal_init: allocating
0.01.422.353 I ggml_metal_init: found device: Apple M4
0.01.422.359 I ggml_metal_init: picking default device: Apple M4
0.01.423.367 I ggml_metal_init: using embedded metal library
0.01.427.208 I ggml_metal_init: GPU name:   Apple M4
0.01.427.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.427.211 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.427.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.427.212 I ggml_metal_init: simdgroup reduction   = true
0.01.427.212 I ggml_metal_init: simdgroup matrix mul. = true
0.01.427.212 I ggml_metal_init: has residency sets    = true
0.01.427.212 I ggml_metal_init: has bfloat            = true
0.01.427.212 I ggml_metal_init: use bfloat            = true
0.01.427.213 I ggml_metal_init: hasUnifiedMemory      = true
0.01.427.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.438.378 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.440.096 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.440.099 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.440.122 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.441.797 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.441.798 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.441.798 I llama_init_from_model: graph nodes  = 967
0.01.441.799 I llama_init_from_model: graph splits = 2
0.01.441.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.441.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.475.961 I 
0.01.476.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.476.005 I perplexity: tokenizing the input ..
0.01.480.990 I perplexity: tokenization took 4.984 ms
0.01.480.997 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.599.519 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.600.782 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.600.818 I llama_perf_context_print:        load time =    1451.22 ms
0.01.600.819 I llama_perf_context_print: prompt eval time =     118.25 ms /   128 tokens (    0.92 ms per token,  1082.47 tokens per second)
0.01.600.820 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.600.821 I llama_perf_context_print:       total time =     124.86 ms /   129 tokens
0.01.601.171 I ggml_metal_free: deallocating

real	0m1.815s
user	0m0.101s
sys	0m0.257s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.013.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.328 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.335 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.338 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.339 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.339 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.340 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.341 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.341 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.342 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.342 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.345 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.345 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.345 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.978 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.979 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.980 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.980 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.980 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.981 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.981 I llama_model_loader: - type  f32:  194 tensors
0.00.039.982 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.983 I print_info: file format = GGUF V3 (latest)
0.00.039.983 I print_info: file type   = Q8_0
0.00.039.985 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.323 I load: special tokens cache size = 25
0.00.057.203 I load: token to piece cache size = 0.2984 MB
0.00.057.219 I print_info: arch             = gptneox
0.00.057.221 I print_info: vocab_only       = 0
0.00.057.221 I print_info: n_ctx_train      = 2048
0.00.057.221 I print_info: n_embd           = 2048
0.00.057.221 I print_info: n_layer          = 24
0.00.057.226 I print_info: n_head           = 16
0.00.057.227 I print_info: n_head_kv        = 16
0.00.057.227 I print_info: n_rot            = 32
0.00.057.227 I print_info: n_swa            = 0
0.00.057.227 I print_info: n_embd_head_k    = 128
0.00.057.228 I print_info: n_embd_head_v    = 128
0.00.057.229 I print_info: n_gqa            = 1
0.00.057.229 I print_info: n_embd_k_gqa     = 2048
0.00.057.230 I print_info: n_embd_v_gqa     = 2048
0.00.057.231 I print_info: f_norm_eps       = 1.0e-05
0.00.057.231 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.232 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.232 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.232 I print_info: f_logit_scale    = 0.0e+00
0.00.057.233 I print_info: n_ff             = 8192
0.00.057.233 I print_info: n_expert         = 0
0.00.057.233 I print_info: n_expert_used    = 0
0.00.057.233 I print_info: causal attn      = 1
0.00.057.233 I print_info: pooling type     = 0
0.00.057.233 I print_info: rope type        = 2
0.00.057.234 I print_info: rope scaling     = linear
0.00.057.234 I print_info: freq_base_train  = 10000.0
0.00.057.235 I print_info: freq_scale_train = 1
0.00.057.235 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.235 I print_info: rope_finetuned   = unknown
0.00.057.235 I print_info: ssm_d_conv       = 0
0.00.057.235 I print_info: ssm_d_inner      = 0
0.00.057.235 I print_info: ssm_d_state      = 0
0.00.057.236 I print_info: ssm_dt_rank      = 0
0.00.057.236 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.236 I print_info: model type       = 1.4B
0.00.057.236 I print_info: model params     = 1.41 B
0.00.057.236 I print_info: general.name     = 1.4B
0.00.057.237 I print_info: vocab type       = BPE
0.00.057.237 I print_info: n_vocab          = 50304
0.00.057.237 I print_info: n_merges         = 50009
0.00.057.238 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.238 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.238 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.238 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.239 I print_info: LF token         = 187 ''
0.00.057.239 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.239 I print_info: max token length = 1024
0.00.057.241 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.273.916 I load_tensors: offloading 24 repeating layers to GPU
0.01.273.920 I load_tensors: offloading output layer to GPU
0.01.273.922 I load_tensors: offloaded 25/25 layers to GPU
0.01.273.945 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.273.946 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.275.002 I llama_init_from_model: n_seq_max     = 1
0.01.275.004 I llama_init_from_model: n_ctx         = 2048
0.01.275.004 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.275.004 I llama_init_from_model: n_batch       = 2048
0.01.275.005 I llama_init_from_model: n_ubatch      = 512
0.01.275.005 I llama_init_from_model: flash_attn    = 0
0.01.275.006 I llama_init_from_model: freq_base     = 10000.0
0.01.275.006 I llama_init_from_model: freq_scale    = 1
0.01.275.007 I ggml_metal_init: allocating
0.01.275.016 I ggml_metal_init: found device: Apple M4
0.01.275.023 I ggml_metal_init: picking default device: Apple M4
0.01.276.147 I ggml_metal_init: using embedded metal library
0.01.282.109 I ggml_metal_init: GPU name:   Apple M4
0.01.282.112 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.282.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.282.114 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.282.115 I ggml_metal_init: simdgroup reduction   = true
0.01.282.115 I ggml_metal_init: simdgroup matrix mul. = true
0.01.282.115 I ggml_metal_init: has residency sets    = true
0.01.282.115 I ggml_metal_init: has bfloat            = true
0.01.282.116 I ggml_metal_init: use bfloat            = true
0.01.282.116 I ggml_metal_init: hasUnifiedMemory      = true
0.01.282.121 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.298.797 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.353.978 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.353.984 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.354.018 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.358.045 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.358.047 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.358.047 I llama_init_from_model: graph nodes  = 967
0.01.358.048 I llama_init_from_model: graph splits = 2
0.01.358.053 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.358.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.358.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.414.380 I main: llama threadpool init, n_threads = 4
0.01.414.431 I 
0.01.414.453 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.414.453 I 
0.01.414.628 I sampler seed: 1234
0.01.414.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.414.647 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.414.649 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.414.649 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.510.756 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.02.510.757 I llama_perf_context_print:        load time =    1399.76 ms
0.02.510.758 I llama_perf_context_print: prompt eval time =      48.97 ms /     7 tokens (    7.00 ms per token,   142.93 tokens per second)
0.02.510.759 I llama_perf_context_print:        eval time =    1044.40 ms /    63 runs   (   16.58 ms per token,    60.32 tokens per second)
0.02.510.760 I llama_perf_context_print:       total time =    1097.09 ms /    70 tokens
0.02.510.997 I ggml_metal_free: deallocating

real	0m2.531s
user	0m0.112s
sys	0m0.272s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.256 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.563 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.566 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.567 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.569 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.573 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.575 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.468 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.319 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.320 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.321 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.322 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.323 I llama_model_loader: - type  f32:  194 tensors
0.00.025.323 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.324 I print_info: file format = GGUF V3 (latest)
0.00.025.324 I print_info: file type   = Q8_0
0.00.025.325 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.369 I load: special tokens cache size = 25
0.00.039.773 I load: token to piece cache size = 0.2984 MB
0.00.039.790 I print_info: arch             = gptneox
0.00.039.791 I print_info: vocab_only       = 0
0.00.039.791 I print_info: n_ctx_train      = 2048
0.00.039.791 I print_info: n_embd           = 2048
0.00.039.791 I print_info: n_layer          = 24
0.00.039.796 I print_info: n_head           = 16
0.00.039.796 I print_info: n_head_kv        = 16
0.00.039.796 I print_info: n_rot            = 32
0.00.039.797 I print_info: n_swa            = 0
0.00.039.797 I print_info: n_embd_head_k    = 128
0.00.039.797 I print_info: n_embd_head_v    = 128
0.00.039.797 I print_info: n_gqa            = 1
0.00.039.798 I print_info: n_embd_k_gqa     = 2048
0.00.039.799 I print_info: n_embd_v_gqa     = 2048
0.00.039.799 I print_info: f_norm_eps       = 1.0e-05
0.00.039.799 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.800 I print_info: f_logit_scale    = 0.0e+00
0.00.039.801 I print_info: n_ff             = 8192
0.00.039.802 I print_info: n_expert         = 0
0.00.039.802 I print_info: n_expert_used    = 0
0.00.039.804 I print_info: causal attn      = 1
0.00.039.804 I print_info: pooling type     = 0
0.00.039.804 I print_info: rope type        = 2
0.00.039.804 I print_info: rope scaling     = linear
0.00.039.805 I print_info: freq_base_train  = 10000.0
0.00.039.805 I print_info: freq_scale_train = 1
0.00.039.805 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.805 I print_info: rope_finetuned   = unknown
0.00.039.806 I print_info: ssm_d_conv       = 0
0.00.039.806 I print_info: ssm_d_inner      = 0
0.00.039.806 I print_info: ssm_d_state      = 0
0.00.039.807 I print_info: ssm_dt_rank      = 0
0.00.039.807 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.808 I print_info: model type       = 1.4B
0.00.039.808 I print_info: model params     = 1.41 B
0.00.039.808 I print_info: general.name     = 1.4B
0.00.039.809 I print_info: vocab type       = BPE
0.00.039.809 I print_info: n_vocab          = 50304
0.00.039.809 I print_info: n_merges         = 50009
0.00.039.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: LF token         = 187 ''
0.00.039.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: max token length = 1024
0.00.039.811 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.900.034 I load_tensors: offloading 24 repeating layers to GPU
0.00.900.041 I load_tensors: offloading output layer to GPU
0.00.900.042 I load_tensors: offloaded 25/25 layers to GPU
0.00.900.069 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.900.072 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.901.425 I llama_init_from_model: n_seq_max     = 1
0.00.901.427 I llama_init_from_model: n_ctx         = 128
0.00.901.428 I llama_init_from_model: n_ctx_per_seq = 128
0.00.901.428 I llama_init_from_model: n_batch       = 128
0.00.901.429 I llama_init_from_model: n_ubatch      = 128
0.00.901.429 I llama_init_from_model: flash_attn    = 0
0.00.901.430 I llama_init_from_model: freq_base     = 10000.0
0.00.901.431 I llama_init_from_model: freq_scale    = 1
0.00.901.432 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.901.433 I ggml_metal_init: allocating
0.00.901.538 I ggml_metal_init: found device: Apple M4
0.00.901.550 I ggml_metal_init: picking default device: Apple M4
0.00.902.844 I ggml_metal_init: using embedded metal library
0.00.908.382 I ggml_metal_init: GPU name:   Apple M4
0.00.908.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.908.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.908.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.908.387 I ggml_metal_init: simdgroup reduction   = true
0.00.908.388 I ggml_metal_init: simdgroup matrix mul. = true
0.00.908.388 I ggml_metal_init: has residency sets    = true
0.00.908.388 I ggml_metal_init: has bfloat            = true
0.00.908.388 I ggml_metal_init: use bfloat            = true
0.00.908.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.908.391 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.924.161 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.927.414 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.927.417 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.927.445 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.930.382 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.930.384 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.930.384 I llama_init_from_model: graph nodes  = 967
0.00.930.385 I llama_init_from_model: graph splits = 2
0.00.930.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.930.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.960.459 I 
0.00.960.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.960.554 I perplexity: tokenizing the input ..
0.00.967.849 I perplexity: tokenization took 7.29 ms
0.00.967.858 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.107.475 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.108.823 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.108.849 I llama_perf_context_print:        load time =     951.19 ms
0.01.108.850 I llama_perf_context_print: prompt eval time =     138.66 ms /   128 tokens (    1.08 ms per token,   923.13 tokens per second)
0.01.108.850 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.108.851 I llama_perf_context_print:       total time =     148.39 ms /   129 tokens
0.01.109.245 I ggml_metal_free: deallocating

real	0m1.125s
user	0m0.078s
sys	0m0.188s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.016.164 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.175 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.180 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.192 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.811 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.812 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.812 I llama_model_loader: - type  f32:  194 tensors
0.00.044.812 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.813 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.813 I print_info: file format = GGUF V3 (latest)
0.00.044.814 I print_info: file type   = Q4_0
0.00.044.815 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.056.223 I load: special tokens cache size = 25
0.00.066.381 I load: token to piece cache size = 0.2984 MB
0.00.066.398 I print_info: arch             = gptneox
0.00.066.399 I print_info: vocab_only       = 0
0.00.066.399 I print_info: n_ctx_train      = 2048
0.00.066.400 I print_info: n_embd           = 2048
0.00.066.400 I print_info: n_layer          = 24
0.00.066.407 I print_info: n_head           = 16
0.00.066.409 I print_info: n_head_kv        = 16
0.00.066.409 I print_info: n_rot            = 32
0.00.066.410 I print_info: n_swa            = 0
0.00.066.410 I print_info: n_embd_head_k    = 128
0.00.066.410 I print_info: n_embd_head_v    = 128
0.00.066.412 I print_info: n_gqa            = 1
0.00.066.414 I print_info: n_embd_k_gqa     = 2048
0.00.066.416 I print_info: n_embd_v_gqa     = 2048
0.00.066.418 I print_info: f_norm_eps       = 1.0e-05
0.00.066.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.420 I print_info: f_logit_scale    = 0.0e+00
0.00.066.422 I print_info: n_ff             = 8192
0.00.066.422 I print_info: n_expert         = 0
0.00.066.423 I print_info: n_expert_used    = 0
0.00.066.423 I print_info: causal attn      = 1
0.00.066.424 I print_info: pooling type     = 0
0.00.066.426 I print_info: rope type        = 2
0.00.066.430 I print_info: rope scaling     = linear
0.00.066.431 I print_info: freq_base_train  = 10000.0
0.00.066.432 I print_info: freq_scale_train = 1
0.00.066.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.436 I print_info: rope_finetuned   = unknown
0.00.066.437 I print_info: ssm_d_conv       = 0
0.00.066.437 I print_info: ssm_d_inner      = 0
0.00.066.437 I print_info: ssm_d_state      = 0
0.00.066.440 I print_info: ssm_dt_rank      = 0
0.00.066.440 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.440 I print_info: model type       = 1.4B
0.00.066.441 I print_info: model params     = 1.41 B
0.00.066.442 I print_info: general.name     = 1.4B
0.00.066.442 I print_info: vocab type       = BPE
0.00.066.443 I print_info: n_vocab          = 50304
0.00.066.443 I print_info: n_merges         = 50009
0.00.066.444 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.445 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.445 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.446 I print_info: LF token         = 187 ''
0.00.066.446 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.446 I print_info: max token length = 1024
0.00.066.447 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.945 I load_tensors: offloading output layer to GPU
0.00.619.946 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.983 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.619.984 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.621.607 I llama_init_from_model: n_seq_max     = 1
0.00.621.610 I llama_init_from_model: n_ctx         = 2048
0.00.621.610 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.621.611 I llama_init_from_model: n_batch       = 2048
0.00.621.611 I llama_init_from_model: n_ubatch      = 512
0.00.621.612 I llama_init_from_model: flash_attn    = 0
0.00.621.614 I llama_init_from_model: freq_base     = 10000.0
0.00.621.615 I llama_init_from_model: freq_scale    = 1
0.00.621.618 I ggml_metal_init: allocating
0.00.621.687 I ggml_metal_init: found device: Apple M4
0.00.621.700 I ggml_metal_init: picking default device: Apple M4
0.00.623.260 I ggml_metal_init: using embedded metal library
0.00.629.932 I ggml_metal_init: GPU name:   Apple M4
0.00.629.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.937 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.939 I ggml_metal_init: simdgroup reduction   = true
0.00.629.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.940 I ggml_metal_init: has residency sets    = true
0.00.629.940 I ggml_metal_init: has bfloat            = true
0.00.629.940 I ggml_metal_init: use bfloat            = true
0.00.629.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.943 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.942 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.704.882 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.704.889 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.704.924 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.483 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.709.486 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.709.486 I llama_init_from_model: graph nodes  = 967
0.00.709.487 I llama_init_from_model: graph splits = 2
0.00.709.493 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.709.615 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.575 I main: llama threadpool init, n_threads = 4
0.00.767.624 I 
0.00.767.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.647 I 
0.00.767.793 I sampler seed: 1234
0.00.767.798 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.842 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.846 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.846 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.456.577 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50141.24 tokens per second)
0.01.456.578 I llama_perf_context_print:        load time =     750.67 ms
0.01.456.579 I llama_perf_context_print: prompt eval time =      49.50 ms /     7 tokens (    7.07 ms per token,   141.42 tokens per second)
0.01.456.579 I llama_perf_context_print:        eval time =     636.31 ms /    63 runs   (   10.10 ms per token,    99.01 tokens per second)
0.01.456.580 I llama_perf_context_print:       total time =     689.74 ms /    70 tokens
0.01.456.828 I ggml_metal_free: deallocating

real	0m1.489s
user	0m0.122s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.833 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.166 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.175 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.176 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.176 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.179 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.185 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.186 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.186 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.910 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.706 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.707 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.709 I llama_model_loader: - type  f32:  194 tensors
0.00.025.709 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.710 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.711 I print_info: file format = GGUF V3 (latest)
0.00.025.711 I print_info: file type   = Q4_0
0.00.025.714 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.771 I load: special tokens cache size = 25
0.00.040.219 I load: token to piece cache size = 0.2984 MB
0.00.040.236 I print_info: arch             = gptneox
0.00.040.237 I print_info: vocab_only       = 0
0.00.040.237 I print_info: n_ctx_train      = 2048
0.00.040.237 I print_info: n_embd           = 2048
0.00.040.238 I print_info: n_layer          = 24
0.00.040.242 I print_info: n_head           = 16
0.00.040.242 I print_info: n_head_kv        = 16
0.00.040.243 I print_info: n_rot            = 32
0.00.040.243 I print_info: n_swa            = 0
0.00.040.243 I print_info: n_embd_head_k    = 128
0.00.040.243 I print_info: n_embd_head_v    = 128
0.00.040.244 I print_info: n_gqa            = 1
0.00.040.244 I print_info: n_embd_k_gqa     = 2048
0.00.040.245 I print_info: n_embd_v_gqa     = 2048
0.00.040.245 I print_info: f_norm_eps       = 1.0e-05
0.00.040.246 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.247 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.249 I print_info: f_logit_scale    = 0.0e+00
0.00.040.250 I print_info: n_ff             = 8192
0.00.040.250 I print_info: n_expert         = 0
0.00.040.250 I print_info: n_expert_used    = 0
0.00.040.250 I print_info: causal attn      = 1
0.00.040.251 I print_info: pooling type     = 0
0.00.040.251 I print_info: rope type        = 2
0.00.040.251 I print_info: rope scaling     = linear
0.00.040.251 I print_info: freq_base_train  = 10000.0
0.00.040.251 I print_info: freq_scale_train = 1
0.00.040.252 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.252 I print_info: rope_finetuned   = unknown
0.00.040.252 I print_info: ssm_d_conv       = 0
0.00.040.252 I print_info: ssm_d_inner      = 0
0.00.040.252 I print_info: ssm_d_state      = 0
0.00.040.252 I print_info: ssm_dt_rank      = 0
0.00.040.252 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.252 I print_info: model type       = 1.4B
0.00.040.253 I print_info: model params     = 1.41 B
0.00.040.253 I print_info: general.name     = 1.4B
0.00.040.253 I print_info: vocab type       = BPE
0.00.040.254 I print_info: n_vocab          = 50304
0.00.040.254 I print_info: n_merges         = 50009
0.00.040.254 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.254 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.254 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.254 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.255 I print_info: LF token         = 187 ''
0.00.040.255 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.261 I print_info: max token length = 1024
0.00.040.263 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.578.846 I load_tensors: offloading 24 repeating layers to GPU
0.00.578.857 I load_tensors: offloading output layer to GPU
0.00.578.858 I load_tensors: offloaded 25/25 layers to GPU
0.00.578.892 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.578.893 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.580.476 I llama_init_from_model: n_seq_max     = 1
0.00.580.479 I llama_init_from_model: n_ctx         = 128
0.00.580.480 I llama_init_from_model: n_ctx_per_seq = 128
0.00.580.480 I llama_init_from_model: n_batch       = 128
0.00.580.481 I llama_init_from_model: n_ubatch      = 128
0.00.580.481 I llama_init_from_model: flash_attn    = 0
0.00.580.483 I llama_init_from_model: freq_base     = 10000.0
0.00.580.484 I llama_init_from_model: freq_scale    = 1
0.00.580.484 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.580.487 I ggml_metal_init: allocating
0.00.580.579 I ggml_metal_init: found device: Apple M4
0.00.580.593 I ggml_metal_init: picking default device: Apple M4
0.00.582.404 I ggml_metal_init: using embedded metal library
0.00.588.461 I ggml_metal_init: GPU name:   Apple M4
0.00.588.469 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.588.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.588.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.588.472 I ggml_metal_init: simdgroup reduction   = true
0.00.588.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.588.473 I ggml_metal_init: has residency sets    = true
0.00.588.473 I ggml_metal_init: has bfloat            = true
0.00.588.473 I ggml_metal_init: use bfloat            = true
0.00.588.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.588.487 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.608.055 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.643 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.611.656 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.611.691 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.614.926 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.614.928 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.614.929 I llama_init_from_model: graph nodes  = 967
0.00.614.929 I llama_init_from_model: graph splits = 2
0.00.614.933 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.614.935 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.524 I 
0.00.640.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.616 I perplexity: tokenizing the input ..
0.00.647.085 I perplexity: tokenization took 6.467 ms
0.00.647.094 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.641 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.783.989 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.784.013 I llama_perf_context_print:        load time =     630.68 ms
0.00.784.015 I llama_perf_context_print: prompt eval time =     135.16 ms /   128 tokens (    1.06 ms per token,   946.99 tokens per second)
0.00.784.015 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.016 I llama_perf_context_print:       total time =     143.49 ms /   129 tokens
0.00.784.399 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.079s
sys	0m0.123s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.756 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.757 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.761 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.762 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.549 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.561 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.286 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.287 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.287 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.288 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.288 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.288 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.289 I llama_model_loader: - type  f32:  194 tensors
0.00.025.289 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.290 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.290 I print_info: file format = GGUF V3 (latest)
0.00.025.291 I print_info: file type   = Q4_1
0.00.025.292 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.202 I load: special tokens cache size = 25
0.00.039.748 I load: token to piece cache size = 0.2984 MB
0.00.039.762 I print_info: arch             = gptneox
0.00.039.763 I print_info: vocab_only       = 0
0.00.039.763 I print_info: n_ctx_train      = 2048
0.00.039.764 I print_info: n_embd           = 2048
0.00.039.764 I print_info: n_layer          = 24
0.00.039.767 I print_info: n_head           = 16
0.00.039.767 I print_info: n_head_kv        = 16
0.00.039.768 I print_info: n_rot            = 32
0.00.039.768 I print_info: n_swa            = 0
0.00.039.768 I print_info: n_embd_head_k    = 128
0.00.039.768 I print_info: n_embd_head_v    = 128
0.00.039.769 I print_info: n_gqa            = 1
0.00.039.770 I print_info: n_embd_k_gqa     = 2048
0.00.039.770 I print_info: n_embd_v_gqa     = 2048
0.00.039.771 I print_info: f_norm_eps       = 1.0e-05
0.00.039.773 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.773 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.773 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.773 I print_info: f_logit_scale    = 0.0e+00
0.00.039.774 I print_info: n_ff             = 8192
0.00.039.774 I print_info: n_expert         = 0
0.00.039.774 I print_info: n_expert_used    = 0
0.00.039.774 I print_info: causal attn      = 1
0.00.039.774 I print_info: pooling type     = 0
0.00.039.775 I print_info: rope type        = 2
0.00.039.776 I print_info: rope scaling     = linear
0.00.039.776 I print_info: freq_base_train  = 10000.0
0.00.039.776 I print_info: freq_scale_train = 1
0.00.039.776 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.777 I print_info: rope_finetuned   = unknown
0.00.039.777 I print_info: ssm_d_conv       = 0
0.00.039.777 I print_info: ssm_d_inner      = 0
0.00.039.780 I print_info: ssm_d_state      = 0
0.00.039.781 I print_info: ssm_dt_rank      = 0
0.00.039.781 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.781 I print_info: model type       = 1.4B
0.00.039.781 I print_info: model params     = 1.41 B
0.00.039.781 I print_info: general.name     = 1.4B
0.00.039.782 I print_info: vocab type       = BPE
0.00.039.782 I print_info: n_vocab          = 50304
0.00.039.782 I print_info: n_merges         = 50009
0.00.039.782 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.783 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.783 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.783 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.783 I print_info: LF token         = 187 ''
0.00.039.783 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.783 I print_info: max token length = 1024
0.00.039.784 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.636.192 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.210 I load_tensors: offloading output layer to GPU
0.00.636.211 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.249 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.636.250 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.637.976 I llama_init_from_model: n_seq_max     = 1
0.00.637.981 I llama_init_from_model: n_ctx         = 2048
0.00.637.981 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.637.982 I llama_init_from_model: n_batch       = 2048
0.00.637.983 I llama_init_from_model: n_ubatch      = 512
0.00.637.983 I llama_init_from_model: flash_attn    = 0
0.00.637.985 I llama_init_from_model: freq_base     = 10000.0
0.00.637.986 I llama_init_from_model: freq_scale    = 1
0.00.637.998 I ggml_metal_init: allocating
0.00.638.068 I ggml_metal_init: found device: Apple M4
0.00.638.081 I ggml_metal_init: picking default device: Apple M4
0.00.639.865 I ggml_metal_init: using embedded metal library
0.00.645.552 I ggml_metal_init: GPU name:   Apple M4
0.00.645.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.561 I ggml_metal_init: simdgroup reduction   = true
0.00.645.562 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.562 I ggml_metal_init: has residency sets    = true
0.00.645.562 I ggml_metal_init: has bfloat            = true
0.00.645.562 I ggml_metal_init: use bfloat            = true
0.00.645.563 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.486 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.722.819 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.722.825 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.722.855 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.177 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.727.179 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.727.179 I llama_init_from_model: graph nodes  = 967
0.00.727.179 I llama_init_from_model: graph splits = 2
0.00.727.185 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.727.307 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.727.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.335 I main: llama threadpool init, n_threads = 4
0.00.782.384 I 
0.00.782.404 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.405 I 
0.00.782.572 I sampler seed: 1234
0.00.782.577 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.592 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.592 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.593 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.507.856 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.01.507.857 I llama_perf_context_print:        load time =     772.87 ms
0.01.507.858 I llama_perf_context_print: prompt eval time =      48.88 ms /     7 tokens (    6.98 ms per token,   143.19 tokens per second)
0.01.507.859 I llama_perf_context_print:        eval time =     673.61 ms /    63 runs   (   10.69 ms per token,    93.53 tokens per second)
0.01.507.860 I llama_perf_context_print:       total time =     726.23 ms /    70 tokens
0.01.508.078 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.006 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.338 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.349 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.351 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.211 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.032 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.033 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.035 I llama_model_loader: - type  f32:  194 tensors
0.00.025.036 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.036 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.037 I print_info: file format = GGUF V3 (latest)
0.00.025.037 I print_info: file type   = Q4_1
0.00.025.038 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.597 I load: special tokens cache size = 25
0.00.040.124 I load: token to piece cache size = 0.2984 MB
0.00.040.142 I print_info: arch             = gptneox
0.00.040.143 I print_info: vocab_only       = 0
0.00.040.143 I print_info: n_ctx_train      = 2048
0.00.040.143 I print_info: n_embd           = 2048
0.00.040.143 I print_info: n_layer          = 24
0.00.040.147 I print_info: n_head           = 16
0.00.040.151 I print_info: n_head_kv        = 16
0.00.040.151 I print_info: n_rot            = 32
0.00.040.151 I print_info: n_swa            = 0
0.00.040.152 I print_info: n_embd_head_k    = 128
0.00.040.152 I print_info: n_embd_head_v    = 128
0.00.040.152 I print_info: n_gqa            = 1
0.00.040.153 I print_info: n_embd_k_gqa     = 2048
0.00.040.154 I print_info: n_embd_v_gqa     = 2048
0.00.040.154 I print_info: f_norm_eps       = 1.0e-05
0.00.040.155 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.155 I print_info: f_logit_scale    = 0.0e+00
0.00.040.156 I print_info: n_ff             = 8192
0.00.040.156 I print_info: n_expert         = 0
0.00.040.156 I print_info: n_expert_used    = 0
0.00.040.156 I print_info: causal attn      = 1
0.00.040.156 I print_info: pooling type     = 0
0.00.040.156 I print_info: rope type        = 2
0.00.040.156 I print_info: rope scaling     = linear
0.00.040.157 I print_info: freq_base_train  = 10000.0
0.00.040.157 I print_info: freq_scale_train = 1
0.00.040.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.157 I print_info: rope_finetuned   = unknown
0.00.040.157 I print_info: ssm_d_conv       = 0
0.00.040.158 I print_info: ssm_d_inner      = 0
0.00.040.158 I print_info: ssm_d_state      = 0
0.00.040.158 I print_info: ssm_dt_rank      = 0
0.00.040.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.158 I print_info: model type       = 1.4B
0.00.040.158 I print_info: model params     = 1.41 B
0.00.040.158 I print_info: general.name     = 1.4B
0.00.040.159 I print_info: vocab type       = BPE
0.00.040.159 I print_info: n_vocab          = 50304
0.00.040.159 I print_info: n_merges         = 50009
0.00.040.160 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: LF token         = 187 ''
0.00.040.161 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: max token length = 1024
0.00.040.161 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.624.285 I load_tensors: offloading 24 repeating layers to GPU
0.00.624.302 I load_tensors: offloading output layer to GPU
0.00.624.302 I load_tensors: offloaded 25/25 layers to GPU
0.00.624.337 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.624.345 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.626.052 I llama_init_from_model: n_seq_max     = 1
0.00.626.055 I llama_init_from_model: n_ctx         = 128
0.00.626.056 I llama_init_from_model: n_ctx_per_seq = 128
0.00.626.056 I llama_init_from_model: n_batch       = 128
0.00.626.057 I llama_init_from_model: n_ubatch      = 128
0.00.626.057 I llama_init_from_model: flash_attn    = 0
0.00.626.059 I llama_init_from_model: freq_base     = 10000.0
0.00.626.060 I llama_init_from_model: freq_scale    = 1
0.00.626.060 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.626.063 I ggml_metal_init: allocating
0.00.626.142 I ggml_metal_init: found device: Apple M4
0.00.626.157 I ggml_metal_init: picking default device: Apple M4
0.00.627.724 I ggml_metal_init: using embedded metal library
0.00.633.770 I ggml_metal_init: GPU name:   Apple M4
0.00.633.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.633.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.633.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.633.784 I ggml_metal_init: simdgroup reduction   = true
0.00.633.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.633.784 I ggml_metal_init: has residency sets    = true
0.00.633.785 I ggml_metal_init: has bfloat            = true
0.00.633.785 I ggml_metal_init: use bfloat            = true
0.00.633.786 I ggml_metal_init: hasUnifiedMemory      = true
0.00.633.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.490 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.657.005 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.657.009 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.657.037 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.660.531 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.660.533 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.660.533 I llama_init_from_model: graph nodes  = 967
0.00.660.534 I llama_init_from_model: graph splits = 2
0.00.660.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.660.537 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.451 I 
0.00.687.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.548 I perplexity: tokenizing the input ..
0.00.694.778 I perplexity: tokenization took 7.226 ms
0.00.694.785 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.831.117 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.832.536 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.832.561 I llama_perf_context_print:        load time =     678.44 ms
0.00.832.562 I llama_perf_context_print: prompt eval time =     135.43 ms /   128 tokens (    1.06 ms per token,   945.17 tokens per second)
0.00.832.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.832.563 I llama_perf_context_print:       total time =     145.11 ms /   129 tokens
0.00.832.936 I ggml_metal_free: deallocating

real	0m0.847s
user	0m0.081s
sys	0m0.121s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.595 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.602 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.610 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.610 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.611 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.611 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.614 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.615 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.017 I llama_model_loader: - type  f32:  194 tensors
0.00.025.017 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.018 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.018 I print_info: file format = GGUF V3 (latest)
0.00.025.019 I print_info: file type   = Q5_0
0.00.025.020 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.206 I load: special tokens cache size = 25
0.00.039.398 I load: token to piece cache size = 0.2984 MB
0.00.039.413 I print_info: arch             = gptneox
0.00.039.414 I print_info: vocab_only       = 0
0.00.039.414 I print_info: n_ctx_train      = 2048
0.00.039.414 I print_info: n_embd           = 2048
0.00.039.415 I print_info: n_layer          = 24
0.00.039.417 I print_info: n_head           = 16
0.00.039.418 I print_info: n_head_kv        = 16
0.00.039.420 I print_info: n_rot            = 32
0.00.039.421 I print_info: n_swa            = 0
0.00.039.421 I print_info: n_embd_head_k    = 128
0.00.039.421 I print_info: n_embd_head_v    = 128
0.00.039.422 I print_info: n_gqa            = 1
0.00.039.423 I print_info: n_embd_k_gqa     = 2048
0.00.039.428 I print_info: n_embd_v_gqa     = 2048
0.00.039.430 I print_info: f_norm_eps       = 1.0e-05
0.00.039.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.431 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.431 I print_info: f_logit_scale    = 0.0e+00
0.00.039.437 I print_info: n_ff             = 8192
0.00.039.437 I print_info: n_expert         = 0
0.00.039.437 I print_info: n_expert_used    = 0
0.00.039.437 I print_info: causal attn      = 1
0.00.039.437 I print_info: pooling type     = 0
0.00.039.439 I print_info: rope type        = 2
0.00.039.440 I print_info: rope scaling     = linear
0.00.039.440 I print_info: freq_base_train  = 10000.0
0.00.039.440 I print_info: freq_scale_train = 1
0.00.039.441 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.441 I print_info: rope_finetuned   = unknown
0.00.039.441 I print_info: ssm_d_conv       = 0
0.00.039.441 I print_info: ssm_d_inner      = 0
0.00.039.441 I print_info: ssm_d_state      = 0
0.00.039.441 I print_info: ssm_dt_rank      = 0
0.00.039.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.441 I print_info: model type       = 1.4B
0.00.039.442 I print_info: model params     = 1.41 B
0.00.039.442 I print_info: general.name     = 1.4B
0.00.039.443 I print_info: vocab type       = BPE
0.00.039.443 I print_info: n_vocab          = 50304
0.00.039.443 I print_info: n_merges         = 50009
0.00.039.443 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.443 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.443 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: LF token         = 187 ''
0.00.039.444 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: max token length = 1024
0.00.039.445 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.656.977 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.986 I load_tensors: offloading output layer to GPU
0.00.656.987 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.014 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.657.016 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.658.542 I llama_init_from_model: n_seq_max     = 1
0.00.658.546 I llama_init_from_model: n_ctx         = 2048
0.00.658.547 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.547 I llama_init_from_model: n_batch       = 2048
0.00.658.548 I llama_init_from_model: n_ubatch      = 512
0.00.658.548 I llama_init_from_model: flash_attn    = 0
0.00.658.550 I llama_init_from_model: freq_base     = 10000.0
0.00.658.551 I llama_init_from_model: freq_scale    = 1
0.00.658.553 I ggml_metal_init: allocating
0.00.658.604 I ggml_metal_init: found device: Apple M4
0.00.658.617 I ggml_metal_init: picking default device: Apple M4
0.00.660.390 I ggml_metal_init: using embedded metal library
0.00.667.441 I ggml_metal_init: GPU name:   Apple M4
0.00.667.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.449 I ggml_metal_init: simdgroup reduction   = true
0.00.667.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.449 I ggml_metal_init: has residency sets    = true
0.00.667.450 I ggml_metal_init: has bfloat            = true
0.00.667.450 I ggml_metal_init: use bfloat            = true
0.00.667.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.686.058 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.387 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.393 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.415 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.749.264 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.749.266 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.749.267 I llama_init_from_model: graph nodes  = 967
0.00.749.267 I llama_init_from_model: graph splits = 2
0.00.749.273 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.749.397 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.397 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.982 I main: llama threadpool init, n_threads = 4
0.00.807.030 I 
0.00.807.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.051 I 
0.00.807.228 I sampler seed: 1234
0.00.807.233 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.248 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.248 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.248 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.596.945 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.01.596.945 I llama_perf_context_print:        load time =     797.34 ms
0.01.596.947 I llama_perf_context_print: prompt eval time =      53.61 ms /     7 tokens (    7.66 ms per token,   130.57 tokens per second)
0.01.596.948 I llama_perf_context_print:        eval time =     733.16 ms /    63 runs   (   11.64 ms per token,    85.93 tokens per second)
0.01.596.948 I llama_perf_context_print:       total time =     790.69 ms /    70 tokens
0.01.597.175 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.110s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.148 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.068 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.068 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.069 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.070 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.071 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.072 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.072 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.074 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.074 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.076 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.076 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.936 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.803 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.804 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.805 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.805 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.805 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.806 I llama_model_loader: - type  f32:  194 tensors
0.00.024.806 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.807 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.807 I print_info: file format = GGUF V3 (latest)
0.00.024.808 I print_info: file type   = Q5_0
0.00.024.809 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.914 I load: special tokens cache size = 25
0.00.039.705 I load: token to piece cache size = 0.2984 MB
0.00.039.722 I print_info: arch             = gptneox
0.00.039.723 I print_info: vocab_only       = 0
0.00.039.723 I print_info: n_ctx_train      = 2048
0.00.039.723 I print_info: n_embd           = 2048
0.00.039.723 I print_info: n_layer          = 24
0.00.039.727 I print_info: n_head           = 16
0.00.039.728 I print_info: n_head_kv        = 16
0.00.039.728 I print_info: n_rot            = 32
0.00.039.728 I print_info: n_swa            = 0
0.00.039.728 I print_info: n_embd_head_k    = 128
0.00.039.729 I print_info: n_embd_head_v    = 128
0.00.039.729 I print_info: n_gqa            = 1
0.00.039.730 I print_info: n_embd_k_gqa     = 2048
0.00.039.730 I print_info: n_embd_v_gqa     = 2048
0.00.039.731 I print_info: f_norm_eps       = 1.0e-05
0.00.039.731 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.732 I print_info: f_logit_scale    = 0.0e+00
0.00.039.732 I print_info: n_ff             = 8192
0.00.039.732 I print_info: n_expert         = 0
0.00.039.733 I print_info: n_expert_used    = 0
0.00.039.733 I print_info: causal attn      = 1
0.00.039.733 I print_info: pooling type     = 0
0.00.039.733 I print_info: rope type        = 2
0.00.039.733 I print_info: rope scaling     = linear
0.00.039.734 I print_info: freq_base_train  = 10000.0
0.00.039.735 I print_info: freq_scale_train = 1
0.00.039.737 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.737 I print_info: rope_finetuned   = unknown
0.00.039.737 I print_info: ssm_d_conv       = 0
0.00.039.737 I print_info: ssm_d_inner      = 0
0.00.039.737 I print_info: ssm_d_state      = 0
0.00.039.737 I print_info: ssm_dt_rank      = 0
0.00.039.737 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.738 I print_info: model type       = 1.4B
0.00.039.738 I print_info: model params     = 1.41 B
0.00.039.738 I print_info: general.name     = 1.4B
0.00.039.739 I print_info: vocab type       = BPE
0.00.039.739 I print_info: n_vocab          = 50304
0.00.039.739 I print_info: n_merges         = 50009
0.00.039.739 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.739 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.739 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.740 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.740 I print_info: LF token         = 187 ''
0.00.039.740 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.740 I print_info: max token length = 1024
0.00.039.741 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.682 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.693 I load_tensors: offloading output layer to GPU
0.00.653.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.729 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.653.735 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.655.265 I llama_init_from_model: n_seq_max     = 1
0.00.655.269 I llama_init_from_model: n_ctx         = 128
0.00.655.269 I llama_init_from_model: n_ctx_per_seq = 128
0.00.655.270 I llama_init_from_model: n_batch       = 128
0.00.655.271 I llama_init_from_model: n_ubatch      = 128
0.00.655.271 I llama_init_from_model: flash_attn    = 0
0.00.655.274 I llama_init_from_model: freq_base     = 10000.0
0.00.655.275 I llama_init_from_model: freq_scale    = 1
0.00.655.275 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.655.282 I ggml_metal_init: allocating
0.00.655.351 I ggml_metal_init: found device: Apple M4
0.00.655.365 I ggml_metal_init: picking default device: Apple M4
0.00.656.898 I ggml_metal_init: using embedded metal library
0.00.663.956 I ggml_metal_init: GPU name:   Apple M4
0.00.663.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.966 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.967 I ggml_metal_init: simdgroup reduction   = true
0.00.663.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.968 I ggml_metal_init: has residency sets    = true
0.00.663.968 I ggml_metal_init: has bfloat            = true
0.00.663.968 I ggml_metal_init: use bfloat            = true
0.00.663.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.311 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.836 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.685.845 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.685.891 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.061 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.689.063 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.689.063 I llama_init_from_model: graph nodes  = 967
0.00.689.064 I llama_init_from_model: graph splits = 2
0.00.689.067 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.689.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.336 I 
0.00.723.450 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.459 I perplexity: tokenizing the input ..
0.00.730.935 I perplexity: tokenization took 7.471 ms
0.00.730.943 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.879.583 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.880.924 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.880.951 I llama_perf_context_print:        load time =     714.18 ms
0.00.880.952 I llama_perf_context_print: prompt eval time =     147.63 ms /   128 tokens (    1.15 ms per token,   867.06 tokens per second)
0.00.880.952 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.880.953 I llama_perf_context_print:       total time =     157.62 ms /   129 tokens
0.00.881.374 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.082s
sys	0m0.143s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.045 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.342 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.344 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.347 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.347 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.347 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.889 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.889 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.889 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.890 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.890 I llama_model_loader: - type  f32:  194 tensors
0.00.026.891 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.891 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.892 I print_info: file format = GGUF V3 (latest)
0.00.026.892 I print_info: file type   = Q5_1
0.00.026.893 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.692 I load: special tokens cache size = 25
0.00.040.934 I load: token to piece cache size = 0.2984 MB
0.00.040.948 I print_info: arch             = gptneox
0.00.040.949 I print_info: vocab_only       = 0
0.00.040.949 I print_info: n_ctx_train      = 2048
0.00.040.950 I print_info: n_embd           = 2048
0.00.040.950 I print_info: n_layer          = 24
0.00.040.953 I print_info: n_head           = 16
0.00.040.954 I print_info: n_head_kv        = 16
0.00.040.954 I print_info: n_rot            = 32
0.00.040.954 I print_info: n_swa            = 0
0.00.040.955 I print_info: n_embd_head_k    = 128
0.00.040.955 I print_info: n_embd_head_v    = 128
0.00.040.957 I print_info: n_gqa            = 1
0.00.040.957 I print_info: n_embd_k_gqa     = 2048
0.00.040.958 I print_info: n_embd_v_gqa     = 2048
0.00.040.959 I print_info: f_norm_eps       = 1.0e-05
0.00.040.959 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.959 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.961 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.961 I print_info: f_logit_scale    = 0.0e+00
0.00.040.961 I print_info: n_ff             = 8192
0.00.040.962 I print_info: n_expert         = 0
0.00.040.962 I print_info: n_expert_used    = 0
0.00.040.965 I print_info: causal attn      = 1
0.00.040.965 I print_info: pooling type     = 0
0.00.040.966 I print_info: rope type        = 2
0.00.040.968 I print_info: rope scaling     = linear
0.00.040.968 I print_info: freq_base_train  = 10000.0
0.00.040.968 I print_info: freq_scale_train = 1
0.00.040.968 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.969 I print_info: rope_finetuned   = unknown
0.00.040.970 I print_info: ssm_d_conv       = 0
0.00.040.970 I print_info: ssm_d_inner      = 0
0.00.040.970 I print_info: ssm_d_state      = 0
0.00.040.970 I print_info: ssm_dt_rank      = 0
0.00.040.970 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.970 I print_info: model type       = 1.4B
0.00.040.970 I print_info: model params     = 1.41 B
0.00.040.971 I print_info: general.name     = 1.4B
0.00.040.971 I print_info: vocab type       = BPE
0.00.040.971 I print_info: n_vocab          = 50304
0.00.040.971 I print_info: n_merges         = 50009
0.00.040.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.973 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.973 I print_info: LF token         = 187 ''
0.00.040.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.973 I print_info: max token length = 1024
0.00.040.974 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.483 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.498 I load_tensors: offloading output layer to GPU
0.00.600.499 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.530 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.600.531 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.602.290 I llama_init_from_model: n_seq_max     = 1
0.00.602.293 I llama_init_from_model: n_ctx         = 2048
0.00.602.294 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.602.294 I llama_init_from_model: n_batch       = 2048
0.00.602.294 I llama_init_from_model: n_ubatch      = 512
0.00.602.295 I llama_init_from_model: flash_attn    = 0
0.00.602.298 I llama_init_from_model: freq_base     = 10000.0
0.00.602.298 I llama_init_from_model: freq_scale    = 1
0.00.602.302 I ggml_metal_init: allocating
0.00.602.385 I ggml_metal_init: found device: Apple M4
0.00.602.405 I ggml_metal_init: picking default device: Apple M4
0.00.603.793 I ggml_metal_init: using embedded metal library
0.00.610.242 I ggml_metal_init: GPU name:   Apple M4
0.00.610.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.248 I ggml_metal_init: simdgroup reduction   = true
0.00.610.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.248 I ggml_metal_init: has residency sets    = true
0.00.610.248 I ggml_metal_init: has bfloat            = true
0.00.610.249 I ggml_metal_init: use bfloat            = true
0.00.610.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.014 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.684.264 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.684.271 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.684.294 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.804 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.805 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.806 I llama_init_from_model: graph nodes  = 967
0.00.688.806 I llama_init_from_model: graph splits = 2
0.00.688.812 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.940 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.480 I main: llama threadpool init, n_threads = 4
0.00.749.529 I 
0.00.749.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.551 I 
0.00.749.709 I sampler seed: 1234
0.00.749.713 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.728 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.728 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.728 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.597.171 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.01.597.171 I llama_perf_context_print:        load time =     737.72 ms
0.01.597.172 I llama_perf_context_print: prompt eval time =      50.47 ms /     7 tokens (    7.21 ms per token,   138.69 tokens per second)
0.01.597.173 I llama_perf_context_print:        eval time =     794.00 ms /    63 runs   (   12.60 ms per token,    79.35 tokens per second)
0.01.597.174 I llama_perf_context_print:       total time =     848.41 ms /    70 tokens
0.01.597.455 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.108s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.268 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.520 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.534 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.539 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.541 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.542 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.542 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.101 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.035.102 I llama_model_loader: - type  f32:  194 tensors
0.00.035.102 I llama_model_loader: - type q5_1:   97 tensors
0.00.035.102 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.103 I print_info: file format = GGUF V3 (latest)
0.00.035.104 I print_info: file type   = Q5_1
0.00.035.104 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.254 I load: special tokens cache size = 25
0.00.050.769 I load: token to piece cache size = 0.2984 MB
0.00.050.784 I print_info: arch             = gptneox
0.00.050.786 I print_info: vocab_only       = 0
0.00.050.786 I print_info: n_ctx_train      = 2048
0.00.050.786 I print_info: n_embd           = 2048
0.00.050.786 I print_info: n_layer          = 24
0.00.050.790 I print_info: n_head           = 16
0.00.050.791 I print_info: n_head_kv        = 16
0.00.050.791 I print_info: n_rot            = 32
0.00.050.791 I print_info: n_swa            = 0
0.00.050.791 I print_info: n_embd_head_k    = 128
0.00.050.791 I print_info: n_embd_head_v    = 128
0.00.050.792 I print_info: n_gqa            = 1
0.00.050.793 I print_info: n_embd_k_gqa     = 2048
0.00.050.793 I print_info: n_embd_v_gqa     = 2048
0.00.050.794 I print_info: f_norm_eps       = 1.0e-05
0.00.050.795 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.795 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.795 I print_info: f_logit_scale    = 0.0e+00
0.00.050.797 I print_info: n_ff             = 8192
0.00.050.797 I print_info: n_expert         = 0
0.00.050.798 I print_info: n_expert_used    = 0
0.00.050.798 I print_info: causal attn      = 1
0.00.050.798 I print_info: pooling type     = 0
0.00.050.798 I print_info: rope type        = 2
0.00.050.798 I print_info: rope scaling     = linear
0.00.050.799 I print_info: freq_base_train  = 10000.0
0.00.050.799 I print_info: freq_scale_train = 1
0.00.050.799 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.799 I print_info: rope_finetuned   = unknown
0.00.050.800 I print_info: ssm_d_conv       = 0
0.00.050.800 I print_info: ssm_d_inner      = 0
0.00.050.800 I print_info: ssm_d_state      = 0
0.00.050.800 I print_info: ssm_dt_rank      = 0
0.00.050.800 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.800 I print_info: model type       = 1.4B
0.00.050.801 I print_info: model params     = 1.41 B
0.00.050.801 I print_info: general.name     = 1.4B
0.00.050.801 I print_info: vocab type       = BPE
0.00.050.802 I print_info: n_vocab          = 50304
0.00.050.802 I print_info: n_merges         = 50009
0.00.050.802 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.802 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.803 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.803 I print_info: LF token         = 187 ''
0.00.050.803 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.803 I print_info: max token length = 1024
0.00.050.804 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.335 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.351 I load_tensors: offloading output layer to GPU
0.00.638.351 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.382 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.638.383 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.640.099 I llama_init_from_model: n_seq_max     = 1
0.00.640.102 I llama_init_from_model: n_ctx         = 128
0.00.640.103 I llama_init_from_model: n_ctx_per_seq = 128
0.00.640.104 I llama_init_from_model: n_batch       = 128
0.00.640.104 I llama_init_from_model: n_ubatch      = 128
0.00.640.104 I llama_init_from_model: flash_attn    = 0
0.00.640.107 I llama_init_from_model: freq_base     = 10000.0
0.00.640.107 I llama_init_from_model: freq_scale    = 1
0.00.640.108 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.640.110 I ggml_metal_init: allocating
0.00.640.188 I ggml_metal_init: found device: Apple M4
0.00.640.201 I ggml_metal_init: picking default device: Apple M4
0.00.641.611 I ggml_metal_init: using embedded metal library
0.00.648.137 I ggml_metal_init: GPU name:   Apple M4
0.00.648.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.142 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.143 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.143 I ggml_metal_init: simdgroup reduction   = true
0.00.648.144 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.144 I ggml_metal_init: has residency sets    = true
0.00.648.144 I ggml_metal_init: has bfloat            = true
0.00.648.144 I ggml_metal_init: use bfloat            = true
0.00.648.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.536 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.037 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.669.044 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.669.085 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.266 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.672.268 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.672.268 I llama_init_from_model: graph nodes  = 967
0.00.672.269 I llama_init_from_model: graph splits = 2
0.00.672.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.672.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.712 I 
0.00.702.802 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.811 I perplexity: tokenizing the input ..
0.00.710.871 I perplexity: tokenization took 8.057 ms
0.00.710.886 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.107 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.860.439 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.860.464 I llama_perf_context_print:        load time =     686.44 ms
0.00.860.465 I llama_perf_context_print: prompt eval time =     147.33 ms /   128 tokens (    1.15 ms per token,   868.79 tokens per second)
0.00.860.465 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.860.466 I llama_perf_context_print:       total time =     157.75 ms /   129 tokens
0.00.860.810 I ggml_metal_free: deallocating

real	0m0.886s
user	0m0.082s
sys	0m0.146s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.791 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.458 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.467 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.471 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.152 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.159 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.864 I llama_model_loader: - type  f32:  194 tensors
0.00.023.865 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.865 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.865 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.866 I print_info: file format = GGUF V3 (latest)
0.00.023.866 I print_info: file type   = Q2_K - Medium
0.00.023.867 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.655 I load: special tokens cache size = 25
0.00.037.817 I load: token to piece cache size = 0.2984 MB
0.00.037.831 I print_info: arch             = gptneox
0.00.037.833 I print_info: vocab_only       = 0
0.00.037.833 I print_info: n_ctx_train      = 2048
0.00.037.833 I print_info: n_embd           = 2048
0.00.037.833 I print_info: n_layer          = 24
0.00.037.836 I print_info: n_head           = 16
0.00.037.837 I print_info: n_head_kv        = 16
0.00.037.837 I print_info: n_rot            = 32
0.00.037.837 I print_info: n_swa            = 0
0.00.037.837 I print_info: n_embd_head_k    = 128
0.00.037.838 I print_info: n_embd_head_v    = 128
0.00.037.838 I print_info: n_gqa            = 1
0.00.037.839 I print_info: n_embd_k_gqa     = 2048
0.00.037.840 I print_info: n_embd_v_gqa     = 2048
0.00.037.840 I print_info: f_norm_eps       = 1.0e-05
0.00.037.841 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.841 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.841 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.841 I print_info: f_logit_scale    = 0.0e+00
0.00.037.842 I print_info: n_ff             = 8192
0.00.037.842 I print_info: n_expert         = 0
0.00.037.842 I print_info: n_expert_used    = 0
0.00.037.842 I print_info: causal attn      = 1
0.00.037.843 I print_info: pooling type     = 0
0.00.037.843 I print_info: rope type        = 2
0.00.037.843 I print_info: rope scaling     = linear
0.00.037.843 I print_info: freq_base_train  = 10000.0
0.00.037.844 I print_info: freq_scale_train = 1
0.00.037.844 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.845 I print_info: rope_finetuned   = unknown
0.00.037.850 I print_info: ssm_d_conv       = 0
0.00.037.852 I print_info: ssm_d_inner      = 0
0.00.037.852 I print_info: ssm_d_state      = 0
0.00.037.853 I print_info: ssm_dt_rank      = 0
0.00.037.853 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.853 I print_info: model type       = 1.4B
0.00.037.854 I print_info: model params     = 1.41 B
0.00.037.854 I print_info: general.name     = 1.4B
0.00.037.854 I print_info: vocab type       = BPE
0.00.037.855 I print_info: n_vocab          = 50304
0.00.037.855 I print_info: n_merges         = 50009
0.00.037.855 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.855 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.855 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.856 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.857 I print_info: LF token         = 187 ''
0.00.037.857 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.857 I print_info: max token length = 1024
0.00.037.857 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.336.513 I load_tensors: offloading 24 repeating layers to GPU
0.00.336.529 I load_tensors: offloading output layer to GPU
0.00.336.530 I load_tensors: offloaded 25/25 layers to GPU
0.00.336.563 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.336.564 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.338.319 I llama_init_from_model: n_seq_max     = 1
0.00.338.322 I llama_init_from_model: n_ctx         = 2048
0.00.338.323 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.338.323 I llama_init_from_model: n_batch       = 2048
0.00.338.324 I llama_init_from_model: n_ubatch      = 512
0.00.338.324 I llama_init_from_model: flash_attn    = 0
0.00.338.327 I llama_init_from_model: freq_base     = 10000.0
0.00.338.327 I llama_init_from_model: freq_scale    = 1
0.00.338.330 I ggml_metal_init: allocating
0.00.338.430 I ggml_metal_init: found device: Apple M4
0.00.338.444 I ggml_metal_init: picking default device: Apple M4
0.00.340.056 I ggml_metal_init: using embedded metal library
0.00.345.723 I ggml_metal_init: GPU name:   Apple M4
0.00.345.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.737 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.738 I ggml_metal_init: simdgroup reduction   = true
0.00.345.738 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.739 I ggml_metal_init: has residency sets    = true
0.00.345.739 I ggml_metal_init: has bfloat            = true
0.00.345.739 I ggml_metal_init: use bfloat            = true
0.00.345.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.747 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.367.464 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.426.054 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.426.077 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.426.107 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.430.359 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.430.361 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.430.361 I llama_init_from_model: graph nodes  = 967
0.00.430.361 I llama_init_from_model: graph splits = 2
0.00.430.367 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.430.483 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.430.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.043 I main: llama threadpool init, n_threads = 4
0.00.490.092 I 
0.00.490.113 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.114 I 
0.00.490.288 I sampler seed: 1234
0.00.490.292 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.490.307 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.490.309 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.490.309 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.170.718 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.170.718 I llama_perf_context_print:        load time =     480.53 ms
0.01.170.719 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.38 tokens per second)
0.01.170.720 I llama_perf_context_print:        eval time =     633.88 ms /    63 runs   (   10.06 ms per token,    99.39 tokens per second)
0.01.170.720 I llama_perf_context_print:       total time =     681.39 ms /    70 tokens
0.01.170.928 I ggml_metal_free: deallocating

real	0m1.188s
user	0m0.112s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.246 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.022 I llama_model_loader: - type  f32:  194 tensors
0.00.026.022 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.022 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.023 I print_info: file format = GGUF V3 (latest)
0.00.026.024 I print_info: file type   = Q2_K - Medium
0.00.026.025 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.422 I load: special tokens cache size = 25
0.00.041.026 I load: token to piece cache size = 0.2984 MB
0.00.041.043 I print_info: arch             = gptneox
0.00.041.044 I print_info: vocab_only       = 0
0.00.041.044 I print_info: n_ctx_train      = 2048
0.00.041.044 I print_info: n_embd           = 2048
0.00.041.045 I print_info: n_layer          = 24
0.00.041.049 I print_info: n_head           = 16
0.00.041.049 I print_info: n_head_kv        = 16
0.00.041.050 I print_info: n_rot            = 32
0.00.041.050 I print_info: n_swa            = 0
0.00.041.050 I print_info: n_embd_head_k    = 128
0.00.041.050 I print_info: n_embd_head_v    = 128
0.00.041.050 I print_info: n_gqa            = 1
0.00.041.051 I print_info: n_embd_k_gqa     = 2048
0.00.041.052 I print_info: n_embd_v_gqa     = 2048
0.00.041.052 I print_info: f_norm_eps       = 1.0e-05
0.00.041.055 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.055 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.056 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.056 I print_info: f_logit_scale    = 0.0e+00
0.00.041.056 I print_info: n_ff             = 8192
0.00.041.056 I print_info: n_expert         = 0
0.00.041.056 I print_info: n_expert_used    = 0
0.00.041.057 I print_info: causal attn      = 1
0.00.041.057 I print_info: pooling type     = 0
0.00.041.057 I print_info: rope type        = 2
0.00.041.057 I print_info: rope scaling     = linear
0.00.041.058 I print_info: freq_base_train  = 10000.0
0.00.041.058 I print_info: freq_scale_train = 1
0.00.041.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.058 I print_info: rope_finetuned   = unknown
0.00.041.058 I print_info: ssm_d_conv       = 0
0.00.041.058 I print_info: ssm_d_inner      = 0
0.00.041.059 I print_info: ssm_d_state      = 0
0.00.041.059 I print_info: ssm_dt_rank      = 0
0.00.041.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.059 I print_info: model type       = 1.4B
0.00.041.059 I print_info: model params     = 1.41 B
0.00.041.060 I print_info: general.name     = 1.4B
0.00.041.060 I print_info: vocab type       = BPE
0.00.041.062 I print_info: n_vocab          = 50304
0.00.041.063 I print_info: n_merges         = 50009
0.00.041.063 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.063 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.063 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.063 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.063 I print_info: LF token         = 187 ''
0.00.041.063 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.066 I print_info: max token length = 1024
0.00.041.066 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.354.862 I load_tensors: offloading 24 repeating layers to GPU
0.00.354.878 I load_tensors: offloading output layer to GPU
0.00.354.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.354.910 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.354.911 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.356.701 I llama_init_from_model: n_seq_max     = 1
0.00.356.704 I llama_init_from_model: n_ctx         = 128
0.00.356.704 I llama_init_from_model: n_ctx_per_seq = 128
0.00.356.705 I llama_init_from_model: n_batch       = 128
0.00.356.705 I llama_init_from_model: n_ubatch      = 128
0.00.356.706 I llama_init_from_model: flash_attn    = 0
0.00.356.707 I llama_init_from_model: freq_base     = 10000.0
0.00.356.708 I llama_init_from_model: freq_scale    = 1
0.00.356.709 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.356.711 I ggml_metal_init: allocating
0.00.356.792 I ggml_metal_init: found device: Apple M4
0.00.356.806 I ggml_metal_init: picking default device: Apple M4
0.00.358.369 I ggml_metal_init: using embedded metal library
0.00.363.901 I ggml_metal_init: GPU name:   Apple M4
0.00.363.911 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.363.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.363.912 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.363.913 I ggml_metal_init: simdgroup reduction   = true
0.00.363.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.363.914 I ggml_metal_init: has residency sets    = true
0.00.363.914 I ggml_metal_init: has bfloat            = true
0.00.363.914 I ggml_metal_init: use bfloat            = true
0.00.363.916 I ggml_metal_init: hasUnifiedMemory      = true
0.00.363.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.385.876 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.389.658 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.389.664 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.389.703 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.393.059 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.393.061 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.393.062 I llama_init_from_model: graph nodes  = 967
0.00.393.062 I llama_init_from_model: graph splits = 2
0.00.393.065 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.393.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.424.152 I 
0.00.424.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.424.253 I perplexity: tokenizing the input ..
0.00.431.009 I perplexity: tokenization took 6.753 ms
0.00.431.017 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.569.336 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.570.763 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.570.784 I llama_perf_context_print:        load time =     415.24 ms
0.00.570.784 I llama_perf_context_print: prompt eval time =     137.45 ms /   128 tokens (    1.07 ms per token,   931.27 tokens per second)
0.00.570.785 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.570.788 I llama_perf_context_print:       total time =     146.64 ms /   129 tokens
0.00.571.166 I ggml_metal_free: deallocating

real	0m0.588s
user	0m0.082s
sys	0m0.088s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.378 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.149 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.162 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.164 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.171 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.979 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.702 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.703 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.703 I llama_model_loader: - type  f32:  194 tensors
0.00.025.704 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.704 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.704 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.704 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.705 I print_info: file format = GGUF V3 (latest)
0.00.025.706 I print_info: file type   = Q3_K - Medium
0.00.025.706 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.836 I load: special tokens cache size = 25
0.00.040.326 I load: token to piece cache size = 0.2984 MB
0.00.040.333 I print_info: arch             = gptneox
0.00.040.333 I print_info: vocab_only       = 0
0.00.040.333 I print_info: n_ctx_train      = 2048
0.00.040.334 I print_info: n_embd           = 2048
0.00.040.334 I print_info: n_layer          = 24
0.00.040.336 I print_info: n_head           = 16
0.00.040.337 I print_info: n_head_kv        = 16
0.00.040.337 I print_info: n_rot            = 32
0.00.040.338 I print_info: n_swa            = 0
0.00.040.338 I print_info: n_embd_head_k    = 128
0.00.040.338 I print_info: n_embd_head_v    = 128
0.00.040.339 I print_info: n_gqa            = 1
0.00.040.339 I print_info: n_embd_k_gqa     = 2048
0.00.040.340 I print_info: n_embd_v_gqa     = 2048
0.00.040.341 I print_info: f_norm_eps       = 1.0e-05
0.00.040.343 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.343 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.343 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.343 I print_info: f_logit_scale    = 0.0e+00
0.00.040.344 I print_info: n_ff             = 8192
0.00.040.344 I print_info: n_expert         = 0
0.00.040.344 I print_info: n_expert_used    = 0
0.00.040.346 I print_info: causal attn      = 1
0.00.040.347 I print_info: pooling type     = 0
0.00.040.347 I print_info: rope type        = 2
0.00.040.348 I print_info: rope scaling     = linear
0.00.040.348 I print_info: freq_base_train  = 10000.0
0.00.040.348 I print_info: freq_scale_train = 1
0.00.040.348 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.349 I print_info: rope_finetuned   = unknown
0.00.040.350 I print_info: ssm_d_conv       = 0
0.00.040.350 I print_info: ssm_d_inner      = 0
0.00.040.350 I print_info: ssm_d_state      = 0
0.00.040.351 I print_info: ssm_dt_rank      = 0
0.00.040.351 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.351 I print_info: model type       = 1.4B
0.00.040.351 I print_info: model params     = 1.41 B
0.00.040.353 I print_info: general.name     = 1.4B
0.00.040.353 I print_info: vocab type       = BPE
0.00.040.355 I print_info: n_vocab          = 50304
0.00.040.355 I print_info: n_merges         = 50009
0.00.040.355 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.355 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.355 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.355 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.356 I print_info: LF token         = 187 ''
0.00.040.356 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.356 I print_info: max token length = 1024
0.00.040.356 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.355 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.371 I load_tensors: offloading output layer to GPU
0.00.445.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.404 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.406 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.447.184 I llama_init_from_model: n_seq_max     = 1
0.00.447.187 I llama_init_from_model: n_ctx         = 2048
0.00.447.187 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.447.188 I llama_init_from_model: n_batch       = 2048
0.00.447.188 I llama_init_from_model: n_ubatch      = 512
0.00.447.188 I llama_init_from_model: flash_attn    = 0
0.00.447.191 I llama_init_from_model: freq_base     = 10000.0
0.00.447.191 I llama_init_from_model: freq_scale    = 1
0.00.447.193 I ggml_metal_init: allocating
0.00.447.273 I ggml_metal_init: found device: Apple M4
0.00.447.286 I ggml_metal_init: picking default device: Apple M4
0.00.448.872 I ggml_metal_init: using embedded metal library
0.00.455.534 I ggml_metal_init: GPU name:   Apple M4
0.00.455.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.540 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.542 I ggml_metal_init: simdgroup reduction   = true
0.00.455.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.543 I ggml_metal_init: has residency sets    = true
0.00.455.543 I ggml_metal_init: has bfloat            = true
0.00.455.543 I ggml_metal_init: use bfloat            = true
0.00.455.545 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.474.822 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.532.442 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.532.451 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.473 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.537.173 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.537.175 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.537.175 I llama_init_from_model: graph nodes  = 967
0.00.537.175 I llama_init_from_model: graph splits = 2
0.00.537.185 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.537.317 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.537.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.595.136 I main: llama threadpool init, n_threads = 4
0.00.595.179 I 
0.00.595.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.595.201 I 
0.00.595.354 I sampler seed: 1234
0.00.595.358 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.595.382 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.595.382 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.595.382 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.336.132 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.336.133 I llama_perf_context_print:        load time =     584.99 ms
0.01.336.134 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.40 tokens per second)
0.01.336.135 I llama_perf_context_print:        eval time =     689.11 ms /    63 runs   (   10.94 ms per token,    91.42 tokens per second)
0.01.336.135 I llama_perf_context_print:       total time =     741.77 ms /    70 tokens
0.01.336.366 I ggml_metal_free: deallocating

real	0m1.355s
user	0m0.111s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.999 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.008 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.012 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.012 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.013 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.014 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.014 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.017 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.018 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.769 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.748 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.030.588 I llama_model_loader: - type  f32:  194 tensors
0.00.030.588 I llama_model_loader: - type q3_K:   25 tensors
0.00.030.588 I llama_model_loader: - type q4_K:   71 tensors
0.00.030.589 I llama_model_loader: - type q5_K:    1 tensors
0.00.030.589 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.590 I print_info: file format = GGUF V3 (latest)
0.00.030.590 I print_info: file type   = Q3_K - Medium
0.00.030.591 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.039.268 I load: special tokens cache size = 25
0.00.045.622 I load: token to piece cache size = 0.2984 MB
0.00.045.636 I print_info: arch             = gptneox
0.00.045.637 I print_info: vocab_only       = 0
0.00.045.637 I print_info: n_ctx_train      = 2048
0.00.045.637 I print_info: n_embd           = 2048
0.00.045.637 I print_info: n_layer          = 24
0.00.045.641 I print_info: n_head           = 16
0.00.045.642 I print_info: n_head_kv        = 16
0.00.045.642 I print_info: n_rot            = 32
0.00.045.642 I print_info: n_swa            = 0
0.00.045.642 I print_info: n_embd_head_k    = 128
0.00.045.642 I print_info: n_embd_head_v    = 128
0.00.045.643 I print_info: n_gqa            = 1
0.00.045.644 I print_info: n_embd_k_gqa     = 2048
0.00.045.647 I print_info: n_embd_v_gqa     = 2048
0.00.045.647 I print_info: f_norm_eps       = 1.0e-05
0.00.045.648 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.648 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.648 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.652 I print_info: f_logit_scale    = 0.0e+00
0.00.045.653 I print_info: n_ff             = 8192
0.00.045.653 I print_info: n_expert         = 0
0.00.045.654 I print_info: n_expert_used    = 0
0.00.045.655 I print_info: causal attn      = 1
0.00.045.656 I print_info: pooling type     = 0
0.00.045.657 I print_info: rope type        = 2
0.00.045.658 I print_info: rope scaling     = linear
0.00.045.659 I print_info: freq_base_train  = 10000.0
0.00.045.659 I print_info: freq_scale_train = 1
0.00.045.659 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.659 I print_info: rope_finetuned   = unknown
0.00.045.660 I print_info: ssm_d_conv       = 0
0.00.045.660 I print_info: ssm_d_inner      = 0
0.00.045.660 I print_info: ssm_d_state      = 0
0.00.045.660 I print_info: ssm_dt_rank      = 0
0.00.045.660 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.660 I print_info: model type       = 1.4B
0.00.045.661 I print_info: model params     = 1.41 B
0.00.045.661 I print_info: general.name     = 1.4B
0.00.045.661 I print_info: vocab type       = BPE
0.00.045.661 I print_info: n_vocab          = 50304
0.00.045.662 I print_info: n_merges         = 50009
0.00.045.662 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.662 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.662 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.662 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.662 I print_info: LF token         = 187 ''
0.00.045.663 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.663 I print_info: max token length = 1024
0.00.045.663 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.583.631 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.643 I load_tensors: offloading output layer to GPU
0.00.583.644 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.676 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.583.677 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.585.240 I llama_init_from_model: n_seq_max     = 1
0.00.585.244 I llama_init_from_model: n_ctx         = 128
0.00.585.244 I llama_init_from_model: n_ctx_per_seq = 128
0.00.585.245 I llama_init_from_model: n_batch       = 128
0.00.585.245 I llama_init_from_model: n_ubatch      = 128
0.00.585.246 I llama_init_from_model: flash_attn    = 0
0.00.585.247 I llama_init_from_model: freq_base     = 10000.0
0.00.585.248 I llama_init_from_model: freq_scale    = 1
0.00.585.248 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.585.251 I ggml_metal_init: allocating
0.00.585.333 I ggml_metal_init: found device: Apple M4
0.00.585.347 I ggml_metal_init: picking default device: Apple M4
0.00.586.916 I ggml_metal_init: using embedded metal library
0.00.593.314 I ggml_metal_init: GPU name:   Apple M4
0.00.593.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.323 I ggml_metal_init: simdgroup reduction   = true
0.00.593.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.324 I ggml_metal_init: has residency sets    = true
0.00.593.324 I ggml_metal_init: has bfloat            = true
0.00.593.324 I ggml_metal_init: use bfloat            = true
0.00.593.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.159 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.715 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.615.720 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.746 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.904 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.618.906 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.618.906 I llama_init_from_model: graph nodes  = 967
0.00.618.907 I llama_init_from_model: graph splits = 2
0.00.618.910 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.618.910 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.664 I 
0.00.650.764 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.772 I perplexity: tokenizing the input ..
0.00.657.910 I perplexity: tokenization took 7.134 ms
0.00.657.918 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.812 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.805.143 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.805.168 I llama_perf_context_print:        load time =     641.84 ms
0.00.805.169 I llama_perf_context_print: prompt eval time =     144.92 ms /   128 tokens (    1.13 ms per token,   883.23 tokens per second)
0.00.805.169 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.170 I llama_perf_context_print:       total time =     154.51 ms /   129 tokens
0.00.805.537 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.080s
sys	0m0.128s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.089 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.427 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.434 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.440 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.441 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.442 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.442 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.946 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.947 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.948 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.948 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.948 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.949 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.949 I llama_model_loader: - type  f32:  194 tensors
0.00.025.950 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.950 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.950 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.951 I print_info: file format = GGUF V3 (latest)
0.00.025.951 I print_info: file type   = Q4_K - Medium
0.00.025.952 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.796 I load: special tokens cache size = 25
0.00.040.134 I load: token to piece cache size = 0.2984 MB
0.00.040.148 I print_info: arch             = gptneox
0.00.040.149 I print_info: vocab_only       = 0
0.00.040.149 I print_info: n_ctx_train      = 2048
0.00.040.150 I print_info: n_embd           = 2048
0.00.040.150 I print_info: n_layer          = 24
0.00.040.153 I print_info: n_head           = 16
0.00.040.153 I print_info: n_head_kv        = 16
0.00.040.154 I print_info: n_rot            = 32
0.00.040.154 I print_info: n_swa            = 0
0.00.040.154 I print_info: n_embd_head_k    = 128
0.00.040.154 I print_info: n_embd_head_v    = 128
0.00.040.155 I print_info: n_gqa            = 1
0.00.040.156 I print_info: n_embd_k_gqa     = 2048
0.00.040.157 I print_info: n_embd_v_gqa     = 2048
0.00.040.157 I print_info: f_norm_eps       = 1.0e-05
0.00.040.158 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.159 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.161 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.161 I print_info: f_logit_scale    = 0.0e+00
0.00.040.161 I print_info: n_ff             = 8192
0.00.040.161 I print_info: n_expert         = 0
0.00.040.163 I print_info: n_expert_used    = 0
0.00.040.163 I print_info: causal attn      = 1
0.00.040.163 I print_info: pooling type     = 0
0.00.040.163 I print_info: rope type        = 2
0.00.040.164 I print_info: rope scaling     = linear
0.00.040.164 I print_info: freq_base_train  = 10000.0
0.00.040.164 I print_info: freq_scale_train = 1
0.00.040.164 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.165 I print_info: rope_finetuned   = unknown
0.00.040.165 I print_info: ssm_d_conv       = 0
0.00.040.165 I print_info: ssm_d_inner      = 0
0.00.040.165 I print_info: ssm_d_state      = 0
0.00.040.165 I print_info: ssm_dt_rank      = 0
0.00.040.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.166 I print_info: model type       = 1.4B
0.00.040.167 I print_info: model params     = 1.41 B
0.00.040.167 I print_info: general.name     = 1.4B
0.00.040.167 I print_info: vocab type       = BPE
0.00.040.167 I print_info: n_vocab          = 50304
0.00.040.167 I print_info: n_merges         = 50009
0.00.040.168 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: LF token         = 187 ''
0.00.040.169 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.169 I print_info: max token length = 1024
0.00.040.171 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.543.817 I load_tensors: offloading 24 repeating layers to GPU
0.00.543.832 I load_tensors: offloading output layer to GPU
0.00.543.833 I load_tensors: offloaded 25/25 layers to GPU
0.00.543.867 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.543.875 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.545.325 I llama_init_from_model: n_seq_max     = 1
0.00.545.329 I llama_init_from_model: n_ctx         = 2048
0.00.545.329 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.545.330 I llama_init_from_model: n_batch       = 2048
0.00.545.330 I llama_init_from_model: n_ubatch      = 512
0.00.545.331 I llama_init_from_model: flash_attn    = 0
0.00.545.333 I llama_init_from_model: freq_base     = 10000.0
0.00.545.334 I llama_init_from_model: freq_scale    = 1
0.00.545.336 I ggml_metal_init: allocating
0.00.545.415 I ggml_metal_init: found device: Apple M4
0.00.545.429 I ggml_metal_init: picking default device: Apple M4
0.00.547.052 I ggml_metal_init: using embedded metal library
0.00.553.072 I ggml_metal_init: GPU name:   Apple M4
0.00.553.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.553.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.553.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.553.079 I ggml_metal_init: simdgroup reduction   = true
0.00.553.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.553.080 I ggml_metal_init: has residency sets    = true
0.00.553.080 I ggml_metal_init: has bfloat            = true
0.00.553.081 I ggml_metal_init: use bfloat            = true
0.00.553.082 I ggml_metal_init: hasUnifiedMemory      = true
0.00.553.083 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.572.235 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.942 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.630.948 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.630.981 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.172 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.636.174 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.636.175 I llama_init_from_model: graph nodes  = 967
0.00.636.175 I llama_init_from_model: graph splits = 2
0.00.636.181 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.636.302 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.636.303 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.688 I main: llama threadpool init, n_threads = 4
0.00.693.737 I 
0.00.693.760 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.760 I 
0.00.693.914 I sampler seed: 1234
0.00.693.919 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.933 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.933 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.934 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.455.036 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49685.09 tokens per second)
0.01.455.036 I llama_perf_context_print:        load time =     682.87 ms
0.01.455.038 I llama_perf_context_print: prompt eval time =      57.41 ms /     7 tokens (    8.20 ms per token,   121.92 tokens per second)
0.01.455.039 I llama_perf_context_print:        eval time =     700.77 ms /    63 runs   (   11.12 ms per token,    89.90 tokens per second)
0.01.455.039 I llama_perf_context_print:       total time =     762.07 ms /    70 tokens
0.01.455.286 I ggml_metal_free: deallocating

real	0m1.475s
user	0m0.108s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.067 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.148 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.030.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.157 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.157 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.158 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.159 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.159 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.160 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.162 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.805 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.843 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.578 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.579 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.580 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.580 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.581 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.581 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.038.582 I llama_model_loader: - type  f32:  194 tensors
0.00.038.582 I llama_model_loader: - type q4_K:   61 tensors
0.00.038.582 I llama_model_loader: - type q5_K:   24 tensors
0.00.038.582 I llama_model_loader: - type q6_K:   13 tensors
0.00.038.583 I print_info: file format = GGUF V3 (latest)
0.00.038.583 I print_info: file type   = Q4_K - Medium
0.00.038.586 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.047.145 I load: special tokens cache size = 25
0.00.055.714 I load: token to piece cache size = 0.2984 MB
0.00.055.729 I print_info: arch             = gptneox
0.00.055.730 I print_info: vocab_only       = 0
0.00.055.730 I print_info: n_ctx_train      = 2048
0.00.055.731 I print_info: n_embd           = 2048
0.00.055.731 I print_info: n_layer          = 24
0.00.055.735 I print_info: n_head           = 16
0.00.055.736 I print_info: n_head_kv        = 16
0.00.055.736 I print_info: n_rot            = 32
0.00.055.736 I print_info: n_swa            = 0
0.00.055.736 I print_info: n_embd_head_k    = 128
0.00.055.737 I print_info: n_embd_head_v    = 128
0.00.055.737 I print_info: n_gqa            = 1
0.00.055.738 I print_info: n_embd_k_gqa     = 2048
0.00.055.739 I print_info: n_embd_v_gqa     = 2048
0.00.055.739 I print_info: f_norm_eps       = 1.0e-05
0.00.055.740 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.740 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.740 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.740 I print_info: f_logit_scale    = 0.0e+00
0.00.055.741 I print_info: n_ff             = 8192
0.00.055.741 I print_info: n_expert         = 0
0.00.055.741 I print_info: n_expert_used    = 0
0.00.055.741 I print_info: causal attn      = 1
0.00.055.741 I print_info: pooling type     = 0
0.00.055.742 I print_info: rope type        = 2
0.00.055.742 I print_info: rope scaling     = linear
0.00.055.742 I print_info: freq_base_train  = 10000.0
0.00.055.742 I print_info: freq_scale_train = 1
0.00.055.742 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.743 I print_info: rope_finetuned   = unknown
0.00.055.743 I print_info: ssm_d_conv       = 0
0.00.055.743 I print_info: ssm_d_inner      = 0
0.00.055.743 I print_info: ssm_d_state      = 0
0.00.055.743 I print_info: ssm_dt_rank      = 0
0.00.055.743 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.743 I print_info: model type       = 1.4B
0.00.055.746 I print_info: model params     = 1.41 B
0.00.055.746 I print_info: general.name     = 1.4B
0.00.055.746 I print_info: vocab type       = BPE
0.00.055.746 I print_info: n_vocab          = 50304
0.00.055.750 I print_info: n_merges         = 50009
0.00.055.750 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.750 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.751 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.751 I print_info: LF token         = 187 ''
0.00.055.751 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.751 I print_info: max token length = 1024
0.00.055.752 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.563.881 I load_tensors: offloading 24 repeating layers to GPU
0.00.563.898 I load_tensors: offloading output layer to GPU
0.00.563.898 I load_tensors: offloaded 25/25 layers to GPU
0.00.563.937 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.563.938 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.565.697 I llama_init_from_model: n_seq_max     = 1
0.00.565.700 I llama_init_from_model: n_ctx         = 128
0.00.565.700 I llama_init_from_model: n_ctx_per_seq = 128
0.00.565.701 I llama_init_from_model: n_batch       = 128
0.00.565.701 I llama_init_from_model: n_ubatch      = 128
0.00.565.702 I llama_init_from_model: flash_attn    = 0
0.00.565.704 I llama_init_from_model: freq_base     = 10000.0
0.00.565.705 I llama_init_from_model: freq_scale    = 1
0.00.565.705 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.565.710 I ggml_metal_init: allocating
0.00.565.844 I ggml_metal_init: found device: Apple M4
0.00.565.859 I ggml_metal_init: picking default device: Apple M4
0.00.567.505 I ggml_metal_init: using embedded metal library
0.00.574.065 I ggml_metal_init: GPU name:   Apple M4
0.00.574.070 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.574.071 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.574.072 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.574.072 I ggml_metal_init: simdgroup reduction   = true
0.00.574.072 I ggml_metal_init: simdgroup matrix mul. = true
0.00.574.073 I ggml_metal_init: has residency sets    = true
0.00.574.073 I ggml_metal_init: has bfloat            = true
0.00.574.073 I ggml_metal_init: use bfloat            = true
0.00.574.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.574.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.591.928 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.595.386 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.595.389 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.595.430 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.610 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.598.612 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.598.612 I llama_init_from_model: graph nodes  = 967
0.00.598.612 I llama_init_from_model: graph splits = 2
0.00.598.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.598.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.563 I 
0.00.629.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.665 I perplexity: tokenizing the input ..
0.00.637.018 I perplexity: tokenization took 7.351 ms
0.00.637.027 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.588 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.783.924 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.783.950 I llama_perf_context_print:        load time =     614.49 ms
0.00.783.951 I llama_perf_context_print: prompt eval time =     145.22 ms /   128 tokens (    1.13 ms per token,   881.39 tokens per second)
0.00.783.952 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.952 I llama_perf_context_print:       total time =     154.39 ms /   129 tokens
0.00.784.306 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.082s
sys	0m0.125s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.264 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.922 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.927 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.927 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.927 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.929 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.931 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.932 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.701 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.493 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.495 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.495 I llama_model_loader: - type  f32:  194 tensors
0.00.025.495 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.496 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.496 I print_info: file format = GGUF V3 (latest)
0.00.025.497 I print_info: file type   = Q5_K - Medium
0.00.025.501 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.318 I load: special tokens cache size = 25
0.00.039.735 I load: token to piece cache size = 0.2984 MB
0.00.039.749 I print_info: arch             = gptneox
0.00.039.750 I print_info: vocab_only       = 0
0.00.039.750 I print_info: n_ctx_train      = 2048
0.00.039.750 I print_info: n_embd           = 2048
0.00.039.751 I print_info: n_layer          = 24
0.00.039.754 I print_info: n_head           = 16
0.00.039.755 I print_info: n_head_kv        = 16
0.00.039.755 I print_info: n_rot            = 32
0.00.039.755 I print_info: n_swa            = 0
0.00.039.755 I print_info: n_embd_head_k    = 128
0.00.039.755 I print_info: n_embd_head_v    = 128
0.00.039.756 I print_info: n_gqa            = 1
0.00.039.757 I print_info: n_embd_k_gqa     = 2048
0.00.039.757 I print_info: n_embd_v_gqa     = 2048
0.00.039.758 I print_info: f_norm_eps       = 1.0e-05
0.00.039.758 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.759 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.759 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.760 I print_info: f_logit_scale    = 0.0e+00
0.00.039.760 I print_info: n_ff             = 8192
0.00.039.760 I print_info: n_expert         = 0
0.00.039.761 I print_info: n_expert_used    = 0
0.00.039.761 I print_info: causal attn      = 1
0.00.039.761 I print_info: pooling type     = 0
0.00.039.761 I print_info: rope type        = 2
0.00.039.761 I print_info: rope scaling     = linear
0.00.039.762 I print_info: freq_base_train  = 10000.0
0.00.039.762 I print_info: freq_scale_train = 1
0.00.039.762 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.762 I print_info: rope_finetuned   = unknown
0.00.039.762 I print_info: ssm_d_conv       = 0
0.00.039.762 I print_info: ssm_d_inner      = 0
0.00.039.764 I print_info: ssm_d_state      = 0
0.00.039.764 I print_info: ssm_dt_rank      = 0
0.00.039.764 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.764 I print_info: model type       = 1.4B
0.00.039.764 I print_info: model params     = 1.41 B
0.00.039.765 I print_info: general.name     = 1.4B
0.00.039.765 I print_info: vocab type       = BPE
0.00.039.765 I print_info: n_vocab          = 50304
0.00.039.765 I print_info: n_merges         = 50009
0.00.039.766 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.766 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.766 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: LF token         = 187 ''
0.00.039.767 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: max token length = 1024
0.00.039.767 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.915 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.919 I load_tensors: offloading output layer to GPU
0.00.593.920 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.943 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.593.944 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.595.451 I llama_init_from_model: n_seq_max     = 1
0.00.595.453 I llama_init_from_model: n_ctx         = 2048
0.00.595.454 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.595.454 I llama_init_from_model: n_batch       = 2048
0.00.595.455 I llama_init_from_model: n_ubatch      = 512
0.00.595.455 I llama_init_from_model: flash_attn    = 0
0.00.595.457 I llama_init_from_model: freq_base     = 10000.0
0.00.595.457 I llama_init_from_model: freq_scale    = 1
0.00.595.459 I ggml_metal_init: allocating
0.00.595.477 I ggml_metal_init: found device: Apple M4
0.00.595.487 I ggml_metal_init: picking default device: Apple M4
0.00.596.838 I ggml_metal_init: using embedded metal library
0.00.602.903 I ggml_metal_init: GPU name:   Apple M4
0.00.602.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.908 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.909 I ggml_metal_init: simdgroup reduction   = true
0.00.602.909 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.909 I ggml_metal_init: has residency sets    = true
0.00.602.910 I ggml_metal_init: has bfloat            = true
0.00.602.910 I ggml_metal_init: use bfloat            = true
0.00.602.911 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.912 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.950 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.637 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.669.643 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.669.665 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.920 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.673.922 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.673.923 I llama_init_from_model: graph nodes  = 967
0.00.673.923 I llama_init_from_model: graph splits = 2
0.00.673.929 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.674.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.674.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.275 I main: llama threadpool init, n_threads = 4
0.00.739.321 I 
0.00.739.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.340 I 
0.00.739.497 I sampler seed: 1234
0.00.739.502 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.543 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.545 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.548 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.590.469 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.590.470 I llama_perf_context_print:        load time =     729.29 ms
0.01.590.471 I llama_perf_context_print: prompt eval time =      52.95 ms /     7 tokens (    7.56 ms per token,   132.21 tokens per second)
0.01.590.472 I llama_perf_context_print:        eval time =     795.11 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.590.472 I llama_perf_context_print:       total time =     851.91 ms /    70 tokens
0.01.590.718 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.108s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.960 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.724 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.726 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.726 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.727 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.727 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.727 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.728 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.729 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.729 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.730 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.730 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.730 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.731 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.499 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.492 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.201 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.202 I llama_model_loader: - type  f32:  194 tensors
0.00.026.202 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.203 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.203 I print_info: file format = GGUF V3 (latest)
0.00.026.204 I print_info: file type   = Q5_K - Medium
0.00.026.205 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.638 I load: special tokens cache size = 25
0.00.041.060 I load: token to piece cache size = 0.2984 MB
0.00.041.078 I print_info: arch             = gptneox
0.00.041.079 I print_info: vocab_only       = 0
0.00.041.079 I print_info: n_ctx_train      = 2048
0.00.041.079 I print_info: n_embd           = 2048
0.00.041.079 I print_info: n_layer          = 24
0.00.041.084 I print_info: n_head           = 16
0.00.041.084 I print_info: n_head_kv        = 16
0.00.041.084 I print_info: n_rot            = 32
0.00.041.084 I print_info: n_swa            = 0
0.00.041.085 I print_info: n_embd_head_k    = 128
0.00.041.085 I print_info: n_embd_head_v    = 128
0.00.041.085 I print_info: n_gqa            = 1
0.00.041.086 I print_info: n_embd_k_gqa     = 2048
0.00.041.087 I print_info: n_embd_v_gqa     = 2048
0.00.041.087 I print_info: f_norm_eps       = 1.0e-05
0.00.041.089 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.089 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.090 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.090 I print_info: f_logit_scale    = 0.0e+00
0.00.041.090 I print_info: n_ff             = 8192
0.00.041.090 I print_info: n_expert         = 0
0.00.041.092 I print_info: n_expert_used    = 0
0.00.041.092 I print_info: causal attn      = 1
0.00.041.092 I print_info: pooling type     = 0
0.00.041.093 I print_info: rope type        = 2
0.00.041.093 I print_info: rope scaling     = linear
0.00.041.093 I print_info: freq_base_train  = 10000.0
0.00.041.093 I print_info: freq_scale_train = 1
0.00.041.093 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.094 I print_info: rope_finetuned   = unknown
0.00.041.094 I print_info: ssm_d_conv       = 0
0.00.041.094 I print_info: ssm_d_inner      = 0
0.00.041.094 I print_info: ssm_d_state      = 0
0.00.041.094 I print_info: ssm_dt_rank      = 0
0.00.041.094 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.094 I print_info: model type       = 1.4B
0.00.041.095 I print_info: model params     = 1.41 B
0.00.041.095 I print_info: general.name     = 1.4B
0.00.041.095 I print_info: vocab type       = BPE
0.00.041.095 I print_info: n_vocab          = 50304
0.00.041.096 I print_info: n_merges         = 50009
0.00.041.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.097 I print_info: LF token         = 187 ''
0.00.041.097 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.097 I print_info: max token length = 1024
0.00.041.098 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.595.455 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.471 I load_tensors: offloading output layer to GPU
0.00.595.472 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.507 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.595.508 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.597.238 I llama_init_from_model: n_seq_max     = 1
0.00.597.243 I llama_init_from_model: n_ctx         = 128
0.00.597.243 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.244 I llama_init_from_model: n_batch       = 128
0.00.597.244 I llama_init_from_model: n_ubatch      = 128
0.00.597.244 I llama_init_from_model: flash_attn    = 0
0.00.597.246 I llama_init_from_model: freq_base     = 10000.0
0.00.597.246 I llama_init_from_model: freq_scale    = 1
0.00.597.247 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.250 I ggml_metal_init: allocating
0.00.597.308 I ggml_metal_init: found device: Apple M4
0.00.597.322 I ggml_metal_init: picking default device: Apple M4
0.00.598.675 I ggml_metal_init: using embedded metal library
0.00.605.049 I ggml_metal_init: GPU name:   Apple M4
0.00.605.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.054 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.055 I ggml_metal_init: simdgroup reduction   = true
0.00.605.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.056 I ggml_metal_init: has residency sets    = true
0.00.605.056 I ggml_metal_init: has bfloat            = true
0.00.605.056 I ggml_metal_init: use bfloat            = true
0.00.605.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.961 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.524 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.625.528 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.625.557 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.715 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.628.717 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.628.717 I llama_init_from_model: graph nodes  = 967
0.00.628.718 I llama_init_from_model: graph splits = 2
0.00.628.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.628.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.187 I 
0.00.659.284 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.291 I perplexity: tokenizing the input ..
0.00.666.580 I perplexity: tokenization took 7.285 ms
0.00.666.588 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.012 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.806.449 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.806.475 I llama_perf_context_print:        load time =     650.22 ms
0.00.806.476 I llama_perf_context_print: prompt eval time =     137.50 ms /   128 tokens (    1.07 ms per token,   930.93 tokens per second)
0.00.806.477 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.806.477 I llama_perf_context_print:       total time =     147.29 ms /   129 tokens
0.00.806.871 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.079s
sys	0m0.140s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.564 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.571 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.572 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.295 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.966 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.967 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.967 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.968 I llama_model_loader: - type  f32:  194 tensors
0.00.023.968 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.969 I print_info: file format = GGUF V3 (latest)
0.00.023.969 I print_info: file type   = Q6_K
0.00.023.970 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.822 I load: special tokens cache size = 25
0.00.038.196 I load: token to piece cache size = 0.2984 MB
0.00.038.210 I print_info: arch             = gptneox
0.00.038.211 I print_info: vocab_only       = 0
0.00.038.211 I print_info: n_ctx_train      = 2048
0.00.038.212 I print_info: n_embd           = 2048
0.00.038.212 I print_info: n_layer          = 24
0.00.038.214 I print_info: n_head           = 16
0.00.038.215 I print_info: n_head_kv        = 16
0.00.038.215 I print_info: n_rot            = 32
0.00.038.215 I print_info: n_swa            = 0
0.00.038.215 I print_info: n_embd_head_k    = 128
0.00.038.215 I print_info: n_embd_head_v    = 128
0.00.038.216 I print_info: n_gqa            = 1
0.00.038.217 I print_info: n_embd_k_gqa     = 2048
0.00.038.218 I print_info: n_embd_v_gqa     = 2048
0.00.038.218 I print_info: f_norm_eps       = 1.0e-05
0.00.038.218 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.219 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.219 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.219 I print_info: f_logit_scale    = 0.0e+00
0.00.038.220 I print_info: n_ff             = 8192
0.00.038.221 I print_info: n_expert         = 0
0.00.038.221 I print_info: n_expert_used    = 0
0.00.038.222 I print_info: causal attn      = 1
0.00.038.222 I print_info: pooling type     = 0
0.00.038.222 I print_info: rope type        = 2
0.00.038.222 I print_info: rope scaling     = linear
0.00.038.227 I print_info: freq_base_train  = 10000.0
0.00.038.229 I print_info: freq_scale_train = 1
0.00.038.229 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.230 I print_info: rope_finetuned   = unknown
0.00.038.230 I print_info: ssm_d_conv       = 0
0.00.038.230 I print_info: ssm_d_inner      = 0
0.00.038.230 I print_info: ssm_d_state      = 0
0.00.038.231 I print_info: ssm_dt_rank      = 0
0.00.038.231 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.231 I print_info: model type       = 1.4B
0.00.038.232 I print_info: model params     = 1.41 B
0.00.038.232 I print_info: general.name     = 1.4B
0.00.038.232 I print_info: vocab type       = BPE
0.00.038.232 I print_info: n_vocab          = 50304
0.00.038.233 I print_info: n_merges         = 50009
0.00.038.233 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.233 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.233 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.233 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.234 I print_info: LF token         = 187 ''
0.00.038.235 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.235 I print_info: max token length = 1024
0.00.038.235 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.247 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.250 I load_tensors: offloading output layer to GPU
0.00.638.251 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.275 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.638.277 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.876 I llama_init_from_model: n_seq_max     = 1
0.00.639.878 I llama_init_from_model: n_ctx         = 2048
0.00.639.879 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.879 I llama_init_from_model: n_batch       = 2048
0.00.639.879 I llama_init_from_model: n_ubatch      = 512
0.00.639.880 I llama_init_from_model: flash_attn    = 0
0.00.639.880 I llama_init_from_model: freq_base     = 10000.0
0.00.639.881 I llama_init_from_model: freq_scale    = 1
0.00.639.882 I ggml_metal_init: allocating
0.00.639.917 I ggml_metal_init: found device: Apple M4
0.00.639.926 I ggml_metal_init: picking default device: Apple M4
0.00.641.185 I ggml_metal_init: using embedded metal library
0.00.647.108 I ggml_metal_init: GPU name:   Apple M4
0.00.647.111 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.112 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.113 I ggml_metal_init: simdgroup reduction   = true
0.00.647.113 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.114 I ggml_metal_init: has residency sets    = true
0.00.647.114 I ggml_metal_init: has bfloat            = true
0.00.647.114 I ggml_metal_init: use bfloat            = true
0.00.647.115 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.606 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.483 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.723.488 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.723.518 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.790 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.727.792 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.727.793 I llama_init_from_model: graph nodes  = 967
0.00.727.793 I llama_init_from_model: graph splits = 2
0.00.727.799 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.727.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.727.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.607 I main: llama threadpool init, n_threads = 4
0.00.790.655 I 
0.00.790.676 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.676 I 
0.00.790.849 I sampler seed: 1234
0.00.790.854 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.869 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.869 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.869 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.676.770 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.676.771 I llama_perf_context_print:        load time =     781.06 ms
0.01.676.772 I llama_perf_context_print: prompt eval time =      57.53 ms /     7 tokens (    8.22 ms per token,   121.68 tokens per second)
0.01.676.772 I llama_perf_context_print:        eval time =     825.45 ms /    63 runs   (   13.10 ms per token,    76.32 tokens per second)
0.01.676.773 I llama_perf_context_print:       total time =     886.89 ms /    70 tokens
0.01.677.050 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.107s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4848 (7cf64f6b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.637 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.642 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.653 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.655 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.655 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.656 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.656 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.656 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.657 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.660 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.660 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.550 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.319 I llama_model_loader: - type  f32:  194 tensors
0.00.024.319 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.320 I print_info: file format = GGUF V3 (latest)
0.00.024.321 I print_info: file type   = Q6_K
0.00.024.322 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.722 I load: special tokens cache size = 25
0.00.039.128 I load: token to piece cache size = 0.2984 MB
0.00.039.143 I print_info: arch             = gptneox
0.00.039.144 I print_info: vocab_only       = 0
0.00.039.144 I print_info: n_ctx_train      = 2048
0.00.039.144 I print_info: n_embd           = 2048
0.00.039.144 I print_info: n_layer          = 24
0.00.039.148 I print_info: n_head           = 16
0.00.039.149 I print_info: n_head_kv        = 16
0.00.039.149 I print_info: n_rot            = 32
0.00.039.149 I print_info: n_swa            = 0
0.00.039.149 I print_info: n_embd_head_k    = 128
0.00.039.149 I print_info: n_embd_head_v    = 128
0.00.039.150 I print_info: n_gqa            = 1
0.00.039.150 I print_info: n_embd_k_gqa     = 2048
0.00.039.151 I print_info: n_embd_v_gqa     = 2048
0.00.039.152 I print_info: f_norm_eps       = 1.0e-05
0.00.039.152 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.152 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.152 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.153 I print_info: f_logit_scale    = 0.0e+00
0.00.039.154 I print_info: n_ff             = 8192
0.00.039.156 I print_info: n_expert         = 0
0.00.039.156 I print_info: n_expert_used    = 0
0.00.039.157 I print_info: causal attn      = 1
0.00.039.157 I print_info: pooling type     = 0
0.00.039.157 I print_info: rope type        = 2
0.00.039.157 I print_info: rope scaling     = linear
0.00.039.157 I print_info: freq_base_train  = 10000.0
0.00.039.158 I print_info: freq_scale_train = 1
0.00.039.158 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.158 I print_info: rope_finetuned   = unknown
0.00.039.158 I print_info: ssm_d_conv       = 0
0.00.039.162 I print_info: ssm_d_inner      = 0
0.00.039.162 I print_info: ssm_d_state      = 0
0.00.039.162 I print_info: ssm_dt_rank      = 0
0.00.039.162 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.162 I print_info: model type       = 1.4B
0.00.039.163 I print_info: model params     = 1.41 B
0.00.039.163 I print_info: general.name     = 1.4B
0.00.039.164 I print_info: vocab type       = BPE
0.00.039.164 I print_info: n_vocab          = 50304
0.00.039.164 I print_info: n_merges         = 50009
0.00.039.164 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: LF token         = 187 ''
0.00.039.165 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: max token length = 1024
0.00.039.167 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.071 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.091 I load_tensors: offloading output layer to GPU
0.00.628.091 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.128 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.628.129 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.629.743 I llama_init_from_model: n_seq_max     = 1
0.00.629.747 I llama_init_from_model: n_ctx         = 128
0.00.629.747 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.748 I llama_init_from_model: n_batch       = 128
0.00.629.748 I llama_init_from_model: n_ubatch      = 128
0.00.629.749 I llama_init_from_model: flash_attn    = 0
0.00.629.751 I llama_init_from_model: freq_base     = 10000.0
0.00.629.752 I llama_init_from_model: freq_scale    = 1
0.00.629.752 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.763 I ggml_metal_init: allocating
0.00.629.841 I ggml_metal_init: found device: Apple M4
0.00.629.856 I ggml_metal_init: picking default device: Apple M4
0.00.631.575 I ggml_metal_init: using embedded metal library
0.00.638.141 I ggml_metal_init: GPU name:   Apple M4
0.00.638.145 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.146 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.147 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.147 I ggml_metal_init: simdgroup reduction   = true
0.00.638.148 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.148 I ggml_metal_init: has residency sets    = true
0.00.638.148 I ggml_metal_init: has bfloat            = true
0.00.638.148 I ggml_metal_init: use bfloat            = true
0.00.638.150 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.153 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.664 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.328 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.332 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.464 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.665.864 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.665.866 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.665.867 I llama_init_from_model: graph nodes  = 967
0.00.665.867 I llama_init_from_model: graph splits = 2
0.00.665.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.870 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.666 I 
0.00.701.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.765 I perplexity: tokenizing the input ..
0.00.708.683 I perplexity: tokenization took 6.916 ms
0.00.708.693 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.335 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.841.665 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.841.688 I llama_perf_context_print:        load time =     692.78 ms
0.00.841.690 I llama_perf_context_print: prompt eval time =     130.84 ms /   128 tokens (    1.02 ms per token,   978.33 tokens per second)
0.00.841.690 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.691 I llama_perf_context_print:       total time =     140.03 ms /   129 tokens
0.00.842.055 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.081s
sys	0m0.150s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4848 (7cf64f6b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1443070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1443077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144307da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144308350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144308900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144308eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144309460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144309a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144309fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14430a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14430a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14430aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14430b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14430c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14430c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14430d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14430d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14430df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14430e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14430edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14430f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14430fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144310350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144310bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144311310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1443117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144311c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1443122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144312790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144312c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144312ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1443135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1443138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144313d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1443141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144314680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144314b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144314fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144315460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144315900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144315da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144316240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1443166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144316b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144316e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144317350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144317860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144318260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144318700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144318ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144319040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1443194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144319980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144319e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14431a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14431a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14431ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14431b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14431b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14431ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14431bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14431c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14431c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14431cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14431cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14431d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14431d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14431ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14431e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14431e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14431eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14431f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14431f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14431fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14431ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1443204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144320a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144320f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1443214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144321a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144321f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1443224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1443229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144322f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144323490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1443239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144323f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144324480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1443249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144324f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144325470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1443259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144325f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144326460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1443269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144326f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144327450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144317d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1443278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144328070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1443285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144328b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144329060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1443295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144329b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14432a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14432a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14432aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14432b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14432b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14432bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14432c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14432c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14432ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14432cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14432d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14432d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14432dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14432e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14432e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14432ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14432ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14432f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14432f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14432fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1443301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144330640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144330ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144330f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144331420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1443318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144331d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144332200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1443326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144332b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144332fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144333480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144333920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144333dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144334260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144334700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144334ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144335040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1443354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144335980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144335e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1443362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144336760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144336c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1443370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144337540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1443379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144337e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144338320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1443387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144338c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144339100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1443395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144339a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144339ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14433a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14433a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14433acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14433b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14433b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14433baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14433bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14433c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14433c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14433cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14433d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14433d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14433db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14433dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14433e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14433e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14433ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14433f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14433f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14433fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144340000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1443404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144340940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144340de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144341280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144341720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144341bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144342060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144342500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1443429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144342e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1443432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144343780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144343cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144344220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144344770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144344cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144345160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144345600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144345aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144345f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1443463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144346880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144346dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144347270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144347710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144347bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144348050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1443484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144348990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1443491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144349730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144349c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14434a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14434a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14434ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14434b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14434b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14434bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14434c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14434c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14434cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14434d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14434d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14434dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14434e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14434e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14434ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14434f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14434f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14434fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144350170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1443506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144350c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144351160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1443516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144351c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144352150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1443526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144352bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144353140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144353690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144353be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144354130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144354680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144354bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144355120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144355670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144355bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144356110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144356660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144356bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144357100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144357650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144357ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1443580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144358640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144358b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1443590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144359630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144359b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14435a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14435a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14435ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14435b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14435b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14435bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14435c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14435c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14435c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14435cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14435d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14435d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14435dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14435e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14435e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14435e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14435ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14435f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14435f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14435fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1443600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x144360560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x144360a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x144360ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x144361340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1443617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x144361c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x144362120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1443625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x144362a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x144362f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144363450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144363b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144364290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1443649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1443650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144365620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144365ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144365f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144366400 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.716.395 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144504b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144504f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144505400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144505870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144505ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144506150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1445065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144506a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144506ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144507310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144507780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144507e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144508990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144509140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144509950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14450a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14450a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14450aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14450b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14450bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14450c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14450cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14450d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14450d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14450e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14450e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14450e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14450ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14450ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14450f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14450f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14450fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144510520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1445109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144510e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144511300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1445117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144511c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1445120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144512580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144512a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144512ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144513360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144513800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144513ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144514140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1445145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144514a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144514f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1445153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144515860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144515d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1445161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144516640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144516ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144516f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144517420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1445176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1445179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144517e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144518280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1445186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144518b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144518fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144519440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1445198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144519d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14451a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14451a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14451aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14451aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14451b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14451b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14451bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14451c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14451c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14451c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14451cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14451d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14451d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14451db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14451dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14451e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14451e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14451ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14451f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14451f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14451fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14451fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144520330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1445207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144520c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144521080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1445214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144521960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144521dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144522240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1445226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144522b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144522f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144523400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144523870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144523ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144524150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1445245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144524a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144524ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144525310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144525780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144525bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144526060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1445264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144526940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144526db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144527220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144527690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144527b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144527f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1445283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144528850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144528cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144529130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1445295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144529a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144529e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14452a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14452a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14452abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14452b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14452b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14452b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14452bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14452c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14452c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14452cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14452cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14452d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14452d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14452dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14452e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14452e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14452e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14452ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14452f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14452f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14452fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144530020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144530490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144530900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144530d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1445311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144531650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144531ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144531f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1445323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144532810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144532c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1445330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144533560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1445339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144533e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1445342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144534720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144534b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144535000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144535470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144535bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144535eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144536320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144536790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144536c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144537070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1445374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144537950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144537dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144538230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1445386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144538b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144538f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1445393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144539860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144539cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14453a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14453a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14453aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14453ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14453b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14453b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14453bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14453c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14453c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14453c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14453cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14453d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14453d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14453daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14453df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14453e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14453e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14453ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144606e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1446072f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144607760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144607cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144608140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1446085b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144608a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144608f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144609460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144609970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14460a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14460a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14460ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14460b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14460b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14460bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14460c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14460cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14460d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14460d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14460dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14460e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14460e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14460ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14460f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14460f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14460fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144610440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144610a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144610fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144611580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144611b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144612100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1446126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144612c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144613240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144613800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144613dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144614380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144614940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144614f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1446154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144615a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144616040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144616600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144616bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144617180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144617740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144617d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1446182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144618880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144618e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144619400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1446199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144619f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14461a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14461ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14461b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14461b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14461bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14461c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14461c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14461cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14461d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14461d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14461dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14461e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14461ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14461ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14461f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14461f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14461fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144620840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144620d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144621240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144621740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144622140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144622640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144622b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144623040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x144623540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x144623a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x144623f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x144624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x144624940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x144624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x144625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x144625840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x144625d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x144626240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144627150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144627870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144627f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1446286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144628970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144629100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1446293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1446298d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144408e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144406c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144409300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144409770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144409be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14440a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14440a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14440a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14440ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14440b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14440b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14440bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14440c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14440d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14440d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14440df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14440e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14440ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14440f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14440fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144410390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144410ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1444111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1444118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144412010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1444122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144412590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144412a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144412e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1444132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1444138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144413db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144414490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144414930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144414dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144415270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144415710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144415bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144416050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1444164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144416990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144416e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1444172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144417770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144417c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1444180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144418550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1444189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144418e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144419330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1444197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144419c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14441a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14441a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14441aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14441aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14441b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14441b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14441b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14441bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14441c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14441c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14441cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14441cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14441d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14441d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14441dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14441e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14441e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14441e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14441ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14441f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14441f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14441fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144420010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144420480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1444208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144420d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1444211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144421640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144421ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144421f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144422390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144422800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144422c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1444230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144423550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1444239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144423e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1444242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144424710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144424b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144424ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144425460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1444258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144425d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1444261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144426620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144426d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144427220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1444277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144427d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144428330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1444288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144428e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144429440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1444299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144429fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14442a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14442ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14442b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14442b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14442bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14442c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14442c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14442cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14442d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14442d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14442dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14442dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14442e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14442e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14442eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14442f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14442f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14442fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1444302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1444307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144430cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1444311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1444316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144431bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1444320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1444325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144432ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144432fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1444334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1444339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144433ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1444343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1444348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144434dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1444352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1444357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144435cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1444361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1444366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144436bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1444370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1444375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144437ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144437fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1444384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1444389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144438ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1444393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1444398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144439dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14443a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14443a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14443acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14443b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14443b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14443bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14443c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14443c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14443cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14443cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14443d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14443d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14443dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14443e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14443e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14443edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14443f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14443f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14443fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1444401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1444406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144440bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1444410c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1444415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144441ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144441fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1444424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1444429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144442ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1444433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1444438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144443dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1444442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1444447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144444cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1444451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144445770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144445d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1444462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144446880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144446d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144447280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144447780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144447e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144448300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1444485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144448b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144449070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144449750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144449bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14444a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14444a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14444ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14444b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14444b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14444bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14444c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14444c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14444ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14444d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14444d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14444ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14444e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14444e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14444eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14444f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14444fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14444ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144450590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144450b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1444510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1444516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144451c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144452200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1444527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144452d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144453310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1444538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144453e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144454420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1444549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144454f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144455530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144455ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144456090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144456640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144456bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1444571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144457750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144457d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1444582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144458860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144458e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1444593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144459970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144459f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14445a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14445aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14445b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14445b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14445bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14445c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14445c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14445cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14445d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14445d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14445ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14445e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14445e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14445eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14445f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14445f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14445fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1444602c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1444607c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144460cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1444611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1444616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144461bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1444620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1444625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144462ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144462fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1444634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1444639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x144463ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1444643c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1444648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x144464dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1444652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1444657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x144465cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1444661c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1444666c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144466bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1444675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144467cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144468410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144468b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144468df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144469580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144469a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144469ec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.788s
user	0m0.279s
sys	0m0.324s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4848 (7cf64f6b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ae0a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ae0ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ae0b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ae0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ae0bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ae0c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ae0ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ae0cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ae0d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ae0da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ae0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ae0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ae0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ae0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ae0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ae10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ae10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ae114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ae11bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ae12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ae12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ae131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ae138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ae14190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ae148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ae14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ae151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ae15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ae15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ae161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ae16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ae16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ae16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ae172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ae17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ae17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ae180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ae18560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ae18a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ae18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ae19340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ae197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ae19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ae1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ae1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ae1a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ae1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ae1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ae1bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ae1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ae1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ae1ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ae1cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ae1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ae1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ae1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ae1e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ae1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ae1eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ae1f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ae1f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ae1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ae1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ae200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ae20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ae20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ae20eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ae21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ae217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ae21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ae22130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ae225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ae22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ae22fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ae23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ae23a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ae23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ae24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ae24a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ae24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ae254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ae25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ae25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ae264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ae26a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ae26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ae274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ae27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ae27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ae284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ae28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ae28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ae294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ae29a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ae29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ae2a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ae2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ae1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ae2ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ae2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ae2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ae2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ae2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ae2cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ae2d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ae2d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ae2db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ae2e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ae2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ae2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ae2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ae2f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ae2fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ae2ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ae30460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ae30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ae30da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ae31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ae316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ae31b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ae32020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ae324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ae32960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ae32e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ae332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ae33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ae33be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ae34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ae34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ae349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ae34e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ae35300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ae357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ae35c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ae360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ae36580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ae36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ae36ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ae37360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ae37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ae37ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ae38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ae385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ae38a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ae38f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ae393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ae39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ae39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ae3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ae3a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ae3aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ae3af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ae3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ae3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ae3bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ae3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ae3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ae3cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ae3cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ae3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ae3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ae3ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ae3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ae3e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ae3eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ae3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ae3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ae3f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ae3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ae402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ae40760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ae40c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ae410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ae41540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ae419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ae41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ae42320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ae427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ae42c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ae43100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ae435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ae43a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ae43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ae44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ae44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ae44cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ae45160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ae45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ae45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ae45f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ae463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ae46880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ae46d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ae47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ae477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ae47d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ae48260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ae48700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ae48ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ae49040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ae494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ae49980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ae49e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ae4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ae4a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ae4acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ae4b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ae4b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ae4ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ae4bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ae4c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ae4ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ae4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ae4d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ae4dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ae4e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ae4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ae4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ae4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ae4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ae4fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ae50290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ae50840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ae50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ae513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ae51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ae51f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ae524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ae52a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ae53010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ae535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ae53b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ae54120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ae546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ae54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ae55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ae557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ae55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ae56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ae568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ae56ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ae57450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ae57a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ae57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ae58560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ae58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ae590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ae59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ae59c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ae5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ae5a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ae5ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ae5b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ae5b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ae5be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ae5c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ae5c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ae5cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ae5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ae5dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ae5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ae5e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ae5ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ae5f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ae5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ae5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ae60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ae60830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ae60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ae61230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ae61730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ae61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ae62130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ae62630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ae62b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ae63030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ae63530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ae63a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ae63f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ae64430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ae64930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ae64e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10ae65330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10ae65830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10ae65d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10ae66230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10ae66730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10ae66c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10ae67130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10ae67630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10ae67b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10ae68030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ae68530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ae68f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ae69660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ae69d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ae6a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ae6a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ae6aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ae6b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ae6b830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.881 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ae0b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ae0b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ae0b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ae0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ae0c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ae0c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ae0cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ae0cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ae0d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ae0d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ae0ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ae0e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ae0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ae0f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ae0ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ae10650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ae10d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ae11490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ae11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ae12380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ae12aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ae131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ae138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ae14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ae14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ae14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ae15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ae15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ae159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ae15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ae162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ae16780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ae16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ae16ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ae17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ae17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ae17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ae18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ae18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ae18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ae18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ae193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ae19880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ae19d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ae1a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ae1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ae1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ae1afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ae1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ae1b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ae1bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ae1c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ae1c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ae1cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ae1d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ae1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ae1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ae1dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ae1dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ae1e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ae1e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ae1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ae1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ae1f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ae1f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ae1fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ae20240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ae206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ae20b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ae20f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ae21400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ae21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ae21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ae22150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ae225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ae22a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ae22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ae23310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ae23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ae23bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ae24060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ae244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ae24940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ae24db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ae25220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ae25690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ae25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ae25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ae263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ae26850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ae26cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ae27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ae275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ae27a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ae27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ae282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ae28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ae28bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ae29040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ae294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ae29920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ae29d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ae2a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ae2a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ae2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ae2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ae2b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ae2b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ae2bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ae2c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ae2c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ae2c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ae2ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ae2d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ae2d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ae2dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ae2e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ae2e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ae2e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ae2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ae2f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ae2f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ae2fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ae2ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ae303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ae30810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ae30c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ae310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ae31560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ae319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ae31e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ae322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ae32720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ae32b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ae33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ae33470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ae338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ae33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ae341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ae34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ae34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ae34f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ae35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ae357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ae35c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ae360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ae36540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ae369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ae36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ae37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ae37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ae37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ae37fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ae38450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ae388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ae38d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ae391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ae39610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ae39a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ae39ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ae3a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ae3a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ae3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ae3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ae3b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ae3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ae3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ae3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ae3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ae3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ae3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ae3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ae3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ae3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ae3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ae3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ae3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ae3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ae3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ae3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ae3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ae401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ae40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ae40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ae40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ae413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ae41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ae41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ae42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ae42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ae429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ae42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ae432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ae43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ae43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ae44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ae44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ae448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ae44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ae451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ae45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ae45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ae46280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ae46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ae46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ae46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ae476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ae47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ae48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ae484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ae48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ae48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ae49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ae49b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ae4a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ae4a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ae4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ae4b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ae4b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ae4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ae4c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ae4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ae4ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ae4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ae4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ae4df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ae4e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ae4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ae4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ae4f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ae4fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ae50180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ae50730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ae50ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ae51290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ae51840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ae51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ae523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ae52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ae52f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ae534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ae53a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ae54010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ae545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ae54b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ae55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ae556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ae55c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ae56230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ae567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ae56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ae57340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ae578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ae57ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ae58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ae58a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ae58fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ae59560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ae59b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ae5a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ae5a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ae5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ae5b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ae5b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ae5bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ae5c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ae5c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ae5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ae5d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ae5d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ae5dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ae5e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ae5e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ae5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ae5f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ae5f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ae5fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ae60040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ae60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ae60a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ae60f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ae61440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12ae61940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12ae61e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12ae62340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12ae62840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12ae62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12ae63240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12ae63740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12ae63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12ae64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12ae64640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ae64b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ae65550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ae65c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ae66390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ae66ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ae66d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ae67500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ae679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ae67e40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ae4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ae56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ae554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ae521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ae4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ae5ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ae5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ae5a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ae58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ae50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ae4dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ae52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ae53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ae59380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ae56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ae5dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ae50b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ae51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ae58dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ae5aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ae53880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ae54990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ae59ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ae56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ae57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ae51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ae52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ae5f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ae5cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ae4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ae57cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ae4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ae4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ae5f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ae54f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ae687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ae5d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ae532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ae55aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ae59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ae510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ae5b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ae4ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ae5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ae5bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ae57710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ae60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ae4ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ae5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ae4e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ae5e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ae58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ae5aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ae5d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ae5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ae543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ae16750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ae6baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ae6bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ae6c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ae6c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ae6c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ae6c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ae6cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ae6ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ae6d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ae6d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ae6d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ae6d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ae6dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ae6deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ae6e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ae6e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ae6e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ae6e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ae6ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ae6ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ae6f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ae6f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ae6f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ae6fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ae6fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ae6ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ae70270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ae70530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ae707f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ae70ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ae70d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ae71030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ae712f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ae715b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ae71870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ae71b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ae71df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ae720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ae72370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ae72630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ae728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ae72bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ae72e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ae73130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ae733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ae736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ae73970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ae73c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ae73ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ae741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ae74470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ae74730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ae749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ae74cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ae74f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ae75230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ae754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ae757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ae75a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ae75d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ae75ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ae762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ae76570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ae76830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ae76af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ae76db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ae77070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ae77330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ae775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ae778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ae77b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ae77e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ae780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ae783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ae78670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ae78930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ae78bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ae78eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ae79170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ae79430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ae796f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ae799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ae79c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ae79f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ae7a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ae7a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ae7a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ae7aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ae7acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ae7afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ae7b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ae7b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ae7b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ae7bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ae7bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ae7c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ae7c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ae7c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ae7c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ae7cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ae7cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ae7d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ae7d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ae7d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ae7d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ae7dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ae7de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ae7e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ae7e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ae7e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ae7e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ae7ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ae7eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ae7f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ae7f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ae7f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ae7f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ae7fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ae7ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ae80230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ae804f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ae807b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ae80a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ae80d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ae80ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ae812b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ae81570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ae81830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ae81af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ae81f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ae82430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ae828d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ae82d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ae83210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ae836b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ae83b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ae840a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ae845f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ae84b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ae85090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ae85350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ae85610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ae85b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ae86010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ae86510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ae86a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ae86f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ae874e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ae879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ae87ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ae885e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ae888a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ae88db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ae89850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ae89b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ae8a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ae8a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ae8ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ae8b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ae8b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ae8bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ae8c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ae8c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ae8ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ae8d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ae8da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ae8e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ae8e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ae8eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ae8f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ae8f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ae8fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ae90290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ae90850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ae90e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ae913d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ae91990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ae91f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ae92510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ae92ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ae93090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ae93650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ae93c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ae941d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ae94790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ae94d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ae95310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ae958d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ae95e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ae96450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ae96a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ae96fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ae97590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ae97b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ae98110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ae986d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ae98c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ae99250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ae99810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ae99dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ae9a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ae9a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ae9af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ae9b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ae9ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ae9c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ae9c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ae9cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ae9d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ae9d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ae9dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ae9e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ae9e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ae9ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ae9f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ae9f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ae9fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10aea0010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10aea0510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10aea0a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10aea0f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10aea1410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10aea1910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10aea1e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10aea2310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10aea2810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10aea2d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10aea3210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10aea3710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10aea3c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10aea4110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10aea4610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10aea4b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10aea5010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10aea5510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10aea5a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10aea6420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10aea6b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10aea7260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10aea7980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10aea7c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10aea83d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10aea8690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10aea8ba0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.961s
user	0m0.231s
sys	0m0.189s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.08 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.51 sec*proc (2 tests)

Total Test time (real) =   1.54 sec
        1.56 real         0.52 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.25 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.57 real         0.13 user         0.08 sys
```
