### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.29 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.13 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.46 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.68 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.24 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.09 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.63 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.23 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.23 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.93 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.99 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.89 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  195.41 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.93 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.38 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 257.49 sec*proc (29 tests)

Total Test time (real) = 257.50 sec

real	4m17.553s
user	8m44.062s
sys	0m7.110s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.98 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.81 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.20 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.20 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.23 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.47 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.86 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.38 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.12 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.02 sec*proc (29 tests)

Total Test time (real) =  55.03 sec

real	0m55.042s
user	1m17.453s
sys	0m6.402s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.107 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.829 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.290 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.297 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.299 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.301 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.301 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.302 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.303 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.304 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.305 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.305 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.306 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.308 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.309 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.310 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.310 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.311 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.311 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.312 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.967 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.969 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.970 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.970 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.971 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.971 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.026.972 I llama_model_loader: - type  f32:  124 tensors
0.00.026.972 I llama_model_loader: - type  f16:   73 tensors
0.00.026.973 I print_info: file format = GGUF V3 (latest)
0.00.026.974 I print_info: file type   = F16
0.00.026.975 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.340 I load: special tokens cache size = 5
0.00.033.467 I load: token to piece cache size = 0.2032 MB
0.00.033.471 I print_info: arch             = bert
0.00.033.472 I print_info: vocab_only       = 0
0.00.033.472 I print_info: n_ctx_train      = 512
0.00.033.472 I print_info: n_embd           = 384
0.00.033.472 I print_info: n_layer          = 12
0.00.033.476 I print_info: n_head           = 12
0.00.033.477 I print_info: n_head_kv        = 12
0.00.033.477 I print_info: n_rot            = 32
0.00.033.477 I print_info: n_swa            = 0
0.00.033.478 I print_info: n_embd_head_k    = 32
0.00.033.478 I print_info: n_embd_head_v    = 32
0.00.033.479 I print_info: n_gqa            = 1
0.00.033.480 I print_info: n_embd_k_gqa     = 384
0.00.033.480 I print_info: n_embd_v_gqa     = 384
0.00.033.481 I print_info: f_norm_eps       = 1.0e-12
0.00.033.482 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.486 I print_info: f_logit_scale    = 0.0e+00
0.00.033.486 I print_info: n_ff             = 1536
0.00.033.487 I print_info: n_expert         = 0
0.00.033.487 I print_info: n_expert_used    = 0
0.00.033.487 I print_info: causal attn      = 0
0.00.033.487 I print_info: pooling type     = 2
0.00.033.488 I print_info: rope type        = 2
0.00.033.488 I print_info: rope scaling     = linear
0.00.033.489 I print_info: freq_base_train  = 10000.0
0.00.033.489 I print_info: freq_scale_train = 1
0.00.033.490 I print_info: n_ctx_orig_yarn  = 512
0.00.033.492 I print_info: rope_finetuned   = unknown
0.00.033.492 I print_info: ssm_d_conv       = 0
0.00.033.492 I print_info: ssm_d_inner      = 0
0.00.033.492 I print_info: ssm_d_state      = 0
0.00.033.492 I print_info: ssm_dt_rank      = 0
0.00.033.492 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.493 I print_info: model type       = 33M
0.00.033.493 I print_info: model params     = 33.21 M
0.00.033.493 I print_info: general.name     = Bge Small
0.00.033.494 I print_info: vocab type       = WPM
0.00.033.495 I print_info: n_vocab          = 30522
0.00.033.495 I print_info: n_merges         = 0
0.00.033.495 I print_info: BOS token        = 101 '[CLS]'
0.00.033.495 I print_info: UNK token        = 100 '[UNK]'
0.00.033.496 I print_info: SEP token        = 102 '[SEP]'
0.00.033.497 I print_info: PAD token        = 0 '[PAD]'
0.00.033.497 I print_info: MASK token       = 103 '[MASK]'
0.00.033.497 I print_info: LF token         = 0 '[PAD]'
0.00.033.503 I print_info: max token length = 21
0.00.036.831 I load_tensors: offloading 12 repeating layers to GPU
0.00.036.833 I load_tensors: offloading output layer to GPU
0.00.036.833 I load_tensors: offloaded 13/13 layers to GPU
0.00.036.856 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.858 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.037.146 I llama_init_from_model: n_seq_max     = 1
0.00.037.147 I llama_init_from_model: n_ctx         = 512
0.00.037.147 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.148 I llama_init_from_model: n_batch       = 2048
0.00.037.148 I llama_init_from_model: n_ubatch      = 2048
0.00.037.148 I llama_init_from_model: flash_attn    = 0
0.00.037.149 I llama_init_from_model: freq_base     = 10000.0
0.00.037.149 I llama_init_from_model: freq_scale    = 1
0.00.037.150 I ggml_metal_init: allocating
0.00.037.161 I ggml_metal_init: found device: Apple M4
0.00.037.165 I ggml_metal_init: picking default device: Apple M4
0.00.037.902 I ggml_metal_init: using embedded metal library
0.00.041.842 I ggml_metal_init: GPU name:   Apple M4
0.00.041.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.846 I ggml_metal_init: simdgroup reduction   = true
0.00.041.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.846 I ggml_metal_init: has residency sets    = true
0.00.041.846 I ggml_metal_init: has bfloat            = true
0.00.041.846 I ggml_metal_init: use bfloat            = true
0.00.041.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.848 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.870 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.054.616 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.054.619 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.640 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.087 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.089 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.089 I llama_init_from_model: graph nodes  = 429
0.00.056.089 I llama_init_from_model: graph splits = 2
0.00.056.091 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.091 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.966 I 
0.00.061.994 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.701 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.067.840 I llama_perf_context_print:        load time =      46.13 ms
0.00.067.841 I llama_perf_context_print: prompt eval time =       5.01 ms /     9 tokens (    0.56 ms per token,  1797.48 tokens per second)
0.00.067.842 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.067.843 I llama_perf_context_print:       total time =       5.88 ms /    10 tokens
0.00.067.987 I ggml_metal_free: deallocating

real	0m0.256s
user	0m0.050s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.042 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.962 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.709 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.714 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.714 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.717 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.717 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.717 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.718 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.719 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.719 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.719 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.720 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.722 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.723 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.723 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.723 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.724 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.724 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.175 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.802 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.804 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.804 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.804 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.804 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.805 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.805 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.806 I llama_model_loader: - type  f32:  124 tensors
0.00.015.806 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.807 I print_info: file format = GGUF V3 (latest)
0.00.015.807 I print_info: file type   = Q8_0
0.00.015.808 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.193 I load: special tokens cache size = 5
0.00.019.508 I load: token to piece cache size = 0.2032 MB
0.00.019.512 I print_info: arch             = bert
0.00.019.512 I print_info: vocab_only       = 0
0.00.019.512 I print_info: n_ctx_train      = 512
0.00.019.513 I print_info: n_embd           = 384
0.00.019.513 I print_info: n_layer          = 12
0.00.019.516 I print_info: n_head           = 12
0.00.019.517 I print_info: n_head_kv        = 12
0.00.019.517 I print_info: n_rot            = 32
0.00.019.517 I print_info: n_swa            = 0
0.00.019.517 I print_info: n_embd_head_k    = 32
0.00.019.520 I print_info: n_embd_head_v    = 32
0.00.019.520 I print_info: n_gqa            = 1
0.00.019.521 I print_info: n_embd_k_gqa     = 384
0.00.019.523 I print_info: n_embd_v_gqa     = 384
0.00.019.523 I print_info: f_norm_eps       = 1.0e-12
0.00.019.524 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.524 I print_info: f_logit_scale    = 0.0e+00
0.00.019.525 I print_info: n_ff             = 1536
0.00.019.525 I print_info: n_expert         = 0
0.00.019.525 I print_info: n_expert_used    = 0
0.00.019.525 I print_info: causal attn      = 0
0.00.019.525 I print_info: pooling type     = 2
0.00.019.527 I print_info: rope type        = 2
0.00.019.527 I print_info: rope scaling     = linear
0.00.019.527 I print_info: freq_base_train  = 10000.0
0.00.019.527 I print_info: freq_scale_train = 1
0.00.019.528 I print_info: n_ctx_orig_yarn  = 512
0.00.019.528 I print_info: rope_finetuned   = unknown
0.00.019.528 I print_info: ssm_d_conv       = 0
0.00.019.528 I print_info: ssm_d_inner      = 0
0.00.019.528 I print_info: ssm_d_state      = 0
0.00.019.528 I print_info: ssm_dt_rank      = 0
0.00.019.528 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.528 I print_info: model type       = 33M
0.00.019.529 I print_info: model params     = 33.21 M
0.00.019.529 I print_info: general.name     = Bge Small
0.00.019.530 I print_info: vocab type       = WPM
0.00.019.530 I print_info: n_vocab          = 30522
0.00.019.530 I print_info: n_merges         = 0
0.00.019.530 I print_info: BOS token        = 101 '[CLS]'
0.00.019.530 I print_info: UNK token        = 100 '[UNK]'
0.00.019.530 I print_info: SEP token        = 102 '[SEP]'
0.00.019.531 I print_info: PAD token        = 0 '[PAD]'
0.00.019.531 I print_info: MASK token       = 103 '[MASK]'
0.00.019.531 I print_info: LF token         = 0 '[PAD]'
0.00.019.531 I print_info: max token length = 21
0.00.021.596 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.597 I load_tensors: offloading output layer to GPU
0.00.021.597 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.605 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.606 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.812 I llama_init_from_model: n_seq_max     = 1
0.00.021.813 I llama_init_from_model: n_ctx         = 512
0.00.021.813 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.814 I llama_init_from_model: n_batch       = 2048
0.00.021.814 I llama_init_from_model: n_ubatch      = 2048
0.00.021.814 I llama_init_from_model: flash_attn    = 0
0.00.021.814 I llama_init_from_model: freq_base     = 10000.0
0.00.021.815 I llama_init_from_model: freq_scale    = 1
0.00.021.815 I ggml_metal_init: allocating
0.00.021.822 I ggml_metal_init: found device: Apple M4
0.00.021.826 I ggml_metal_init: picking default device: Apple M4
0.00.022.442 I ggml_metal_init: using embedded metal library
0.00.025.065 I ggml_metal_init: GPU name:   Apple M4
0.00.025.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.069 I ggml_metal_init: simdgroup reduction   = true
0.00.025.069 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.070 I ggml_metal_init: has residency sets    = true
0.00.025.070 I ggml_metal_init: has bfloat            = true
0.00.025.071 I ggml_metal_init: use bfloat            = true
0.00.025.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.073 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.685 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.036.344 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.036.346 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.036.360 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.037.496 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.037.498 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.037.498 I llama_init_from_model: graph nodes  = 429
0.00.037.498 I llama_init_from_model: graph splits = 2
0.00.037.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.037.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.816 I 
0.00.041.841 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.042.443 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.924 I llama_perf_context_print:        load time =      31.85 ms
0.00.045.926 I llama_perf_context_print: prompt eval time =       3.35 ms /     9 tokens (    0.37 ms per token,  2690.58 tokens per second)
0.00.045.927 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.927 I llama_perf_context_print:       total time =       4.11 ms /    10 tokens
0.00.046.134 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.266 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.606 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.611 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.613 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.621 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.622 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.622 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.623 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.624 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.625 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.626 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.626 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.627 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.630 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.631 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.631 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.634 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.635 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.893 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.057 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.627 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.628 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.628 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.629 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.629 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.629 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.630 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.630 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.631 I llama_model_loader: - type  f32:   40 tensors
0.00.048.631 I llama_model_loader: - type  f16:   30 tensors
0.00.048.632 I print_info: file format = GGUF V3 (latest)
0.00.048.632 I print_info: file type   = F16
0.00.048.634 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.843 W load: empty token at index 5
0.00.057.872 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.405 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.442 I load: special tokens cache size = 5
0.00.323.912 I load: token to piece cache size = 1.5060 MB
0.00.323.917 I print_info: arch             = jina-bert-v2
0.00.323.917 I print_info: vocab_only       = 0
0.00.323.917 I print_info: n_ctx_train      = 8192
0.00.323.918 I print_info: n_embd           = 384
0.00.323.918 I print_info: n_layer          = 4
0.00.323.924 I print_info: n_head           = 12
0.00.323.924 I print_info: n_head_kv        = 12
0.00.323.924 I print_info: n_rot            = 32
0.00.323.924 I print_info: n_swa            = 0
0.00.323.925 I print_info: n_embd_head_k    = 32
0.00.323.925 I print_info: n_embd_head_v    = 32
0.00.323.925 I print_info: n_gqa            = 1
0.00.323.926 I print_info: n_embd_k_gqa     = 384
0.00.323.926 I print_info: n_embd_v_gqa     = 384
0.00.323.929 I print_info: f_norm_eps       = 1.0e-12
0.00.323.930 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.323.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.323.930 I print_info: f_max_alibi_bias = 8.0e+00
0.00.323.932 I print_info: f_logit_scale    = 0.0e+00
0.00.323.933 I print_info: n_ff             = 1536
0.00.323.934 I print_info: n_expert         = 0
0.00.323.934 I print_info: n_expert_used    = 0
0.00.323.934 I print_info: causal attn      = 0
0.00.323.935 I print_info: pooling type     = -1
0.00.323.935 I print_info: rope type        = -1
0.00.323.935 I print_info: rope scaling     = linear
0.00.323.936 I print_info: freq_base_train  = 10000.0
0.00.323.937 I print_info: freq_scale_train = 1
0.00.323.938 I print_info: n_ctx_orig_yarn  = 8192
0.00.323.938 I print_info: rope_finetuned   = unknown
0.00.323.938 I print_info: ssm_d_conv       = 0
0.00.323.938 I print_info: ssm_d_inner      = 0
0.00.323.938 I print_info: ssm_d_state      = 0
0.00.323.938 I print_info: ssm_dt_rank      = 0
0.00.323.938 I print_info: ssm_dt_b_c_rms   = 0
0.00.323.939 I print_info: model type       = 33M
0.00.323.939 I print_info: model params     = 32.90 M
0.00.323.940 I print_info: general.name     = Jina Bert Implementation
0.00.323.941 I print_info: vocab type       = BPE
0.00.323.941 I print_info: n_vocab          = 61056
0.00.323.941 I print_info: n_merges         = 39382
0.00.323.941 I print_info: BOS token        = 0 '<s>'
0.00.323.942 I print_info: EOS token        = 2 '</s>'
0.00.323.942 I print_info: UNK token        = 3 '<unk>'
0.00.323.942 I print_info: SEP token        = 2 '</s>'
0.00.323.943 I print_info: PAD token        = 1 '<pad>'
0.00.323.943 I print_info: MASK token       = 4 '<mask>'
0.00.323.943 I print_info: EOG token        = 2 '</s>'
0.00.323.944 I print_info: max token length = 45
0.00.325.594 I load_tensors: offloading 4 repeating layers to GPU
0.00.325.595 I load_tensors: offloading output layer to GPU
0.00.325.595 I load_tensors: offloaded 5/5 layers to GPU
0.00.325.619 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.325.620 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.325.857 I llama_init_from_model: n_seq_max     = 1
0.00.325.858 I llama_init_from_model: n_ctx         = 8192
0.00.325.858 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.325.859 I llama_init_from_model: n_batch       = 2048
0.00.325.859 I llama_init_from_model: n_ubatch      = 2048
0.00.325.859 I llama_init_from_model: flash_attn    = 0
0.00.325.859 I llama_init_from_model: freq_base     = 10000.0
0.00.325.860 I llama_init_from_model: freq_scale    = 1
0.00.325.860 I ggml_metal_init: allocating
0.00.325.864 I ggml_metal_init: found device: Apple M4
0.00.325.868 I ggml_metal_init: picking default device: Apple M4
0.00.326.603 I ggml_metal_init: using embedded metal library
0.00.329.480 I ggml_metal_init: GPU name:   Apple M4
0.00.329.482 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.329.482 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.329.483 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.329.483 I ggml_metal_init: simdgroup reduction   = true
0.00.329.483 I ggml_metal_init: simdgroup matrix mul. = true
0.00.329.484 I ggml_metal_init: has residency sets    = true
0.00.329.484 I ggml_metal_init: has bfloat            = true
0.00.329.484 I ggml_metal_init: use bfloat            = true
0.00.329.484 I ggml_metal_init: hasUnifiedMemory      = true
0.00.329.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.339.066 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.342.327 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.342.328 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.342.349 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.349.072 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.349.074 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.349.074 I llama_init_from_model: graph nodes  = 154
0.00.349.075 I llama_init_from_model: graph splits = 2
0.00.349.076 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.349.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.761 I 
0.00.357.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.357.901 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.357.902 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.357.905 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.357.905 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.357.913 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.357.913 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.450 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.183 I llama_perf_context_print:        load time =     335.80 ms
0.00.362.184 I llama_perf_context_print: prompt eval time =       3.72 ms /    62 tokens (    0.06 ms per token, 16648.76 tokens per second)
0.00.362.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.186 I llama_perf_context_print:       total time =       4.42 ms /    63 tokens
0.00.362.443 I ggml_metal_free: deallocating

real	0m1.086s
user	0m0.331s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.151 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.328 I main: llama backend init
0.00.000.335 I main: load the model and apply lora adapter, if any
0.00.031.715 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.032 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.046 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.072 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.072 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.073 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.075 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.076 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.081 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.082 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.082 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.063.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.023 I llama_model_loader: - type  f32:  194 tensors
0.00.063.024 I llama_model_loader: - type  f16:   98 tensors
0.00.063.025 I print_info: file format = GGUF V3 (latest)
0.00.063.026 I print_info: file type   = all F32 (guessed)
0.00.063.028 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.077.826 I load: special tokens cache size = 25
0.00.086.965 I load: token to piece cache size = 0.2984 MB
0.00.086.969 I print_info: arch             = gptneox
0.00.086.969 I print_info: vocab_only       = 0
0.00.086.969 I print_info: n_ctx_train      = 2048
0.00.086.969 I print_info: n_embd           = 2048
0.00.086.970 I print_info: n_layer          = 24
0.00.086.973 I print_info: n_head           = 16
0.00.086.974 I print_info: n_head_kv        = 16
0.00.086.976 I print_info: n_rot            = 32
0.00.086.976 I print_info: n_swa            = 0
0.00.086.977 I print_info: n_embd_head_k    = 128
0.00.086.977 I print_info: n_embd_head_v    = 128
0.00.086.978 I print_info: n_gqa            = 1
0.00.086.978 I print_info: n_embd_k_gqa     = 2048
0.00.086.979 I print_info: n_embd_v_gqa     = 2048
0.00.086.980 I print_info: f_norm_eps       = 1.0e-05
0.00.086.980 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.981 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.981 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.981 I print_info: f_logit_scale    = 0.0e+00
0.00.086.982 I print_info: n_ff             = 8192
0.00.086.982 I print_info: n_expert         = 0
0.00.086.982 I print_info: n_expert_used    = 0
0.00.086.982 I print_info: causal attn      = 1
0.00.086.982 I print_info: pooling type     = 0
0.00.086.982 I print_info: rope type        = 2
0.00.086.983 I print_info: rope scaling     = linear
0.00.086.983 I print_info: freq_base_train  = 10000.0
0.00.086.983 I print_info: freq_scale_train = 1
0.00.086.984 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.984 I print_info: rope_finetuned   = unknown
0.00.086.984 I print_info: ssm_d_conv       = 0
0.00.086.985 I print_info: ssm_d_inner      = 0
0.00.086.985 I print_info: ssm_d_state      = 0
0.00.086.985 I print_info: ssm_dt_rank      = 0
0.00.086.985 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.985 I print_info: model type       = 1.4B
0.00.086.986 I print_info: model params     = 1.41 B
0.00.086.986 I print_info: general.name     = 1.4B
0.00.086.986 I print_info: vocab type       = BPE
0.00.086.987 I print_info: n_vocab          = 50304
0.00.086.987 I print_info: n_merges         = 50009
0.00.086.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.987 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.988 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.988 I print_info: LF token         = 187 'Ċ'
0.00.086.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.992 I print_info: max token length = 1024
0.00.141.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.141.982 I load_tensors: offloading output layer to GPU
0.00.141.983 I load_tensors: offloaded 25/25 layers to GPU
0.00.142.007 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.142.008 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.142.314 I llama_init_from_model: n_seq_max     = 1
0.00.142.315 I llama_init_from_model: n_ctx         = 2048
0.00.142.315 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.142.315 I llama_init_from_model: n_batch       = 2048
0.00.142.316 I llama_init_from_model: n_ubatch      = 512
0.00.142.316 I llama_init_from_model: flash_attn    = 0
0.00.142.316 I llama_init_from_model: freq_base     = 10000.0
0.00.142.316 I llama_init_from_model: freq_scale    = 1
0.00.142.317 I ggml_metal_init: allocating
0.00.142.350 I ggml_metal_init: found device: Apple M4
0.00.142.355 I ggml_metal_init: picking default device: Apple M4
0.00.142.976 I ggml_metal_init: using embedded metal library
0.00.151.913 I ggml_metal_init: GPU name:   Apple M4
0.00.151.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.151.915 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.151.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.151.916 I ggml_metal_init: simdgroup reduction   = true
0.00.151.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.151.916 I ggml_metal_init: has residency sets    = true
0.00.151.916 I ggml_metal_init: has bfloat            = true
0.00.151.917 I ggml_metal_init: use bfloat            = true
0.00.151.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.151.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.175.929 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.204.454 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.204.462 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.204.506 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.208.305 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.208.308 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.208.308 I llama_init_from_model: graph nodes  = 967
0.00.208.308 I llama_init_from_model: graph splits = 2
0.00.208.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.208.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.208.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.273.638 I main: llama threadpool init, n_threads = 4
0.00.273.680 I 
0.00.273.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.273.715 I 
0.00.273.891 I sampler seed: 1234
0.00.273.896 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.273.920 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.273.922 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.273.922 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.117.741 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.02.117.741 I llama_perf_context_print:        load time =     241.06 ms
0.02.117.742 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.29 tokens per second)
0.02.117.743 I llama_perf_context_print:        eval time =    1797.23 ms /    63 runs   (   28.53 ms per token,    35.05 tokens per second)
0.02.117.743 I llama_perf_context_print:       total time =    1844.95 ms /    70 tokens
0.02.117.943 I ggml_metal_free: deallocating

real	0m2.421s
user	0m0.132s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.487 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.205 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.164 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.170 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.171 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.172 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.176 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.182 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.185 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.186 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.190 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.632 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.669 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.964 I llama_model_loader: - type  f32:  194 tensors
0.00.054.964 I llama_model_loader: - type  f16:   98 tensors
0.00.054.965 I print_info: file format = GGUF V3 (latest)
0.00.054.965 I print_info: file type   = all F32 (guessed)
0.00.054.967 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.058 I load: special tokens cache size = 25
0.00.076.293 I load: token to piece cache size = 0.2984 MB
0.00.076.296 I print_info: arch             = gptneox
0.00.076.296 I print_info: vocab_only       = 0
0.00.076.297 I print_info: n_ctx_train      = 2048
0.00.076.297 I print_info: n_embd           = 2048
0.00.076.297 I print_info: n_layer          = 24
0.00.076.300 I print_info: n_head           = 16
0.00.076.301 I print_info: n_head_kv        = 16
0.00.076.302 I print_info: n_rot            = 32
0.00.076.302 I print_info: n_swa            = 0
0.00.076.302 I print_info: n_embd_head_k    = 128
0.00.076.302 I print_info: n_embd_head_v    = 128
0.00.076.303 I print_info: n_gqa            = 1
0.00.076.304 I print_info: n_embd_k_gqa     = 2048
0.00.076.304 I print_info: n_embd_v_gqa     = 2048
0.00.076.305 I print_info: f_norm_eps       = 1.0e-05
0.00.076.305 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.306 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.306 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.306 I print_info: f_logit_scale    = 0.0e+00
0.00.076.307 I print_info: n_ff             = 8192
0.00.076.307 I print_info: n_expert         = 0
0.00.076.307 I print_info: n_expert_used    = 0
0.00.076.307 I print_info: causal attn      = 1
0.00.076.307 I print_info: pooling type     = 0
0.00.076.308 I print_info: rope type        = 2
0.00.076.308 I print_info: rope scaling     = linear
0.00.076.310 I print_info: freq_base_train  = 10000.0
0.00.076.311 I print_info: freq_scale_train = 1
0.00.076.311 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.311 I print_info: rope_finetuned   = unknown
0.00.076.311 I print_info: ssm_d_conv       = 0
0.00.076.311 I print_info: ssm_d_inner      = 0
0.00.076.311 I print_info: ssm_d_state      = 0
0.00.076.312 I print_info: ssm_dt_rank      = 0
0.00.076.312 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.312 I print_info: model type       = 1.4B
0.00.076.312 I print_info: model params     = 1.41 B
0.00.076.313 I print_info: general.name     = 1.4B
0.00.076.313 I print_info: vocab type       = BPE
0.00.076.313 I print_info: n_vocab          = 50304
0.00.076.314 I print_info: n_merges         = 50009
0.00.076.314 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.314 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.314 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.315 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.315 I print_info: LF token         = 187 'Ċ'
0.00.076.315 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.315 I print_info: max token length = 1024
0.01.456.326 I load_tensors: offloading 24 repeating layers to GPU
0.01.456.329 I load_tensors: offloading output layer to GPU
0.01.456.330 I load_tensors: offloaded 25/25 layers to GPU
0.01.456.358 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.456.360 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.457.115 I llama_init_from_model: n_seq_max     = 1
0.01.457.116 I llama_init_from_model: n_ctx         = 128
0.01.457.116 I llama_init_from_model: n_ctx_per_seq = 128
0.01.457.116 I llama_init_from_model: n_batch       = 128
0.01.457.117 I llama_init_from_model: n_ubatch      = 128
0.01.457.117 I llama_init_from_model: flash_attn    = 0
0.01.457.120 I llama_init_from_model: freq_base     = 10000.0
0.01.457.122 I llama_init_from_model: freq_scale    = 1
0.01.457.124 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.457.125 I ggml_metal_init: allocating
0.01.457.168 I ggml_metal_init: found device: Apple M4
0.01.457.174 I ggml_metal_init: picking default device: Apple M4
0.01.458.244 I ggml_metal_init: using embedded metal library
0.01.462.061 I ggml_metal_init: GPU name:   Apple M4
0.01.462.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.462.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.462.065 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.462.066 I ggml_metal_init: simdgroup reduction   = true
0.01.462.066 I ggml_metal_init: simdgroup matrix mul. = true
0.01.462.066 I ggml_metal_init: has residency sets    = true
0.01.462.066 I ggml_metal_init: has bfloat            = true
0.01.462.066 I ggml_metal_init: use bfloat            = true
0.01.462.067 I ggml_metal_init: hasUnifiedMemory      = true
0.01.462.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.472.894 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.474.594 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.474.596 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.474.620 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.476.321 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.476.322 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.476.322 I llama_init_from_model: graph nodes  = 967
0.01.476.323 I llama_init_from_model: graph splits = 2
0.01.476.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.476.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.511.365 I 
0.01.511.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.511.405 I perplexity: tokenizing the input ..
0.01.516.422 I perplexity: tokenization took 5.016 ms
0.01.516.426 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.634.700 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.636.040 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.636.071 I llama_perf_context_print:        load time =    1488.15 ms
0.01.636.072 I llama_perf_context_print: prompt eval time =     118.01 ms /   128 tokens (    0.92 ms per token,  1084.66 tokens per second)
0.01.636.072 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.636.073 I llama_perf_context_print:       total time =     124.71 ms /   129 tokens
0.01.636.472 I ggml_metal_free: deallocating

real	0m1.831s
user	0m0.098s
sys	0m0.265s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.831 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.209 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.210 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.212 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.215 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.215 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.215 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.137 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.054 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.055 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.056 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.056 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.057 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.057 I llama_model_loader: - type  f32:  194 tensors
0.00.033.058 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.059 I print_info: file format = GGUF V3 (latest)
0.00.033.059 I print_info: file type   = Q8_0
0.00.033.060 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.343 I load: special tokens cache size = 25
0.00.047.513 I load: token to piece cache size = 0.2984 MB
0.00.047.518 I print_info: arch             = gptneox
0.00.047.519 I print_info: vocab_only       = 0
0.00.047.519 I print_info: n_ctx_train      = 2048
0.00.047.521 I print_info: n_embd           = 2048
0.00.047.522 I print_info: n_layer          = 24
0.00.047.528 I print_info: n_head           = 16
0.00.047.529 I print_info: n_head_kv        = 16
0.00.047.529 I print_info: n_rot            = 32
0.00.047.529 I print_info: n_swa            = 0
0.00.047.530 I print_info: n_embd_head_k    = 128
0.00.047.530 I print_info: n_embd_head_v    = 128
0.00.047.530 I print_info: n_gqa            = 1
0.00.047.531 I print_info: n_embd_k_gqa     = 2048
0.00.047.533 I print_info: n_embd_v_gqa     = 2048
0.00.047.534 I print_info: f_norm_eps       = 1.0e-05
0.00.047.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.535 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.535 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.535 I print_info: f_logit_scale    = 0.0e+00
0.00.047.536 I print_info: n_ff             = 8192
0.00.047.536 I print_info: n_expert         = 0
0.00.047.536 I print_info: n_expert_used    = 0
0.00.047.536 I print_info: causal attn      = 1
0.00.047.538 I print_info: pooling type     = 0
0.00.047.538 I print_info: rope type        = 2
0.00.047.538 I print_info: rope scaling     = linear
0.00.047.539 I print_info: freq_base_train  = 10000.0
0.00.047.539 I print_info: freq_scale_train = 1
0.00.047.539 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.539 I print_info: rope_finetuned   = unknown
0.00.047.540 I print_info: ssm_d_conv       = 0
0.00.047.540 I print_info: ssm_d_inner      = 0
0.00.047.541 I print_info: ssm_d_state      = 0
0.00.047.541 I print_info: ssm_dt_rank      = 0
0.00.047.541 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.541 I print_info: model type       = 1.4B
0.00.047.542 I print_info: model params     = 1.41 B
0.00.047.542 I print_info: general.name     = 1.4B
0.00.047.543 I print_info: vocab type       = BPE
0.00.047.543 I print_info: n_vocab          = 50304
0.00.047.543 I print_info: n_merges         = 50009
0.00.047.543 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.543 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.544 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.544 I print_info: LF token         = 187 'Ċ'
0.00.047.544 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.544 I print_info: max token length = 1024
0.01.189.421 I load_tensors: offloading 24 repeating layers to GPU
0.01.189.426 I load_tensors: offloading output layer to GPU
0.01.189.427 I load_tensors: offloaded 25/25 layers to GPU
0.01.189.450 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.189.452 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.190.116 I llama_init_from_model: n_seq_max     = 1
0.01.190.118 I llama_init_from_model: n_ctx         = 2048
0.01.190.118 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.190.118 I llama_init_from_model: n_batch       = 2048
0.01.190.119 I llama_init_from_model: n_ubatch      = 512
0.01.190.119 I llama_init_from_model: flash_attn    = 0
0.01.190.120 I llama_init_from_model: freq_base     = 10000.0
0.01.190.120 I llama_init_from_model: freq_scale    = 1
0.01.190.121 I ggml_metal_init: allocating
0.01.190.133 I ggml_metal_init: found device: Apple M4
0.01.190.141 I ggml_metal_init: picking default device: Apple M4
0.01.191.306 I ggml_metal_init: using embedded metal library
0.01.196.578 I ggml_metal_init: GPU name:   Apple M4
0.01.196.582 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.196.583 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.196.584 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.196.584 I ggml_metal_init: simdgroup reduction   = true
0.01.196.584 I ggml_metal_init: simdgroup matrix mul. = true
0.01.196.584 I ggml_metal_init: has residency sets    = true
0.01.196.585 I ggml_metal_init: has bfloat            = true
0.01.196.585 I ggml_metal_init: use bfloat            = true
0.01.196.586 I ggml_metal_init: hasUnifiedMemory      = true
0.01.196.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.212.168 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.258.823 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.258.833 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.258.878 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.263.961 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.263.963 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.263.964 I llama_init_from_model: graph nodes  = 967
0.01.263.964 I llama_init_from_model: graph splits = 2
0.01.263.970 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.264.100 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.264.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.320.868 I main: llama threadpool init, n_threads = 4
0.01.320.915 I 
0.01.320.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.320.940 I 
0.01.321.114 I sampler seed: 1234
0.01.321.119 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.321.140 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.321.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.321.141 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.419.042 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.02.419.042 I llama_perf_context_print:        load time =    1310.38 ms
0.02.419.043 I llama_perf_context_print: prompt eval time =      49.00 ms /     7 tokens (    7.00 ms per token,   142.87 tokens per second)
0.02.419.044 I llama_perf_context_print:        eval time =    1045.99 ms /    63 runs   (   16.60 ms per token,    60.23 tokens per second)
0.02.419.045 I llama_perf_context_print:       total time =    1098.83 ms /    70 tokens
0.02.419.314 I ggml_metal_free: deallocating

real	0m2.439s
user	0m0.107s
sys	0m0.257s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.031 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.041 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.042 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.043 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.043 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.044 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.044 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.044 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.046 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.047 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.047 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.954 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.980 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.805 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.807 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.807 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.809 I llama_model_loader: - type  f32:  194 tensors
0.00.025.809 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.810 I print_info: file format = GGUF V3 (latest)
0.00.025.810 I print_info: file type   = Q8_0
0.00.025.811 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.097 I load: special tokens cache size = 25
0.00.040.129 I load: token to piece cache size = 0.2984 MB
0.00.040.134 I print_info: arch             = gptneox
0.00.040.134 I print_info: vocab_only       = 0
0.00.040.134 I print_info: n_ctx_train      = 2048
0.00.040.135 I print_info: n_embd           = 2048
0.00.040.135 I print_info: n_layer          = 24
0.00.040.139 I print_info: n_head           = 16
0.00.040.140 I print_info: n_head_kv        = 16
0.00.040.140 I print_info: n_rot            = 32
0.00.040.140 I print_info: n_swa            = 0
0.00.040.140 I print_info: n_embd_head_k    = 128
0.00.040.141 I print_info: n_embd_head_v    = 128
0.00.040.141 I print_info: n_gqa            = 1
0.00.040.142 I print_info: n_embd_k_gqa     = 2048
0.00.040.143 I print_info: n_embd_v_gqa     = 2048
0.00.040.143 I print_info: f_norm_eps       = 1.0e-05
0.00.040.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.144 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.144 I print_info: f_logit_scale    = 0.0e+00
0.00.040.144 I print_info: n_ff             = 8192
0.00.040.145 I print_info: n_expert         = 0
0.00.040.145 I print_info: n_expert_used    = 0
0.00.040.145 I print_info: causal attn      = 1
0.00.040.145 I print_info: pooling type     = 0
0.00.040.145 I print_info: rope type        = 2
0.00.040.145 I print_info: rope scaling     = linear
0.00.040.146 I print_info: freq_base_train  = 10000.0
0.00.040.147 I print_info: freq_scale_train = 1
0.00.040.147 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.147 I print_info: rope_finetuned   = unknown
0.00.040.148 I print_info: ssm_d_conv       = 0
0.00.040.148 I print_info: ssm_d_inner      = 0
0.00.040.148 I print_info: ssm_d_state      = 0
0.00.040.148 I print_info: ssm_dt_rank      = 0
0.00.040.148 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.148 I print_info: model type       = 1.4B
0.00.040.149 I print_info: model params     = 1.41 B
0.00.040.149 I print_info: general.name     = 1.4B
0.00.040.149 I print_info: vocab type       = BPE
0.00.040.150 I print_info: n_vocab          = 50304
0.00.040.150 I print_info: n_merges         = 50009
0.00.040.150 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.150 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.151 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.151 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.152 I print_info: LF token         = 187 'Ċ'
0.00.040.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.152 I print_info: max token length = 1024
0.00.940.183 I load_tensors: offloading 24 repeating layers to GPU
0.00.940.188 I load_tensors: offloading output layer to GPU
0.00.940.189 I load_tensors: offloaded 25/25 layers to GPU
0.00.940.218 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.940.221 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.941.459 I llama_init_from_model: n_seq_max     = 1
0.00.941.461 I llama_init_from_model: n_ctx         = 128
0.00.941.462 I llama_init_from_model: n_ctx_per_seq = 128
0.00.941.462 I llama_init_from_model: n_batch       = 128
0.00.941.462 I llama_init_from_model: n_ubatch      = 128
0.00.941.465 I llama_init_from_model: flash_attn    = 0
0.00.941.466 I llama_init_from_model: freq_base     = 10000.0
0.00.941.466 I llama_init_from_model: freq_scale    = 1
0.00.941.467 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.941.468 I ggml_metal_init: allocating
0.00.941.529 I ggml_metal_init: found device: Apple M4
0.00.941.537 I ggml_metal_init: picking default device: Apple M4
0.00.942.846 I ggml_metal_init: using embedded metal library
0.00.948.065 I ggml_metal_init: GPU name:   Apple M4
0.00.948.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.948.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.948.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.948.074 I ggml_metal_init: simdgroup reduction   = true
0.00.948.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.948.074 I ggml_metal_init: has residency sets    = true
0.00.948.075 I ggml_metal_init: has bfloat            = true
0.00.948.075 I ggml_metal_init: use bfloat            = true
0.00.948.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.948.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.963.278 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.966.639 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.966.648 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.966.697 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.969.707 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.969.709 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.969.709 I llama_init_from_model: graph nodes  = 967
0.00.969.710 I llama_init_from_model: graph splits = 2
0.00.969.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.969.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.997.659 I 
0.00.997.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.997.737 I perplexity: tokenizing the input ..
0.01.004.817 I perplexity: tokenization took 7.076 ms
0.01.004.823 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.142.842 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.144.174 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.144.199 I llama_perf_context_print:        load time =     987.79 ms
0.01.144.200 I llama_perf_context_print: prompt eval time =     137.16 ms /   128 tokens (    1.07 ms per token,   933.20 tokens per second)
0.01.144.201 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.144.201 I llama_perf_context_print:       total time =     146.54 ms /   129 tokens
0.01.144.606 I ggml_metal_free: deallocating

real	0m1.161s
user	0m0.076s
sys	0m0.189s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.016.265 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.915 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.922 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.925 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.925 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.927 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.927 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.928 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.928 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.929 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.929 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.931 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.931 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.931 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.163 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.330 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.551 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.552 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.554 I llama_model_loader: - type  f32:  194 tensors
0.00.043.554 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.554 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.555 I print_info: file format = GGUF V3 (latest)
0.00.043.557 I print_info: file type   = Q4_0
0.00.043.559 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.053.902 I load: special tokens cache size = 25
0.00.061.887 I load: token to piece cache size = 0.2984 MB
0.00.061.891 I print_info: arch             = gptneox
0.00.061.892 I print_info: vocab_only       = 0
0.00.061.892 I print_info: n_ctx_train      = 2048
0.00.061.892 I print_info: n_embd           = 2048
0.00.061.893 I print_info: n_layer          = 24
0.00.061.897 I print_info: n_head           = 16
0.00.061.898 I print_info: n_head_kv        = 16
0.00.061.898 I print_info: n_rot            = 32
0.00.061.899 I print_info: n_swa            = 0
0.00.061.899 I print_info: n_embd_head_k    = 128
0.00.061.899 I print_info: n_embd_head_v    = 128
0.00.061.900 I print_info: n_gqa            = 1
0.00.061.901 I print_info: n_embd_k_gqa     = 2048
0.00.061.902 I print_info: n_embd_v_gqa     = 2048
0.00.061.902 I print_info: f_norm_eps       = 1.0e-05
0.00.061.903 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.903 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.903 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.904 I print_info: f_logit_scale    = 0.0e+00
0.00.061.904 I print_info: n_ff             = 8192
0.00.061.905 I print_info: n_expert         = 0
0.00.061.905 I print_info: n_expert_used    = 0
0.00.061.905 I print_info: causal attn      = 1
0.00.061.905 I print_info: pooling type     = 0
0.00.061.905 I print_info: rope type        = 2
0.00.061.909 I print_info: rope scaling     = linear
0.00.061.909 I print_info: freq_base_train  = 10000.0
0.00.061.909 I print_info: freq_scale_train = 1
0.00.061.910 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.910 I print_info: rope_finetuned   = unknown
0.00.061.910 I print_info: ssm_d_conv       = 0
0.00.061.910 I print_info: ssm_d_inner      = 0
0.00.061.911 I print_info: ssm_d_state      = 0
0.00.061.911 I print_info: ssm_dt_rank      = 0
0.00.061.911 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.911 I print_info: model type       = 1.4B
0.00.061.912 I print_info: model params     = 1.41 B
0.00.061.912 I print_info: general.name     = 1.4B
0.00.061.913 I print_info: vocab type       = BPE
0.00.061.914 I print_info: n_vocab          = 50304
0.00.061.914 I print_info: n_merges         = 50009
0.00.061.921 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.921 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.922 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.922 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.922 I print_info: LF token         = 187 'Ċ'
0.00.061.923 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.925 I print_info: max token length = 1024
0.00.703.490 I load_tensors: offloading 24 repeating layers to GPU
0.00.703.504 I load_tensors: offloading output layer to GPU
0.00.703.505 I load_tensors: offloaded 25/25 layers to GPU
0.00.703.538 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.703.539 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.705.173 I llama_init_from_model: n_seq_max     = 1
0.00.705.180 I llama_init_from_model: n_ctx         = 2048
0.00.705.181 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.705.181 I llama_init_from_model: n_batch       = 2048
0.00.705.181 I llama_init_from_model: n_ubatch      = 512
0.00.705.182 I llama_init_from_model: flash_attn    = 0
0.00.705.185 I llama_init_from_model: freq_base     = 10000.0
0.00.705.185 I llama_init_from_model: freq_scale    = 1
0.00.705.191 I ggml_metal_init: allocating
0.00.705.302 I ggml_metal_init: found device: Apple M4
0.00.705.317 I ggml_metal_init: picking default device: Apple M4
0.00.707.252 I ggml_metal_init: using embedded metal library
0.00.713.876 I ggml_metal_init: GPU name:   Apple M4
0.00.713.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.713.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.713.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.713.883 I ggml_metal_init: simdgroup reduction   = true
0.00.713.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.713.883 I ggml_metal_init: has residency sets    = true
0.00.713.883 I ggml_metal_init: has bfloat            = true
0.00.713.884 I ggml_metal_init: use bfloat            = true
0.00.713.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.713.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.731.906 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.784.227 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.784.236 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.784.281 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.788.591 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.788.593 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.788.593 I llama_init_from_model: graph nodes  = 967
0.00.788.594 I llama_init_from_model: graph splits = 2
0.00.788.599 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.788.712 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.788.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.845.019 I main: llama threadpool init, n_threads = 4
0.00.845.061 I 
0.00.845.085 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.845.087 I 
0.00.845.259 I sampler seed: 1234
0.00.845.263 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.845.302 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.845.305 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.845.306 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.538.662 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50070.52 tokens per second)
0.01.538.663 I llama_perf_context_print:        load time =     828.09 ms
0.01.538.664 I llama_perf_context_print: prompt eval time =      49.36 ms /     7 tokens (    7.05 ms per token,   141.83 tokens per second)
0.01.538.666 I llama_perf_context_print:        eval time =     641.00 ms /    63 runs   (   10.17 ms per token,    98.28 tokens per second)
0.01.538.666 I llama_perf_context_print:       total time =     694.31 ms /    70 tokens
0.01.538.928 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.117s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.278 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.335 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.428 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.440 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.440 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.441 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.444 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.444 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.359 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.211 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.213 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.213 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.214 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.214 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.214 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.215 I llama_model_loader: - type  f32:  194 tensors
0.00.026.215 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.215 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.216 I print_info: file format = GGUF V3 (latest)
0.00.026.217 I print_info: file type   = Q4_0
0.00.026.218 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.121 I load: special tokens cache size = 25
0.00.040.247 I load: token to piece cache size = 0.2984 MB
0.00.040.250 I print_info: arch             = gptneox
0.00.040.251 I print_info: vocab_only       = 0
0.00.040.251 I print_info: n_ctx_train      = 2048
0.00.040.251 I print_info: n_embd           = 2048
0.00.040.251 I print_info: n_layer          = 24
0.00.040.255 I print_info: n_head           = 16
0.00.040.259 I print_info: n_head_kv        = 16
0.00.040.259 I print_info: n_rot            = 32
0.00.040.259 I print_info: n_swa            = 0
0.00.040.259 I print_info: n_embd_head_k    = 128
0.00.040.259 I print_info: n_embd_head_v    = 128
0.00.040.260 I print_info: n_gqa            = 1
0.00.040.261 I print_info: n_embd_k_gqa     = 2048
0.00.040.261 I print_info: n_embd_v_gqa     = 2048
0.00.040.262 I print_info: f_norm_eps       = 1.0e-05
0.00.040.262 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.262 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.263 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.263 I print_info: f_logit_scale    = 0.0e+00
0.00.040.263 I print_info: n_ff             = 8192
0.00.040.264 I print_info: n_expert         = 0
0.00.040.264 I print_info: n_expert_used    = 0
0.00.040.264 I print_info: causal attn      = 1
0.00.040.264 I print_info: pooling type     = 0
0.00.040.264 I print_info: rope type        = 2
0.00.040.265 I print_info: rope scaling     = linear
0.00.040.266 I print_info: freq_base_train  = 10000.0
0.00.040.267 I print_info: freq_scale_train = 1
0.00.040.268 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.268 I print_info: rope_finetuned   = unknown
0.00.040.268 I print_info: ssm_d_conv       = 0
0.00.040.268 I print_info: ssm_d_inner      = 0
0.00.040.268 I print_info: ssm_d_state      = 0
0.00.040.268 I print_info: ssm_dt_rank      = 0
0.00.040.268 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.269 I print_info: model type       = 1.4B
0.00.040.269 I print_info: model params     = 1.41 B
0.00.040.269 I print_info: general.name     = 1.4B
0.00.040.270 I print_info: vocab type       = BPE
0.00.040.270 I print_info: n_vocab          = 50304
0.00.040.270 I print_info: n_merges         = 50009
0.00.040.271 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.271 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.271 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.271 I print_info: LF token         = 187 'Ċ'
0.00.040.272 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.272 I print_info: max token length = 1024
0.00.599.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.525 I load_tensors: offloading output layer to GPU
0.00.599.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.558 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.599.559 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.601.001 I llama_init_from_model: n_seq_max     = 1
0.00.601.006 I llama_init_from_model: n_ctx         = 128
0.00.601.006 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.007 I llama_init_from_model: n_batch       = 128
0.00.601.008 I llama_init_from_model: n_ubatch      = 128
0.00.601.008 I llama_init_from_model: flash_attn    = 0
0.00.601.010 I llama_init_from_model: freq_base     = 10000.0
0.00.601.011 I llama_init_from_model: freq_scale    = 1
0.00.601.011 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.018 I ggml_metal_init: allocating
0.00.601.105 I ggml_metal_init: found device: Apple M4
0.00.601.119 I ggml_metal_init: picking default device: Apple M4
0.00.602.905 I ggml_metal_init: using embedded metal library
0.00.608.611 I ggml_metal_init: GPU name:   Apple M4
0.00.608.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.618 I ggml_metal_init: simdgroup reduction   = true
0.00.608.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.619 I ggml_metal_init: has residency sets    = true
0.00.608.619 I ggml_metal_init: has bfloat            = true
0.00.608.620 I ggml_metal_init: use bfloat            = true
0.00.608.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.591 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.978 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.981 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.028 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.255 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.257 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.257 I llama_init_from_model: graph nodes  = 967
0.00.634.258 I llama_init_from_model: graph splits = 2
0.00.634.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.899 I 
0.00.662.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.007 I perplexity: tokenizing the input ..
0.00.669.949 I perplexity: tokenization took 6.939 ms
0.00.669.955 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.390 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.804.727 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.804.757 I llama_perf_context_print:        load time =     652.55 ms
0.00.804.761 I llama_perf_context_print: prompt eval time =     132.88 ms /   128 tokens (    1.04 ms per token,   963.24 tokens per second)
0.00.804.761 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.762 I llama_perf_context_print:       total time =     141.86 ms /   129 tokens
0.00.805.144 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.079s
sys	0m0.126s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.763 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.414 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.029.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.420 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.420 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.421 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.421 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.423 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.427 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.428 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.334 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.426 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.379 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.380 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.380 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.038.381 I llama_model_loader: - type  f32:  194 tensors
0.00.038.382 I llama_model_loader: - type q4_1:   97 tensors
0.00.038.382 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.382 I print_info: file format = GGUF V3 (latest)
0.00.038.383 I print_info: file type   = Q4_1
0.00.038.384 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.143 I load: special tokens cache size = 25
0.00.054.113 I load: token to piece cache size = 0.2984 MB
0.00.054.116 I print_info: arch             = gptneox
0.00.054.116 I print_info: vocab_only       = 0
0.00.054.116 I print_info: n_ctx_train      = 2048
0.00.054.117 I print_info: n_embd           = 2048
0.00.054.117 I print_info: n_layer          = 24
0.00.054.120 I print_info: n_head           = 16
0.00.054.121 I print_info: n_head_kv        = 16
0.00.054.121 I print_info: n_rot            = 32
0.00.054.121 I print_info: n_swa            = 0
0.00.054.121 I print_info: n_embd_head_k    = 128
0.00.054.121 I print_info: n_embd_head_v    = 128
0.00.054.122 I print_info: n_gqa            = 1
0.00.054.123 I print_info: n_embd_k_gqa     = 2048
0.00.054.123 I print_info: n_embd_v_gqa     = 2048
0.00.054.124 I print_info: f_norm_eps       = 1.0e-05
0.00.054.124 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.124 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.124 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.125 I print_info: f_logit_scale    = 0.0e+00
0.00.054.127 I print_info: n_ff             = 8192
0.00.054.127 I print_info: n_expert         = 0
0.00.054.127 I print_info: n_expert_used    = 0
0.00.054.127 I print_info: causal attn      = 1
0.00.054.127 I print_info: pooling type     = 0
0.00.054.129 I print_info: rope type        = 2
0.00.054.130 I print_info: rope scaling     = linear
0.00.054.131 I print_info: freq_base_train  = 10000.0
0.00.054.131 I print_info: freq_scale_train = 1
0.00.054.131 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.131 I print_info: rope_finetuned   = unknown
0.00.054.131 I print_info: ssm_d_conv       = 0
0.00.054.132 I print_info: ssm_d_inner      = 0
0.00.054.132 I print_info: ssm_d_state      = 0
0.00.054.132 I print_info: ssm_dt_rank      = 0
0.00.054.132 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.132 I print_info: model type       = 1.4B
0.00.054.133 I print_info: model params     = 1.41 B
0.00.054.133 I print_info: general.name     = 1.4B
0.00.054.135 I print_info: vocab type       = BPE
0.00.054.135 I print_info: n_vocab          = 50304
0.00.054.135 I print_info: n_merges         = 50009
0.00.054.135 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.136 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.145 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.145 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.146 I print_info: LF token         = 187 'Ċ'
0.00.054.146 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.146 I print_info: max token length = 1024
0.00.755.281 I load_tensors: offloading 24 repeating layers to GPU
0.00.755.296 I load_tensors: offloading output layer to GPU
0.00.755.296 I load_tensors: offloaded 25/25 layers to GPU
0.00.755.331 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.755.342 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.756.886 I llama_init_from_model: n_seq_max     = 1
0.00.756.892 I llama_init_from_model: n_ctx         = 2048
0.00.756.892 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.756.893 I llama_init_from_model: n_batch       = 2048
0.00.756.893 I llama_init_from_model: n_ubatch      = 512
0.00.756.894 I llama_init_from_model: flash_attn    = 0
0.00.756.895 I llama_init_from_model: freq_base     = 10000.0
0.00.756.896 I llama_init_from_model: freq_scale    = 1
0.00.756.902 I ggml_metal_init: allocating
0.00.756.981 I ggml_metal_init: found device: Apple M4
0.00.756.995 I ggml_metal_init: picking default device: Apple M4
0.00.758.913 I ggml_metal_init: using embedded metal library
0.00.765.503 I ggml_metal_init: GPU name:   Apple M4
0.00.765.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.765.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.765.510 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.765.510 I ggml_metal_init: simdgroup reduction   = true
0.00.765.511 I ggml_metal_init: simdgroup matrix mul. = true
0.00.765.511 I ggml_metal_init: has residency sets    = true
0.00.765.511 I ggml_metal_init: has bfloat            = true
0.00.765.512 I ggml_metal_init: use bfloat            = true
0.00.765.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.765.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.784.000 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.838.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.838.012 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.838.056 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.842.005 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.842.007 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.842.007 I llama_init_from_model: graph nodes  = 967
0.00.842.007 I llama_init_from_model: graph splits = 2
0.00.842.014 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.842.138 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.842.138 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.896.005 I main: llama threadpool init, n_threads = 4
0.00.896.050 I 
0.00.896.076 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.896.076 I 
0.00.896.251 I sampler seed: 1234
0.00.896.256 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.896.276 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.896.277 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.896.277 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.636.744 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.636.745 I llama_perf_context_print:        load time =     886.58 ms
0.01.636.746 I llama_perf_context_print: prompt eval time =      48.94 ms /     7 tokens (    6.99 ms per token,   143.05 tokens per second)
0.01.636.747 I llama_perf_context_print:        eval time =     688.72 ms /    63 runs   (   10.93 ms per token,    91.47 tokens per second)
0.01.636.748 I llama_perf_context_print:       total time =     741.40 ms /    70 tokens
0.01.636.986 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.112s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.958 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.783 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.784 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.784 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.786 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.786 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.787 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.787 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.788 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.790 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.790 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.704 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.563 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.564 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.564 I llama_model_loader: - type  f32:  194 tensors
0.00.024.565 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.565 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.566 I print_info: file format = GGUF V3 (latest)
0.00.024.566 I print_info: file type   = Q4_1
0.00.024.568 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.695 I load: special tokens cache size = 25
0.00.038.677 I load: token to piece cache size = 0.2984 MB
0.00.038.680 I print_info: arch             = gptneox
0.00.038.680 I print_info: vocab_only       = 0
0.00.038.680 I print_info: n_ctx_train      = 2048
0.00.038.681 I print_info: n_embd           = 2048
0.00.038.681 I print_info: n_layer          = 24
0.00.038.684 I print_info: n_head           = 16
0.00.038.686 I print_info: n_head_kv        = 16
0.00.038.686 I print_info: n_rot            = 32
0.00.038.686 I print_info: n_swa            = 0
0.00.038.687 I print_info: n_embd_head_k    = 128
0.00.038.687 I print_info: n_embd_head_v    = 128
0.00.038.687 I print_info: n_gqa            = 1
0.00.038.688 I print_info: n_embd_k_gqa     = 2048
0.00.038.689 I print_info: n_embd_v_gqa     = 2048
0.00.038.689 I print_info: f_norm_eps       = 1.0e-05
0.00.038.690 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.690 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.690 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.690 I print_info: f_logit_scale    = 0.0e+00
0.00.038.691 I print_info: n_ff             = 8192
0.00.038.691 I print_info: n_expert         = 0
0.00.038.691 I print_info: n_expert_used    = 0
0.00.038.692 I print_info: causal attn      = 1
0.00.038.692 I print_info: pooling type     = 0
0.00.038.692 I print_info: rope type        = 2
0.00.038.692 I print_info: rope scaling     = linear
0.00.038.693 I print_info: freq_base_train  = 10000.0
0.00.038.693 I print_info: freq_scale_train = 1
0.00.038.693 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.693 I print_info: rope_finetuned   = unknown
0.00.038.694 I print_info: ssm_d_conv       = 0
0.00.038.694 I print_info: ssm_d_inner      = 0
0.00.038.694 I print_info: ssm_d_state      = 0
0.00.038.694 I print_info: ssm_dt_rank      = 0
0.00.038.694 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.695 I print_info: model type       = 1.4B
0.00.038.695 I print_info: model params     = 1.41 B
0.00.038.695 I print_info: general.name     = 1.4B
0.00.038.696 I print_info: vocab type       = BPE
0.00.038.696 I print_info: n_vocab          = 50304
0.00.038.696 I print_info: n_merges         = 50009
0.00.038.696 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.696 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: LF token         = 187 'Ċ'
0.00.038.698 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.698 I print_info: max token length = 1024
0.00.643.780 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.794 I load_tensors: offloading output layer to GPU
0.00.643.794 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.826 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.643.827 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.645.048 I llama_init_from_model: n_seq_max     = 1
0.00.645.058 I llama_init_from_model: n_ctx         = 128
0.00.645.058 I llama_init_from_model: n_ctx_per_seq = 128
0.00.645.059 I llama_init_from_model: n_batch       = 128
0.00.645.059 I llama_init_from_model: n_ubatch      = 128
0.00.645.060 I llama_init_from_model: flash_attn    = 0
0.00.645.062 I llama_init_from_model: freq_base     = 10000.0
0.00.645.063 I llama_init_from_model: freq_scale    = 1
0.00.645.063 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.645.069 I ggml_metal_init: allocating
0.00.645.138 I ggml_metal_init: found device: Apple M4
0.00.645.153 I ggml_metal_init: picking default device: Apple M4
0.00.647.287 I ggml_metal_init: using embedded metal library
0.00.653.958 I ggml_metal_init: GPU name:   Apple M4
0.00.653.964 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.965 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.966 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.967 I ggml_metal_init: simdgroup reduction   = true
0.00.653.967 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.967 I ggml_metal_init: has residency sets    = true
0.00.653.967 I ggml_metal_init: has bfloat            = true
0.00.653.968 I ggml_metal_init: use bfloat            = true
0.00.653.969 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.971 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.691 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.676.186 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.676.190 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.676.232 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.422 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.679.423 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.679.424 I llama_init_from_model: graph nodes  = 967
0.00.679.424 I llama_init_from_model: graph splits = 2
0.00.679.427 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.679.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.767 I 
0.00.708.865 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.882 I perplexity: tokenizing the input ..
0.00.716.123 I perplexity: tokenization took 7.237 ms
0.00.716.130 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.489 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.854.913 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.854.939 I llama_perf_context_print:        load time =     699.80 ms
0.00.854.940 I llama_perf_context_print: prompt eval time =     136.49 ms /   128 tokens (    1.07 ms per token,   937.83 tokens per second)
0.00.854.941 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.941 I llama_perf_context_print:       total time =     146.18 ms /   129 tokens
0.00.855.358 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.080s
sys	0m0.134s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.015.929 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.882 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.035.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.896 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.897 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.897 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.897 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.897 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.898 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.898 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.899 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.899 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.899 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.900 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.900 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.902 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.902 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.620 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.622 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.622 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.622 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.623 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.045.623 I llama_model_loader: - type  f32:  194 tensors
0.00.045.624 I llama_model_loader: - type q5_0:   97 tensors
0.00.045.624 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.624 I print_info: file format = GGUF V3 (latest)
0.00.045.625 I print_info: file type   = Q5_0
0.00.045.626 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.055.090 I load: special tokens cache size = 25
0.00.063.516 I load: token to piece cache size = 0.2984 MB
0.00.063.519 I print_info: arch             = gptneox
0.00.063.519 I print_info: vocab_only       = 0
0.00.063.519 I print_info: n_ctx_train      = 2048
0.00.063.520 I print_info: n_embd           = 2048
0.00.063.520 I print_info: n_layer          = 24
0.00.063.524 I print_info: n_head           = 16
0.00.063.524 I print_info: n_head_kv        = 16
0.00.063.525 I print_info: n_rot            = 32
0.00.063.525 I print_info: n_swa            = 0
0.00.063.525 I print_info: n_embd_head_k    = 128
0.00.063.525 I print_info: n_embd_head_v    = 128
0.00.063.526 I print_info: n_gqa            = 1
0.00.063.527 I print_info: n_embd_k_gqa     = 2048
0.00.063.528 I print_info: n_embd_v_gqa     = 2048
0.00.063.528 I print_info: f_norm_eps       = 1.0e-05
0.00.063.529 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.529 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.529 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.529 I print_info: f_logit_scale    = 0.0e+00
0.00.063.530 I print_info: n_ff             = 8192
0.00.063.530 I print_info: n_expert         = 0
0.00.063.530 I print_info: n_expert_used    = 0
0.00.063.531 I print_info: causal attn      = 1
0.00.063.531 I print_info: pooling type     = 0
0.00.063.531 I print_info: rope type        = 2
0.00.063.534 I print_info: rope scaling     = linear
0.00.063.534 I print_info: freq_base_train  = 10000.0
0.00.063.534 I print_info: freq_scale_train = 1
0.00.063.535 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.535 I print_info: rope_finetuned   = unknown
0.00.063.535 I print_info: ssm_d_conv       = 0
0.00.063.535 I print_info: ssm_d_inner      = 0
0.00.063.535 I print_info: ssm_d_state      = 0
0.00.063.535 I print_info: ssm_dt_rank      = 0
0.00.063.536 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.536 I print_info: model type       = 1.4B
0.00.063.536 I print_info: model params     = 1.41 B
0.00.063.536 I print_info: general.name     = 1.4B
0.00.063.537 I print_info: vocab type       = BPE
0.00.063.537 I print_info: n_vocab          = 50304
0.00.063.537 I print_info: n_merges         = 50009
0.00.063.538 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.538 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.538 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.538 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.539 I print_info: LF token         = 187 'Ċ'
0.00.063.539 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.539 I print_info: max token length = 1024
0.00.741.967 I load_tensors: offloading 24 repeating layers to GPU
0.00.741.984 I load_tensors: offloading output layer to GPU
0.00.741.985 I load_tensors: offloaded 25/25 layers to GPU
0.00.742.018 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.742.019 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.743.397 I llama_init_from_model: n_seq_max     = 1
0.00.743.403 I llama_init_from_model: n_ctx         = 2048
0.00.743.403 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.743.404 I llama_init_from_model: n_batch       = 2048
0.00.743.404 I llama_init_from_model: n_ubatch      = 512
0.00.743.405 I llama_init_from_model: flash_attn    = 0
0.00.743.407 I llama_init_from_model: freq_base     = 10000.0
0.00.743.407 I llama_init_from_model: freq_scale    = 1
0.00.743.410 I ggml_metal_init: allocating
0.00.743.483 I ggml_metal_init: found device: Apple M4
0.00.743.496 I ggml_metal_init: picking default device: Apple M4
0.00.745.273 I ggml_metal_init: using embedded metal library
0.00.751.703 I ggml_metal_init: GPU name:   Apple M4
0.00.751.707 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.751.707 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.751.708 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.751.712 I ggml_metal_init: simdgroup reduction   = true
0.00.751.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.751.713 I ggml_metal_init: has residency sets    = true
0.00.751.713 I ggml_metal_init: has bfloat            = true
0.00.751.713 I ggml_metal_init: use bfloat            = true
0.00.751.714 I ggml_metal_init: hasUnifiedMemory      = true
0.00.751.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.769.244 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.823.718 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.823.726 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.823.760 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.828.046 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.828.048 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.828.048 I llama_init_from_model: graph nodes  = 967
0.00.828.049 I llama_init_from_model: graph splits = 2
0.00.828.053 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.828.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.828.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.886.224 I main: llama threadpool init, n_threads = 4
0.00.886.269 I 
0.00.886.292 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.886.295 I 
0.00.886.451 I sampler seed: 1234
0.00.886.456 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.886.475 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.886.475 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.886.476 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.695.297 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.695.298 I llama_perf_context_print:        load time =     869.65 ms
0.01.695.303 I llama_perf_context_print: prompt eval time =      52.94 ms /     7 tokens (    7.56 ms per token,   132.24 tokens per second)
0.01.695.305 I llama_perf_context_print:        eval time =     753.00 ms /    63 runs   (   11.95 ms per token,    83.67 tokens per second)
0.01.695.306 I llama_perf_context_print:       total time =     809.72 ms /    70 tokens
0.01.695.589 I ggml_metal_free: deallocating

real	0m1.716s
user	0m0.115s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.011 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.359 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.361 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.367 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.367 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.369 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.369 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.369 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.011 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.012 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.013 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.013 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.013 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.014 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.014 I llama_model_loader: - type  f32:  194 tensors
0.00.026.014 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.015 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.015 I print_info: file format = GGUF V3 (latest)
0.00.026.016 I print_info: file type   = Q5_0
0.00.026.017 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.989 I load: special tokens cache size = 25
0.00.039.969 I load: token to piece cache size = 0.2984 MB
0.00.039.972 I print_info: arch             = gptneox
0.00.039.972 I print_info: vocab_only       = 0
0.00.039.972 I print_info: n_ctx_train      = 2048
0.00.039.972 I print_info: n_embd           = 2048
0.00.039.973 I print_info: n_layer          = 24
0.00.039.976 I print_info: n_head           = 16
0.00.039.977 I print_info: n_head_kv        = 16
0.00.039.977 I print_info: n_rot            = 32
0.00.039.977 I print_info: n_swa            = 0
0.00.039.979 I print_info: n_embd_head_k    = 128
0.00.039.979 I print_info: n_embd_head_v    = 128
0.00.039.980 I print_info: n_gqa            = 1
0.00.039.980 I print_info: n_embd_k_gqa     = 2048
0.00.039.981 I print_info: n_embd_v_gqa     = 2048
0.00.039.981 I print_info: f_norm_eps       = 1.0e-05
0.00.039.982 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.982 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.982 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.982 I print_info: f_logit_scale    = 0.0e+00
0.00.039.983 I print_info: n_ff             = 8192
0.00.039.983 I print_info: n_expert         = 0
0.00.039.983 I print_info: n_expert_used    = 0
0.00.039.983 I print_info: causal attn      = 1
0.00.039.983 I print_info: pooling type     = 0
0.00.039.984 I print_info: rope type        = 2
0.00.039.986 I print_info: rope scaling     = linear
0.00.039.986 I print_info: freq_base_train  = 10000.0
0.00.039.987 I print_info: freq_scale_train = 1
0.00.039.987 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.987 I print_info: rope_finetuned   = unknown
0.00.039.987 I print_info: ssm_d_conv       = 0
0.00.039.987 I print_info: ssm_d_inner      = 0
0.00.039.987 I print_info: ssm_d_state      = 0
0.00.039.987 I print_info: ssm_dt_rank      = 0
0.00.039.988 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.988 I print_info: model type       = 1.4B
0.00.039.988 I print_info: model params     = 1.41 B
0.00.039.988 I print_info: general.name     = 1.4B
0.00.039.989 I print_info: vocab type       = BPE
0.00.039.989 I print_info: n_vocab          = 50304
0.00.039.989 I print_info: n_merges         = 50009
0.00.039.989 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.993 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.994 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.994 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.994 I print_info: LF token         = 187 'Ċ'
0.00.039.994 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.994 I print_info: max token length = 1024
0.00.706.858 I load_tensors: offloading 24 repeating layers to GPU
0.00.706.875 I load_tensors: offloading output layer to GPU
0.00.706.876 I load_tensors: offloaded 25/25 layers to GPU
0.00.706.908 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.706.910 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.708.357 I llama_init_from_model: n_seq_max     = 1
0.00.708.362 I llama_init_from_model: n_ctx         = 128
0.00.708.363 I llama_init_from_model: n_ctx_per_seq = 128
0.00.708.363 I llama_init_from_model: n_batch       = 128
0.00.708.364 I llama_init_from_model: n_ubatch      = 128
0.00.708.365 I llama_init_from_model: flash_attn    = 0
0.00.708.367 I llama_init_from_model: freq_base     = 10000.0
0.00.708.367 I llama_init_from_model: freq_scale    = 1
0.00.708.368 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.708.375 I ggml_metal_init: allocating
0.00.708.453 I ggml_metal_init: found device: Apple M4
0.00.708.466 I ggml_metal_init: picking default device: Apple M4
0.00.710.232 I ggml_metal_init: using embedded metal library
0.00.717.046 I ggml_metal_init: GPU name:   Apple M4
0.00.717.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.056 I ggml_metal_init: simdgroup reduction   = true
0.00.717.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.057 I ggml_metal_init: has residency sets    = true
0.00.717.057 I ggml_metal_init: has bfloat            = true
0.00.717.057 I ggml_metal_init: use bfloat            = true
0.00.717.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.734.748 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.738.243 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.738.249 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.738.296 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.703 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.741.705 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.741.706 I llama_init_from_model: graph nodes  = 967
0.00.741.706 I llama_init_from_model: graph splits = 2
0.00.741.709 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.741.709 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.094 I 
0.00.775.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.190 I perplexity: tokenizing the input ..
0.00.782.146 I perplexity: tokenization took 6.952 ms
0.00.782.153 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.930.613 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.931.959 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.931.983 I llama_perf_context_print:        load time =     765.07 ms
0.00.931.984 I llama_perf_context_print: prompt eval time =     147.57 ms /   128 tokens (    1.15 ms per token,   867.41 tokens per second)
0.00.931.985 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.931.985 I llama_perf_context_print:       total time =     156.89 ms /   129 tokens
0.00.932.362 I ggml_metal_free: deallocating

real	0m0.948s
user	0m0.079s
sys	0m0.143s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.007 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.028.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.045 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.046 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.046 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.047 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.048 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.048 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.049 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.049 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.049 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.050 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.050 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.712 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.713 I llama_model_loader: - type  f32:  194 tensors
0.00.036.713 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.713 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.714 I print_info: file format = GGUF V3 (latest)
0.00.036.714 I print_info: file type   = Q5_1
0.00.036.715 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.174 I load: special tokens cache size = 25
0.00.051.581 I load: token to piece cache size = 0.2984 MB
0.00.051.584 I print_info: arch             = gptneox
0.00.051.585 I print_info: vocab_only       = 0
0.00.051.585 I print_info: n_ctx_train      = 2048
0.00.051.585 I print_info: n_embd           = 2048
0.00.051.585 I print_info: n_layer          = 24
0.00.051.588 I print_info: n_head           = 16
0.00.051.589 I print_info: n_head_kv        = 16
0.00.051.589 I print_info: n_rot            = 32
0.00.051.589 I print_info: n_swa            = 0
0.00.051.590 I print_info: n_embd_head_k    = 128
0.00.051.590 I print_info: n_embd_head_v    = 128
0.00.051.592 I print_info: n_gqa            = 1
0.00.051.593 I print_info: n_embd_k_gqa     = 2048
0.00.051.594 I print_info: n_embd_v_gqa     = 2048
0.00.051.594 I print_info: f_norm_eps       = 1.0e-05
0.00.051.595 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.595 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.595 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.595 I print_info: f_logit_scale    = 0.0e+00
0.00.051.596 I print_info: n_ff             = 8192
0.00.051.596 I print_info: n_expert         = 0
0.00.051.596 I print_info: n_expert_used    = 0
0.00.051.596 I print_info: causal attn      = 1
0.00.051.596 I print_info: pooling type     = 0
0.00.051.596 I print_info: rope type        = 2
0.00.051.596 I print_info: rope scaling     = linear
0.00.051.597 I print_info: freq_base_train  = 10000.0
0.00.051.597 I print_info: freq_scale_train = 1
0.00.051.597 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.597 I print_info: rope_finetuned   = unknown
0.00.051.598 I print_info: ssm_d_conv       = 0
0.00.051.598 I print_info: ssm_d_inner      = 0
0.00.051.599 I print_info: ssm_d_state      = 0
0.00.051.599 I print_info: ssm_dt_rank      = 0
0.00.051.599 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.599 I print_info: model type       = 1.4B
0.00.051.600 I print_info: model params     = 1.41 B
0.00.051.600 I print_info: general.name     = 1.4B
0.00.051.600 I print_info: vocab type       = BPE
0.00.051.601 I print_info: n_vocab          = 50304
0.00.051.601 I print_info: n_merges         = 50009
0.00.051.601 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.601 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.601 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.604 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.605 I print_info: LF token         = 187 'Ċ'
0.00.051.605 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.605 I print_info: max token length = 1024
0.00.689.462 I load_tensors: offloading 24 repeating layers to GPU
0.00.689.477 I load_tensors: offloading output layer to GPU
0.00.689.477 I load_tensors: offloaded 25/25 layers to GPU
0.00.689.517 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.689.518 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.690.965 I llama_init_from_model: n_seq_max     = 1
0.00.690.973 I llama_init_from_model: n_ctx         = 2048
0.00.690.974 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.690.974 I llama_init_from_model: n_batch       = 2048
0.00.690.975 I llama_init_from_model: n_ubatch      = 512
0.00.690.975 I llama_init_from_model: flash_attn    = 0
0.00.690.977 I llama_init_from_model: freq_base     = 10000.0
0.00.690.978 I llama_init_from_model: freq_scale    = 1
0.00.690.983 I ggml_metal_init: allocating
0.00.691.115 I ggml_metal_init: found device: Apple M4
0.00.691.129 I ggml_metal_init: picking default device: Apple M4
0.00.693.052 I ggml_metal_init: using embedded metal library
0.00.699.558 I ggml_metal_init: GPU name:   Apple M4
0.00.699.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.699.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.699.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.699.564 I ggml_metal_init: simdgroup reduction   = true
0.00.699.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.699.565 I ggml_metal_init: has residency sets    = true
0.00.699.565 I ggml_metal_init: has bfloat            = true
0.00.699.565 I ggml_metal_init: use bfloat            = true
0.00.699.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.699.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.716.811 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.771.455 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.771.462 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.771.498 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.776.005 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.776.007 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.776.007 I llama_init_from_model: graph nodes  = 967
0.00.776.008 I llama_init_from_model: graph splits = 2
0.00.776.014 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.776.145 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.776.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.428 I main: llama threadpool init, n_threads = 4
0.00.836.470 I 
0.00.836.496 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.496 I 
0.00.836.653 I sampler seed: 1234
0.00.836.658 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.692 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.696 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.696 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.690.248 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50569.80 tokens per second)
0.01.690.249 I llama_perf_context_print:        load time =     826.75 ms
0.01.690.249 I llama_perf_context_print: prompt eval time =      52.19 ms /     7 tokens (    7.46 ms per token,   134.13 tokens per second)
0.01.690.250 I llama_perf_context_print:        eval time =     798.33 ms /    63 runs   (   12.67 ms per token,    78.91 tokens per second)
0.01.690.250 I llama_perf_context_print:       total time =     854.49 ms /    70 tokens
0.01.690.525 I ggml_metal_free: deallocating

real	0m1.707s
user	0m0.111s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.406 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.004 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.005 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.006 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.008 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.008 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.008 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.009 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.009 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.010 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.011 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.012 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.012 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.838 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.609 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.610 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.612 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.612 I llama_model_loader: - type  f32:  194 tensors
0.00.024.612 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.613 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.613 I print_info: file format = GGUF V3 (latest)
0.00.024.614 I print_info: file type   = Q5_1
0.00.024.615 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.840 I load: special tokens cache size = 25
0.00.038.803 I load: token to piece cache size = 0.2984 MB
0.00.038.805 I print_info: arch             = gptneox
0.00.038.805 I print_info: vocab_only       = 0
0.00.038.806 I print_info: n_ctx_train      = 2048
0.00.038.806 I print_info: n_embd           = 2048
0.00.038.806 I print_info: n_layer          = 24
0.00.038.809 I print_info: n_head           = 16
0.00.038.809 I print_info: n_head_kv        = 16
0.00.038.810 I print_info: n_rot            = 32
0.00.038.810 I print_info: n_swa            = 0
0.00.038.811 I print_info: n_embd_head_k    = 128
0.00.038.811 I print_info: n_embd_head_v    = 128
0.00.038.812 I print_info: n_gqa            = 1
0.00.038.813 I print_info: n_embd_k_gqa     = 2048
0.00.038.813 I print_info: n_embd_v_gqa     = 2048
0.00.038.814 I print_info: f_norm_eps       = 1.0e-05
0.00.038.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.815 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.815 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.815 I print_info: f_logit_scale    = 0.0e+00
0.00.038.816 I print_info: n_ff             = 8192
0.00.038.816 I print_info: n_expert         = 0
0.00.038.816 I print_info: n_expert_used    = 0
0.00.038.816 I print_info: causal attn      = 1
0.00.038.816 I print_info: pooling type     = 0
0.00.038.816 I print_info: rope type        = 2
0.00.038.817 I print_info: rope scaling     = linear
0.00.038.817 I print_info: freq_base_train  = 10000.0
0.00.038.817 I print_info: freq_scale_train = 1
0.00.038.818 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.818 I print_info: rope_finetuned   = unknown
0.00.038.819 I print_info: ssm_d_conv       = 0
0.00.038.820 I print_info: ssm_d_inner      = 0
0.00.038.820 I print_info: ssm_d_state      = 0
0.00.038.820 I print_info: ssm_dt_rank      = 0
0.00.038.820 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.820 I print_info: model type       = 1.4B
0.00.038.821 I print_info: model params     = 1.41 B
0.00.038.821 I print_info: general.name     = 1.4B
0.00.038.821 I print_info: vocab type       = BPE
0.00.038.821 I print_info: n_vocab          = 50304
0.00.038.821 I print_info: n_merges         = 50009
0.00.038.822 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: LF token         = 187 'Ċ'
0.00.038.823 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: max token length = 1024
0.00.665.000 I load_tensors: offloading 24 repeating layers to GPU
0.00.665.016 I load_tensors: offloading output layer to GPU
0.00.665.017 I load_tensors: offloaded 25/25 layers to GPU
0.00.665.055 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.665.057 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.666.607 I llama_init_from_model: n_seq_max     = 1
0.00.666.611 I llama_init_from_model: n_ctx         = 128
0.00.666.611 I llama_init_from_model: n_ctx_per_seq = 128
0.00.666.612 I llama_init_from_model: n_batch       = 128
0.00.666.613 I llama_init_from_model: n_ubatch      = 128
0.00.666.613 I llama_init_from_model: flash_attn    = 0
0.00.666.615 I llama_init_from_model: freq_base     = 10000.0
0.00.666.615 I llama_init_from_model: freq_scale    = 1
0.00.666.616 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.666.618 I ggml_metal_init: allocating
0.00.666.687 I ggml_metal_init: found device: Apple M4
0.00.666.701 I ggml_metal_init: picking default device: Apple M4
0.00.668.306 I ggml_metal_init: using embedded metal library
0.00.674.766 I ggml_metal_init: GPU name:   Apple M4
0.00.674.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.772 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.772 I ggml_metal_init: simdgroup reduction   = true
0.00.674.773 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.773 I ggml_metal_init: has residency sets    = true
0.00.674.773 I ggml_metal_init: has bfloat            = true
0.00.674.773 I ggml_metal_init: use bfloat            = true
0.00.674.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.266 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.797 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.801 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.841 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.699.400 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.699.402 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.699.402 I llama_init_from_model: graph nodes  = 967
0.00.699.403 I llama_init_from_model: graph splits = 2
0.00.699.407 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.699.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.831 I 
0.00.729.921 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.929 I perplexity: tokenizing the input ..
0.00.737.044 I perplexity: tokenization took 7.114 ms
0.00.737.050 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.883.334 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.884.772 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.884.791 I llama_perf_context_print:        load time =     720.42 ms
0.00.884.792 I llama_perf_context_print: prompt eval time =     146.03 ms /   128 tokens (    1.14 ms per token,   876.56 tokens per second)
0.00.884.793 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.884.793 I llama_perf_context_print:       total time =     154.97 ms /   129 tokens
0.00.885.155 I ggml_metal_free: deallocating

real	0m0.898s
user	0m0.077s
sys	0m0.143s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.470 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.137 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.140 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.140 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.142 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.142 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.142 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.143 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.146 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.146 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.147 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.151 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.012 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.894 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.896 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.896 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.896 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.897 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.898 I llama_model_loader: - type  f32:  194 tensors
0.00.025.898 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.898 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.898 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.899 I print_info: file format = GGUF V3 (latest)
0.00.025.900 I print_info: file type   = Q2_K - Medium
0.00.025.901 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.047 I load: special tokens cache size = 25
0.00.040.096 I load: token to piece cache size = 0.2984 MB
0.00.040.102 I print_info: arch             = gptneox
0.00.040.103 I print_info: vocab_only       = 0
0.00.040.103 I print_info: n_ctx_train      = 2048
0.00.040.103 I print_info: n_embd           = 2048
0.00.040.103 I print_info: n_layer          = 24
0.00.040.108 I print_info: n_head           = 16
0.00.040.109 I print_info: n_head_kv        = 16
0.00.040.109 I print_info: n_rot            = 32
0.00.040.109 I print_info: n_swa            = 0
0.00.040.109 I print_info: n_embd_head_k    = 128
0.00.040.109 I print_info: n_embd_head_v    = 128
0.00.040.110 I print_info: n_gqa            = 1
0.00.040.111 I print_info: n_embd_k_gqa     = 2048
0.00.040.112 I print_info: n_embd_v_gqa     = 2048
0.00.040.112 I print_info: f_norm_eps       = 1.0e-05
0.00.040.113 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.113 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.113 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.113 I print_info: f_logit_scale    = 0.0e+00
0.00.040.114 I print_info: n_ff             = 8192
0.00.040.114 I print_info: n_expert         = 0
0.00.040.114 I print_info: n_expert_used    = 0
0.00.040.114 I print_info: causal attn      = 1
0.00.040.114 I print_info: pooling type     = 0
0.00.040.115 I print_info: rope type        = 2
0.00.040.115 I print_info: rope scaling     = linear
0.00.040.115 I print_info: freq_base_train  = 10000.0
0.00.040.115 I print_info: freq_scale_train = 1
0.00.040.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.116 I print_info: rope_finetuned   = unknown
0.00.040.116 I print_info: ssm_d_conv       = 0
0.00.040.116 I print_info: ssm_d_inner      = 0
0.00.040.116 I print_info: ssm_d_state      = 0
0.00.040.116 I print_info: ssm_dt_rank      = 0
0.00.040.119 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.119 I print_info: model type       = 1.4B
0.00.040.119 I print_info: model params     = 1.41 B
0.00.040.120 I print_info: general.name     = 1.4B
0.00.040.120 I print_info: vocab type       = BPE
0.00.040.120 I print_info: n_vocab          = 50304
0.00.040.122 I print_info: n_merges         = 50009
0.00.040.122 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.122 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.122 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.122 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.123 I print_info: LF token         = 187 'Ċ'
0.00.040.123 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.123 I print_info: max token length = 1024
0.00.340.060 I load_tensors: offloading 24 repeating layers to GPU
0.00.340.066 I load_tensors: offloading output layer to GPU
0.00.340.067 I load_tensors: offloaded 25/25 layers to GPU
0.00.340.087 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.340.088 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.340.808 I llama_init_from_model: n_seq_max     = 1
0.00.340.813 I llama_init_from_model: n_ctx         = 2048
0.00.340.814 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.340.814 I llama_init_from_model: n_batch       = 2048
0.00.340.814 I llama_init_from_model: n_ubatch      = 512
0.00.340.815 I llama_init_from_model: flash_attn    = 0
0.00.340.816 I llama_init_from_model: freq_base     = 10000.0
0.00.340.820 I llama_init_from_model: freq_scale    = 1
0.00.340.822 I ggml_metal_init: allocating
0.00.340.878 I ggml_metal_init: found device: Apple M4
0.00.340.889 I ggml_metal_init: picking default device: Apple M4
0.00.342.012 I ggml_metal_init: using embedded metal library
0.00.346.252 I ggml_metal_init: GPU name:   Apple M4
0.00.346.258 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.259 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.260 I ggml_metal_init: simdgroup reduction   = true
0.00.346.260 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.262 I ggml_metal_init: has residency sets    = true
0.00.346.262 I ggml_metal_init: has bfloat            = true
0.00.346.270 I ggml_metal_init: use bfloat            = true
0.00.346.271 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.274 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.364.159 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.397.143 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.397.149 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.397.182 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.401.967 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.401.968 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.401.969 I llama_init_from_model: graph nodes  = 967
0.00.401.969 I llama_init_from_model: graph splits = 2
0.00.401.975 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.402.099 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.402.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.461.978 I main: llama threadpool init, n_threads = 4
0.00.462.023 I 
0.00.462.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.462.047 I 
0.00.462.225 I sampler seed: 1234
0.00.462.229 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.462.240 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.462.241 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.462.241 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.150.435 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.150.436 I llama_perf_context_print:        load time =     450.84 ms
0.01.150.437 I llama_perf_context_print: prompt eval time =      44.19 ms /     7 tokens (    6.31 ms per token,   158.40 tokens per second)
0.01.150.438 I llama_perf_context_print:        eval time =     641.11 ms /    63 runs   (   10.18 ms per token,    98.27 tokens per second)
0.01.150.438 I llama_perf_context_print:       total time =     689.12 ms /    70 tokens
0.01.150.682 I ggml_metal_free: deallocating

real	0m1.169s
user	0m0.106s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.455 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.234 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.236 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.241 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.719 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.720 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.720 I llama_model_loader: - type  f32:  194 tensors
0.00.026.720 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.721 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.721 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.722 I print_info: file format = GGUF V3 (latest)
0.00.026.722 I print_info: file type   = Q2_K - Medium
0.00.026.723 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.967 I load: special tokens cache size = 25
0.00.040.721 I load: token to piece cache size = 0.2984 MB
0.00.040.723 I print_info: arch             = gptneox
0.00.040.724 I print_info: vocab_only       = 0
0.00.040.724 I print_info: n_ctx_train      = 2048
0.00.040.724 I print_info: n_embd           = 2048
0.00.040.724 I print_info: n_layer          = 24
0.00.040.727 I print_info: n_head           = 16
0.00.040.727 I print_info: n_head_kv        = 16
0.00.040.727 I print_info: n_rot            = 32
0.00.040.728 I print_info: n_swa            = 0
0.00.040.728 I print_info: n_embd_head_k    = 128
0.00.040.728 I print_info: n_embd_head_v    = 128
0.00.040.729 I print_info: n_gqa            = 1
0.00.040.730 I print_info: n_embd_k_gqa     = 2048
0.00.040.730 I print_info: n_embd_v_gqa     = 2048
0.00.040.731 I print_info: f_norm_eps       = 1.0e-05
0.00.040.731 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.732 I print_info: f_logit_scale    = 0.0e+00
0.00.040.732 I print_info: n_ff             = 8192
0.00.040.733 I print_info: n_expert         = 0
0.00.040.733 I print_info: n_expert_used    = 0
0.00.040.733 I print_info: causal attn      = 1
0.00.040.733 I print_info: pooling type     = 0
0.00.040.733 I print_info: rope type        = 2
0.00.040.733 I print_info: rope scaling     = linear
0.00.040.734 I print_info: freq_base_train  = 10000.0
0.00.040.734 I print_info: freq_scale_train = 1
0.00.040.742 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.744 I print_info: rope_finetuned   = unknown
0.00.040.744 I print_info: ssm_d_conv       = 0
0.00.040.744 I print_info: ssm_d_inner      = 0
0.00.040.744 I print_info: ssm_d_state      = 0
0.00.040.745 I print_info: ssm_dt_rank      = 0
0.00.040.745 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.745 I print_info: model type       = 1.4B
0.00.040.746 I print_info: model params     = 1.41 B
0.00.040.746 I print_info: general.name     = 1.4B
0.00.040.746 I print_info: vocab type       = BPE
0.00.040.747 I print_info: n_vocab          = 50304
0.00.040.748 I print_info: n_merges         = 50009
0.00.040.748 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.748 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.749 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.749 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.749 I print_info: LF token         = 187 'Ċ'
0.00.040.751 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.751 I print_info: max token length = 1024
0.00.353.788 I load_tensors: offloading 24 repeating layers to GPU
0.00.353.800 I load_tensors: offloading output layer to GPU
0.00.353.801 I load_tensors: offloaded 25/25 layers to GPU
0.00.353.829 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.353.830 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.355.236 I llama_init_from_model: n_seq_max     = 1
0.00.355.243 I llama_init_from_model: n_ctx         = 128
0.00.355.244 I llama_init_from_model: n_ctx_per_seq = 128
0.00.355.245 I llama_init_from_model: n_batch       = 128
0.00.355.245 I llama_init_from_model: n_ubatch      = 128
0.00.355.245 I llama_init_from_model: flash_attn    = 0
0.00.355.246 I llama_init_from_model: freq_base     = 10000.0
0.00.355.247 I llama_init_from_model: freq_scale    = 1
0.00.355.247 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.355.250 I ggml_metal_init: allocating
0.00.355.301 I ggml_metal_init: found device: Apple M4
0.00.355.314 I ggml_metal_init: picking default device: Apple M4
0.00.357.014 I ggml_metal_init: using embedded metal library
0.00.362.772 I ggml_metal_init: GPU name:   Apple M4
0.00.362.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.362.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.362.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.362.790 I ggml_metal_init: simdgroup reduction   = true
0.00.362.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.362.790 I ggml_metal_init: has residency sets    = true
0.00.362.790 I ggml_metal_init: has bfloat            = true
0.00.362.791 I ggml_metal_init: use bfloat            = true
0.00.362.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.362.798 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.384.848 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.388.501 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.388.509 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.388.567 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.392.031 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.392.033 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.392.034 I llama_init_from_model: graph nodes  = 967
0.00.392.034 I llama_init_from_model: graph splits = 2
0.00.392.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.392.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.419.169 I 
0.00.419.260 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.419.268 I perplexity: tokenizing the input ..
0.00.426.401 I perplexity: tokenization took 7.13 ms
0.00.426.407 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.452 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.864 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.885 I llama_perf_context_print:        load time =     407.70 ms
0.00.561.887 I llama_perf_context_print: prompt eval time =     133.06 ms /   128 tokens (    1.04 ms per token,   962.01 tokens per second)
0.00.561.888 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.888 I llama_perf_context_print:       total time =     142.72 ms /   129 tokens
0.00.562.232 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.081s
sys	0m0.101s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.987 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.669 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.671 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.672 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.672 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.673 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.673 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.542 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.403 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.405 I llama_model_loader: - type  f32:  194 tensors
0.00.025.405 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.405 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.406 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.406 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.406 I print_info: file format = GGUF V3 (latest)
0.00.025.407 I print_info: file type   = Q3_K - Medium
0.00.025.408 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.510 I load: special tokens cache size = 25
0.00.039.556 I load: token to piece cache size = 0.2984 MB
0.00.039.559 I print_info: arch             = gptneox
0.00.039.559 I print_info: vocab_only       = 0
0.00.039.560 I print_info: n_ctx_train      = 2048
0.00.039.560 I print_info: n_embd           = 2048
0.00.039.560 I print_info: n_layer          = 24
0.00.039.563 I print_info: n_head           = 16
0.00.039.564 I print_info: n_head_kv        = 16
0.00.039.566 I print_info: n_rot            = 32
0.00.039.566 I print_info: n_swa            = 0
0.00.039.566 I print_info: n_embd_head_k    = 128
0.00.039.566 I print_info: n_embd_head_v    = 128
0.00.039.567 I print_info: n_gqa            = 1
0.00.039.568 I print_info: n_embd_k_gqa     = 2048
0.00.039.569 I print_info: n_embd_v_gqa     = 2048
0.00.039.569 I print_info: f_norm_eps       = 1.0e-05
0.00.039.569 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.570 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.570 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.570 I print_info: f_logit_scale    = 0.0e+00
0.00.039.570 I print_info: n_ff             = 8192
0.00.039.571 I print_info: n_expert         = 0
0.00.039.571 I print_info: n_expert_used    = 0
0.00.039.572 I print_info: causal attn      = 1
0.00.039.574 I print_info: pooling type     = 0
0.00.039.574 I print_info: rope type        = 2
0.00.039.574 I print_info: rope scaling     = linear
0.00.039.575 I print_info: freq_base_train  = 10000.0
0.00.039.579 I print_info: freq_scale_train = 1
0.00.039.580 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.580 I print_info: rope_finetuned   = unknown
0.00.039.580 I print_info: ssm_d_conv       = 0
0.00.039.580 I print_info: ssm_d_inner      = 0
0.00.039.581 I print_info: ssm_d_state      = 0
0.00.039.582 I print_info: ssm_dt_rank      = 0
0.00.039.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.582 I print_info: model type       = 1.4B
0.00.039.582 I print_info: model params     = 1.41 B
0.00.039.582 I print_info: general.name     = 1.4B
0.00.039.583 I print_info: vocab type       = BPE
0.00.039.583 I print_info: n_vocab          = 50304
0.00.039.583 I print_info: n_merges         = 50009
0.00.039.583 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.583 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.583 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: LF token         = 187 'Ċ'
0.00.039.587 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: max token length = 1024
0.00.445.403 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.419 I load_tensors: offloading output layer to GPU
0.00.445.420 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.452 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.453 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.446.955 I llama_init_from_model: n_seq_max     = 1
0.00.446.959 I llama_init_from_model: n_ctx         = 2048
0.00.446.960 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.446.960 I llama_init_from_model: n_batch       = 2048
0.00.446.960 I llama_init_from_model: n_ubatch      = 512
0.00.446.961 I llama_init_from_model: flash_attn    = 0
0.00.446.967 I llama_init_from_model: freq_base     = 10000.0
0.00.446.968 I llama_init_from_model: freq_scale    = 1
0.00.446.974 I ggml_metal_init: allocating
0.00.447.050 I ggml_metal_init: found device: Apple M4
0.00.447.064 I ggml_metal_init: picking default device: Apple M4
0.00.448.938 I ggml_metal_init: using embedded metal library
0.00.455.279 I ggml_metal_init: GPU name:   Apple M4
0.00.455.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.287 I ggml_metal_init: simdgroup reduction   = true
0.00.455.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.287 I ggml_metal_init: has residency sets    = true
0.00.455.288 I ggml_metal_init: has bfloat            = true
0.00.455.288 I ggml_metal_init: use bfloat            = true
0.00.455.289 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.997 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.527.719 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.527.725 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.527.806 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.533.806 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.533.809 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.533.809 I llama_init_from_model: graph nodes  = 967
0.00.533.810 I llama_init_from_model: graph splits = 2
0.00.533.815 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.533.945 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.533.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.705 I main: llama threadpool init, n_threads = 4
0.00.591.750 I 
0.00.591.775 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.775 I 
0.00.591.946 I sampler seed: 1234
0.00.591.950 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.591.962 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.591.962 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.591.963 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.343.614 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.343.615 I llama_perf_context_print:        load time =     582.06 ms
0.01.343.615 I llama_perf_context_print: prompt eval time =      49.71 ms /     7 tokens (    7.10 ms per token,   140.82 tokens per second)
0.01.343.616 I llama_perf_context_print:        eval time =     698.96 ms /    63 runs   (   11.09 ms per token,    90.13 tokens per second)
0.01.343.619 I llama_perf_context_print:       total time =     752.57 ms /    70 tokens
0.01.343.858 I ggml_metal_free: deallocating

real	0m1.360s
user	0m0.110s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.392 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.479 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.482 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.484 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.484 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.488 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.343 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.089 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.089 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.090 I llama_model_loader: - type  f32:  194 tensors
0.00.025.091 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.091 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.091 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.092 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.092 I print_info: file format = GGUF V3 (latest)
0.00.025.093 I print_info: file type   = Q3_K - Medium
0.00.025.094 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.216 I load: special tokens cache size = 25
0.00.039.295 I load: token to piece cache size = 0.2984 MB
0.00.039.299 I print_info: arch             = gptneox
0.00.039.299 I print_info: vocab_only       = 0
0.00.039.299 I print_info: n_ctx_train      = 2048
0.00.039.299 I print_info: n_embd           = 2048
0.00.039.300 I print_info: n_layer          = 24
0.00.039.303 I print_info: n_head           = 16
0.00.039.304 I print_info: n_head_kv        = 16
0.00.039.304 I print_info: n_rot            = 32
0.00.039.304 I print_info: n_swa            = 0
0.00.039.305 I print_info: n_embd_head_k    = 128
0.00.039.307 I print_info: n_embd_head_v    = 128
0.00.039.308 I print_info: n_gqa            = 1
0.00.039.309 I print_info: n_embd_k_gqa     = 2048
0.00.039.309 I print_info: n_embd_v_gqa     = 2048
0.00.039.310 I print_info: f_norm_eps       = 1.0e-05
0.00.039.310 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.310 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.311 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.312 I print_info: f_logit_scale    = 0.0e+00
0.00.039.312 I print_info: n_ff             = 8192
0.00.039.312 I print_info: n_expert         = 0
0.00.039.312 I print_info: n_expert_used    = 0
0.00.039.314 I print_info: causal attn      = 1
0.00.039.314 I print_info: pooling type     = 0
0.00.039.314 I print_info: rope type        = 2
0.00.039.314 I print_info: rope scaling     = linear
0.00.039.314 I print_info: freq_base_train  = 10000.0
0.00.039.315 I print_info: freq_scale_train = 1
0.00.039.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.315 I print_info: rope_finetuned   = unknown
0.00.039.315 I print_info: ssm_d_conv       = 0
0.00.039.315 I print_info: ssm_d_inner      = 0
0.00.039.315 I print_info: ssm_d_state      = 0
0.00.039.315 I print_info: ssm_dt_rank      = 0
0.00.039.315 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.316 I print_info: model type       = 1.4B
0.00.039.316 I print_info: model params     = 1.41 B
0.00.039.320 I print_info: general.name     = 1.4B
0.00.039.322 I print_info: vocab type       = BPE
0.00.039.322 I print_info: n_vocab          = 50304
0.00.039.322 I print_info: n_merges         = 50009
0.00.039.322 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.323 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.323 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.323 I print_info: LF token         = 187 'Ċ'
0.00.039.323 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.323 I print_info: max token length = 1024
0.00.453.218 I load_tensors: offloading 24 repeating layers to GPU
0.00.453.226 I load_tensors: offloading output layer to GPU
0.00.453.227 I load_tensors: offloaded 25/25 layers to GPU
0.00.453.241 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.453.242 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.454.081 I llama_init_from_model: n_seq_max     = 1
0.00.454.085 I llama_init_from_model: n_ctx         = 128
0.00.454.085 I llama_init_from_model: n_ctx_per_seq = 128
0.00.454.086 I llama_init_from_model: n_batch       = 128
0.00.454.086 I llama_init_from_model: n_ubatch      = 128
0.00.454.086 I llama_init_from_model: flash_attn    = 0
0.00.454.087 I llama_init_from_model: freq_base     = 10000.0
0.00.454.088 I llama_init_from_model: freq_scale    = 1
0.00.454.089 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.454.090 I ggml_metal_init: allocating
0.00.454.127 I ggml_metal_init: found device: Apple M4
0.00.454.136 I ggml_metal_init: picking default device: Apple M4
0.00.455.135 I ggml_metal_init: using embedded metal library
0.00.459.270 I ggml_metal_init: GPU name:   Apple M4
0.00.459.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.459.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.459.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.459.280 I ggml_metal_init: simdgroup reduction   = true
0.00.459.281 I ggml_metal_init: simdgroup matrix mul. = true
0.00.459.281 I ggml_metal_init: has residency sets    = true
0.00.459.281 I ggml_metal_init: has bfloat            = true
0.00.459.282 I ggml_metal_init: use bfloat            = true
0.00.459.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.459.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.293 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.891 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.476.893 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.476.920 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.478.525 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.478.527 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.478.527 I llama_init_from_model: graph nodes  = 967
0.00.478.527 I llama_init_from_model: graph splits = 2
0.00.478.528 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.119 I 
0.00.502.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.155 I perplexity: tokenizing the input ..
0.00.506.093 I perplexity: tokenization took 3.936 ms
0.00.506.097 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.651.049 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.652.682 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.652.705 I llama_perf_context_print:        load time =     492.72 ms
0.00.652.707 I llama_perf_context_print: prompt eval time =     144.72 ms /   128 tokens (    1.13 ms per token,   884.48 tokens per second)
0.00.652.707 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.652.708 I llama_perf_context_print:       total time =     150.59 ms /   129 tokens
0.00.653.042 I ggml_metal_free: deallocating

real	0m0.668s
user	0m0.071s
sys	0m0.081s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.857 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.092 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.103 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.105 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.107 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.107 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.107 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.108 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.109 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.109 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.109 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.115 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.115 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.116 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.961 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.962 I llama_model_loader: - type  f32:  194 tensors
0.00.025.962 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.963 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.963 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.964 I print_info: file format = GGUF V3 (latest)
0.00.025.964 I print_info: file type   = Q4_K - Medium
0.00.025.965 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.139 I load: special tokens cache size = 25
0.00.040.142 I load: token to piece cache size = 0.2984 MB
0.00.040.144 I print_info: arch             = gptneox
0.00.040.145 I print_info: vocab_only       = 0
0.00.040.145 I print_info: n_ctx_train      = 2048
0.00.040.145 I print_info: n_embd           = 2048
0.00.040.145 I print_info: n_layer          = 24
0.00.040.147 I print_info: n_head           = 16
0.00.040.148 I print_info: n_head_kv        = 16
0.00.040.148 I print_info: n_rot            = 32
0.00.040.148 I print_info: n_swa            = 0
0.00.040.149 I print_info: n_embd_head_k    = 128
0.00.040.149 I print_info: n_embd_head_v    = 128
0.00.040.149 I print_info: n_gqa            = 1
0.00.040.150 I print_info: n_embd_k_gqa     = 2048
0.00.040.151 I print_info: n_embd_v_gqa     = 2048
0.00.040.154 I print_info: f_norm_eps       = 1.0e-05
0.00.040.154 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.155 I print_info: f_logit_scale    = 0.0e+00
0.00.040.155 I print_info: n_ff             = 8192
0.00.040.157 I print_info: n_expert         = 0
0.00.040.157 I print_info: n_expert_used    = 0
0.00.040.157 I print_info: causal attn      = 1
0.00.040.157 I print_info: pooling type     = 0
0.00.040.158 I print_info: rope type        = 2
0.00.040.158 I print_info: rope scaling     = linear
0.00.040.158 I print_info: freq_base_train  = 10000.0
0.00.040.159 I print_info: freq_scale_train = 1
0.00.040.160 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.160 I print_info: rope_finetuned   = unknown
0.00.040.160 I print_info: ssm_d_conv       = 0
0.00.040.160 I print_info: ssm_d_inner      = 0
0.00.040.161 I print_info: ssm_d_state      = 0
0.00.040.161 I print_info: ssm_dt_rank      = 0
0.00.040.161 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.161 I print_info: model type       = 1.4B
0.00.040.161 I print_info: model params     = 1.41 B
0.00.040.162 I print_info: general.name     = 1.4B
0.00.040.162 I print_info: vocab type       = BPE
0.00.040.162 I print_info: n_vocab          = 50304
0.00.040.163 I print_info: n_merges         = 50009
0.00.040.163 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.163 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.163 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.164 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.166 I print_info: LF token         = 187 'Ċ'
0.00.040.166 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.166 I print_info: max token length = 1024
0.00.532.948 I load_tensors: offloading 24 repeating layers to GPU
0.00.532.963 I load_tensors: offloading output layer to GPU
0.00.532.963 I load_tensors: offloaded 25/25 layers to GPU
0.00.532.999 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.000 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.534.417 I llama_init_from_model: n_seq_max     = 1
0.00.534.422 I llama_init_from_model: n_ctx         = 2048
0.00.534.423 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.534.423 I llama_init_from_model: n_batch       = 2048
0.00.534.423 I llama_init_from_model: n_ubatch      = 512
0.00.534.424 I llama_init_from_model: flash_attn    = 0
0.00.534.426 I llama_init_from_model: freq_base     = 10000.0
0.00.534.427 I llama_init_from_model: freq_scale    = 1
0.00.534.429 I ggml_metal_init: allocating
0.00.534.507 I ggml_metal_init: found device: Apple M4
0.00.534.521 I ggml_metal_init: picking default device: Apple M4
0.00.536.390 I ggml_metal_init: using embedded metal library
0.00.542.996 I ggml_metal_init: GPU name:   Apple M4
0.00.543.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.002 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.003 I ggml_metal_init: simdgroup reduction   = true
0.00.543.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.003 I ggml_metal_init: has residency sets    = true
0.00.543.003 I ggml_metal_init: has bfloat            = true
0.00.543.004 I ggml_metal_init: use bfloat            = true
0.00.543.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.381 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.495 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.614.501 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.536 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.243 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.619.245 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.619.246 I llama_init_from_model: graph nodes  = 967
0.00.619.246 I llama_init_from_model: graph splits = 2
0.00.619.253 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.112 I main: llama threadpool init, n_threads = 4
0.00.679.161 I 
0.00.679.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.185 I 
0.00.679.339 I sampler seed: 1234
0.00.679.343 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.354 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.355 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.355 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.445.944 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49339.82 tokens per second)
0.01.445.945 I llama_perf_context_print:        load time =     669.60 ms
0.01.445.946 I llama_perf_context_print: prompt eval time =      57.78 ms /     7 tokens (    8.25 ms per token,   121.15 tokens per second)
0.01.445.947 I llama_perf_context_print:        eval time =     705.71 ms /    63 runs   (   11.20 ms per token,    89.27 tokens per second)
0.01.445.948 I llama_perf_context_print:       total time =     767.48 ms /    70 tokens
0.01.446.178 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.109s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.851 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.064 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.073 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.073 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.073 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.073 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.075 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.075 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.075 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.076 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.076 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.077 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.079 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.079 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.992 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.805 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.806 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.807 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.807 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.807 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.808 I llama_model_loader: - type  f32:  194 tensors
0.00.024.809 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.809 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.809 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.810 I print_info: file format = GGUF V3 (latest)
0.00.024.810 I print_info: file type   = Q4_K - Medium
0.00.024.812 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.264 I load: special tokens cache size = 25
0.00.039.437 I load: token to piece cache size = 0.2984 MB
0.00.039.440 I print_info: arch             = gptneox
0.00.039.441 I print_info: vocab_only       = 0
0.00.039.441 I print_info: n_ctx_train      = 2048
0.00.039.441 I print_info: n_embd           = 2048
0.00.039.441 I print_info: n_layer          = 24
0.00.039.446 I print_info: n_head           = 16
0.00.039.447 I print_info: n_head_kv        = 16
0.00.039.447 I print_info: n_rot            = 32
0.00.039.447 I print_info: n_swa            = 0
0.00.039.448 I print_info: n_embd_head_k    = 128
0.00.039.448 I print_info: n_embd_head_v    = 128
0.00.039.448 I print_info: n_gqa            = 1
0.00.039.449 I print_info: n_embd_k_gqa     = 2048
0.00.039.450 I print_info: n_embd_v_gqa     = 2048
0.00.039.450 I print_info: f_norm_eps       = 1.0e-05
0.00.039.451 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.451 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.451 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.451 I print_info: f_logit_scale    = 0.0e+00
0.00.039.452 I print_info: n_ff             = 8192
0.00.039.452 I print_info: n_expert         = 0
0.00.039.452 I print_info: n_expert_used    = 0
0.00.039.452 I print_info: causal attn      = 1
0.00.039.453 I print_info: pooling type     = 0
0.00.039.453 I print_info: rope type        = 2
0.00.039.453 I print_info: rope scaling     = linear
0.00.039.453 I print_info: freq_base_train  = 10000.0
0.00.039.454 I print_info: freq_scale_train = 1
0.00.039.454 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.454 I print_info: rope_finetuned   = unknown
0.00.039.454 I print_info: ssm_d_conv       = 0
0.00.039.454 I print_info: ssm_d_inner      = 0
0.00.039.455 I print_info: ssm_d_state      = 0
0.00.039.455 I print_info: ssm_dt_rank      = 0
0.00.039.455 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.455 I print_info: model type       = 1.4B
0.00.039.456 I print_info: model params     = 1.41 B
0.00.039.456 I print_info: general.name     = 1.4B
0.00.039.456 I print_info: vocab type       = BPE
0.00.039.457 I print_info: n_vocab          = 50304
0.00.039.457 I print_info: n_merges         = 50009
0.00.039.457 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.457 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.457 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.457 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: LF token         = 187 'Ċ'
0.00.039.458 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: max token length = 1024
0.00.571.973 I load_tensors: offloading 24 repeating layers to GPU
0.00.571.979 I load_tensors: offloading output layer to GPU
0.00.571.980 I load_tensors: offloaded 25/25 layers to GPU
0.00.572.003 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.572.004 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.573.085 I llama_init_from_model: n_seq_max     = 1
0.00.573.088 I llama_init_from_model: n_ctx         = 128
0.00.573.091 I llama_init_from_model: n_ctx_per_seq = 128
0.00.573.091 I llama_init_from_model: n_batch       = 128
0.00.573.091 I llama_init_from_model: n_ubatch      = 128
0.00.573.092 I llama_init_from_model: flash_attn    = 0
0.00.573.093 I llama_init_from_model: freq_base     = 10000.0
0.00.573.094 I llama_init_from_model: freq_scale    = 1
0.00.573.094 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.573.096 I ggml_metal_init: allocating
0.00.573.157 I ggml_metal_init: found device: Apple M4
0.00.573.168 I ggml_metal_init: picking default device: Apple M4
0.00.574.337 I ggml_metal_init: using embedded metal library
0.00.578.906 I ggml_metal_init: GPU name:   Apple M4
0.00.578.917 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.578.918 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.578.918 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.578.919 I ggml_metal_init: simdgroup reduction   = true
0.00.578.919 I ggml_metal_init: simdgroup matrix mul. = true
0.00.578.919 I ggml_metal_init: has residency sets    = true
0.00.578.920 I ggml_metal_init: has bfloat            = true
0.00.578.920 I ggml_metal_init: use bfloat            = true
0.00.578.921 I ggml_metal_init: hasUnifiedMemory      = true
0.00.578.924 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.591.970 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.593.940 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.593.946 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.593.974 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.595.662 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.595.663 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.595.663 I llama_init_from_model: graph nodes  = 967
0.00.595.663 I llama_init_from_model: graph splits = 2
0.00.595.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.595.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.132 I 
0.00.619.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.170 I perplexity: tokenizing the input ..
0.00.623.021 I perplexity: tokenization took 3.849 ms
0.00.623.024 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.264 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.757.637 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.757.659 I llama_perf_context_print:        load time =     610.28 ms
0.00.757.660 I llama_perf_context_print: prompt eval time =     133.01 ms /   128 tokens (    1.04 ms per token,   962.32 tokens per second)
0.00.757.660 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.757.661 I llama_perf_context_print:       total time =     138.53 ms /   129 tokens
0.00.758.069 I ggml_metal_free: deallocating

real	0m0.773s
user	0m0.069s
sys	0m0.109s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.116 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.904 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.915 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.916 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.917 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.922 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.923 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.923 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.793 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.758 I llama_model_loader: - type  f32:  194 tensors
0.00.026.759 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.759 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.760 I print_info: file format = GGUF V3 (latest)
0.00.026.760 I print_info: file type   = Q5_K - Medium
0.00.026.761 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.731 I load: special tokens cache size = 25
0.00.040.710 I load: token to piece cache size = 0.2984 MB
0.00.040.712 I print_info: arch             = gptneox
0.00.040.713 I print_info: vocab_only       = 0
0.00.040.713 I print_info: n_ctx_train      = 2048
0.00.040.713 I print_info: n_embd           = 2048
0.00.040.713 I print_info: n_layer          = 24
0.00.040.716 I print_info: n_head           = 16
0.00.040.717 I print_info: n_head_kv        = 16
0.00.040.717 I print_info: n_rot            = 32
0.00.040.717 I print_info: n_swa            = 0
0.00.040.717 I print_info: n_embd_head_k    = 128
0.00.040.717 I print_info: n_embd_head_v    = 128
0.00.040.718 I print_info: n_gqa            = 1
0.00.040.721 I print_info: n_embd_k_gqa     = 2048
0.00.040.722 I print_info: n_embd_v_gqa     = 2048
0.00.040.722 I print_info: f_norm_eps       = 1.0e-05
0.00.040.723 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.723 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.723 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.723 I print_info: f_logit_scale    = 0.0e+00
0.00.040.724 I print_info: n_ff             = 8192
0.00.040.724 I print_info: n_expert         = 0
0.00.040.724 I print_info: n_expert_used    = 0
0.00.040.724 I print_info: causal attn      = 1
0.00.040.725 I print_info: pooling type     = 0
0.00.040.726 I print_info: rope type        = 2
0.00.040.726 I print_info: rope scaling     = linear
0.00.040.726 I print_info: freq_base_train  = 10000.0
0.00.040.727 I print_info: freq_scale_train = 1
0.00.040.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.727 I print_info: rope_finetuned   = unknown
0.00.040.727 I print_info: ssm_d_conv       = 0
0.00.040.727 I print_info: ssm_d_inner      = 0
0.00.040.729 I print_info: ssm_d_state      = 0
0.00.040.729 I print_info: ssm_dt_rank      = 0
0.00.040.729 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.729 I print_info: model type       = 1.4B
0.00.040.729 I print_info: model params     = 1.41 B
0.00.040.730 I print_info: general.name     = 1.4B
0.00.040.730 I print_info: vocab type       = BPE
0.00.040.730 I print_info: n_vocab          = 50304
0.00.040.731 I print_info: n_merges         = 50009
0.00.040.731 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.735 I print_info: LF token         = 187 'Ċ'
0.00.040.735 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.735 I print_info: max token length = 1024
0.00.596.357 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.369 I load_tensors: offloading output layer to GPU
0.00.596.370 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.405 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.596.406 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.597.667 I llama_init_from_model: n_seq_max     = 1
0.00.597.670 I llama_init_from_model: n_ctx         = 2048
0.00.597.671 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.597.671 I llama_init_from_model: n_batch       = 2048
0.00.597.672 I llama_init_from_model: n_ubatch      = 512
0.00.597.672 I llama_init_from_model: flash_attn    = 0
0.00.597.673 I llama_init_from_model: freq_base     = 10000.0
0.00.597.673 I llama_init_from_model: freq_scale    = 1
0.00.597.675 I ggml_metal_init: allocating
0.00.597.690 I ggml_metal_init: found device: Apple M4
0.00.597.702 I ggml_metal_init: picking default device: Apple M4
0.00.599.145 I ggml_metal_init: using embedded metal library
0.00.605.409 I ggml_metal_init: GPU name:   Apple M4
0.00.605.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.415 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.415 I ggml_metal_init: simdgroup reduction   = true
0.00.605.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.416 I ggml_metal_init: has residency sets    = true
0.00.605.416 I ggml_metal_init: has bfloat            = true
0.00.605.416 I ggml_metal_init: use bfloat            = true
0.00.605.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.550 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.850 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.672.856 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.672.928 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.678.098 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.678.100 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.678.101 I llama_init_from_model: graph nodes  = 967
0.00.678.101 I llama_init_from_model: graph splits = 2
0.00.678.107 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.678.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.678.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.407 I main: llama threadpool init, n_threads = 4
0.00.741.449 I 
0.00.741.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.475 I 
0.00.741.633 I sampler seed: 1234
0.00.741.637 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.648 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.648 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.650 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.587.023 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.587.024 I llama_perf_context_print:        load time =     730.57 ms
0.01.587.025 I llama_perf_context_print: prompt eval time =      51.23 ms /     7 tokens (    7.32 ms per token,   136.63 tokens per second)
0.01.587.025 I llama_perf_context_print:        eval time =     791.29 ms /    63 runs   (   12.56 ms per token,    79.62 tokens per second)
0.01.587.026 I llama_perf_context_print:       total time =     846.33 ms /    70 tokens
0.01.587.254 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.108s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.649 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.510 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.518 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.519 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.519 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.520 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.520 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.521 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.521 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.522 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.522 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.523 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.523 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.525 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.526 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.357 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.413 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.282 I llama_model_loader: - type  f32:  194 tensors
0.00.025.282 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.283 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.283 I print_info: file format = GGUF V3 (latest)
0.00.025.284 I print_info: file type   = Q5_K - Medium
0.00.025.285 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.696 I load: special tokens cache size = 25
0.00.039.774 I load: token to piece cache size = 0.2984 MB
0.00.039.779 I print_info: arch             = gptneox
0.00.039.779 I print_info: vocab_only       = 0
0.00.039.779 I print_info: n_ctx_train      = 2048
0.00.039.779 I print_info: n_embd           = 2048
0.00.039.779 I print_info: n_layer          = 24
0.00.039.784 I print_info: n_head           = 16
0.00.039.785 I print_info: n_head_kv        = 16
0.00.039.785 I print_info: n_rot            = 32
0.00.039.785 I print_info: n_swa            = 0
0.00.039.785 I print_info: n_embd_head_k    = 128
0.00.039.785 I print_info: n_embd_head_v    = 128
0.00.039.786 I print_info: n_gqa            = 1
0.00.039.787 I print_info: n_embd_k_gqa     = 2048
0.00.039.788 I print_info: n_embd_v_gqa     = 2048
0.00.039.788 I print_info: f_norm_eps       = 1.0e-05
0.00.039.789 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.789 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.789 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.789 I print_info: f_logit_scale    = 0.0e+00
0.00.039.790 I print_info: n_ff             = 8192
0.00.039.790 I print_info: n_expert         = 0
0.00.039.790 I print_info: n_expert_used    = 0
0.00.039.790 I print_info: causal attn      = 1
0.00.039.790 I print_info: pooling type     = 0
0.00.039.790 I print_info: rope type        = 2
0.00.039.791 I print_info: rope scaling     = linear
0.00.039.791 I print_info: freq_base_train  = 10000.0
0.00.039.791 I print_info: freq_scale_train = 1
0.00.039.792 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.792 I print_info: rope_finetuned   = unknown
0.00.039.792 I print_info: ssm_d_conv       = 0
0.00.039.792 I print_info: ssm_d_inner      = 0
0.00.039.792 I print_info: ssm_d_state      = 0
0.00.039.792 I print_info: ssm_dt_rank      = 0
0.00.039.792 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.793 I print_info: model type       = 1.4B
0.00.039.793 I print_info: model params     = 1.41 B
0.00.039.793 I print_info: general.name     = 1.4B
0.00.039.795 I print_info: vocab type       = BPE
0.00.039.795 I print_info: n_vocab          = 50304
0.00.039.795 I print_info: n_merges         = 50009
0.00.039.795 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.795 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.795 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.796 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.798 I print_info: LF token         = 187 'Ċ'
0.00.039.798 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.798 I print_info: max token length = 1024
0.00.581.923 I load_tensors: offloading 24 repeating layers to GPU
0.00.581.940 I load_tensors: offloading output layer to GPU
0.00.581.941 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.974 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.581.976 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.583.418 I llama_init_from_model: n_seq_max     = 1
0.00.583.423 I llama_init_from_model: n_ctx         = 128
0.00.583.423 I llama_init_from_model: n_ctx_per_seq = 128
0.00.583.424 I llama_init_from_model: n_batch       = 128
0.00.583.424 I llama_init_from_model: n_ubatch      = 128
0.00.583.424 I llama_init_from_model: flash_attn    = 0
0.00.583.426 I llama_init_from_model: freq_base     = 10000.0
0.00.583.427 I llama_init_from_model: freq_scale    = 1
0.00.583.428 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.583.430 I ggml_metal_init: allocating
0.00.583.499 I ggml_metal_init: found device: Apple M4
0.00.583.512 I ggml_metal_init: picking default device: Apple M4
0.00.585.074 I ggml_metal_init: using embedded metal library
0.00.591.719 I ggml_metal_init: GPU name:   Apple M4
0.00.591.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.591.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.591.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.591.726 I ggml_metal_init: simdgroup reduction   = true
0.00.591.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.591.727 I ggml_metal_init: has residency sets    = true
0.00.591.727 I ggml_metal_init: has bfloat            = true
0.00.591.727 I ggml_metal_init: use bfloat            = true
0.00.591.728 I ggml_metal_init: hasUnifiedMemory      = true
0.00.591.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.609.495 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.612.977 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.612.980 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.613.050 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.616.438 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.616.440 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.616.440 I llama_init_from_model: graph nodes  = 967
0.00.616.441 I llama_init_from_model: graph splits = 2
0.00.616.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.616.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.478 I 
0.00.648.558 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.567 I perplexity: tokenizing the input ..
0.00.655.740 I perplexity: tokenization took 7.169 ms
0.00.655.748 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.153 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.797.494 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.797.518 I llama_perf_context_print:        load time =     638.82 ms
0.00.797.521 I llama_perf_context_print: prompt eval time =     139.69 ms /   128 tokens (    1.09 ms per token,   916.28 tokens per second)
0.00.797.522 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.522 I llama_perf_context_print:       total time =     149.04 ms /   129 tokens
0.00.797.892 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.079s
sys	0m0.124s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.732 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.120 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.122 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.032 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.978 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.978 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.979 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.979 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.980 I llama_model_loader: - type  f32:  194 tensors
0.00.025.980 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.981 I print_info: file format = GGUF V3 (latest)
0.00.025.981 I print_info: file type   = Q6_K
0.00.025.982 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.294 I load: special tokens cache size = 25
0.00.040.379 I load: token to piece cache size = 0.2984 MB
0.00.040.382 I print_info: arch             = gptneox
0.00.040.382 I print_info: vocab_only       = 0
0.00.040.382 I print_info: n_ctx_train      = 2048
0.00.040.382 I print_info: n_embd           = 2048
0.00.040.383 I print_info: n_layer          = 24
0.00.040.385 I print_info: n_head           = 16
0.00.040.386 I print_info: n_head_kv        = 16
0.00.040.386 I print_info: n_rot            = 32
0.00.040.386 I print_info: n_swa            = 0
0.00.040.387 I print_info: n_embd_head_k    = 128
0.00.040.387 I print_info: n_embd_head_v    = 128
0.00.040.388 I print_info: n_gqa            = 1
0.00.040.389 I print_info: n_embd_k_gqa     = 2048
0.00.040.389 I print_info: n_embd_v_gqa     = 2048
0.00.040.391 I print_info: f_norm_eps       = 1.0e-05
0.00.040.391 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.391 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.392 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.392 I print_info: f_logit_scale    = 0.0e+00
0.00.040.393 I print_info: n_ff             = 8192
0.00.040.393 I print_info: n_expert         = 0
0.00.040.393 I print_info: n_expert_used    = 0
0.00.040.393 I print_info: causal attn      = 1
0.00.040.393 I print_info: pooling type     = 0
0.00.040.393 I print_info: rope type        = 2
0.00.040.396 I print_info: rope scaling     = linear
0.00.040.396 I print_info: freq_base_train  = 10000.0
0.00.040.397 I print_info: freq_scale_train = 1
0.00.040.397 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.397 I print_info: rope_finetuned   = unknown
0.00.040.397 I print_info: ssm_d_conv       = 0
0.00.040.397 I print_info: ssm_d_inner      = 0
0.00.040.398 I print_info: ssm_d_state      = 0
0.00.040.398 I print_info: ssm_dt_rank      = 0
0.00.040.398 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.398 I print_info: model type       = 1.4B
0.00.040.399 I print_info: model params     = 1.41 B
0.00.040.399 I print_info: general.name     = 1.4B
0.00.040.399 I print_info: vocab type       = BPE
0.00.040.399 I print_info: n_vocab          = 50304
0.00.040.399 I print_info: n_merges         = 50009
0.00.040.400 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.401 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.401 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: LF token         = 187 'Ċ'
0.00.040.402 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: max token length = 1024
0.00.645.307 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.311 I load_tensors: offloading output layer to GPU
0.00.645.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.334 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.645.335 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.646.549 I llama_init_from_model: n_seq_max     = 1
0.00.646.552 I llama_init_from_model: n_ctx         = 2048
0.00.646.552 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.646.553 I llama_init_from_model: n_batch       = 2048
0.00.646.553 I llama_init_from_model: n_ubatch      = 512
0.00.646.554 I llama_init_from_model: flash_attn    = 0
0.00.646.555 I llama_init_from_model: freq_base     = 10000.0
0.00.646.555 I llama_init_from_model: freq_scale    = 1
0.00.646.556 I ggml_metal_init: allocating
0.00.646.589 I ggml_metal_init: found device: Apple M4
0.00.646.604 I ggml_metal_init: picking default device: Apple M4
0.00.648.043 I ggml_metal_init: using embedded metal library
0.00.653.873 I ggml_metal_init: GPU name:   Apple M4
0.00.653.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.877 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.878 I ggml_metal_init: simdgroup reduction   = true
0.00.653.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.879 I ggml_metal_init: has residency sets    = true
0.00.653.879 I ggml_metal_init: has bfloat            = true
0.00.653.879 I ggml_metal_init: use bfloat            = true
0.00.653.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.223 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.591 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.600 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.721.635 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.018 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.727.020 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.727.020 I llama_init_from_model: graph nodes  = 967
0.00.727.020 I llama_init_from_model: graph splits = 2
0.00.727.025 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.727.155 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.727.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.791.876 I main: llama threadpool init, n_threads = 4
0.00.791.921 I 
0.00.791.947 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.791.948 I 
0.00.792.134 I sampler seed: 1234
0.00.792.139 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.792.150 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.792.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.792.156 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.669.359 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.669.359 I llama_perf_context_print:        load time =     782.47 ms
0.01.669.360 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.73 tokens per second)
0.01.669.361 I llama_perf_context_print:        eval time =     819.89 ms /    63 runs   (   13.01 ms per token,    76.84 tokens per second)
0.01.669.362 I llama_perf_context_print:       total time =     878.15 ms /    70 tokens
0.01.669.628 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.108s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4631 (7c9e0ca5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.878 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.862 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.870 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.871 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.871 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.871 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.872 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.873 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.873 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.873 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.874 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.876 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.876 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.876 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.686 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.502 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.502 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.503 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.503 I llama_model_loader: - type  f32:  194 tensors
0.00.024.504 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.504 I print_info: file format = GGUF V3 (latest)
0.00.024.505 I print_info: file type   = Q6_K
0.00.024.506 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.802 I load: special tokens cache size = 25
0.00.038.497 I load: token to piece cache size = 0.2984 MB
0.00.038.500 I print_info: arch             = gptneox
0.00.038.500 I print_info: vocab_only       = 0
0.00.038.501 I print_info: n_ctx_train      = 2048
0.00.038.501 I print_info: n_embd           = 2048
0.00.038.501 I print_info: n_layer          = 24
0.00.038.504 I print_info: n_head           = 16
0.00.038.505 I print_info: n_head_kv        = 16
0.00.038.505 I print_info: n_rot            = 32
0.00.038.505 I print_info: n_swa            = 0
0.00.038.505 I print_info: n_embd_head_k    = 128
0.00.038.508 I print_info: n_embd_head_v    = 128
0.00.038.509 I print_info: n_gqa            = 1
0.00.038.509 I print_info: n_embd_k_gqa     = 2048
0.00.038.510 I print_info: n_embd_v_gqa     = 2048
0.00.038.511 I print_info: f_norm_eps       = 1.0e-05
0.00.038.512 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.512 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.512 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.513 I print_info: f_logit_scale    = 0.0e+00
0.00.038.513 I print_info: n_ff             = 8192
0.00.038.513 I print_info: n_expert         = 0
0.00.038.513 I print_info: n_expert_used    = 0
0.00.038.514 I print_info: causal attn      = 1
0.00.038.514 I print_info: pooling type     = 0
0.00.038.514 I print_info: rope type        = 2
0.00.038.514 I print_info: rope scaling     = linear
0.00.038.514 I print_info: freq_base_train  = 10000.0
0.00.038.515 I print_info: freq_scale_train = 1
0.00.038.515 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.515 I print_info: rope_finetuned   = unknown
0.00.038.515 I print_info: ssm_d_conv       = 0
0.00.038.515 I print_info: ssm_d_inner      = 0
0.00.038.516 I print_info: ssm_d_state      = 0
0.00.038.516 I print_info: ssm_dt_rank      = 0
0.00.038.516 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.516 I print_info: model type       = 1.4B
0.00.038.517 I print_info: model params     = 1.41 B
0.00.038.517 I print_info: general.name     = 1.4B
0.00.038.519 I print_info: vocab type       = BPE
0.00.038.519 I print_info: n_vocab          = 50304
0.00.038.519 I print_info: n_merges         = 50009
0.00.038.520 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.520 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.523 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.523 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.524 I print_info: LF token         = 187 'Ċ'
0.00.038.524 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.524 I print_info: max token length = 1024
0.00.619.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.413 I load_tensors: offloading output layer to GPU
0.00.619.414 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.438 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.619.440 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.620.770 I llama_init_from_model: n_seq_max     = 1
0.00.620.772 I llama_init_from_model: n_ctx         = 128
0.00.620.773 I llama_init_from_model: n_ctx_per_seq = 128
0.00.620.773 I llama_init_from_model: n_batch       = 128
0.00.620.773 I llama_init_from_model: n_ubatch      = 128
0.00.620.774 I llama_init_from_model: flash_attn    = 0
0.00.620.775 I llama_init_from_model: freq_base     = 10000.0
0.00.620.775 I llama_init_from_model: freq_scale    = 1
0.00.620.776 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.777 I ggml_metal_init: allocating
0.00.620.793 I ggml_metal_init: found device: Apple M4
0.00.620.802 I ggml_metal_init: picking default device: Apple M4
0.00.622.131 I ggml_metal_init: using embedded metal library
0.00.627.910 I ggml_metal_init: GPU name:   Apple M4
0.00.627.914 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.914 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.915 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.916 I ggml_metal_init: simdgroup reduction   = true
0.00.627.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.916 I ggml_metal_init: has residency sets    = true
0.00.627.916 I ggml_metal_init: has bfloat            = true
0.00.627.916 I ggml_metal_init: use bfloat            = true
0.00.627.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.989 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.413 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.422 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.466 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.662 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.664 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.664 I llama_init_from_model: graph nodes  = 967
0.00.650.665 I llama_init_from_model: graph splits = 2
0.00.650.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.518 I 
0.00.686.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.605 I perplexity: tokenizing the input ..
0.00.693.157 I perplexity: tokenization took 6.55 ms
0.00.693.163 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.199 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.834.612 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.834.641 I llama_perf_context_print:        load time =     677.63 ms
0.00.834.642 I llama_perf_context_print: prompt eval time =     139.65 ms /   128 tokens (    1.09 ms per token,   916.60 tokens per second)
0.00.834.643 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.643 I llama_perf_context_print:       total time =     148.13 ms /   129 tokens
0.00.835.048 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.076s
sys	0m0.150s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4631 (7c9e0ca5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10fb04a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10fb05160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10fb05710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10fb05cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10fb06270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10fb06820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10fb06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10fb07380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10fb07930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10fb07e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10fb08330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10fb08830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10fb09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10fb09b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10fb0a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10fb0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10fb0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10fb0b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10fb0bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10fb0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10fb0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10fb0d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10fb0dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10fb0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10fb0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10fb0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10fb0f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10fb101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10fb10700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10fb109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10fb10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10fb11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10fb119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10fb11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10fb121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10fb12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10fb12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10fb12f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10fb13430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10fb138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10fb13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10fb14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10fb146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10fb14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10fb14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10fb15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10fb15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10fb16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10fb16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10fb16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10fb17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10fb17b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10fb181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10fb187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10fb18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10fb19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10fb198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10fb19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10fb1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10fb1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10fb1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10fb1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10fb1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10fb1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10fb1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10fb1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10fb1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10fb1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10fb1d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10fb1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10fb1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10fb1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10fb1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10fb1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10fb1ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10fb1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10fb1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10fb1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10fb203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10fb20910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10fb20e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10fb213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10fb21900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10fb21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10fb223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10fb228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10fb22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10fb23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10fb238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10fb23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10fb24380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10fb248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10fb24e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10fb25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10fb258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10fb25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10fb26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10fb16040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10fb267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10fb26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10fb274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10fb27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10fb27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10fb284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10fb28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10fb28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10fb294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10fb29a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10fb29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10fb2a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10fb2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10fb2af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10fb2b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10fb2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10fb2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10fb2c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10fb2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10fb2cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10fb2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10fb2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10fb2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10fb2de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10fb2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10fb2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10fb2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10fb2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10fb2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10fb2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10fb2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10fb30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10fb307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10fb30c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10fb31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10fb315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10fb31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10fb31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10fb32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10fb32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10fb32cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10fb33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10fb33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10fb33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10fb33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10fb343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10fb34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10fb34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10fb351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10fb35670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10fb35b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10fb35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10fb36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10fb368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10fb36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10fb37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10fb376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10fb37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10fb38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10fb384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10fb38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10fb38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10fb39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10fb39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10fb39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10fb3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10fb3a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10fb3a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10fb3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10fb3b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10fb3b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10fb3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10fb3c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10fb3c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10fb3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10fb3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10fb3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10fb3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10fb3dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10fb3e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10fb3e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10fb3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10fb3ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10fb3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10fb3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10fb3fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10fb40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10fb40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10fb40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10fb40f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10fb41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10fb418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10fb41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10fb421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10fb42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10fb42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10fb43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10fb43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10fb43bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10fb43e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10fb444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10fb44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10fb450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10fb458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10fb45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10fb46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10fb46620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10fb46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10fb47420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10fb478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10fb47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10fb48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10fb489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10fb48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10fb49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10fb499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10fb49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10fb4a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10fb4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10fb4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10fb4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10fb4b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10fb4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10fb4c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10fb4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10fb4cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10fb4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10fb4d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10fb4deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10fb4e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10fb4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10fb4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10fb4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10fb4f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10fb4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10fb503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10fb50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10fb50e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10fb513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10fb51920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10fb51e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10fb523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10fb52910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10fb52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10fb533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10fb53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10fb53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10fb543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10fb548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10fb54e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10fb55390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10fb558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10fb55e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10fb56380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10fb568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10fb56e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10fb57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10fb578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10fb57e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10fb58360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10fb588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10fb58e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10fb59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10fb598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10fb59df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10fb5a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10fb5a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10fb5ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10fb5b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10fb5b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10fb5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10fb5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10fb5c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10fb5ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10fb5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10fb5d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10fb5d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10fb5dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10fb5e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10fb5e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10fb5eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10fb5ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10fb5f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10fb5f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10fb5fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10fb60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10fb60c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10fb61340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10fb61a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10fb61d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10fb62510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10fb627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10fb62de0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.730.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.730.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146204ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146205150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1462055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146205a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146205ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146206310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146206780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146206bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146207060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1462074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146207940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146208040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146208b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146209310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146209b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14620a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14620a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14620b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14620b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14620bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14620c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14620cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14620d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14620db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14620e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14620e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14620e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14620ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14620f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14620f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14620f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14620fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146210350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146210610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146210a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146210ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146211360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1462117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146211c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1462120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146212520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146212990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146212e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146213270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1462136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146213b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146213fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146214430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1462148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146214d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146215180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1462155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146215a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146215ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146216340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1462167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146216d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146217220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146217690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146217b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146217f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1462183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146218850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146218cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146219130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1462195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146219a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146219e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14621a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14621a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14621abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14621b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14621b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14621b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14621bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14621c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14621c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14621cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14621cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14621d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14621d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14621dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14621e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14621e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14621e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14621ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14621f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14621f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14621fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146220020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146220490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146220900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146220d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1462211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146221650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146221ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146221f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1462223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146222810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146222c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1462230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146223560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1462239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146223e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1462242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146224720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146224b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146225000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146225470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1462258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146225d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1462261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146226630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146226aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146226f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146227380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1462277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146227c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1462280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146228540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1462289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146228e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146229290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146229700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146229b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146229fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14622a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14622a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14622ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14622b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14622b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14622ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14622bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14622c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14622c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14622cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14622d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14622d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14622d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14622de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14622e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14622e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14622eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14622efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14622f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14622f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14622fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146230180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1462305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146230a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146230ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146231340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1462317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146231c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146232090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146232500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146232970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146232de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146233250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1462336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146233b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146233fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146234410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146234880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146234cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146235160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146235d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146236050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146236310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146236780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146236bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146237060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1462374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146237940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146237db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146238220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146238690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146238b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146238f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1462393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146239850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146239cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14623a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14623a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14623aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14623ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14623b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14623b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14623bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14623c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14623c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14623c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14623cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14623d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14623d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14623dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14623df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14623e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14623e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14623eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14623f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14623f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14623fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14623fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146240460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1462408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146240d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1462411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1462416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146241be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146242750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146242a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146242fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146243590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146243b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146244110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1462446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146244c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146245250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146245810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146245dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146246390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146246950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146246f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1462474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146247a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146248050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146248610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146248bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146249190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146249750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146249d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14624a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14624a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14624ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14624b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14624b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14624bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14624c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14624cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14624d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14624d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14624dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14624e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14624e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14624ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14624f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14624f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14624fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146250490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146250a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146251010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1462515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146251b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146252150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146252710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146252cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146253290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146253850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146253e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1462543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146254990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146254f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146255510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146255ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146256090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146256650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146256c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146257110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146257610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146257b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146258010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146258510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146258a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146258f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146259410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146259910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146259e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14625a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14625a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14625ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14625b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14625b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14625c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14625c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14625cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14625d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14625d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14625e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14625e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14625ea00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1247044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1247056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1247063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1247078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1247083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12470a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12470a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12470b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12470b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12470bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12470c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12470cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12470d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12470db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12470de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12470e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12470e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12470e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12470ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12470f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12470f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12470fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12470ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1247107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1247110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1247119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1247138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1247141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1247157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1247160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1247185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12471a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12471a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12471a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12471adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12471b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12471b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12471bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12471bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12471c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12471c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12471ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12471d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12471d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12471da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12471de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12471e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12471e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12471ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12471f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12471f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12471f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12471fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1247213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1247229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1247232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124723e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1247258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1247261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124726ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124727360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1247277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124727c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1247280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124728520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1247296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12472a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12472a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12472ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12472b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12472b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12472ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12472bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12472c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12472c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12472cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12472d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12472d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12472d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12472dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12472e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12472e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12472eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12472efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12472f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12472f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12472fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1247305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124730a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1247324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124732950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1247336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1247343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124734860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1247355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1247374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124738210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124738f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1247393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12473a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12473a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12473aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12473ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12473b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12473b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12473bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12473c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12473c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12473c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12473cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12473d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12473d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12473dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12473df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12473e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x135609800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13560bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13560c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13560c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13560ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13560d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13560d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13560dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13560e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13560e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13560eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13560f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13560f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13560fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1356102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135610830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135610d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1356112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135611820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135611d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1356122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135612810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135612d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1356132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135613800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135613d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1356142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1356147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135614d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135615290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1356157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135615d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135616280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1356167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135616d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135617270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1356177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135617d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135618260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1356187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135618d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x135619250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1356197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x135619cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13561a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13561a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13561ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13561b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13561b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13561bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13561c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13561c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13561ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13561d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13561d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13561dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13561e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13561e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13561eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13561f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13561f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13561fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1356201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135620730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135620c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1356211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135622060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135622500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1356229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1356232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135623780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135623c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1356240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135624560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135624a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135624ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1356257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135625c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1356261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1356268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135627010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135627730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135627e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135628110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135628900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135628bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1356291d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.280s
sys	0m0.325s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4631 (7c9e0ca5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12690a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12690ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12690b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12690b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12690bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12690c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12690c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12690cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12690d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12690d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12690dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12690e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12690ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12690f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12690fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126910420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126910b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126911260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126911980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126912150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126912870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126912f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1269136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126913f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126914670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126914930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126914f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126915bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1269160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1269163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126916850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126916b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1269173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1269178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126917ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126918040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1269184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126918980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126918e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1269192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126919760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126919c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12691a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12691a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12691a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12691ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12691b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12691bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12691c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12691c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12691cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12691d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12691db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12691e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12691e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12691ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12691f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12691f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12691fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126920390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126920650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126920af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126920f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126921430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1269218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126921d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126922210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1269226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126922b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126922ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126923490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126923930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126923dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126924320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126924870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126924dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126925310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126925860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126925db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126926300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126926850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126926da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1269272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126927840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126927d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1269282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126928830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126928d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1269292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126929820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126929d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12692a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12692a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12692ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12692b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12692b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12692bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12691ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12692c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12692c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12692cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12692d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12692d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12692deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12692e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12692e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12692eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12692f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12692f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12692fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1269303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126930930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126930e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126931320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1269317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126931c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126932100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1269325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126932a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126932ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126933380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126933820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126933cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126934160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126934600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126934aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126934f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1269353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126935880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126935d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1269361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126936660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126936b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126936fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126937440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1269378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126937d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126938220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1269386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126938b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126939000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1269394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126939940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126939de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12693a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12693a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12693abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12693b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12693b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12693b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12693be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12693c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12693c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12693cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12693d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12693d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12693da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12693dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12693e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12693e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12693ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12693f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12693f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12693fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12693ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1269403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126940840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126940ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126941180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126941620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126941ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126941f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126942400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1269428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126942d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1269431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126943680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126943b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126943fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126944460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126944900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126944da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126945240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1269456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126945b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126946020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1269464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126946960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126946e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1269472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126947740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126947be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126948080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1269485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126948b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126949070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1269495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126949880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126949e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12694a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12694aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12694b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12694b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12694ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12694c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12694c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12694ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12694d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12694d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12694dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12694e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12694e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12694ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12694f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12694f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12694fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126950380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1269508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126950e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126951370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1269518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126951e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126952360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1269528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126952e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126953350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1269538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126953df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126954340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126954890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126954de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126955330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126955880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126955dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126956320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126956870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126956dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126957310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126957860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126957db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126958300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126958850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126958da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1269592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126959840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126959d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12695a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12695a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12695ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12695b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12695b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12695bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12695c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12695c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12695cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12695d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12695d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12695dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12695e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12695e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12695ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12695f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12695f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12695fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126960280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1269607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126960d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1269611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126961660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126961b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126961fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126962440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1269628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126962d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126963220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1269636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126963b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126964000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1269644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126964940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126964de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126965280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1269657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126965ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126966610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126966d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126967450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126967710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126967f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1269681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1269687d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126968480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12694a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126949b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12694a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12691d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12691d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12691f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12694c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126914bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12691b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12691c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12691c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12691aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12691cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126913bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12691fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12692c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1269679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126916dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126917090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12694c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12694ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126915200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1269154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126915780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126968c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126968ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1269691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126969470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126969730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1269699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126969cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126969f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12696a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12696a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12696a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12696aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12696ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12696aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12696b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12696b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12696b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12696baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12696bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12696c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12696c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12696c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12696c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12696cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12696ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12696d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12696d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12696d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12696d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12696dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12696deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12696e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12696e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12696e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12696e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12696ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12696ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12696f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12696f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12696f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12696fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12696fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12696ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126970270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126970530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1269707f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126970ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126970d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126971030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1269712f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1269715b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126971870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126971b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126971df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1269720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126972370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126972630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1269728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126972bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126972e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126973130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1269733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1269736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126973970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126973c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126973ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1269741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126974470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126974730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1269749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126974cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126974f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126975230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1269754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1269757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126975a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126975d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126975ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1269762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126976570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126976830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126976af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126976db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126977070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126977330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1269775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1269778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126977b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126977e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1269780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1269783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126978670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126978930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126978bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126978eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126979170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126979430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1269796f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1269799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126979c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126979f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12697a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12697a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12697a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12697aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12697acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12697afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12697b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12697b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12697b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12697bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12697bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12697c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12697c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12697c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12697c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12697cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12697cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12697d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12697d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12697d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12697d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12697dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12697de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12697e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12697e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12697e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12697e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12697ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12697eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12697f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12697f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12697f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12697f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12697fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12697ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126980230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1269804f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1269807b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126980a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126980d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126980ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1269812b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126981570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126981830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126981af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126981db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126982070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126982330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1269825f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1269828b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126982b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126982e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1269830f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1269833b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126983670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126983930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126983bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126983eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126984170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126984430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1269846f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1269849b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126984c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126984f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1269851f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1269854b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126985770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126985a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126985cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126985fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126986270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126986530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1269867f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126986ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126986d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126987030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1269872f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1269875b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126987870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126987db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126988070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126988510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1269889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126988e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126989600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1269898c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126989b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126989ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12698a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12698a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12698ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12698b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12698b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12698ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12698bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12698c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12698c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12698cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12698d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12698d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12698d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12698de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12698e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12698e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12698eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12698efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12698f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12698f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12698fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126990190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126990600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126990a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126990ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126991350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1269917c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126991c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1269920a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126992510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126992980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126992df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126993260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1269936d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126993b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126993fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126994420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126994890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126994d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126995170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1269955e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126995a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126995ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126996330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1269967a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126996c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126997080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1269974f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126997960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126997dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126998240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1269986b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126998b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126998f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126999400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126999870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126999ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12699a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12699a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12699aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12699aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12699b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12699b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12699bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12699c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12699c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12699c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12699cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12699d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12699dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12699e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12699ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12699f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12699f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12699fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12699ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1269a0570 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124f08600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124f08a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124f08ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124f09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124f097c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124f09c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124f0a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x124f0a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124f0a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124f0adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124f0b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124f0b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124f0c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124f0cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124f0d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124f0dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124f0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124f0e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124f0f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124f0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124f0ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124f10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124f10d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124f11480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124f11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124f11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124f12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124f12590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124f12a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124f12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124f13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124f13880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124f13cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124f13fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124f14420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124f14890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124f14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124f152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124f157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124f15cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124f161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124f166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124f16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124f170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124f175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124f17a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124f17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124f18340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124f187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124f18c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124f19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124f19500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124f19970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124f19de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124f1a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124f1aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124f1aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124f1b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124f1b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124f1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124f1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124f1c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124f1cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124f1d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124f1d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124f1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124f1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124f1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124f1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124f1edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124f1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124f1f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124f1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124f200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124f20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124f20b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124f210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124f21630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124f21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124f220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124f22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124f22b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124f230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124f23610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124f23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124f240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124f24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124f24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124f250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124f255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124f25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124f26090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124f265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124f26b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124f27080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124f275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124f27b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124f28070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124f285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124f28b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x124f29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124f295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x124f29b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124f2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124f2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124f2aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124f2b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124f2b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124f2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124f2c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124f2c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124f2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124f2d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124f2d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124f2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124f2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124f2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124f2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124f2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124f2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124f2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124f2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124f2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124f30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124f307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124f30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124f310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124f31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124f31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124f32360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124f32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124f32ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124f33140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124f335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124f33a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124f33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124f343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124f34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124f34d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124f351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124f35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124f35ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124f35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124f36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124f368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124f36d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124f37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124f376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124f37b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124f37fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124f38480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124f38920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124f38dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124f39260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124f39700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124f39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124f3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124f3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124f3a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124f3ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124f3b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124f3b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124f3bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124f3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124f3c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124f3c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124f3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124f3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124f3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124f3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124f3e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124f3e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124f3ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124f3eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124f3f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124f3f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124f3fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124f40160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124f40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124f40aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124f40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124f413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124f41880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124f41d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124f421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124f42660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124f42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124f42fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124f43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124f438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124f43d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124f44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124f44770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124f44cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124f45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124f45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124f45a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124f46030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124f46640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124f46c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x124f47440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x124f478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124f47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124f481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x124f487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124f48fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124f49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124f498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124f49d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124f4a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124f4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124f4afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124f4b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124f4ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124f4bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124f4c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124f4ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124f4cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124f4d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124f4da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124f4dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124f4e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124f4ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124f4efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124f4f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124f4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124f4ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124f504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124f50a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124f50f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124f514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124f51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124f51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124f524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124f52a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124f52f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124f534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124f53a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124f53f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124f544a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124f549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124f54f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124f55490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124f559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124f55f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124f569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124f56f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124f57470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124f579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124f57f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124f58460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124f589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124f58f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124f59450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124f599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124f59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124f5a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124f5a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124f5aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124f5b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124f5b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124f5bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124f5c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124f5c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124f5cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124f5d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124f5d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124f5dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124f5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124f5e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124f5ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124f5ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124f5f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124f5f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124f5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124f601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124f60640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124f60ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124f60f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124f61420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124f61970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124f62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124f627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124f62ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124f635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124f638b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x124f640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124f64360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124f64970 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.947s
user	0m0.235s
sys	0m0.181s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.46 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.91 sec*proc (2 tests)

Total Test time (real) =   1.92 sec
        1.94 real         0.52 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.08 sys
```
