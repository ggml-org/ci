Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (3.0s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.272s
user	0m1.074s
sys	0m1.544s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Built target build_info
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Built target llava
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target test-c
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-log
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-perf
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-batched
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-imatrix
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-cli
[ 80%] Built target llama-passkey
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-parallel
[ 81%] Generating index.html.gz.hpp
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-retrieval
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-tts
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-llava-clip-quantize-cli
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.147s
user	0m6.494s
sys	0m10.108s

main: quantize time =  7504.49 ms
main:    total time =  7504.49 ms

main: quantize time =  4256.99 ms
main:    total time =  4256.99 ms

main: quantize time =  3362.50 ms
main:    total time =  3362.50 ms

main: quantize time =  3446.37 ms
main:    total time =  3446.37 ms

main: quantize time =  2958.18 ms
main:    total time =  2958.18 ms

main: quantize time =  5045.35 ms
main:    total time =  5045.35 ms

main: quantize time =  5856.25 ms
main:    total time =  5856.25 ms

main: quantize time =  6794.40 ms
main:    total time =  6794.40 ms

main: quantize time =  5787.73 ms
main:    total time =  5787.73 ms

main: quantize time =  4409.50 ms
main:    total time =  4409.50 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.175 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.366 I main: llama backend init
0.00.000.374 I main: load the model and apply lora adapter, if any
0.00.053.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.066.612 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.635 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.638 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.641 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.641 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.658 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.659 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.659 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.660 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.085.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.342 I llama_model_loader: - type  f32:  194 tensors
0.00.085.342 I llama_model_loader: - type  f16:   98 tensors
0.00.085.343 I print_info: file format = GGUF V3 (latest)
0.00.085.344 I print_info: file type   = all F32 (guessed)
0.00.085.346 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.099.013 I load: special tokens cache size = 25
0.00.109.484 I load: token to piece cache size = 0.2984 MB
0.00.109.507 I print_info: arch             = gptneox
0.00.109.508 I print_info: vocab_only       = 0
0.00.109.508 I print_info: n_ctx_train      = 2048
0.00.109.508 I print_info: n_embd           = 2048
0.00.109.509 I print_info: n_layer          = 24
0.00.109.512 I print_info: n_head           = 16
0.00.109.512 I print_info: n_head_kv        = 16
0.00.109.513 I print_info: n_rot            = 32
0.00.109.513 I print_info: n_swa            = 0
0.00.109.513 I print_info: n_embd_head_k    = 128
0.00.109.513 I print_info: n_embd_head_v    = 128
0.00.109.514 I print_info: n_gqa            = 1
0.00.109.515 I print_info: n_embd_k_gqa     = 2048
0.00.109.516 I print_info: n_embd_v_gqa     = 2048
0.00.109.517 I print_info: f_norm_eps       = 1.0e-05
0.00.109.517 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.109.517 I print_info: f_clamp_kqv      = 0.0e+00
0.00.109.518 I print_info: f_max_alibi_bias = 0.0e+00
0.00.109.518 I print_info: f_logit_scale    = 0.0e+00
0.00.109.518 I print_info: n_ff             = 8192
0.00.109.519 I print_info: n_expert         = 0
0.00.109.521 I print_info: n_expert_used    = 0
0.00.109.521 I print_info: causal attn      = 1
0.00.109.521 I print_info: pooling type     = 0
0.00.109.521 I print_info: rope type        = 2
0.00.109.522 I print_info: rope scaling     = linear
0.00.109.522 I print_info: freq_base_train  = 10000.0
0.00.109.522 I print_info: freq_scale_train = 1
0.00.109.524 I print_info: n_ctx_orig_yarn  = 2048
0.00.109.524 I print_info: rope_finetuned   = unknown
0.00.109.524 I print_info: ssm_d_conv       = 0
0.00.109.524 I print_info: ssm_d_inner      = 0
0.00.109.525 I print_info: ssm_d_state      = 0
0.00.109.525 I print_info: ssm_dt_rank      = 0
0.00.109.525 I print_info: ssm_dt_b_c_rms   = 0
0.00.109.525 I print_info: model type       = 1.4B
0.00.109.525 I print_info: model params     = 1.41 B
0.00.109.526 I print_info: general.name     = 1.4B
0.00.109.526 I print_info: vocab type       = BPE
0.00.109.526 I print_info: n_vocab          = 50304
0.00.109.526 I print_info: n_merges         = 50009
0.00.109.527 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.109.527 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.109.527 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.109.527 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.109.528 I print_info: LF token         = 187 'Ċ'
0.00.109.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.109.528 I print_info: max token length = 1024
0.00.109.529 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.150.044 I load_tensors: offloading 24 repeating layers to GPU
0.00.150.048 I load_tensors: offloading output layer to GPU
0.00.150.048 I load_tensors: offloaded 25/25 layers to GPU
0.00.150.074 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.150.075 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.150.504 I llama_init_from_model: n_seq_max     = 1
0.00.150.505 I llama_init_from_model: n_ctx         = 2048
0.00.150.505 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.150.505 I llama_init_from_model: n_batch       = 2048
0.00.150.506 I llama_init_from_model: n_ubatch      = 512
0.00.150.506 I llama_init_from_model: flash_attn    = 0
0.00.150.506 I llama_init_from_model: freq_base     = 10000.0
0.00.150.506 I llama_init_from_model: freq_scale    = 1
0.00.150.508 I ggml_metal_init: allocating
0.00.150.547 I ggml_metal_init: found device: Apple M4
0.00.150.553 I ggml_metal_init: picking default device: Apple M4
0.00.151.102 I ggml_metal_init: using embedded metal library
0.00.172.845 I ggml_metal_init: GPU name:   Apple M4
0.00.172.847 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.172.847 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.172.847 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.172.848 I ggml_metal_init: simdgroup reduction   = true
0.00.172.848 I ggml_metal_init: simdgroup matrix mul. = true
0.00.172.848 I ggml_metal_init: has residency sets    = true
0.00.172.848 I ggml_metal_init: has bfloat            = true
0.00.172.848 I ggml_metal_init: use bfloat            = true
0.00.172.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.172.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.264.996 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.293.276 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.293.282 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.293.314 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.297.654 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.297.657 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.297.657 I llama_init_from_model: graph nodes  = 967
0.00.297.657 I llama_init_from_model: graph splits = 2
0.00.297.664 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.297.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.297.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.362.977 I main: llama threadpool init, n_threads = 4
0.00.363.038 I 
0.00.363.068 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.363.069 I 
0.00.363.263 I sampler seed: 1234
0.00.363.267 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.363.302 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.363.303 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.363.304 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.196.719 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.02.196.720 I llama_perf_context_print:        load time =     308.15 ms
0.02.196.721 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.25 tokens per second)
0.02.196.721 I llama_perf_context_print:        eval time =    1786.85 ms /    63 runs   (   28.36 ms per token,    35.26 tokens per second)
0.02.196.724 I llama_perf_context_print:       total time =    1834.72 ms /    70 tokens
0.02.196.971 I ggml_metal_free: deallocating

real	0m2.536s
user	0m0.134s
sys	0m0.135s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.878 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.301 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.310 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.311 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.311 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.216 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.036 I llama_model_loader: - type  f32:  194 tensors
0.00.028.037 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.037 I print_info: file format = GGUF V3 (latest)
0.00.028.038 I print_info: file type   = Q8_0
0.00.028.042 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.502 I load: special tokens cache size = 25
0.00.042.921 I load: token to piece cache size = 0.2984 MB
0.00.042.939 I print_info: arch             = gptneox
0.00.042.940 I print_info: vocab_only       = 0
0.00.042.941 I print_info: n_ctx_train      = 2048
0.00.042.941 I print_info: n_embd           = 2048
0.00.042.941 I print_info: n_layer          = 24
0.00.042.948 I print_info: n_head           = 16
0.00.042.949 I print_info: n_head_kv        = 16
0.00.042.949 I print_info: n_rot            = 32
0.00.042.949 I print_info: n_swa            = 0
0.00.042.949 I print_info: n_embd_head_k    = 128
0.00.042.949 I print_info: n_embd_head_v    = 128
0.00.042.950 I print_info: n_gqa            = 1
0.00.042.951 I print_info: n_embd_k_gqa     = 2048
0.00.042.952 I print_info: n_embd_v_gqa     = 2048
0.00.042.953 I print_info: f_norm_eps       = 1.0e-05
0.00.042.953 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.956 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.956 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.957 I print_info: f_logit_scale    = 0.0e+00
0.00.042.957 I print_info: n_ff             = 8192
0.00.042.957 I print_info: n_expert         = 0
0.00.042.957 I print_info: n_expert_used    = 0
0.00.042.957 I print_info: causal attn      = 1
0.00.042.958 I print_info: pooling type     = 0
0.00.042.958 I print_info: rope type        = 2
0.00.042.958 I print_info: rope scaling     = linear
0.00.042.958 I print_info: freq_base_train  = 10000.0
0.00.042.959 I print_info: freq_scale_train = 1
0.00.042.959 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.959 I print_info: rope_finetuned   = unknown
0.00.042.959 I print_info: ssm_d_conv       = 0
0.00.042.959 I print_info: ssm_d_inner      = 0
0.00.042.963 I print_info: ssm_d_state      = 0
0.00.042.964 I print_info: ssm_dt_rank      = 0
0.00.042.964 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.964 I print_info: model type       = 1.4B
0.00.042.965 I print_info: model params     = 1.41 B
0.00.042.965 I print_info: general.name     = 1.4B
0.00.042.966 I print_info: vocab type       = BPE
0.00.042.966 I print_info: n_vocab          = 50304
0.00.042.966 I print_info: n_merges         = 50009
0.00.042.966 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.966 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.967 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.967 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.967 I print_info: LF token         = 187 'Ċ'
0.00.042.967 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.967 I print_info: max token length = 1024
0.00.042.968 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.336.742 I load_tensors: offloading 24 repeating layers to GPU
0.01.336.747 I load_tensors: offloading output layer to GPU
0.01.336.749 I load_tensors: offloaded 25/25 layers to GPU
0.01.336.773 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.336.776 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.337.834 I llama_init_from_model: n_seq_max     = 1
0.01.337.836 I llama_init_from_model: n_ctx         = 2048
0.01.337.836 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.337.836 I llama_init_from_model: n_batch       = 2048
0.01.337.837 I llama_init_from_model: n_ubatch      = 512
0.01.337.837 I llama_init_from_model: flash_attn    = 0
0.01.337.838 I llama_init_from_model: freq_base     = 10000.0
0.01.337.839 I llama_init_from_model: freq_scale    = 1
0.01.337.840 I ggml_metal_init: allocating
0.01.337.865 I ggml_metal_init: found device: Apple M4
0.01.337.876 I ggml_metal_init: picking default device: Apple M4
0.01.339.074 I ggml_metal_init: using embedded metal library
0.01.345.056 I ggml_metal_init: GPU name:   Apple M4
0.01.345.059 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.345.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.345.060 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.345.060 I ggml_metal_init: simdgroup reduction   = true
0.01.345.061 I ggml_metal_init: simdgroup matrix mul. = true
0.01.345.061 I ggml_metal_init: has residency sets    = true
0.01.345.061 I ggml_metal_init: has bfloat            = true
0.01.345.061 I ggml_metal_init: use bfloat            = true
0.01.345.062 I ggml_metal_init: hasUnifiedMemory      = true
0.01.345.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.361.444 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.412.818 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.412.829 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.412.907 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.416.969 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.416.971 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.416.971 I llama_init_from_model: graph nodes  = 967
0.01.416.971 I llama_init_from_model: graph splits = 2
0.01.416.979 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.417.092 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.417.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.474.347 I main: llama threadpool init, n_threads = 4
0.01.474.393 I 
0.01.474.414 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.474.417 I 
0.01.474.573 I sampler seed: 1234
0.01.474.577 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.474.621 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.474.624 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.474.625 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.559.044 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52014.65 tokens per second)
0.02.559.045 I llama_perf_context_print:        load time =    1463.72 ms
0.02.559.046 I llama_perf_context_print: prompt eval time =      49.02 ms /     7 tokens (    7.00 ms per token,   142.78 tokens per second)
0.02.559.047 I llama_perf_context_print:        eval time =    1032.41 ms /    63 runs   (   16.39 ms per token,    61.02 tokens per second)
0.02.559.048 I llama_perf_context_print:       total time =    1085.44 ms /    70 tokens
0.02.559.278 I ggml_metal_free: deallocating

real	0m2.582s
user	0m0.108s
sys	0m0.289s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.074 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.110 I main: llama backend init
0.00.000.112 I main: load the model and apply lora adapter, if any
0.00.020.099 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.838 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.848 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.849 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.849 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.850 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.851 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.852 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.852 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.852 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.853 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.853 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.854 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.855 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.855 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.856 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.591 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.888 I llama_model_loader: - type  f32:  194 tensors
0.00.051.889 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.889 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.890 I print_info: file format = GGUF V3 (latest)
0.00.051.895 I print_info: file type   = Q4_0
0.00.051.897 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.069.392 I load: special tokens cache size = 25
0.00.081.619 I load: token to piece cache size = 0.2984 MB
0.00.081.635 I print_info: arch             = gptneox
0.00.081.636 I print_info: vocab_only       = 0
0.00.081.637 I print_info: n_ctx_train      = 2048
0.00.081.637 I print_info: n_embd           = 2048
0.00.081.637 I print_info: n_layer          = 24
0.00.081.642 I print_info: n_head           = 16
0.00.081.643 I print_info: n_head_kv        = 16
0.00.081.643 I print_info: n_rot            = 32
0.00.081.645 I print_info: n_swa            = 0
0.00.081.646 I print_info: n_embd_head_k    = 128
0.00.081.646 I print_info: n_embd_head_v    = 128
0.00.081.647 I print_info: n_gqa            = 1
0.00.081.648 I print_info: n_embd_k_gqa     = 2048
0.00.081.649 I print_info: n_embd_v_gqa     = 2048
0.00.081.650 I print_info: f_norm_eps       = 1.0e-05
0.00.081.650 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.650 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.651 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.653 I print_info: f_logit_scale    = 0.0e+00
0.00.081.654 I print_info: n_ff             = 8192
0.00.081.654 I print_info: n_expert         = 0
0.00.081.654 I print_info: n_expert_used    = 0
0.00.081.654 I print_info: causal attn      = 1
0.00.081.655 I print_info: pooling type     = 0
0.00.081.655 I print_info: rope type        = 2
0.00.081.657 I print_info: rope scaling     = linear
0.00.081.660 I print_info: freq_base_train  = 10000.0
0.00.081.660 I print_info: freq_scale_train = 1
0.00.081.660 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.661 I print_info: rope_finetuned   = unknown
0.00.081.661 I print_info: ssm_d_conv       = 0
0.00.081.661 I print_info: ssm_d_inner      = 0
0.00.081.661 I print_info: ssm_d_state      = 0
0.00.081.662 I print_info: ssm_dt_rank      = 0
0.00.081.662 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.662 I print_info: model type       = 1.4B
0.00.081.663 I print_info: model params     = 1.41 B
0.00.081.663 I print_info: general.name     = 1.4B
0.00.081.665 I print_info: vocab type       = BPE
0.00.081.666 I print_info: n_vocab          = 50304
0.00.081.666 I print_info: n_merges         = 50009
0.00.081.668 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.668 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.669 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.669 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.669 I print_info: LF token         = 187 'Ċ'
0.00.081.670 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.670 I print_info: max token length = 1024
0.00.081.670 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.640 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.656 I load_tensors: offloading output layer to GPU
0.00.652.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.692 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.652.697 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.654.055 I llama_init_from_model: n_seq_max     = 1
0.00.654.058 I llama_init_from_model: n_ctx         = 2048
0.00.654.059 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.060 I llama_init_from_model: n_batch       = 2048
0.00.654.060 I llama_init_from_model: n_ubatch      = 512
0.00.654.060 I llama_init_from_model: flash_attn    = 0
0.00.654.062 I llama_init_from_model: freq_base     = 10000.0
0.00.654.063 I llama_init_from_model: freq_scale    = 1
0.00.654.067 I ggml_metal_init: allocating
0.00.654.149 I ggml_metal_init: found device: Apple M4
0.00.654.163 I ggml_metal_init: picking default device: Apple M4
0.00.655.741 I ggml_metal_init: using embedded metal library
0.00.661.724 I ggml_metal_init: GPU name:   Apple M4
0.00.661.728 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.729 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.730 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.731 I ggml_metal_init: simdgroup reduction   = true
0.00.661.731 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.732 I ggml_metal_init: has residency sets    = true
0.00.661.732 I ggml_metal_init: has bfloat            = true
0.00.661.732 I ggml_metal_init: use bfloat            = true
0.00.661.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.738 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.969 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.735.976 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.000 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.846 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.740.848 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.740.849 I llama_init_from_model: graph nodes  = 967
0.00.740.849 I llama_init_from_model: graph splits = 2
0.00.740.855 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.985 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.986 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.965 I main: llama threadpool init, n_threads = 4
0.00.795.009 I 
0.00.795.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.032 I 
0.00.795.185 I sampler seed: 1234
0.00.795.189 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.204 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.206 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.206 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.477.921 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50750.54 tokens per second)
0.01.477.922 I llama_perf_context_print:        load time =     774.14 ms
0.01.477.923 I llama_perf_context_print: prompt eval time =      48.98 ms /     7 tokens (    7.00 ms per token,   142.93 tokens per second)
0.01.477.924 I llama_perf_context_print:        eval time =     630.97 ms /    63 runs   (   10.02 ms per token,    99.85 tokens per second)
0.01.477.924 I llama_perf_context_print:       total time =     683.68 ms /    70 tokens
0.01.478.206 I ggml_metal_free: deallocating

real	0m1.520s
user	0m0.132s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.804 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.884 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.593 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.594 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.594 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.594 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.595 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.595 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.595 I llama_model_loader: - type  f32:  194 tensors
0.00.033.596 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.596 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.596 I print_info: file format = GGUF V3 (latest)
0.00.033.597 I print_info: file type   = Q4_1
0.00.033.598 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.041.972 I load: special tokens cache size = 25
0.00.048.362 I load: token to piece cache size = 0.2984 MB
0.00.048.376 I print_info: arch             = gptneox
0.00.048.377 I print_info: vocab_only       = 0
0.00.048.377 I print_info: n_ctx_train      = 2048
0.00.048.378 I print_info: n_embd           = 2048
0.00.048.378 I print_info: n_layer          = 24
0.00.048.381 I print_info: n_head           = 16
0.00.048.381 I print_info: n_head_kv        = 16
0.00.048.382 I print_info: n_rot            = 32
0.00.048.382 I print_info: n_swa            = 0
0.00.048.382 I print_info: n_embd_head_k    = 128
0.00.048.382 I print_info: n_embd_head_v    = 128
0.00.048.383 I print_info: n_gqa            = 1
0.00.048.384 I print_info: n_embd_k_gqa     = 2048
0.00.048.385 I print_info: n_embd_v_gqa     = 2048
0.00.048.385 I print_info: f_norm_eps       = 1.0e-05
0.00.048.385 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.386 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.388 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.388 I print_info: f_logit_scale    = 0.0e+00
0.00.048.388 I print_info: n_ff             = 8192
0.00.048.389 I print_info: n_expert         = 0
0.00.048.389 I print_info: n_expert_used    = 0
0.00.048.389 I print_info: causal attn      = 1
0.00.048.389 I print_info: pooling type     = 0
0.00.048.389 I print_info: rope type        = 2
0.00.048.390 I print_info: rope scaling     = linear
0.00.048.390 I print_info: freq_base_train  = 10000.0
0.00.048.391 I print_info: freq_scale_train = 1
0.00.048.391 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.392 I print_info: rope_finetuned   = unknown
0.00.048.392 I print_info: ssm_d_conv       = 0
0.00.048.392 I print_info: ssm_d_inner      = 0
0.00.048.392 I print_info: ssm_d_state      = 0
0.00.048.392 I print_info: ssm_dt_rank      = 0
0.00.048.392 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.393 I print_info: model type       = 1.4B
0.00.048.393 I print_info: model params     = 1.41 B
0.00.048.393 I print_info: general.name     = 1.4B
0.00.048.394 I print_info: vocab type       = BPE
0.00.048.394 I print_info: n_vocab          = 50304
0.00.048.394 I print_info: n_merges         = 50009
0.00.048.394 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.394 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.395 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.396 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.396 I print_info: LF token         = 187 'Ċ'
0.00.048.396 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.396 I print_info: max token length = 1024
0.00.048.397 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.804.328 I load_tensors: offloading 24 repeating layers to GPU
0.00.804.337 I load_tensors: offloading output layer to GPU
0.00.804.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.804.363 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.804.364 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.805.486 I llama_init_from_model: n_seq_max     = 1
0.00.805.494 I llama_init_from_model: n_ctx         = 2048
0.00.805.495 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.805.495 I llama_init_from_model: n_batch       = 2048
0.00.805.496 I llama_init_from_model: n_ubatch      = 512
0.00.805.496 I llama_init_from_model: flash_attn    = 0
0.00.805.498 I llama_init_from_model: freq_base     = 10000.0
0.00.805.498 I llama_init_from_model: freq_scale    = 1
0.00.805.501 I ggml_metal_init: allocating
0.00.805.563 I ggml_metal_init: found device: Apple M4
0.00.805.600 I ggml_metal_init: picking default device: Apple M4
0.00.806.819 I ggml_metal_init: using embedded metal library
0.00.811.919 I ggml_metal_init: GPU name:   Apple M4
0.00.811.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.811.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.811.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.811.928 I ggml_metal_init: simdgroup reduction   = true
0.00.811.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.811.929 I ggml_metal_init: has residency sets    = true
0.00.811.929 I ggml_metal_init: has bfloat            = true
0.00.811.929 I ggml_metal_init: use bfloat            = true
0.00.811.931 I ggml_metal_init: hasUnifiedMemory      = true
0.00.811.933 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.824.375 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.856.829 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.856.835 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.856.866 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.861.496 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.861.497 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.861.498 I llama_init_from_model: graph nodes  = 967
0.00.861.498 I llama_init_from_model: graph splits = 2
0.00.861.504 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.861.633 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.861.634 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.916.356 I main: llama threadpool init, n_threads = 4
0.00.916.400 I 
0.00.916.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.916.421 I 
0.00.916.576 I sampler seed: 1234
0.00.916.581 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.916.596 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.916.597 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.916.597 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.649.650 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55642.63 tokens per second)
0.01.649.650 I llama_perf_context_print:        load time =     906.79 ms
0.01.649.651 I llama_perf_context_print: prompt eval time =      48.79 ms /     7 tokens (    6.97 ms per token,   143.47 tokens per second)
0.01.649.652 I llama_perf_context_print:        eval time =     681.45 ms /    63 runs   (   10.82 ms per token,    92.45 tokens per second)
0.01.649.652 I llama_perf_context_print:       total time =     734.05 ms /    70 tokens
0.01.649.853 I ggml_metal_free: deallocating

real	0m1.666s
user	0m0.102s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.747 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.392 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.398 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.398 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.399 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.401 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.401 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.140 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.035.111 I llama_model_loader: - type  f32:  194 tensors
0.00.035.112 I llama_model_loader: - type q5_0:   97 tensors
0.00.035.112 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.112 I print_info: file format = GGUF V3 (latest)
0.00.035.113 I print_info: file type   = Q5_0
0.00.035.114 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.911 I load: special tokens cache size = 25
0.00.051.438 I load: token to piece cache size = 0.2984 MB
0.00.051.453 I print_info: arch             = gptneox
0.00.051.454 I print_info: vocab_only       = 0
0.00.051.454 I print_info: n_ctx_train      = 2048
0.00.051.455 I print_info: n_embd           = 2048
0.00.051.455 I print_info: n_layer          = 24
0.00.051.458 I print_info: n_head           = 16
0.00.051.458 I print_info: n_head_kv        = 16
0.00.051.459 I print_info: n_rot            = 32
0.00.051.459 I print_info: n_swa            = 0
0.00.051.459 I print_info: n_embd_head_k    = 128
0.00.051.459 I print_info: n_embd_head_v    = 128
0.00.051.460 I print_info: n_gqa            = 1
0.00.051.460 I print_info: n_embd_k_gqa     = 2048
0.00.051.461 I print_info: n_embd_v_gqa     = 2048
0.00.051.462 I print_info: f_norm_eps       = 1.0e-05
0.00.051.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.465 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.465 I print_info: f_logit_scale    = 0.0e+00
0.00.051.466 I print_info: n_ff             = 8192
0.00.051.466 I print_info: n_expert         = 0
0.00.051.466 I print_info: n_expert_used    = 0
0.00.051.466 I print_info: causal attn      = 1
0.00.051.467 I print_info: pooling type     = 0
0.00.051.468 I print_info: rope type        = 2
0.00.051.470 I print_info: rope scaling     = linear
0.00.051.470 I print_info: freq_base_train  = 10000.0
0.00.051.470 I print_info: freq_scale_train = 1
0.00.051.470 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.478 I print_info: rope_finetuned   = unknown
0.00.051.479 I print_info: ssm_d_conv       = 0
0.00.051.479 I print_info: ssm_d_inner      = 0
0.00.051.479 I print_info: ssm_d_state      = 0
0.00.051.479 I print_info: ssm_dt_rank      = 0
0.00.051.479 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.481 I print_info: model type       = 1.4B
0.00.051.481 I print_info: model params     = 1.41 B
0.00.051.481 I print_info: general.name     = 1.4B
0.00.051.482 I print_info: vocab type       = BPE
0.00.051.482 I print_info: n_vocab          = 50304
0.00.051.482 I print_info: n_merges         = 50009
0.00.051.483 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.484 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.484 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.485 I print_info: LF token         = 187 'Ċ'
0.00.051.485 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.485 I print_info: max token length = 1024
0.00.051.486 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.738.900 I load_tensors: offloading 24 repeating layers to GPU
0.00.738.917 I load_tensors: offloading output layer to GPU
0.00.738.918 I load_tensors: offloaded 25/25 layers to GPU
0.00.738.953 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.738.954 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.740.686 I llama_init_from_model: n_seq_max     = 1
0.00.740.689 I llama_init_from_model: n_ctx         = 2048
0.00.740.690 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.740.690 I llama_init_from_model: n_batch       = 2048
0.00.740.691 I llama_init_from_model: n_ubatch      = 512
0.00.740.691 I llama_init_from_model: flash_attn    = 0
0.00.740.693 I llama_init_from_model: freq_base     = 10000.0
0.00.740.694 I llama_init_from_model: freq_scale    = 1
0.00.740.696 I ggml_metal_init: allocating
0.00.740.773 I ggml_metal_init: found device: Apple M4
0.00.740.786 I ggml_metal_init: picking default device: Apple M4
0.00.742.374 I ggml_metal_init: using embedded metal library
0.00.748.988 I ggml_metal_init: GPU name:   Apple M4
0.00.748.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.748.994 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.748.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.748.995 I ggml_metal_init: simdgroup reduction   = true
0.00.748.995 I ggml_metal_init: simdgroup matrix mul. = true
0.00.748.996 I ggml_metal_init: has residency sets    = true
0.00.748.996 I ggml_metal_init: has bfloat            = true
0.00.748.996 I ggml_metal_init: use bfloat            = true
0.00.748.997 I ggml_metal_init: hasUnifiedMemory      = true
0.00.749.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.767.887 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.821.523 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.821.529 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.821.562 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.825.752 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.825.754 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.825.754 I llama_init_from_model: graph nodes  = 967
0.00.825.754 I llama_init_from_model: graph splits = 2
0.00.825.760 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.825.892 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.825.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.883.099 I main: llama threadpool init, n_threads = 4
0.00.883.145 I 
0.00.883.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.883.169 I 
0.00.883.324 I sampler seed: 1234
0.00.883.329 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.883.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.883.344 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.883.344 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.674.232 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49824.56 tokens per second)
0.01.674.233 I llama_perf_context_print:        load time =     873.63 ms
0.01.674.234 I llama_perf_context_print: prompt eval time =      42.83 ms /     7 tokens (    6.12 ms per token,   163.45 tokens per second)
0.01.674.235 I llama_perf_context_print:        eval time =     745.08 ms /    63 runs   (   11.83 ms per token,    84.56 tokens per second)
0.01.674.236 I llama_perf_context_print:       total time =     791.85 ms /    70 tokens
0.01.674.480 I ggml_metal_free: deallocating

real	0m1.690s
user	0m0.112s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.156 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.467 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.472 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.276 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.041 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.041 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.042 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.042 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.042 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.043 I llama_model_loader: - type  f32:  194 tensors
0.00.027.043 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.043 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.044 I print_info: file format = GGUF V3 (latest)
0.00.027.044 I print_info: file type   = Q5_1
0.00.027.045 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.874 I load: special tokens cache size = 25
0.00.041.270 I load: token to piece cache size = 0.2984 MB
0.00.041.284 I print_info: arch             = gptneox
0.00.041.285 I print_info: vocab_only       = 0
0.00.041.285 I print_info: n_ctx_train      = 2048
0.00.041.285 I print_info: n_embd           = 2048
0.00.041.285 I print_info: n_layer          = 24
0.00.041.288 I print_info: n_head           = 16
0.00.041.289 I print_info: n_head_kv        = 16
0.00.041.289 I print_info: n_rot            = 32
0.00.041.289 I print_info: n_swa            = 0
0.00.041.290 I print_info: n_embd_head_k    = 128
0.00.041.290 I print_info: n_embd_head_v    = 128
0.00.041.290 I print_info: n_gqa            = 1
0.00.041.291 I print_info: n_embd_k_gqa     = 2048
0.00.041.294 I print_info: n_embd_v_gqa     = 2048
0.00.041.295 I print_info: f_norm_eps       = 1.0e-05
0.00.041.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.296 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.297 I print_info: f_logit_scale    = 0.0e+00
0.00.041.297 I print_info: n_ff             = 8192
0.00.041.297 I print_info: n_expert         = 0
0.00.041.297 I print_info: n_expert_used    = 0
0.00.041.298 I print_info: causal attn      = 1
0.00.041.298 I print_info: pooling type     = 0
0.00.041.299 I print_info: rope type        = 2
0.00.041.300 I print_info: rope scaling     = linear
0.00.041.300 I print_info: freq_base_train  = 10000.0
0.00.041.301 I print_info: freq_scale_train = 1
0.00.041.301 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.301 I print_info: rope_finetuned   = unknown
0.00.041.301 I print_info: ssm_d_conv       = 0
0.00.041.301 I print_info: ssm_d_inner      = 0
0.00.041.301 I print_info: ssm_d_state      = 0
0.00.041.302 I print_info: ssm_dt_rank      = 0
0.00.041.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.303 I print_info: model type       = 1.4B
0.00.041.303 I print_info: model params     = 1.41 B
0.00.041.303 I print_info: general.name     = 1.4B
0.00.041.304 I print_info: vocab type       = BPE
0.00.041.304 I print_info: n_vocab          = 50304
0.00.041.304 I print_info: n_merges         = 50009
0.00.041.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.306 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.306 I print_info: LF token         = 187 'Ċ'
0.00.041.306 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.306 I print_info: max token length = 1024
0.00.041.307 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.104 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.114 I load_tensors: offloading output layer to GPU
0.00.604.116 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.147 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.148 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.605.748 I llama_init_from_model: n_seq_max     = 1
0.00.605.751 I llama_init_from_model: n_ctx         = 2048
0.00.605.751 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.605.752 I llama_init_from_model: n_batch       = 2048
0.00.605.752 I llama_init_from_model: n_ubatch      = 512
0.00.605.753 I llama_init_from_model: flash_attn    = 0
0.00.605.754 I llama_init_from_model: freq_base     = 10000.0
0.00.605.755 I llama_init_from_model: freq_scale    = 1
0.00.605.756 I ggml_metal_init: allocating
0.00.605.774 I ggml_metal_init: found device: Apple M4
0.00.605.788 I ggml_metal_init: picking default device: Apple M4
0.00.607.034 I ggml_metal_init: using embedded metal library
0.00.613.480 I ggml_metal_init: GPU name:   Apple M4
0.00.613.484 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.485 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.486 I ggml_metal_init: simdgroup reduction   = true
0.00.613.486 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.487 I ggml_metal_init: has residency sets    = true
0.00.613.487 I ggml_metal_init: has bfloat            = true
0.00.613.487 I ggml_metal_init: use bfloat            = true
0.00.613.488 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.489 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.726 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.770 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.778 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.805 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.276 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.686.279 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.686.279 I llama_init_from_model: graph nodes  = 967
0.00.686.279 I llama_init_from_model: graph splits = 2
0.00.686.284 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.686.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.686.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.783 I main: llama threadpool init, n_threads = 4
0.00.741.832 I 
0.00.741.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.854 I 
0.00.742.022 I sampler seed: 1234
0.00.742.027 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.043 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.043 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.043 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.579.131 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.579.131 I llama_perf_context_print:        load time =     729.90 ms
0.01.579.132 I llama_perf_context_print: prompt eval time =      41.97 ms /     7 tokens (    6.00 ms per token,   166.79 tokens per second)
0.01.579.133 I llama_perf_context_print:        eval time =     792.18 ms /    63 runs   (   12.57 ms per token,    79.53 tokens per second)
0.01.579.134 I llama_perf_context_print:       total time =     838.07 ms /    70 tokens
0.01.579.420 I ggml_metal_free: deallocating

real	0m1.599s
user	0m0.109s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.358 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.024 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.030 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.031 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.033 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.033 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.034 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.034 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.037 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.042 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.853 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.804 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.488 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.489 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.489 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.490 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.491 I llama_model_loader: - type  f32:  194 tensors
0.00.024.491 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.491 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.492 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.492 I print_info: file format = GGUF V3 (latest)
0.00.024.493 I print_info: file type   = Q2_K - Medium
0.00.024.494 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.356 I load: special tokens cache size = 25
0.00.038.788 I load: token to piece cache size = 0.2984 MB
0.00.038.797 I print_info: arch             = gptneox
0.00.038.798 I print_info: vocab_only       = 0
0.00.038.798 I print_info: n_ctx_train      = 2048
0.00.038.798 I print_info: n_embd           = 2048
0.00.038.799 I print_info: n_layer          = 24
0.00.038.801 I print_info: n_head           = 16
0.00.038.802 I print_info: n_head_kv        = 16
0.00.038.802 I print_info: n_rot            = 32
0.00.038.802 I print_info: n_swa            = 0
0.00.038.802 I print_info: n_embd_head_k    = 128
0.00.038.803 I print_info: n_embd_head_v    = 128
0.00.038.803 I print_info: n_gqa            = 1
0.00.038.806 I print_info: n_embd_k_gqa     = 2048
0.00.038.806 I print_info: n_embd_v_gqa     = 2048
0.00.038.807 I print_info: f_norm_eps       = 1.0e-05
0.00.038.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.808 I print_info: f_logit_scale    = 0.0e+00
0.00.038.809 I print_info: n_ff             = 8192
0.00.038.809 I print_info: n_expert         = 0
0.00.038.809 I print_info: n_expert_used    = 0
0.00.038.809 I print_info: causal attn      = 1
0.00.038.810 I print_info: pooling type     = 0
0.00.038.810 I print_info: rope type        = 2
0.00.038.810 I print_info: rope scaling     = linear
0.00.038.810 I print_info: freq_base_train  = 10000.0
0.00.038.811 I print_info: freq_scale_train = 1
0.00.038.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.811 I print_info: rope_finetuned   = unknown
0.00.038.811 I print_info: ssm_d_conv       = 0
0.00.038.811 I print_info: ssm_d_inner      = 0
0.00.038.812 I print_info: ssm_d_state      = 0
0.00.038.812 I print_info: ssm_dt_rank      = 0
0.00.038.813 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.813 I print_info: model type       = 1.4B
0.00.038.813 I print_info: model params     = 1.41 B
0.00.038.813 I print_info: general.name     = 1.4B
0.00.038.814 I print_info: vocab type       = BPE
0.00.038.814 I print_info: n_vocab          = 50304
0.00.038.814 I print_info: n_merges         = 50009
0.00.038.814 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.814 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.814 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.815 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: LF token         = 187 'Ċ'
0.00.038.824 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.824 I print_info: max token length = 1024
0.00.038.825 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.335.903 I load_tensors: offloading 24 repeating layers to GPU
0.00.335.918 I load_tensors: offloading output layer to GPU
0.00.335.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.335.954 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.335.956 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.693 I llama_init_from_model: n_seq_max     = 1
0.00.337.695 I llama_init_from_model: n_ctx         = 2048
0.00.337.696 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.337.696 I llama_init_from_model: n_batch       = 2048
0.00.337.696 I llama_init_from_model: n_ubatch      = 512
0.00.337.697 I llama_init_from_model: flash_attn    = 0
0.00.337.699 I llama_init_from_model: freq_base     = 10000.0
0.00.337.700 I llama_init_from_model: freq_scale    = 1
0.00.337.702 I ggml_metal_init: allocating
0.00.337.810 I ggml_metal_init: found device: Apple M4
0.00.337.824 I ggml_metal_init: picking default device: Apple M4
0.00.339.460 I ggml_metal_init: using embedded metal library
0.00.345.208 I ggml_metal_init: GPU name:   Apple M4
0.00.345.218 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.219 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.220 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.220 I ggml_metal_init: simdgroup reduction   = true
0.00.345.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.221 I ggml_metal_init: has residency sets    = true
0.00.345.222 I ggml_metal_init: has bfloat            = true
0.00.345.222 I ggml_metal_init: use bfloat            = true
0.00.345.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.227 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.366.715 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.415.887 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.415.896 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.415.926 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.420.500 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.420.501 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.420.502 I llama_init_from_model: graph nodes  = 967
0.00.420.502 I llama_init_from_model: graph splits = 2
0.00.420.507 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.420.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.420.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.744 I main: llama threadpool init, n_threads = 4
0.00.477.793 I 
0.00.477.821 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.822 I 
0.00.477.996 I sampler seed: 1234
0.00.478.001 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.478.016 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.478.017 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.478.017 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.147.495 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.147.496 I llama_perf_context_print:        load time =     467.65 ms
0.01.147.497 I llama_perf_context_print: prompt eval time =      35.52 ms /     7 tokens (    5.07 ms per token,   197.09 tokens per second)
0.01.147.498 I llama_perf_context_print:        eval time =     631.19 ms /    63 runs   (   10.02 ms per token,    99.81 tokens per second)
0.01.147.498 I llama_perf_context_print:       total time =     670.48 ms /    70 tokens
0.01.147.738 I ggml_metal_free: deallocating

real	0m1.164s
user	0m0.113s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.921 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.926 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.942 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.762 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.760 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.469 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.470 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.471 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.471 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.471 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.472 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.472 I llama_model_loader: - type  f32:  194 tensors
0.00.025.473 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.473 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.473 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.473 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.474 I print_info: file format = GGUF V3 (latest)
0.00.025.474 I print_info: file type   = Q3_K - Medium
0.00.025.477 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.686 I load: special tokens cache size = 25
0.00.039.942 I load: token to piece cache size = 0.2984 MB
0.00.039.957 I print_info: arch             = gptneox
0.00.039.958 I print_info: vocab_only       = 0
0.00.039.958 I print_info: n_ctx_train      = 2048
0.00.039.958 I print_info: n_embd           = 2048
0.00.039.958 I print_info: n_layer          = 24
0.00.039.961 I print_info: n_head           = 16
0.00.039.962 I print_info: n_head_kv        = 16
0.00.039.962 I print_info: n_rot            = 32
0.00.039.962 I print_info: n_swa            = 0
0.00.039.962 I print_info: n_embd_head_k    = 128
0.00.039.962 I print_info: n_embd_head_v    = 128
0.00.039.963 I print_info: n_gqa            = 1
0.00.039.964 I print_info: n_embd_k_gqa     = 2048
0.00.039.965 I print_info: n_embd_v_gqa     = 2048
0.00.039.965 I print_info: f_norm_eps       = 1.0e-05
0.00.039.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.967 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.967 I print_info: f_logit_scale    = 0.0e+00
0.00.039.967 I print_info: n_ff             = 8192
0.00.039.968 I print_info: n_expert         = 0
0.00.039.968 I print_info: n_expert_used    = 0
0.00.039.970 I print_info: causal attn      = 1
0.00.039.970 I print_info: pooling type     = 0
0.00.039.970 I print_info: rope type        = 2
0.00.039.970 I print_info: rope scaling     = linear
0.00.039.970 I print_info: freq_base_train  = 10000.0
0.00.039.971 I print_info: freq_scale_train = 1
0.00.039.971 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.971 I print_info: rope_finetuned   = unknown
0.00.039.971 I print_info: ssm_d_conv       = 0
0.00.039.971 I print_info: ssm_d_inner      = 0
0.00.039.971 I print_info: ssm_d_state      = 0
0.00.039.971 I print_info: ssm_dt_rank      = 0
0.00.039.971 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.972 I print_info: model type       = 1.4B
0.00.039.976 I print_info: model params     = 1.41 B
0.00.039.976 I print_info: general.name     = 1.4B
0.00.039.976 I print_info: vocab type       = BPE
0.00.039.976 I print_info: n_vocab          = 50304
0.00.039.978 I print_info: n_merges         = 50009
0.00.039.978 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.978 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.978 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.978 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.979 I print_info: LF token         = 187 'Ċ'
0.00.039.979 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.979 I print_info: max token length = 1024
0.00.039.979 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.448.732 I load_tensors: offloading 24 repeating layers to GPU
0.00.448.748 I load_tensors: offloading output layer to GPU
0.00.448.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.448.781 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.448.782 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.450.259 I llama_init_from_model: n_seq_max     = 1
0.00.450.268 I llama_init_from_model: n_ctx         = 2048
0.00.450.269 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.450.269 I llama_init_from_model: n_batch       = 2048
0.00.450.270 I llama_init_from_model: n_ubatch      = 512
0.00.450.270 I llama_init_from_model: flash_attn    = 0
0.00.450.273 I llama_init_from_model: freq_base     = 10000.0
0.00.450.273 I llama_init_from_model: freq_scale    = 1
0.00.450.276 I ggml_metal_init: allocating
0.00.450.324 I ggml_metal_init: found device: Apple M4
0.00.450.343 I ggml_metal_init: picking default device: Apple M4
0.00.452.457 I ggml_metal_init: using embedded metal library
0.00.458.928 I ggml_metal_init: GPU name:   Apple M4
0.00.458.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.458.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.458.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.458.950 I ggml_metal_init: simdgroup reduction   = true
0.00.458.950 I ggml_metal_init: simdgroup matrix mul. = true
0.00.458.951 I ggml_metal_init: has residency sets    = true
0.00.458.951 I ggml_metal_init: has bfloat            = true
0.00.458.951 I ggml_metal_init: use bfloat            = true
0.00.458.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.458.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.479.586 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.549.839 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.549.845 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.549.868 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.554.930 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.554.932 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.554.932 I llama_init_from_model: graph nodes  = 967
0.00.554.932 I llama_init_from_model: graph splits = 2
0.00.554.939 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.555.098 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.555.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.969 I main: llama threadpool init, n_threads = 4
0.00.611.019 I 
0.00.611.040 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.043 I 
0.00.611.201 I sampler seed: 1234
0.00.611.205 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.611.257 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.611.259 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.611.260 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.353.091 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.353.092 I llama_perf_context_print:        load time =     601.04 ms
0.01.353.094 I llama_perf_context_print: prompt eval time =      45.30 ms /     7 tokens (    6.47 ms per token,   154.54 tokens per second)
0.01.353.094 I llama_perf_context_print:        eval time =     693.66 ms /    63 runs   (   11.01 ms per token,    90.82 tokens per second)
0.01.353.095 I llama_perf_context_print:       total time =     742.86 ms /    70 tokens
0.01.353.321 I ggml_metal_free: deallocating

real	0m1.369s
user	0m0.111s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.988 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.649 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.656 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.657 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.657 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.662 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.223 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.224 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.226 I llama_model_loader: - type  f32:  194 tensors
0.00.025.226 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.226 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.227 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.227 I print_info: file format = GGUF V3 (latest)
0.00.025.228 I print_info: file type   = Q4_K - Medium
0.00.025.229 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.413 I load: special tokens cache size = 25
0.00.039.548 I load: token to piece cache size = 0.2984 MB
0.00.039.563 I print_info: arch             = gptneox
0.00.039.564 I print_info: vocab_only       = 0
0.00.039.564 I print_info: n_ctx_train      = 2048
0.00.039.565 I print_info: n_embd           = 2048
0.00.039.565 I print_info: n_layer          = 24
0.00.039.572 I print_info: n_head           = 16
0.00.039.573 I print_info: n_head_kv        = 16
0.00.039.573 I print_info: n_rot            = 32
0.00.039.574 I print_info: n_swa            = 0
0.00.039.574 I print_info: n_embd_head_k    = 128
0.00.039.574 I print_info: n_embd_head_v    = 128
0.00.039.575 I print_info: n_gqa            = 1
0.00.039.576 I print_info: n_embd_k_gqa     = 2048
0.00.039.576 I print_info: n_embd_v_gqa     = 2048
0.00.039.577 I print_info: f_norm_eps       = 1.0e-05
0.00.039.579 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.579 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.579 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.579 I print_info: f_logit_scale    = 0.0e+00
0.00.039.580 I print_info: n_ff             = 8192
0.00.039.580 I print_info: n_expert         = 0
0.00.039.580 I print_info: n_expert_used    = 0
0.00.039.580 I print_info: causal attn      = 1
0.00.039.580 I print_info: pooling type     = 0
0.00.039.580 I print_info: rope type        = 2
0.00.039.580 I print_info: rope scaling     = linear
0.00.039.581 I print_info: freq_base_train  = 10000.0
0.00.039.584 I print_info: freq_scale_train = 1
0.00.039.584 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.584 I print_info: rope_finetuned   = unknown
0.00.039.585 I print_info: ssm_d_conv       = 0
0.00.039.585 I print_info: ssm_d_inner      = 0
0.00.039.585 I print_info: ssm_d_state      = 0
0.00.039.585 I print_info: ssm_dt_rank      = 0
0.00.039.585 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.585 I print_info: model type       = 1.4B
0.00.039.586 I print_info: model params     = 1.41 B
0.00.039.586 I print_info: general.name     = 1.4B
0.00.039.586 I print_info: vocab type       = BPE
0.00.039.586 I print_info: n_vocab          = 50304
0.00.039.587 I print_info: n_merges         = 50009
0.00.039.587 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.593 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.594 I print_info: LF token         = 187 'Ċ'
0.00.039.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.595 I print_info: max token length = 1024
0.00.039.596 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.514.147 I load_tensors: offloading 24 repeating layers to GPU
0.00.514.162 I load_tensors: offloading output layer to GPU
0.00.514.163 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.198 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.199 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.515.955 I llama_init_from_model: n_seq_max     = 1
0.00.515.958 I llama_init_from_model: n_ctx         = 2048
0.00.515.958 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.515.959 I llama_init_from_model: n_batch       = 2048
0.00.515.959 I llama_init_from_model: n_ubatch      = 512
0.00.515.959 I llama_init_from_model: flash_attn    = 0
0.00.515.961 I llama_init_from_model: freq_base     = 10000.0
0.00.515.962 I llama_init_from_model: freq_scale    = 1
0.00.515.972 I ggml_metal_init: allocating
0.00.516.079 I ggml_metal_init: found device: Apple M4
0.00.516.093 I ggml_metal_init: picking default device: Apple M4
0.00.517.682 I ggml_metal_init: using embedded metal library
0.00.524.348 I ggml_metal_init: GPU name:   Apple M4
0.00.524.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.524.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.524.353 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.524.354 I ggml_metal_init: simdgroup reduction   = true
0.00.524.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.524.355 I ggml_metal_init: has residency sets    = true
0.00.524.355 I ggml_metal_init: has bfloat            = true
0.00.524.355 I ggml_metal_init: use bfloat            = true
0.00.524.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.524.358 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.441 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.596.566 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.596.572 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.596.595 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.601.504 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.601.506 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.601.507 I llama_init_from_model: graph nodes  = 967
0.00.601.507 I llama_init_from_model: graph splits = 2
0.00.601.513 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.601.634 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.601.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.833 I main: llama threadpool init, n_threads = 4
0.00.661.884 I 
0.00.661.903 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.905 I 
0.00.662.076 I sampler seed: 1234
0.00.662.081 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.662.097 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.662.097 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.662.097 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.433.657 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.433.658 I llama_perf_context_print:        load time =     651.12 ms
0.01.433.660 I llama_perf_context_print: prompt eval time =      56.85 ms /     7 tokens (    8.12 ms per token,   123.14 tokens per second)
0.01.433.665 I llama_perf_context_print:        eval time =     711.74 ms /    63 runs   (   11.30 ms per token,    88.52 tokens per second)
0.01.433.668 I llama_perf_context_print:       total time =     772.55 ms /    70 tokens
0.01.433.943 I ggml_metal_free: deallocating

real	0m1.455s
user	0m0.110s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.720 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.374 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.381 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.382 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.382 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.383 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.384 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.384 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.385 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.385 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.385 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.386 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.387 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.388 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.388 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.114 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.796 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.797 I llama_model_loader: - type  f32:  194 tensors
0.00.023.797 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.797 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.798 I print_info: file format = GGUF V3 (latest)
0.00.023.798 I print_info: file type   = Q5_K - Medium
0.00.023.799 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.638 I load: special tokens cache size = 25
0.00.037.805 I load: token to piece cache size = 0.2984 MB
0.00.037.819 I print_info: arch             = gptneox
0.00.037.820 I print_info: vocab_only       = 0
0.00.037.820 I print_info: n_ctx_train      = 2048
0.00.037.820 I print_info: n_embd           = 2048
0.00.037.820 I print_info: n_layer          = 24
0.00.037.823 I print_info: n_head           = 16
0.00.037.824 I print_info: n_head_kv        = 16
0.00.037.824 I print_info: n_rot            = 32
0.00.037.824 I print_info: n_swa            = 0
0.00.037.824 I print_info: n_embd_head_k    = 128
0.00.037.824 I print_info: n_embd_head_v    = 128
0.00.037.825 I print_info: n_gqa            = 1
0.00.037.826 I print_info: n_embd_k_gqa     = 2048
0.00.037.826 I print_info: n_embd_v_gqa     = 2048
0.00.037.827 I print_info: f_norm_eps       = 1.0e-05
0.00.037.828 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.828 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.828 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.828 I print_info: f_logit_scale    = 0.0e+00
0.00.037.830 I print_info: n_ff             = 8192
0.00.037.830 I print_info: n_expert         = 0
0.00.037.830 I print_info: n_expert_used    = 0
0.00.037.830 I print_info: causal attn      = 1
0.00.037.830 I print_info: pooling type     = 0
0.00.037.830 I print_info: rope type        = 2
0.00.037.831 I print_info: rope scaling     = linear
0.00.037.831 I print_info: freq_base_train  = 10000.0
0.00.037.831 I print_info: freq_scale_train = 1
0.00.037.831 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.831 I print_info: rope_finetuned   = unknown
0.00.037.831 I print_info: ssm_d_conv       = 0
0.00.037.832 I print_info: ssm_d_inner      = 0
0.00.037.832 I print_info: ssm_d_state      = 0
0.00.037.832 I print_info: ssm_dt_rank      = 0
0.00.037.832 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.832 I print_info: model type       = 1.4B
0.00.037.832 I print_info: model params     = 1.41 B
0.00.037.832 I print_info: general.name     = 1.4B
0.00.037.833 I print_info: vocab type       = BPE
0.00.037.833 I print_info: n_vocab          = 50304
0.00.037.833 I print_info: n_merges         = 50009
0.00.037.833 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.834 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.834 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.834 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.834 I print_info: LF token         = 187 'Ċ'
0.00.037.834 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.834 I print_info: max token length = 1024
0.00.037.835 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.613 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.626 I load_tensors: offloading output layer to GPU
0.00.591.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.660 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.662 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.593.437 I llama_init_from_model: n_seq_max     = 1
0.00.593.440 I llama_init_from_model: n_ctx         = 2048
0.00.593.441 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.593.441 I llama_init_from_model: n_batch       = 2048
0.00.593.442 I llama_init_from_model: n_ubatch      = 512
0.00.593.442 I llama_init_from_model: flash_attn    = 0
0.00.593.445 I llama_init_from_model: freq_base     = 10000.0
0.00.593.445 I llama_init_from_model: freq_scale    = 1
0.00.593.448 I ggml_metal_init: allocating
0.00.593.544 I ggml_metal_init: found device: Apple M4
0.00.593.563 I ggml_metal_init: picking default device: Apple M4
0.00.595.328 I ggml_metal_init: using embedded metal library
0.00.601.981 I ggml_metal_init: GPU name:   Apple M4
0.00.601.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.986 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.987 I ggml_metal_init: simdgroup reduction   = true
0.00.601.988 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.988 I ggml_metal_init: has residency sets    = true
0.00.601.988 I ggml_metal_init: has bfloat            = true
0.00.601.988 I ggml_metal_init: use bfloat            = true
0.00.601.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.991 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.793 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.595 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.672.604 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.672.633 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.089 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.677.091 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.677.092 I llama_init_from_model: graph nodes  = 967
0.00.677.092 I llama_init_from_model: graph splits = 2
0.00.677.097 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.677.229 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.230 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.337 I main: llama threadpool init, n_threads = 4
0.00.739.386 I 
0.00.739.407 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.408 I 
0.00.739.580 I sampler seed: 1234
0.00.739.585 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.600 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.600 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.600 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.588.551 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.588.552 I llama_perf_context_print:        load time =     729.88 ms
0.01.588.553 I llama_perf_context_print: prompt eval time =      52.61 ms /     7 tokens (    7.52 ms per token,   133.06 tokens per second)
0.01.588.555 I llama_perf_context_print:        eval time =     793.40 ms /    63 runs   (   12.59 ms per token,    79.41 tokens per second)
0.01.588.555 I llama_perf_context_print:       total time =     849.95 ms /    70 tokens
0.01.588.787 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.110s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.211 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.063 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.063 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.065 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.065 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.818 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.597 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.598 I llama_model_loader: - type  f32:  194 tensors
0.00.024.598 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.598 I print_info: file format = GGUF V3 (latest)
0.00.024.599 I print_info: file type   = Q6_K
0.00.024.600 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.502 I load: special tokens cache size = 25
0.00.038.911 I load: token to piece cache size = 0.2984 MB
0.00.038.920 I print_info: arch             = gptneox
0.00.038.922 I print_info: vocab_only       = 0
0.00.038.922 I print_info: n_ctx_train      = 2048
0.00.038.922 I print_info: n_embd           = 2048
0.00.038.922 I print_info: n_layer          = 24
0.00.038.925 I print_info: n_head           = 16
0.00.038.926 I print_info: n_head_kv        = 16
0.00.038.926 I print_info: n_rot            = 32
0.00.038.927 I print_info: n_swa            = 0
0.00.038.927 I print_info: n_embd_head_k    = 128
0.00.038.930 I print_info: n_embd_head_v    = 128
0.00.038.930 I print_info: n_gqa            = 1
0.00.038.931 I print_info: n_embd_k_gqa     = 2048
0.00.038.932 I print_info: n_embd_v_gqa     = 2048
0.00.038.932 I print_info: f_norm_eps       = 1.0e-05
0.00.038.933 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.933 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.933 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.933 I print_info: f_logit_scale    = 0.0e+00
0.00.038.934 I print_info: n_ff             = 8192
0.00.038.934 I print_info: n_expert         = 0
0.00.038.934 I print_info: n_expert_used    = 0
0.00.038.935 I print_info: causal attn      = 1
0.00.038.935 I print_info: pooling type     = 0
0.00.038.935 I print_info: rope type        = 2
0.00.038.935 I print_info: rope scaling     = linear
0.00.038.935 I print_info: freq_base_train  = 10000.0
0.00.038.936 I print_info: freq_scale_train = 1
0.00.038.936 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.936 I print_info: rope_finetuned   = unknown
0.00.038.936 I print_info: ssm_d_conv       = 0
0.00.038.936 I print_info: ssm_d_inner      = 0
0.00.038.939 I print_info: ssm_d_state      = 0
0.00.038.940 I print_info: ssm_dt_rank      = 0
0.00.038.940 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.940 I print_info: model type       = 1.4B
0.00.038.940 I print_info: model params     = 1.41 B
0.00.038.940 I print_info: general.name     = 1.4B
0.00.038.941 I print_info: vocab type       = BPE
0.00.038.941 I print_info: n_vocab          = 50304
0.00.038.942 I print_info: n_merges         = 50009
0.00.038.942 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.943 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.943 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.943 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: LF token         = 187 'Ċ'
0.00.038.944 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: max token length = 1024
0.00.038.944 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.648 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.651 I load_tensors: offloading output layer to GPU
0.00.632.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.675 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.632.678 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.634.075 I llama_init_from_model: n_seq_max     = 1
0.00.634.077 I llama_init_from_model: n_ctx         = 2048
0.00.634.078 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.634.078 I llama_init_from_model: n_batch       = 2048
0.00.634.079 I llama_init_from_model: n_ubatch      = 512
0.00.634.079 I llama_init_from_model: flash_attn    = 0
0.00.634.080 I llama_init_from_model: freq_base     = 10000.0
0.00.634.081 I llama_init_from_model: freq_scale    = 1
0.00.634.082 I ggml_metal_init: allocating
0.00.634.119 I ggml_metal_init: found device: Apple M4
0.00.634.129 I ggml_metal_init: picking default device: Apple M4
0.00.635.391 I ggml_metal_init: using embedded metal library
0.00.641.454 I ggml_metal_init: GPU name:   Apple M4
0.00.641.458 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.459 I ggml_metal_init: simdgroup reduction   = true
0.00.641.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.460 I ggml_metal_init: has residency sets    = true
0.00.641.460 I ggml_metal_init: has bfloat            = true
0.00.641.460 I ggml_metal_init: use bfloat            = true
0.00.641.461 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.462 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.424 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.784 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.709.791 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.709.821 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.530 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.533 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.533 I llama_init_from_model: graph nodes  = 967
0.00.714.533 I llama_init_from_model: graph splits = 2
0.00.714.541 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.751 I main: llama threadpool init, n_threads = 4
0.00.782.803 I 
0.00.782.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.828 I 
0.00.783.010 I sampler seed: 1234
0.00.783.014 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.057 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.060 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.061 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.659.231 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.01.659.231 I llama_perf_context_print:        load time =     772.79 ms
0.01.659.232 I llama_perf_context_print: prompt eval time =      57.81 ms /     7 tokens (    8.26 ms per token,   121.09 tokens per second)
0.01.659.233 I llama_perf_context_print:        eval time =     815.52 ms /    63 runs   (   12.94 ms per token,    77.25 tokens per second)
0.01.659.233 I llama_perf_context_print:       total time =     877.23 ms /    70 tokens
0.01.659.509 I ggml_metal_free: deallocating

real	0m1.676s
user	0m0.108s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.864 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.231 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.237 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.239 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.241 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.241 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.242 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.243 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.244 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.245 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.245 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.247 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.249 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.775 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.957 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.959 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.961 I llama_model_loader: - type  f32:  194 tensors
0.00.052.962 I llama_model_loader: - type  f16:   98 tensors
0.00.052.962 I print_info: file format = GGUF V3 (latest)
0.00.052.963 I print_info: file type   = all F32 (guessed)
0.00.052.964 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.289 I load: special tokens cache size = 25
0.00.072.625 I load: token to piece cache size = 0.2984 MB
0.00.072.641 I print_info: arch             = gptneox
0.00.072.642 I print_info: vocab_only       = 0
0.00.072.642 I print_info: n_ctx_train      = 2048
0.00.072.642 I print_info: n_embd           = 2048
0.00.072.642 I print_info: n_layer          = 24
0.00.072.645 I print_info: n_head           = 16
0.00.072.646 I print_info: n_head_kv        = 16
0.00.072.646 I print_info: n_rot            = 32
0.00.072.647 I print_info: n_swa            = 0
0.00.072.647 I print_info: n_embd_head_k    = 128
0.00.072.647 I print_info: n_embd_head_v    = 128
0.00.072.648 I print_info: n_gqa            = 1
0.00.072.649 I print_info: n_embd_k_gqa     = 2048
0.00.072.649 I print_info: n_embd_v_gqa     = 2048
0.00.072.650 I print_info: f_norm_eps       = 1.0e-05
0.00.072.650 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.650 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.650 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.651 I print_info: f_logit_scale    = 0.0e+00
0.00.072.651 I print_info: n_ff             = 8192
0.00.072.651 I print_info: n_expert         = 0
0.00.072.652 I print_info: n_expert_used    = 0
0.00.072.652 I print_info: causal attn      = 1
0.00.072.652 I print_info: pooling type     = 0
0.00.072.652 I print_info: rope type        = 2
0.00.072.652 I print_info: rope scaling     = linear
0.00.072.653 I print_info: freq_base_train  = 10000.0
0.00.072.653 I print_info: freq_scale_train = 1
0.00.072.653 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.653 I print_info: rope_finetuned   = unknown
0.00.072.654 I print_info: ssm_d_conv       = 0
0.00.072.654 I print_info: ssm_d_inner      = 0
0.00.072.654 I print_info: ssm_d_state      = 0
0.00.072.654 I print_info: ssm_dt_rank      = 0
0.00.072.654 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.654 I print_info: model type       = 1.4B
0.00.072.655 I print_info: model params     = 1.41 B
0.00.072.655 I print_info: general.name     = 1.4B
0.00.072.656 I print_info: vocab type       = BPE
0.00.072.656 I print_info: n_vocab          = 50304
0.00.072.656 I print_info: n_merges         = 50009
0.00.072.656 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.656 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.657 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.657 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.657 I print_info: LF token         = 187 'Ċ'
0.00.072.657 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.658 I print_info: max token length = 1024
0.00.072.658 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.403.994 I load_tensors: offloading 24 repeating layers to GPU
0.01.403.999 I load_tensors: offloading output layer to GPU
0.01.403.999 I load_tensors: offloaded 25/25 layers to GPU
0.01.404.021 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.404.023 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.405.044 I llama_init_from_model: n_seq_max     = 1
0.01.405.045 I llama_init_from_model: n_ctx         = 128
0.01.405.045 I llama_init_from_model: n_ctx_per_seq = 128
0.01.405.046 I llama_init_from_model: n_batch       = 128
0.01.405.046 I llama_init_from_model: n_ubatch      = 128
0.01.405.046 I llama_init_from_model: flash_attn    = 0
0.01.405.047 I llama_init_from_model: freq_base     = 10000.0
0.01.405.047 I llama_init_from_model: freq_scale    = 1
0.01.405.047 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.405.048 I ggml_metal_init: allocating
0.01.405.074 I ggml_metal_init: found device: Apple M4
0.01.405.080 I ggml_metal_init: picking default device: Apple M4
0.01.405.985 I ggml_metal_init: using embedded metal library
0.01.410.076 I ggml_metal_init: GPU name:   Apple M4
0.01.410.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.410.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.410.080 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.410.080 I ggml_metal_init: simdgroup reduction   = true
0.01.410.080 I ggml_metal_init: simdgroup matrix mul. = true
0.01.410.081 I ggml_metal_init: has residency sets    = true
0.01.410.081 I ggml_metal_init: has bfloat            = true
0.01.410.081 I ggml_metal_init: use bfloat            = true
0.01.410.081 I ggml_metal_init: hasUnifiedMemory      = true
0.01.410.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.422.197 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.423.962 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.423.964 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.423.978 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.425.688 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.425.689 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.425.690 I llama_init_from_model: graph nodes  = 967
0.01.425.690 I llama_init_from_model: graph splits = 2
0.01.425.691 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.425.692 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.460.815 I 
0.01.460.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.460.862 I perplexity: tokenizing the input ..
0.01.466.008 I perplexity: tokenization took 5.145 ms
0.01.466.012 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.584.287 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.585.647 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.585.680 I llama_perf_context_print:        load time =    1437.78 ms
0.01.585.681 I llama_perf_context_print: prompt eval time =     118.01 ms /   128 tokens (    0.92 ms per token,  1084.64 tokens per second)
0.01.585.682 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.585.683 I llama_perf_context_print:       total time =     124.87 ms /   129 tokens
0.01.586.065 I ggml_metal_free: deallocating

real	0m1.802s
user	0m0.096s
sys	0m0.255s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.226 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.610 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.620 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.621 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.621 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.622 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.622 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.624 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.624 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.625 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.625 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.627 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.499 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.346 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.348 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.348 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.349 I llama_model_loader: - type  f32:  194 tensors
0.00.025.349 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.350 I print_info: file format = GGUF V3 (latest)
0.00.025.351 I print_info: file type   = Q8_0
0.00.025.352 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.827 I load: special tokens cache size = 25
0.00.040.186 I load: token to piece cache size = 0.2984 MB
0.00.040.203 I print_info: arch             = gptneox
0.00.040.204 I print_info: vocab_only       = 0
0.00.040.204 I print_info: n_ctx_train      = 2048
0.00.040.204 I print_info: n_embd           = 2048
0.00.040.204 I print_info: n_layer          = 24
0.00.040.214 I print_info: n_head           = 16
0.00.040.215 I print_info: n_head_kv        = 16
0.00.040.234 I print_info: n_rot            = 32
0.00.040.235 I print_info: n_swa            = 0
0.00.040.235 I print_info: n_embd_head_k    = 128
0.00.040.235 I print_info: n_embd_head_v    = 128
0.00.040.237 I print_info: n_gqa            = 1
0.00.040.237 I print_info: n_embd_k_gqa     = 2048
0.00.040.238 I print_info: n_embd_v_gqa     = 2048
0.00.040.238 I print_info: f_norm_eps       = 1.0e-05
0.00.040.239 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.239 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.239 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.239 I print_info: f_logit_scale    = 0.0e+00
0.00.040.240 I print_info: n_ff             = 8192
0.00.040.240 I print_info: n_expert         = 0
0.00.040.240 I print_info: n_expert_used    = 0
0.00.040.240 I print_info: causal attn      = 1
0.00.040.241 I print_info: pooling type     = 0
0.00.040.241 I print_info: rope type        = 2
0.00.040.241 I print_info: rope scaling     = linear
0.00.040.243 I print_info: freq_base_train  = 10000.0
0.00.040.243 I print_info: freq_scale_train = 1
0.00.040.243 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.244 I print_info: rope_finetuned   = unknown
0.00.040.244 I print_info: ssm_d_conv       = 0
0.00.040.244 I print_info: ssm_d_inner      = 0
0.00.040.244 I print_info: ssm_d_state      = 0
0.00.040.244 I print_info: ssm_dt_rank      = 0
0.00.040.244 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.244 I print_info: model type       = 1.4B
0.00.040.248 I print_info: model params     = 1.41 B
0.00.040.248 I print_info: general.name     = 1.4B
0.00.040.249 I print_info: vocab type       = BPE
0.00.040.249 I print_info: n_vocab          = 50304
0.00.040.249 I print_info: n_merges         = 50009
0.00.040.249 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.249 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.249 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: LF token         = 187 'Ċ'
0.00.040.250 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: max token length = 1024
0.00.040.251 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.881.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.881.258 I load_tensors: offloading output layer to GPU
0.00.881.259 I load_tensors: offloaded 25/25 layers to GPU
0.00.881.282 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.881.285 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.882.344 I llama_init_from_model: n_seq_max     = 1
0.00.882.347 I llama_init_from_model: n_ctx         = 128
0.00.882.347 I llama_init_from_model: n_ctx_per_seq = 128
0.00.882.347 I llama_init_from_model: n_batch       = 128
0.00.882.347 I llama_init_from_model: n_ubatch      = 128
0.00.882.348 I llama_init_from_model: flash_attn    = 0
0.00.882.349 I llama_init_from_model: freq_base     = 10000.0
0.00.882.350 I llama_init_from_model: freq_scale    = 1
0.00.882.350 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.882.352 I ggml_metal_init: allocating
0.00.882.412 I ggml_metal_init: found device: Apple M4
0.00.882.424 I ggml_metal_init: picking default device: Apple M4
0.00.883.624 I ggml_metal_init: using embedded metal library
0.00.889.302 I ggml_metal_init: GPU name:   Apple M4
0.00.889.305 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.889.306 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.889.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.889.308 I ggml_metal_init: simdgroup reduction   = true
0.00.889.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.889.308 I ggml_metal_init: has residency sets    = true
0.00.889.308 I ggml_metal_init: has bfloat            = true
0.00.889.308 I ggml_metal_init: use bfloat            = true
0.00.889.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.889.319 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.905.308 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.908.616 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.908.620 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.908.667 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.911.789 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.911.790 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.911.791 I llama_init_from_model: graph nodes  = 967
0.00.911.791 I llama_init_from_model: graph splits = 2
0.00.911.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.911.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.941.462 I 
0.00.941.544 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.941.551 I perplexity: tokenizing the input ..
0.00.948.335 I perplexity: tokenization took 6.783 ms
0.00.948.339 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.086.549 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.088.173 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.088.200 I llama_perf_context_print:        load time =     932.23 ms
0.01.088.202 I llama_perf_context_print: prompt eval time =     137.97 ms /   128 tokens (    1.08 ms per token,   927.70 tokens per second)
0.01.088.203 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.088.203 I llama_perf_context_print:       total time =     146.74 ms /   129 tokens
0.01.088.609 I ggml_metal_free: deallocating

real	0m1.103s
user	0m0.075s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.116 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.587 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.589 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.333 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.094 I llama_model_loader: - type  f32:  194 tensors
0.00.026.094 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.094 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.095 I print_info: file format = GGUF V3 (latest)
0.00.026.096 I print_info: file type   = Q4_0
0.00.026.096 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.252 I load: special tokens cache size = 25
0.00.040.714 I load: token to piece cache size = 0.2984 MB
0.00.040.731 I print_info: arch             = gptneox
0.00.040.732 I print_info: vocab_only       = 0
0.00.040.732 I print_info: n_ctx_train      = 2048
0.00.040.732 I print_info: n_embd           = 2048
0.00.040.732 I print_info: n_layer          = 24
0.00.040.737 I print_info: n_head           = 16
0.00.040.737 I print_info: n_head_kv        = 16
0.00.040.738 I print_info: n_rot            = 32
0.00.040.738 I print_info: n_swa            = 0
0.00.040.738 I print_info: n_embd_head_k    = 128
0.00.040.738 I print_info: n_embd_head_v    = 128
0.00.040.739 I print_info: n_gqa            = 1
0.00.040.742 I print_info: n_embd_k_gqa     = 2048
0.00.040.742 I print_info: n_embd_v_gqa     = 2048
0.00.040.743 I print_info: f_norm_eps       = 1.0e-05
0.00.040.743 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.743 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.743 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.744 I print_info: f_logit_scale    = 0.0e+00
0.00.040.744 I print_info: n_ff             = 8192
0.00.040.744 I print_info: n_expert         = 0
0.00.040.745 I print_info: n_expert_used    = 0
0.00.040.745 I print_info: causal attn      = 1
0.00.040.745 I print_info: pooling type     = 0
0.00.040.745 I print_info: rope type        = 2
0.00.040.745 I print_info: rope scaling     = linear
0.00.040.746 I print_info: freq_base_train  = 10000.0
0.00.040.746 I print_info: freq_scale_train = 1
0.00.040.746 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.746 I print_info: rope_finetuned   = unknown
0.00.040.746 I print_info: ssm_d_conv       = 0
0.00.040.746 I print_info: ssm_d_inner      = 0
0.00.040.746 I print_info: ssm_d_state      = 0
0.00.040.747 I print_info: ssm_dt_rank      = 0
0.00.040.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.747 I print_info: model type       = 1.4B
0.00.040.747 I print_info: model params     = 1.41 B
0.00.040.747 I print_info: general.name     = 1.4B
0.00.040.749 I print_info: vocab type       = BPE
0.00.040.749 I print_info: n_vocab          = 50304
0.00.040.749 I print_info: n_merges         = 50009
0.00.040.749 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.750 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.750 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.750 I print_info: LF token         = 187 'Ċ'
0.00.040.750 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.751 I print_info: max token length = 1024
0.00.040.751 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.728 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.744 I load_tensors: offloading output layer to GPU
0.00.593.745 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.784 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.593.785 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.595.259 I llama_init_from_model: n_seq_max     = 1
0.00.595.261 I llama_init_from_model: n_ctx         = 128
0.00.595.262 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.263 I llama_init_from_model: n_batch       = 128
0.00.595.263 I llama_init_from_model: n_ubatch      = 128
0.00.595.263 I llama_init_from_model: flash_attn    = 0
0.00.595.266 I llama_init_from_model: freq_base     = 10000.0
0.00.595.266 I llama_init_from_model: freq_scale    = 1
0.00.595.267 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.270 I ggml_metal_init: allocating
0.00.595.368 I ggml_metal_init: found device: Apple M4
0.00.595.382 I ggml_metal_init: picking default device: Apple M4
0.00.596.982 I ggml_metal_init: using embedded metal library
0.00.602.496 I ggml_metal_init: GPU name:   Apple M4
0.00.602.504 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.505 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.507 I ggml_metal_init: simdgroup reduction   = true
0.00.602.507 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.508 I ggml_metal_init: has residency sets    = true
0.00.602.508 I ggml_metal_init: has bfloat            = true
0.00.602.508 I ggml_metal_init: use bfloat            = true
0.00.602.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.881 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.626.575 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.626.585 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.626.631 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.629.928 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.629.930 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.629.931 I llama_init_from_model: graph nodes  = 967
0.00.629.931 I llama_init_from_model: graph splits = 2
0.00.629.934 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.629.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.074 I 
0.00.658.175 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.182 I perplexity: tokenizing the input ..
0.00.665.305 I perplexity: tokenization took 7.12 ms
0.00.665.322 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.175 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.799.524 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.799.550 I llama_perf_context_print:        load time =     647.95 ms
0.00.799.551 I llama_perf_context_print: prompt eval time =     131.95 ms /   128 tokens (    1.03 ms per token,   970.04 tokens per second)
0.00.799.551 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.552 I llama_perf_context_print:       total time =     141.48 ms /   129 tokens
0.00.799.933 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.081s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.928 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.493 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.506 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.506 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.507 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.509 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.509 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.509 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.197 I llama_model_loader: - type  f32:  194 tensors
0.00.025.198 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.205 I print_info: file format = GGUF V3 (latest)
0.00.025.206 I print_info: file type   = Q4_1
0.00.025.207 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.712 I load: special tokens cache size = 25
0.00.040.130 I load: token to piece cache size = 0.2984 MB
0.00.040.147 I print_info: arch             = gptneox
0.00.040.148 I print_info: vocab_only       = 0
0.00.040.148 I print_info: n_ctx_train      = 2048
0.00.040.148 I print_info: n_embd           = 2048
0.00.040.148 I print_info: n_layer          = 24
0.00.040.152 I print_info: n_head           = 16
0.00.040.153 I print_info: n_head_kv        = 16
0.00.040.153 I print_info: n_rot            = 32
0.00.040.153 I print_info: n_swa            = 0
0.00.040.153 I print_info: n_embd_head_k    = 128
0.00.040.153 I print_info: n_embd_head_v    = 128
0.00.040.154 I print_info: n_gqa            = 1
0.00.040.155 I print_info: n_embd_k_gqa     = 2048
0.00.040.155 I print_info: n_embd_v_gqa     = 2048
0.00.040.156 I print_info: f_norm_eps       = 1.0e-05
0.00.040.156 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.156 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.156 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.157 I print_info: f_logit_scale    = 0.0e+00
0.00.040.157 I print_info: n_ff             = 8192
0.00.040.157 I print_info: n_expert         = 0
0.00.040.158 I print_info: n_expert_used    = 0
0.00.040.158 I print_info: causal attn      = 1
0.00.040.158 I print_info: pooling type     = 0
0.00.040.158 I print_info: rope type        = 2
0.00.040.158 I print_info: rope scaling     = linear
0.00.040.158 I print_info: freq_base_train  = 10000.0
0.00.040.159 I print_info: freq_scale_train = 1
0.00.040.159 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.159 I print_info: rope_finetuned   = unknown
0.00.040.159 I print_info: ssm_d_conv       = 0
0.00.040.159 I print_info: ssm_d_inner      = 0
0.00.040.159 I print_info: ssm_d_state      = 0
0.00.040.159 I print_info: ssm_dt_rank      = 0
0.00.040.160 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.160 I print_info: model type       = 1.4B
0.00.040.160 I print_info: model params     = 1.41 B
0.00.040.160 I print_info: general.name     = 1.4B
0.00.040.161 I print_info: vocab type       = BPE
0.00.040.161 I print_info: n_vocab          = 50304
0.00.040.161 I print_info: n_merges         = 50009
0.00.040.161 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.162 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.162 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.162 I print_info: LF token         = 187 'Ċ'
0.00.040.162 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.162 I print_info: max token length = 1024
0.00.040.163 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.268 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.283 I load_tensors: offloading output layer to GPU
0.00.608.284 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.320 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.608.321 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.609.480 I llama_init_from_model: n_seq_max     = 1
0.00.609.482 I llama_init_from_model: n_ctx         = 128
0.00.609.483 I llama_init_from_model: n_ctx_per_seq = 128
0.00.609.484 I llama_init_from_model: n_batch       = 128
0.00.609.484 I llama_init_from_model: n_ubatch      = 128
0.00.609.484 I llama_init_from_model: flash_attn    = 0
0.00.609.486 I llama_init_from_model: freq_base     = 10000.0
0.00.609.487 I llama_init_from_model: freq_scale    = 1
0.00.609.487 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.490 I ggml_metal_init: allocating
0.00.609.615 I ggml_metal_init: found device: Apple M4
0.00.609.629 I ggml_metal_init: picking default device: Apple M4
0.00.611.246 I ggml_metal_init: using embedded metal library
0.00.617.054 I ggml_metal_init: GPU name:   Apple M4
0.00.617.068 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.070 I ggml_metal_init: simdgroup reduction   = true
0.00.617.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.071 I ggml_metal_init: has residency sets    = true
0.00.617.071 I ggml_metal_init: has bfloat            = true
0.00.617.071 I ggml_metal_init: use bfloat            = true
0.00.617.073 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.816 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.346 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.641.353 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.641.391 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.644.560 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.644.562 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.644.563 I llama_init_from_model: graph nodes  = 967
0.00.644.563 I llama_init_from_model: graph splits = 2
0.00.644.567 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.567 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.026 I 
0.00.673.118 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.126 I perplexity: tokenizing the input ..
0.00.679.979 I perplexity: tokenization took 6.85 ms
0.00.679.983 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.190 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.816.538 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.816.559 I llama_perf_context_print:        load time =     664.09 ms
0.00.816.560 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.39 tokens per second)
0.00.816.561 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.561 I llama_perf_context_print:       total time =     143.54 ms /   129 tokens
0.00.816.938 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.080s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.886 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.199 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.212 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.213 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.213 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.865 I llama_model_loader: - type  f32:  194 tensors
0.00.024.865 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.865 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.866 I print_info: file format = GGUF V3 (latest)
0.00.024.866 I print_info: file type   = Q5_0
0.00.024.868 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.983 I load: special tokens cache size = 25
0.00.039.479 I load: token to piece cache size = 0.2984 MB
0.00.039.496 I print_info: arch             = gptneox
0.00.039.497 I print_info: vocab_only       = 0
0.00.039.497 I print_info: n_ctx_train      = 2048
0.00.039.497 I print_info: n_embd           = 2048
0.00.039.497 I print_info: n_layer          = 24
0.00.039.501 I print_info: n_head           = 16
0.00.039.502 I print_info: n_head_kv        = 16
0.00.039.502 I print_info: n_rot            = 32
0.00.039.502 I print_info: n_swa            = 0
0.00.039.502 I print_info: n_embd_head_k    = 128
0.00.039.502 I print_info: n_embd_head_v    = 128
0.00.039.503 I print_info: n_gqa            = 1
0.00.039.503 I print_info: n_embd_k_gqa     = 2048
0.00.039.504 I print_info: n_embd_v_gqa     = 2048
0.00.039.507 I print_info: f_norm_eps       = 1.0e-05
0.00.039.507 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.507 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.507 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.507 I print_info: f_logit_scale    = 0.0e+00
0.00.039.508 I print_info: n_ff             = 8192
0.00.039.508 I print_info: n_expert         = 0
0.00.039.508 I print_info: n_expert_used    = 0
0.00.039.509 I print_info: causal attn      = 1
0.00.039.509 I print_info: pooling type     = 0
0.00.039.510 I print_info: rope type        = 2
0.00.039.510 I print_info: rope scaling     = linear
0.00.039.510 I print_info: freq_base_train  = 10000.0
0.00.039.511 I print_info: freq_scale_train = 1
0.00.039.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.511 I print_info: rope_finetuned   = unknown
0.00.039.511 I print_info: ssm_d_conv       = 0
0.00.039.511 I print_info: ssm_d_inner      = 0
0.00.039.513 I print_info: ssm_d_state      = 0
0.00.039.513 I print_info: ssm_dt_rank      = 0
0.00.039.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.513 I print_info: model type       = 1.4B
0.00.039.514 I print_info: model params     = 1.41 B
0.00.039.514 I print_info: general.name     = 1.4B
0.00.039.514 I print_info: vocab type       = BPE
0.00.039.515 I print_info: n_vocab          = 50304
0.00.039.515 I print_info: n_merges         = 50009
0.00.039.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.518 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.518 I print_info: LF token         = 187 'Ċ'
0.00.039.518 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.518 I print_info: max token length = 1024
0.00.039.519 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.743 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.760 I load_tensors: offloading output layer to GPU
0.00.652.761 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.808 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.652.809 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.654.476 I llama_init_from_model: n_seq_max     = 1
0.00.654.483 I llama_init_from_model: n_ctx         = 128
0.00.654.483 I llama_init_from_model: n_ctx_per_seq = 128
0.00.654.484 I llama_init_from_model: n_batch       = 128
0.00.654.484 I llama_init_from_model: n_ubatch      = 128
0.00.654.484 I llama_init_from_model: flash_attn    = 0
0.00.654.486 I llama_init_from_model: freq_base     = 10000.0
0.00.654.487 I llama_init_from_model: freq_scale    = 1
0.00.654.487 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.654.491 I ggml_metal_init: allocating
0.00.654.589 I ggml_metal_init: found device: Apple M4
0.00.654.604 I ggml_metal_init: picking default device: Apple M4
0.00.656.529 I ggml_metal_init: using embedded metal library
0.00.663.750 I ggml_metal_init: GPU name:   Apple M4
0.00.663.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.760 I ggml_metal_init: simdgroup reduction   = true
0.00.663.760 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.760 I ggml_metal_init: has residency sets    = true
0.00.663.761 I ggml_metal_init: has bfloat            = true
0.00.663.761 I ggml_metal_init: use bfloat            = true
0.00.663.762 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.774 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.757 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.445 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.686.453 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.686.486 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.986 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.689.988 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.689.988 I llama_init_from_model: graph nodes  = 967
0.00.689.989 I llama_init_from_model: graph splits = 2
0.00.689.993 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.689.993 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.197 I 
0.00.718.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.290 I perplexity: tokenizing the input ..
0.00.725.330 I perplexity: tokenization took 7.038 ms
0.00.725.346 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.224 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.862.727 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.862.749 I llama_perf_context_print:        load time =     709.30 ms
0.00.862.750 I llama_perf_context_print: prompt eval time =     134.94 ms /   128 tokens (    1.05 ms per token,   948.54 tokens per second)
0.00.862.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.862.751 I llama_perf_context_print:       total time =     144.56 ms /   129 tokens
0.00.863.123 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.080s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.969 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.899 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.904 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.906 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.907 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.907 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.908 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.908 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.912 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.913 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.914 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.916 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.916 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.847 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.923 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.872 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.874 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.876 I llama_model_loader: - type  f32:  194 tensors
0.00.026.876 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.876 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.877 I print_info: file format = GGUF V3 (latest)
0.00.026.878 I print_info: file type   = Q5_1
0.00.026.879 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.365 I load: special tokens cache size = 25
0.00.042.048 I load: token to piece cache size = 0.2984 MB
0.00.042.060 I print_info: arch             = gptneox
0.00.042.061 I print_info: vocab_only       = 0
0.00.042.061 I print_info: n_ctx_train      = 2048
0.00.042.061 I print_info: n_embd           = 2048
0.00.042.061 I print_info: n_layer          = 24
0.00.042.065 I print_info: n_head           = 16
0.00.042.066 I print_info: n_head_kv        = 16
0.00.042.066 I print_info: n_rot            = 32
0.00.042.066 I print_info: n_swa            = 0
0.00.042.066 I print_info: n_embd_head_k    = 128
0.00.042.066 I print_info: n_embd_head_v    = 128
0.00.042.067 I print_info: n_gqa            = 1
0.00.042.068 I print_info: n_embd_k_gqa     = 2048
0.00.042.068 I print_info: n_embd_v_gqa     = 2048
0.00.042.069 I print_info: f_norm_eps       = 1.0e-05
0.00.042.069 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.069 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.069 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.070 I print_info: f_logit_scale    = 0.0e+00
0.00.042.070 I print_info: n_ff             = 8192
0.00.042.073 I print_info: n_expert         = 0
0.00.042.073 I print_info: n_expert_used    = 0
0.00.042.073 I print_info: causal attn      = 1
0.00.042.073 I print_info: pooling type     = 0
0.00.042.073 I print_info: rope type        = 2
0.00.042.074 I print_info: rope scaling     = linear
0.00.042.074 I print_info: freq_base_train  = 10000.0
0.00.042.074 I print_info: freq_scale_train = 1
0.00.042.074 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.075 I print_info: rope_finetuned   = unknown
0.00.042.075 I print_info: ssm_d_conv       = 0
0.00.042.075 I print_info: ssm_d_inner      = 0
0.00.042.075 I print_info: ssm_d_state      = 0
0.00.042.075 I print_info: ssm_dt_rank      = 0
0.00.042.075 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.075 I print_info: model type       = 1.4B
0.00.042.076 I print_info: model params     = 1.41 B
0.00.042.076 I print_info: general.name     = 1.4B
0.00.042.076 I print_info: vocab type       = BPE
0.00.042.076 I print_info: n_vocab          = 50304
0.00.042.078 I print_info: n_merges         = 50009
0.00.042.078 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.078 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.078 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.079 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.079 I print_info: LF token         = 187 'Ċ'
0.00.042.079 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.079 I print_info: max token length = 1024
0.00.042.082 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.943 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.958 I load_tensors: offloading output layer to GPU
0.00.604.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.991 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.992 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.606.389 I llama_init_from_model: n_seq_max     = 1
0.00.606.392 I llama_init_from_model: n_ctx         = 128
0.00.606.392 I llama_init_from_model: n_ctx_per_seq = 128
0.00.606.392 I llama_init_from_model: n_batch       = 128
0.00.606.393 I llama_init_from_model: n_ubatch      = 128
0.00.606.393 I llama_init_from_model: flash_attn    = 0
0.00.606.395 I llama_init_from_model: freq_base     = 10000.0
0.00.606.395 I llama_init_from_model: freq_scale    = 1
0.00.606.396 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.606.398 I ggml_metal_init: allocating
0.00.606.486 I ggml_metal_init: found device: Apple M4
0.00.606.507 I ggml_metal_init: picking default device: Apple M4
0.00.607.858 I ggml_metal_init: using embedded metal library
0.00.614.155 I ggml_metal_init: GPU name:   Apple M4
0.00.614.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.160 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.162 I ggml_metal_init: simdgroup reduction   = true
0.00.614.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.162 I ggml_metal_init: has residency sets    = true
0.00.614.163 I ggml_metal_init: has bfloat            = true
0.00.614.163 I ggml_metal_init: use bfloat            = true
0.00.614.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.125 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.605 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.634.617 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.634.651 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.637.863 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.637.864 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.637.865 I llama_init_from_model: graph nodes  = 967
0.00.637.866 I llama_init_from_model: graph splits = 2
0.00.637.868 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.593 I 
0.00.667.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.690 I perplexity: tokenizing the input ..
0.00.675.113 I perplexity: tokenization took 7.42 ms
0.00.675.125 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.546 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.821.879 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.821.901 I llama_perf_context_print:        load time =     656.61 ms
0.00.821.902 I llama_perf_context_print: prompt eval time =     144.53 ms /   128 tokens (    1.13 ms per token,   885.65 tokens per second)
0.00.821.903 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.904 I llama_perf_context_print:       total time =     154.31 ms /   129 tokens
0.00.822.266 I ggml_metal_free: deallocating

real	0m0.839s
user	0m0.080s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.949 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.956 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.957 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.957 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.959 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.959 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.960 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.961 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.961 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.961 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.962 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.962 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.963 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.965 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.824 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.864 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.630 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.632 I llama_model_loader: - type  f32:  194 tensors
0.00.024.632 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.633 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.633 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.634 I print_info: file format = GGUF V3 (latest)
0.00.024.634 I print_info: file type   = Q2_K - Medium
0.00.024.635 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.045 I load: special tokens cache size = 25
0.00.039.401 I load: token to piece cache size = 0.2984 MB
0.00.039.418 I print_info: arch             = gptneox
0.00.039.419 I print_info: vocab_only       = 0
0.00.039.419 I print_info: n_ctx_train      = 2048
0.00.039.419 I print_info: n_embd           = 2048
0.00.039.419 I print_info: n_layer          = 24
0.00.039.423 I print_info: n_head           = 16
0.00.039.424 I print_info: n_head_kv        = 16
0.00.039.424 I print_info: n_rot            = 32
0.00.039.424 I print_info: n_swa            = 0
0.00.039.425 I print_info: n_embd_head_k    = 128
0.00.039.425 I print_info: n_embd_head_v    = 128
0.00.039.425 I print_info: n_gqa            = 1
0.00.039.426 I print_info: n_embd_k_gqa     = 2048
0.00.039.426 I print_info: n_embd_v_gqa     = 2048
0.00.039.427 I print_info: f_norm_eps       = 1.0e-05
0.00.039.427 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.427 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.427 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.428 I print_info: f_logit_scale    = 0.0e+00
0.00.039.428 I print_info: n_ff             = 8192
0.00.039.429 I print_info: n_expert         = 0
0.00.039.429 I print_info: n_expert_used    = 0
0.00.039.429 I print_info: causal attn      = 1
0.00.039.429 I print_info: pooling type     = 0
0.00.039.429 I print_info: rope type        = 2
0.00.039.429 I print_info: rope scaling     = linear
0.00.039.430 I print_info: freq_base_train  = 10000.0
0.00.039.430 I print_info: freq_scale_train = 1
0.00.039.430 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.430 I print_info: rope_finetuned   = unknown
0.00.039.430 I print_info: ssm_d_conv       = 0
0.00.039.430 I print_info: ssm_d_inner      = 0
0.00.039.430 I print_info: ssm_d_state      = 0
0.00.039.431 I print_info: ssm_dt_rank      = 0
0.00.039.431 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.431 I print_info: model type       = 1.4B
0.00.039.431 I print_info: model params     = 1.41 B
0.00.039.431 I print_info: general.name     = 1.4B
0.00.039.432 I print_info: vocab type       = BPE
0.00.039.432 I print_info: n_vocab          = 50304
0.00.039.432 I print_info: n_merges         = 50009
0.00.039.433 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.433 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.433 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.433 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.433 I print_info: LF token         = 187 'Ċ'
0.00.039.434 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.438 I print_info: max token length = 1024
0.00.039.438 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.333.166 I load_tensors: offloading 24 repeating layers to GPU
0.00.333.179 I load_tensors: offloading output layer to GPU
0.00.333.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.333.218 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.333.219 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.334.698 I llama_init_from_model: n_seq_max     = 1
0.00.334.701 I llama_init_from_model: n_ctx         = 128
0.00.334.701 I llama_init_from_model: n_ctx_per_seq = 128
0.00.334.702 I llama_init_from_model: n_batch       = 128
0.00.334.702 I llama_init_from_model: n_ubatch      = 128
0.00.334.702 I llama_init_from_model: flash_attn    = 0
0.00.334.704 I llama_init_from_model: freq_base     = 10000.0
0.00.334.704 I llama_init_from_model: freq_scale    = 1
0.00.334.705 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.334.709 I ggml_metal_init: allocating
0.00.334.826 I ggml_metal_init: found device: Apple M4
0.00.334.842 I ggml_metal_init: picking default device: Apple M4
0.00.336.528 I ggml_metal_init: using embedded metal library
0.00.341.938 I ggml_metal_init: GPU name:   Apple M4
0.00.341.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.952 I ggml_metal_init: simdgroup reduction   = true
0.00.341.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.952 I ggml_metal_init: has residency sets    = true
0.00.341.953 I ggml_metal_init: has bfloat            = true
0.00.341.953 I ggml_metal_init: use bfloat            = true
0.00.341.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.959 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.363.870 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.367.437 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.367.455 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.367.508 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.370.918 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.370.920 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.370.921 I llama_init_from_model: graph nodes  = 967
0.00.370.922 I llama_init_from_model: graph splits = 2
0.00.370.925 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.370.925 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.399.896 I 
0.00.399.993 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.400.002 I perplexity: tokenizing the input ..
0.00.407.187 I perplexity: tokenization took 7.182 ms
0.00.407.194 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.540.309 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.541.654 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.541.678 I llama_perf_context_print:        load time =     390.89 ms
0.00.541.679 I llama_perf_context_print: prompt eval time =     132.16 ms /   128 tokens (    1.03 ms per token,   968.55 tokens per second)
0.00.541.679 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.541.680 I llama_perf_context_print:       total time =     141.78 ms /   129 tokens
0.00.542.072 I ggml_metal_free: deallocating

real	0m0.556s
user	0m0.083s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.024 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.308 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.315 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.317 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.317 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.320 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.320 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.320 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.321 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.324 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.325 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.326 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.326 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.208 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.037 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.039 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.040 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.040 I llama_model_loader: - type  f32:  194 tensors
0.00.025.041 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.041 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.041 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.042 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.042 I print_info: file format = GGUF V3 (latest)
0.00.025.043 I print_info: file type   = Q3_K - Medium
0.00.025.044 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.520 I load: special tokens cache size = 25
0.00.040.041 I load: token to piece cache size = 0.2984 MB
0.00.040.059 I print_info: arch             = gptneox
0.00.040.060 I print_info: vocab_only       = 0
0.00.040.060 I print_info: n_ctx_train      = 2048
0.00.040.060 I print_info: n_embd           = 2048
0.00.040.061 I print_info: n_layer          = 24
0.00.040.065 I print_info: n_head           = 16
0.00.040.067 I print_info: n_head_kv        = 16
0.00.040.067 I print_info: n_rot            = 32
0.00.040.067 I print_info: n_swa            = 0
0.00.040.067 I print_info: n_embd_head_k    = 128
0.00.040.068 I print_info: n_embd_head_v    = 128
0.00.040.068 I print_info: n_gqa            = 1
0.00.040.069 I print_info: n_embd_k_gqa     = 2048
0.00.040.069 I print_info: n_embd_v_gqa     = 2048
0.00.040.070 I print_info: f_norm_eps       = 1.0e-05
0.00.040.070 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.070 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.071 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.071 I print_info: f_logit_scale    = 0.0e+00
0.00.040.071 I print_info: n_ff             = 8192
0.00.040.071 I print_info: n_expert         = 0
0.00.040.072 I print_info: n_expert_used    = 0
0.00.040.072 I print_info: causal attn      = 1
0.00.040.072 I print_info: pooling type     = 0
0.00.040.074 I print_info: rope type        = 2
0.00.040.075 I print_info: rope scaling     = linear
0.00.040.075 I print_info: freq_base_train  = 10000.0
0.00.040.075 I print_info: freq_scale_train = 1
0.00.040.075 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.075 I print_info: rope_finetuned   = unknown
0.00.040.076 I print_info: ssm_d_conv       = 0
0.00.040.076 I print_info: ssm_d_inner      = 0
0.00.040.076 I print_info: ssm_d_state      = 0
0.00.040.076 I print_info: ssm_dt_rank      = 0
0.00.040.076 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.076 I print_info: model type       = 1.4B
0.00.040.078 I print_info: model params     = 1.41 B
0.00.040.078 I print_info: general.name     = 1.4B
0.00.040.078 I print_info: vocab type       = BPE
0.00.040.079 I print_info: n_vocab          = 50304
0.00.040.079 I print_info: n_merges         = 50009
0.00.040.079 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.079 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.079 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.081 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.081 I print_info: LF token         = 187 'Ċ'
0.00.040.081 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.081 I print_info: max token length = 1024
0.00.040.082 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.899 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.913 I load_tensors: offloading output layer to GPU
0.00.440.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.948 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.949 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.732 I llama_init_from_model: n_seq_max     = 1
0.00.442.735 I llama_init_from_model: n_ctx         = 128
0.00.442.735 I llama_init_from_model: n_ctx_per_seq = 128
0.00.442.736 I llama_init_from_model: n_batch       = 128
0.00.442.736 I llama_init_from_model: n_ubatch      = 128
0.00.442.737 I llama_init_from_model: flash_attn    = 0
0.00.442.739 I llama_init_from_model: freq_base     = 10000.0
0.00.442.746 I llama_init_from_model: freq_scale    = 1
0.00.442.746 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.442.753 I ggml_metal_init: allocating
0.00.442.871 I ggml_metal_init: found device: Apple M4
0.00.442.885 I ggml_metal_init: picking default device: Apple M4
0.00.444.499 I ggml_metal_init: using embedded metal library
0.00.451.178 I ggml_metal_init: GPU name:   Apple M4
0.00.451.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.190 I ggml_metal_init: simdgroup reduction   = true
0.00.451.191 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.191 I ggml_metal_init: has residency sets    = true
0.00.451.191 I ggml_metal_init: has bfloat            = true
0.00.451.192 I ggml_metal_init: use bfloat            = true
0.00.451.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.706 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.474.415 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.474.428 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.474.487 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.477.764 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.477.766 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.477.767 I llama_init_from_model: graph nodes  = 967
0.00.477.767 I llama_init_from_model: graph splits = 2
0.00.477.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.477.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.036 I 
0.00.505.119 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.129 I perplexity: tokenizing the input ..
0.00.512.274 I perplexity: tokenization took 7.142 ms
0.00.512.281 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.657.774 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.659.474 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.659.497 I llama_perf_context_print:        load time =     496.00 ms
0.00.659.498 I llama_perf_context_print: prompt eval time =     144.61 ms /   128 tokens (    1.13 ms per token,   885.14 tokens per second)
0.00.659.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.659.499 I llama_perf_context_print:       total time =     154.47 ms /   129 tokens
0.00.659.917 I ggml_metal_free: deallocating

real	0m0.673s
user	0m0.080s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.105 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.120 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.993 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.016 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.798 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.799 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.801 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.801 I llama_model_loader: - type  f32:  194 tensors
0.00.025.802 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.802 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.802 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.803 I print_info: file format = GGUF V3 (latest)
0.00.025.803 I print_info: file type   = Q4_K - Medium
0.00.025.805 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.935 I load: special tokens cache size = 25
0.00.040.434 I load: token to piece cache size = 0.2984 MB
0.00.040.452 I print_info: arch             = gptneox
0.00.040.453 I print_info: vocab_only       = 0
0.00.040.453 I print_info: n_ctx_train      = 2048
0.00.040.453 I print_info: n_embd           = 2048
0.00.040.453 I print_info: n_layer          = 24
0.00.040.457 I print_info: n_head           = 16
0.00.040.458 I print_info: n_head_kv        = 16
0.00.040.458 I print_info: n_rot            = 32
0.00.040.458 I print_info: n_swa            = 0
0.00.040.458 I print_info: n_embd_head_k    = 128
0.00.040.458 I print_info: n_embd_head_v    = 128
0.00.040.459 I print_info: n_gqa            = 1
0.00.040.459 I print_info: n_embd_k_gqa     = 2048
0.00.040.460 I print_info: n_embd_v_gqa     = 2048
0.00.040.460 I print_info: f_norm_eps       = 1.0e-05
0.00.040.461 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.461 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.461 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.461 I print_info: f_logit_scale    = 0.0e+00
0.00.040.462 I print_info: n_ff             = 8192
0.00.040.462 I print_info: n_expert         = 0
0.00.040.462 I print_info: n_expert_used    = 0
0.00.040.462 I print_info: causal attn      = 1
0.00.040.462 I print_info: pooling type     = 0
0.00.040.462 I print_info: rope type        = 2
0.00.040.466 I print_info: rope scaling     = linear
0.00.040.466 I print_info: freq_base_train  = 10000.0
0.00.040.466 I print_info: freq_scale_train = 1
0.00.040.466 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.467 I print_info: rope_finetuned   = unknown
0.00.040.467 I print_info: ssm_d_conv       = 0
0.00.040.467 I print_info: ssm_d_inner      = 0
0.00.040.467 I print_info: ssm_d_state      = 0
0.00.040.467 I print_info: ssm_dt_rank      = 0
0.00.040.467 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.467 I print_info: model type       = 1.4B
0.00.040.468 I print_info: model params     = 1.41 B
0.00.040.468 I print_info: general.name     = 1.4B
0.00.040.468 I print_info: vocab type       = BPE
0.00.040.468 I print_info: n_vocab          = 50304
0.00.040.469 I print_info: n_merges         = 50009
0.00.040.469 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.474 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.474 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.474 I print_info: LF token         = 187 'Ċ'
0.00.040.475 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.475 I print_info: max token length = 1024
0.00.040.475 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.520.834 I load_tensors: offloading 24 repeating layers to GPU
0.00.520.849 I load_tensors: offloading output layer to GPU
0.00.520.850 I load_tensors: offloaded 25/25 layers to GPU
0.00.520.887 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.520.888 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.522.308 I llama_init_from_model: n_seq_max     = 1
0.00.522.310 I llama_init_from_model: n_ctx         = 128
0.00.522.311 I llama_init_from_model: n_ctx_per_seq = 128
0.00.522.311 I llama_init_from_model: n_batch       = 128
0.00.522.312 I llama_init_from_model: n_ubatch      = 128
0.00.522.312 I llama_init_from_model: flash_attn    = 0
0.00.522.314 I llama_init_from_model: freq_base     = 10000.0
0.00.522.314 I llama_init_from_model: freq_scale    = 1
0.00.522.315 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.522.319 I ggml_metal_init: allocating
0.00.522.399 I ggml_metal_init: found device: Apple M4
0.00.522.412 I ggml_metal_init: picking default device: Apple M4
0.00.523.928 I ggml_metal_init: using embedded metal library
0.00.530.700 I ggml_metal_init: GPU name:   Apple M4
0.00.530.710 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.530.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.530.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.530.712 I ggml_metal_init: simdgroup reduction   = true
0.00.530.712 I ggml_metal_init: simdgroup matrix mul. = true
0.00.530.713 I ggml_metal_init: has residency sets    = true
0.00.530.713 I ggml_metal_init: has bfloat            = true
0.00.530.713 I ggml_metal_init: use bfloat            = true
0.00.530.714 I ggml_metal_init: hasUnifiedMemory      = true
0.00.530.719 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.221 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.552.755 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.552.770 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.552.838 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.556.040 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.556.042 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.556.043 I llama_init_from_model: graph nodes  = 967
0.00.556.043 I llama_init_from_model: graph splits = 2
0.00.556.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.556.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.203 I 
0.00.583.287 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.295 I perplexity: tokenizing the input ..
0.00.589.431 I perplexity: tokenization took 6.134 ms
0.00.589.437 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.721.756 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.723.092 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.723.122 I llama_perf_context_print:        load time =     573.09 ms
0.00.723.123 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.51 tokens per second)
0.00.723.124 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.723.125 I llama_perf_context_print:       total time =     139.92 ms /   129 tokens
0.00.723.568 I ggml_metal_free: deallocating

real	0m0.740s
user	0m0.079s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.827 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.717 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.725 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.725 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.727 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.728 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.731 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.732 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.732 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.441 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.188 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.189 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.190 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.190 I llama_model_loader: - type  f32:  194 tensors
0.00.024.191 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.191 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.192 I print_info: file format = GGUF V3 (latest)
0.00.024.192 I print_info: file type   = Q5_K - Medium
0.00.024.193 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.747 I load: special tokens cache size = 25
0.00.039.079 I load: token to piece cache size = 0.2984 MB
0.00.039.097 I print_info: arch             = gptneox
0.00.039.098 I print_info: vocab_only       = 0
0.00.039.098 I print_info: n_ctx_train      = 2048
0.00.039.098 I print_info: n_embd           = 2048
0.00.039.098 I print_info: n_layer          = 24
0.00.039.102 I print_info: n_head           = 16
0.00.039.103 I print_info: n_head_kv        = 16
0.00.039.103 I print_info: n_rot            = 32
0.00.039.103 I print_info: n_swa            = 0
0.00.039.104 I print_info: n_embd_head_k    = 128
0.00.039.104 I print_info: n_embd_head_v    = 128
0.00.039.104 I print_info: n_gqa            = 1
0.00.039.105 I print_info: n_embd_k_gqa     = 2048
0.00.039.105 I print_info: n_embd_v_gqa     = 2048
0.00.039.106 I print_info: f_norm_eps       = 1.0e-05
0.00.039.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.106 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.107 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.107 I print_info: f_logit_scale    = 0.0e+00
0.00.039.107 I print_info: n_ff             = 8192
0.00.039.108 I print_info: n_expert         = 0
0.00.039.108 I print_info: n_expert_used    = 0
0.00.039.108 I print_info: causal attn      = 1
0.00.039.108 I print_info: pooling type     = 0
0.00.039.108 I print_info: rope type        = 2
0.00.039.108 I print_info: rope scaling     = linear
0.00.039.109 I print_info: freq_base_train  = 10000.0
0.00.039.109 I print_info: freq_scale_train = 1
0.00.039.109 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.109 I print_info: rope_finetuned   = unknown
0.00.039.110 I print_info: ssm_d_conv       = 0
0.00.039.110 I print_info: ssm_d_inner      = 0
0.00.039.110 I print_info: ssm_d_state      = 0
0.00.039.110 I print_info: ssm_dt_rank      = 0
0.00.039.110 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.113 I print_info: model type       = 1.4B
0.00.039.113 I print_info: model params     = 1.41 B
0.00.039.113 I print_info: general.name     = 1.4B
0.00.039.114 I print_info: vocab type       = BPE
0.00.039.114 I print_info: n_vocab          = 50304
0.00.039.114 I print_info: n_merges         = 50009
0.00.039.115 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.115 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.115 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.115 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.115 I print_info: LF token         = 187 'Ċ'
0.00.039.116 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.116 I print_info: max token length = 1024
0.00.039.116 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.585.140 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.151 I load_tensors: offloading output layer to GPU
0.00.585.152 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.179 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.585.180 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.586.768 I llama_init_from_model: n_seq_max     = 1
0.00.586.772 I llama_init_from_model: n_ctx         = 128
0.00.586.772 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.773 I llama_init_from_model: n_batch       = 128
0.00.586.773 I llama_init_from_model: n_ubatch      = 128
0.00.586.774 I llama_init_from_model: flash_attn    = 0
0.00.586.776 I llama_init_from_model: freq_base     = 10000.0
0.00.586.776 I llama_init_from_model: freq_scale    = 1
0.00.586.777 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.780 I ggml_metal_init: allocating
0.00.586.864 I ggml_metal_init: found device: Apple M4
0.00.586.879 I ggml_metal_init: picking default device: Apple M4
0.00.588.397 I ggml_metal_init: using embedded metal library
0.00.595.181 I ggml_metal_init: GPU name:   Apple M4
0.00.595.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.191 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.193 I ggml_metal_init: simdgroup reduction   = true
0.00.595.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.193 I ggml_metal_init: has residency sets    = true
0.00.595.194 I ggml_metal_init: has bfloat            = true
0.00.595.194 I ggml_metal_init: use bfloat            = true
0.00.595.195 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.027 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.606 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.613 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.641 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.861 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.863 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.864 I llama_init_from_model: graph nodes  = 967
0.00.619.864 I llama_init_from_model: graph splits = 2
0.00.619.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.276 I 
0.00.649.362 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.368 I perplexity: tokenizing the input ..
0.00.656.611 I perplexity: tokenization took 7.239 ms
0.00.656.620 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.739 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.100 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.136 I llama_perf_context_print:        load time =     640.44 ms
0.00.796.137 I llama_perf_context_print: prompt eval time =     137.22 ms /   128 tokens (    1.07 ms per token,   932.79 tokens per second)
0.00.796.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.138 I llama_perf_context_print:       total time =     146.86 ms /   129 tokens
0.00.796.547 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.080s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.822 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.374 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.375 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.378 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.379 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.168 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.093 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.093 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.094 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.094 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.095 I llama_model_loader: - type  f32:  194 tensors
0.00.024.095 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.096 I print_info: file format = GGUF V3 (latest)
0.00.024.096 I print_info: file type   = Q6_K
0.00.024.097 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.189 I load: special tokens cache size = 25
0.00.038.463 I load: token to piece cache size = 0.2984 MB
0.00.038.480 I print_info: arch             = gptneox
0.00.038.481 I print_info: vocab_only       = 0
0.00.038.481 I print_info: n_ctx_train      = 2048
0.00.038.481 I print_info: n_embd           = 2048
0.00.038.481 I print_info: n_layer          = 24
0.00.038.487 I print_info: n_head           = 16
0.00.038.488 I print_info: n_head_kv        = 16
0.00.038.488 I print_info: n_rot            = 32
0.00.038.488 I print_info: n_swa            = 0
0.00.038.488 I print_info: n_embd_head_k    = 128
0.00.038.489 I print_info: n_embd_head_v    = 128
0.00.038.489 I print_info: n_gqa            = 1
0.00.038.490 I print_info: n_embd_k_gqa     = 2048
0.00.038.491 I print_info: n_embd_v_gqa     = 2048
0.00.038.491 I print_info: f_norm_eps       = 1.0e-05
0.00.038.492 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.492 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.492 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.492 I print_info: f_logit_scale    = 0.0e+00
0.00.038.493 I print_info: n_ff             = 8192
0.00.038.493 I print_info: n_expert         = 0
0.00.038.494 I print_info: n_expert_used    = 0
0.00.038.494 I print_info: causal attn      = 1
0.00.038.494 I print_info: pooling type     = 0
0.00.038.494 I print_info: rope type        = 2
0.00.038.494 I print_info: rope scaling     = linear
0.00.038.495 I print_info: freq_base_train  = 10000.0
0.00.038.495 I print_info: freq_scale_train = 1
0.00.038.495 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.495 I print_info: rope_finetuned   = unknown
0.00.038.495 I print_info: ssm_d_conv       = 0
0.00.038.495 I print_info: ssm_d_inner      = 0
0.00.038.495 I print_info: ssm_d_state      = 0
0.00.038.496 I print_info: ssm_dt_rank      = 0
0.00.038.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.496 I print_info: model type       = 1.4B
0.00.038.496 I print_info: model params     = 1.41 B
0.00.038.496 I print_info: general.name     = 1.4B
0.00.038.497 I print_info: vocab type       = BPE
0.00.038.499 I print_info: n_vocab          = 50304
0.00.038.499 I print_info: n_merges         = 50009
0.00.038.499 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.499 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.499 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.499 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.500 I print_info: LF token         = 187 'Ċ'
0.00.038.500 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.500 I print_info: max token length = 1024
0.00.038.501 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.568 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.584 I load_tensors: offloading output layer to GPU
0.00.613.585 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.621 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.613.622 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.615.256 I llama_init_from_model: n_seq_max     = 1
0.00.615.259 I llama_init_from_model: n_ctx         = 128
0.00.615.260 I llama_init_from_model: n_ctx_per_seq = 128
0.00.615.260 I llama_init_from_model: n_batch       = 128
0.00.615.261 I llama_init_from_model: n_ubatch      = 128
0.00.615.261 I llama_init_from_model: flash_attn    = 0
0.00.615.263 I llama_init_from_model: freq_base     = 10000.0
0.00.615.264 I llama_init_from_model: freq_scale    = 1
0.00.615.264 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.615.267 I ggml_metal_init: allocating
0.00.615.370 I ggml_metal_init: found device: Apple M4
0.00.615.394 I ggml_metal_init: picking default device: Apple M4
0.00.616.717 I ggml_metal_init: using embedded metal library
0.00.623.219 I ggml_metal_init: GPU name:   Apple M4
0.00.623.223 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.224 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.225 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.225 I ggml_metal_init: simdgroup reduction   = true
0.00.623.226 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.226 I ggml_metal_init: has residency sets    = true
0.00.623.226 I ggml_metal_init: has bfloat            = true
0.00.623.226 I ggml_metal_init: use bfloat            = true
0.00.623.227 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.231 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.236 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.644.775 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.644.780 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.644.827 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.648.133 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.648.134 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.648.135 I llama_init_from_model: graph nodes  = 967
0.00.648.136 I llama_init_from_model: graph splits = 2
0.00.648.139 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.648.139 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.575 I 
0.00.682.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.642 I perplexity: tokenizing the input ..
0.00.689.032 I perplexity: tokenization took 6.387 ms
0.00.689.044 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.744 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.822.289 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.822.312 I llama_perf_context_print:        load time =     673.75 ms
0.00.822.313 I llama_perf_context_print: prompt eval time =     130.79 ms /   128 tokens (    1.02 ms per token,   978.65 tokens per second)
0.00.822.314 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.822.314 I llama_perf_context_print:       total time =     139.74 ms /   129 tokens
0.00.822.662 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.078s
sys	0m0.129s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.296 I build: 4854 (7c7f3b7f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.943 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.836 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.846 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.849 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.850 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.852 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.862 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.862 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.863 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.864 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.865 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.016 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.866 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.868 I llama_model_loader: - type  f32:  194 tensors
0.00.047.870 I llama_model_loader: - type  f16:   98 tensors
0.00.047.871 I print_info: file format = GGUF V3 (latest)
0.00.047.872 I print_info: file type   = all F32 (guessed)
0.00.047.873 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.057.250 I load: special tokens cache size = 25
0.00.064.234 I load: token to piece cache size = 0.2984 MB
0.00.064.252 I print_info: arch             = gptneox
0.00.064.252 I print_info: vocab_only       = 0
0.00.064.253 I print_info: n_ctx_train      = 2048
0.00.064.253 I print_info: n_embd           = 2048
0.00.064.253 I print_info: n_layer          = 24
0.00.064.257 I print_info: n_head           = 16
0.00.064.258 I print_info: n_head_kv        = 16
0.00.064.258 I print_info: n_rot            = 32
0.00.064.258 I print_info: n_swa            = 0
0.00.064.258 I print_info: n_embd_head_k    = 128
0.00.064.258 I print_info: n_embd_head_v    = 128
0.00.064.259 I print_info: n_gqa            = 1
0.00.064.260 I print_info: n_embd_k_gqa     = 2048
0.00.064.260 I print_info: n_embd_v_gqa     = 2048
0.00.064.261 I print_info: f_norm_eps       = 1.0e-05
0.00.064.261 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.261 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.262 I print_info: f_logit_scale    = 0.0e+00
0.00.064.262 I print_info: n_ff             = 8192
0.00.064.262 I print_info: n_expert         = 0
0.00.064.263 I print_info: n_expert_used    = 0
0.00.064.264 I print_info: causal attn      = 1
0.00.064.267 I print_info: pooling type     = 0
0.00.064.267 I print_info: rope type        = 2
0.00.064.267 I print_info: rope scaling     = linear
0.00.064.267 I print_info: freq_base_train  = 10000.0
0.00.064.268 I print_info: freq_scale_train = 1
0.00.064.268 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.268 I print_info: rope_finetuned   = unknown
0.00.064.268 I print_info: ssm_d_conv       = 0
0.00.064.268 I print_info: ssm_d_inner      = 0
0.00.064.268 I print_info: ssm_d_state      = 0
0.00.064.268 I print_info: ssm_dt_rank      = 0
0.00.064.268 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.269 I print_info: model type       = 1.4B
0.00.064.270 I print_info: model params     = 1.41 B
0.00.064.271 I print_info: general.name     = 1.4B
0.00.064.271 I print_info: vocab type       = BPE
0.00.064.271 I print_info: n_vocab          = 50304
0.00.064.271 I print_info: n_merges         = 50009
0.00.064.272 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.274 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.274 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.274 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.274 I print_info: LF token         = 187 'Ċ'
0.00.064.274 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.275 I print_info: max token length = 1024
0.00.064.275 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.529.414 I load_tensors: offloading 24 repeating layers to GPU
0.01.529.418 I load_tensors: offloading output layer to GPU
0.01.529.419 I load_tensors: offloaded 25/25 layers to GPU
0.01.529.435 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.529.435 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.529.838 I llama_init_from_model: n_seq_max     = 1
0.01.529.839 I llama_init_from_model: n_ctx         = 128
0.01.529.839 I llama_init_from_model: n_ctx_per_seq = 128
0.01.529.839 I llama_init_from_model: n_batch       = 128
0.01.529.839 I llama_init_from_model: n_ubatch      = 128
0.01.529.839 I llama_init_from_model: flash_attn    = 0
0.01.529.840 I llama_init_from_model: freq_base     = 10000.0
0.01.529.840 I llama_init_from_model: freq_scale    = 1
0.01.529.840 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.529.841 I ggml_metal_init: allocating
0.01.529.869 I ggml_metal_init: found device: Apple M4
0.01.529.874 I ggml_metal_init: picking default device: Apple M4
0.01.530.410 I ggml_metal_init: using embedded metal library
0.01.532.967 I ggml_metal_init: GPU name:   Apple M4
0.01.532.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.532.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.532.969 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.532.970 I ggml_metal_init: simdgroup reduction   = true
0.01.532.970 I ggml_metal_init: simdgroup matrix mul. = true
0.01.532.970 I ggml_metal_init: has residency sets    = true
0.01.532.970 I ggml_metal_init: has bfloat            = true
0.01.532.970 I ggml_metal_init: use bfloat            = true
0.01.532.971 I ggml_metal_init: hasUnifiedMemory      = true
0.01.532.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.542.616 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.544.327 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.544.329 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.544.353 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.545.958 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.545.959 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.545.959 I llama_init_from_model: graph nodes  = 967
0.01.545.959 I llama_init_from_model: graph splits = 2
0.01.545.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.545.961 I 
0.01.546.005 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.546.006 I compute_imatrix: tokenizing the input ..
0.01.550.136 I compute_imatrix: tokenization took 4.129 ms
0.01.550.138 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.834.514 I compute_imatrix: 0.28 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.836.909 I llama_perf_context_print:        load time =    1812.57 ms
0.01.836.910 I llama_perf_context_print: prompt eval time =     282.50 ms /   128 tokens (    2.21 ms per token,   453.10 tokens per second)
0.01.836.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.836.912 I llama_perf_context_print:       total time =    1814.96 ms /   129 tokens
0.01.837.461 I ggml_metal_free: deallocating

real	0m2.071s
user	0m0.125s
sys	0m0.205s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4854 (7c7f3b7f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148504cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148505350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1485057c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148505c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1485060a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148506510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148506980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148506df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148507260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1485076d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148507b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1485081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148508d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1485094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148509cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14850a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14850ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14850b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14850b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14850c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14850c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14850cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14850d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14850df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14850e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14850e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14850ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14850f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14850f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14850fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1485102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1485107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148510ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148511370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148511810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148511de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148512280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148512720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148512bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148513060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148513500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1485139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148513e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1485142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1485145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148514860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148514cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148515860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148515c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148516100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1485165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148516a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148516ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148517380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148517820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148517cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148518160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148518420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148518920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14850f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148519080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1485195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148519ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148519fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14851a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14851a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14851aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14851b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14851b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14851bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14851c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14851c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14851ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14851d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14851d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14851dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14851e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14851e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14851ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14851f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14851f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14851ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1485204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148520a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148521040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1485215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148521ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148522150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148522700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148522cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148523260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148523810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148523dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148524370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148524920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148524ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148525480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1485152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148525be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148526050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1485264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148526a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148527020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1485275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148527b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148528130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1485286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148528c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148529240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1485297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148529da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14852a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14852a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14852aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14852b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14852b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14852bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14852c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14852c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14852ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14852d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14852d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14852dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14852e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14852e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14852eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14852efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14852f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14852f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14852feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1485303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1485308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148530db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1485312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1485317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148531cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1485321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1485326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148532bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1485330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1485335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148533ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148533fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1485344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1485349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148534eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1485353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1485358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148535db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1485362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1485367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148536cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1485371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1485376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148537bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1485380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1485385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148538ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148538fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1485394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1485399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148539eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14853a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14853a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14853adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14853b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14853b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14853bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14853c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14853c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14853cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14853d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14853d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14853dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14853dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14853e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14853e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14853eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14853f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14853f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14853fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1485402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1485407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148540cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1485411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1485416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148541bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1485420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1485425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148542ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148542fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1485434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1485439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148543eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148544460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148544a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148544fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148545570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148545a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148545f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148546470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148546b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148546ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1485472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148547860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148547d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148548440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1485488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148548d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148549220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148549a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148549d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14854a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14854a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14854ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14854b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14854b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14854bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14854c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14854cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14854d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14854d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14854dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14854e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14854e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14854ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14854f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14854f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14854fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148550390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148550940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148550ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1485514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148551a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148552000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1485525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148552b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148553110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1485536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148553c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148554220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1485547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148554d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148555330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1485558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148555e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148556440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1485569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148556fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148557550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148557b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1485580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148558660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148558c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1485591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148559770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148559d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14855a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14855a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14855ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14855b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14855b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14855bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14855c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14855caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14855d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14855d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14855dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14855e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14855e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14855eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14855efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14855f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14855f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14855feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1485603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1485608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148560db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1485612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1485617b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148561cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1485621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1485626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x148562bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1485630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1485635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x148563ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x148563fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1485644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1485649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x148564eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1485653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1485658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1485662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1485669e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148567100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148567820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148567ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148568270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148568710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148568bb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.721.192 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.721.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148405ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148406150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1484065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148408fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148409420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1484096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148409b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148409fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14840a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14840a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14840ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14840b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14840bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14840c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14840cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14840d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14840dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14840e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14840eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14840f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14840f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1484100f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148410810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148410f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148411650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148411910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148411bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148412040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1484124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148412920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148412ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1484133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148413ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148413f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148414410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1484148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148414d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1484151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148415690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148415b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148415fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148416470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148416910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148416db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148417250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1484176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148417b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148418030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1484184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148418970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148418e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1484192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148419750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148419bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14841a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14841a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14841a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14841ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14841af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14841b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14841b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14841bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14841c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14841c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14841c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14841ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14841d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14841d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14841dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14841e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14841e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14841e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14841ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14841f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14841f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14841fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14841ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1484203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148420810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148420c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1484210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148421560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1484219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148421e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1484222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148422720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148422b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148423000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148423470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1484238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148423d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1484241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148424630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148424aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148424f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148425380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1484257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148425c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1484260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148426540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1484269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148426e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148427290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148427700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148427b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148427fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148428450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1484288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148428d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1484291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148429610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148429a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148429ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14842a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14842a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14842ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14842b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14842b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14842b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14842be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14842c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14842c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14842cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14842cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14842d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14842d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14842dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14842e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14842e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14842ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14842eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14842f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14842f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14842fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148430090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148430500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148430970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148430de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148431250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1484316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148431b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148431fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148432410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148432880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148432cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148433160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1484335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148433a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148433eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148434320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148434790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148434c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148435070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1484354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148435950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148435dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148436230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1484366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148436b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148436f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1484373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148437860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148437cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148438140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1484385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148438a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1484391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148439460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1484398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148439d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14843a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14843a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14843aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14843af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14843b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14843b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14843bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14843c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14843c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14843c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14843ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14843d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14843d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14843db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14843dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14843e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14843e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14843ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14843f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14843f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14843fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14843fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148440350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1484407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148440c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1484410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148441510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148441980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148441df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148442260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1484426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148442b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148443310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1484435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148443b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148444080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148444760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148444c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1484450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148445540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148445d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148446050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148446600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148446bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148447160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148447710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148447cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148448270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148448820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148448dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148449380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148449930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148449ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14844a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14844aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14844aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14844b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14844bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14844c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14844c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14844cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14844d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14844d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14844dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14844e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14844e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14844ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14844f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14844f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14844ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148450540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148450af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1484510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148451650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148451c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1484521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148452760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148452d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1484532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148453870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148453e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1484543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148454980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148454f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1484554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148455a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148456040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1484565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148456ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148457150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148457700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148457cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148458260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148458810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148458dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148459370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148459920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148459ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14845a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14845a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14845add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14845b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14845b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14845bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14845c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14845c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14845cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14845d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14845d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14845dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14845dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14845e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14845e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14845eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14845f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14845f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14845fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1484602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1484607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x148460cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1484611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1484616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148461bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1484625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148462d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148463420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148463b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148463e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148464590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148464a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148464ed0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137604ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137604f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1376053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137605820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137605c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137606100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137606570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1376069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137606e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1376072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137607730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137607db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1376088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137609080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137609890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137609fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13760a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13760adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13760b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13760bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13760c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13760cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13760d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13760d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13760e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13760e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13760e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13760ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13760eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13760f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13760f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13760fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137610500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1376109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137610e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1376112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137611780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137611c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1376120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137612560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137612a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137612ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137613340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1376137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137613c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137614120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1376145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137614a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137614f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1376153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137615840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137615ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137616180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137616ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137616f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137617400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1376176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137617980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137617df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137618260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1376186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137618b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137618fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137619420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137619890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137619d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13761a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13761a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13761aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13761aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13761b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13761b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13761bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13761c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13761c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13761c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13761cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13761d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13761d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13761db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13761df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13761e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13761e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13761ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13761f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13761f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13761fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13761fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137620310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137620780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137620bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137621060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1376214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137621940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137621db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137622220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137622690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137622db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137623290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137623840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137623df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1376243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137624950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137624f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1376254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137626010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1376265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137627120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1376276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137628230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137628730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137628c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137629130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137629630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137629b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13762a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13762a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13762aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13762af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13762b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13762b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13762be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13762c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13762c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13762cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13762d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13762d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13762dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13762e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13762e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13762eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13762f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13762f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13762fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13762ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137630430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137630930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137630e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137631330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137631830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137631d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137632230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137632730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137632c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137633130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137633630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137633b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137634030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137634530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137634a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137634f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137635430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137635930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137635e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137636330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137636830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137636d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137637730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137637c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137638130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137638630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137638b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137639030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137639530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137639a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137639f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13763a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13763a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13763ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13763b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13763b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13763bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13763c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13763c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13763cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13763d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13763d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13763db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13763e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13763e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13763ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13763ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13763f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13763f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13763fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137640330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137640830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137640d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137641230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1376417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137641d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137642340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1376428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1376432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1376437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137643ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137644370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137644630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137644be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1376450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1376457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137645c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137646100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1376465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137646df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1376470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137647c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1376481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137648770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137648d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1376492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137649880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137649e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13764a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13764a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13764af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13764b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13764baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13764c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13764c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13764cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13764d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13764d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13764dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13764e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13764e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13764edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13764f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13764f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13764fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137650490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137650a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137650ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1376515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137651b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137652100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1376526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137652c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137653210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1376537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137653d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137654320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1376548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137654e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137655430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1376559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137655f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137656540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137656af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1376570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137657650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137657c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1376581b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137658760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137658d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1376592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137659870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137659e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13765a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13765a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13765af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13765b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13765b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13765be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13765c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13765c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13765cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13765d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13765d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13765dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13765e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13765e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13765eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13765f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13765f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13765fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13765ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x137660430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x137660930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x137660e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x137661330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x137661830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x137661d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x137662230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x137662730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137662c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137663640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137663d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137664480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137664ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137664e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1376655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137665a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137665f30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.800s
user	0m0.267s
sys	0m0.330s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4854 (7c7f3b7f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13960bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13960c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13960ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13960cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13960d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13960db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13960e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13960e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13960ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13960f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13960f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13960fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139610640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139610df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139611600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139611d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139612440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139612b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139613280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139613a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139614170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139614890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139614fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139615850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139615f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1396168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139616f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1396173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139617890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139617b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139618240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139618500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1396189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139618e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1396192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139619780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139619c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13961a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13961a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13961aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13961aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13961b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13961b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13961baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13961bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13961c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13961cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13961d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13961d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13961dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13961e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13961e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13961ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13961ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13961f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13961f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13961fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139620250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1396206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1396209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1396212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139621790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139621c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1396220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139622570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139622a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139622eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139623350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1396237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139623c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139624130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139624680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139624bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139625120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139625670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139625bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139626110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139626660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139626bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139627100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139627650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139627ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1396280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139628640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139628b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1396290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139629630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139629b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13962a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13962a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13962ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13962b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13962b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13962bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13962c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13961c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13962c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13962ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13962d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13962d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13962dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13962e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13962e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13962ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13962f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13962f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13962fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1396301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139630740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139630c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1396311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139631680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139631b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139631fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139632460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139632900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139632da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139633240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1396336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139633b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139634020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1396344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139634e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1396352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139635740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139635be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139636080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139636520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1396369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139636e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139637300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1396377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139637c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1396380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139638580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139638a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139638ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139639360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139639800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139639ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13963a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13963a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13963aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13963af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13963b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13963b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13963bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13963c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13963c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13963cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13963cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13963d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13963d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13963dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13963e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13963e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13963eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13963efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13963f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13963f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13963fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139640260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139640700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139640ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139641040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1396414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139641980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139641e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1396422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139642760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139642c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1396430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139643540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1396439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139643e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139644320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1396447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139644c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139645100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1396455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139645a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139645ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139646380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139646820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139646cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139647160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139647600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139647aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1396483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139648930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139648e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1396493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139649920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139649dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13964a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13964a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13964aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13964b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13964b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13964ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13964bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13964c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13964c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13964ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13964d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13964d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13964de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13964e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13964e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13964ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13964f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13964f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13964fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139650290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139650840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1396513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139651950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1396524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139652a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139653010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1396535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139653b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139654120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1396546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139654c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139655230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1396557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139655d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139656340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1396568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139656ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139657450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139657a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139657fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139658560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139658b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1396590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139659670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139659c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13965a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13965a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13965ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13965b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13965b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13965be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13965c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13965c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13965cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13965d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13965dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13965e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13965e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13965ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13965f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13965f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13965fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139660280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139660830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139660de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139661390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139661940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139661ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1396623f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1396628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139662df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1396632f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1396637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139663cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1396641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1396646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139664bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1396650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1396655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139665af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139665ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1396664f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1396669f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x139666ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1396673f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1396678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x139667df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1396682f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1396687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x139668cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1396691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1396696f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139669bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13966a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13966ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13966b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13966bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13966be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13966c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13966ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13966cef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.185 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.190 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a804ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a805150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a8055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a805a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a805ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a806310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a806780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a806bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a807060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a8074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a807940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a808020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a808b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a8092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a809b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a80a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a80a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a80b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a80b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a80bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a80c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a80cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a80d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a80dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a80e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a80e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a80e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a80ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a80f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a80f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a80fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a810090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a810770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a810c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a8110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a811550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a8119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a811e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a812330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a8127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a812c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a813110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a8135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a813a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a813ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a814390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a814830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a814cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a815170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a815610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a815ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a815f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a8163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a816890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a816d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a8171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a817670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a817930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a817bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a818060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a8184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a818940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a818db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a819220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a819690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a819b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a819f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a81a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a81a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a81acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a81b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a81b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a81ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a81be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a81c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a81c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a81cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a81d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a81d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a81d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a81dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a81e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a81e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a81eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a81ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a81f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a81f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a81fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a820110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a820580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a8209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a820e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a8212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a821740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a821bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a822020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a822490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a822900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a822d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a8231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a823650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a823ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a823f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a8243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a824810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a824c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a8250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a825560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a8259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a825e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a8262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a826720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a826b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a827000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a827470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a8278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a827d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a8281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a828630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a828aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a828f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a829380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a8297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a829c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a82a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a82a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a82a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a82ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a82b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a82b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a82bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a82bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a82c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a82c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a82cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a82d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a82d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a82da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a82def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a82e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a82e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a82ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a82f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a82f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a82f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a82fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a830270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a8306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a830b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a830fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a831430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a8318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a831d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a832180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a8325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a832a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a832ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a833340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a8337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a833c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a834090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a834500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a834970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a834de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a835250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a8356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a835e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a836100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a836570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a8369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a836e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a8372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a837730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a837ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a838010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a838480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a8388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a838d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a8391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a839640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a839ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a839f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a83a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a83a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a83ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a83b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a83b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a83b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a83be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a83c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a83c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a83cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a83cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a83d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a83d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a83dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a83e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a83e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a83ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a83ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a83f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a83f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a83ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a840270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a840820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a840d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a841400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a8418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a841d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a8421e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a842a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a842cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a8432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a843850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a843e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a8443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a844960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a844f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a8454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a845a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a846020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a8465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a846b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a847130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a8476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a847c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a848240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a8487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a848da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a849350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a849900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a849eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a84a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a84aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a84afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a84b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a84bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a84c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a84c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a84cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a84d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a84d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a84dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a84e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a84e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a84ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a84f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a84f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a84ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a850510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a850ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a851070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a851620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a851bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a852180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a852730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a852ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a853290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a853840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a853df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a8543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a854950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a854f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a8554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a855a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a856010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a8565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a856b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a857070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a857570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a857a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a857f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a858470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a858970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a858e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a859370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a859870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a859d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a85a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a85a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a85ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a85b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13a85b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13a85bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13a85c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13a85c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13a85ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13a85cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13a85d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13a85d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13a85de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13a85e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a85e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a85f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a85f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a8600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a8607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a860aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a861230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a8616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a861b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139707180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1397075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139707a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139707ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139708340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13970add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13970b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13970b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13970bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13970bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13970c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13970cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13970d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13970dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13970e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13970ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13970f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13970faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139710210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1397109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139711100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139711820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139711f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139712660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139712d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139713040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139713300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139713770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139713be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139714050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139714b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139715200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1397156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139715fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139716480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139716920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139716dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139717260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139717700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139717ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139718040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1397184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139718980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139718e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1397192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139719760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139719c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13971a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13971a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13971a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13971ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13971b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13971b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13971bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13971c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13971c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13971c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13971caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13971cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13971d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13971d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13971dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13971e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13971e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13971ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13971ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13971f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13971f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13971fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139720030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1397204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139720910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139720d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1397211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139721660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139721ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139721f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1397223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139722820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139722c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139723100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139723570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1397239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139723e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1397242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139724730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139724ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139725010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139725480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1397258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139725d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1397261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139726640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139726ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139726f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139727390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139727ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139727f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139728540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139728af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1397290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139729650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139729c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13972a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13972a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13972ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13972b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13972b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13972be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13972c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13972c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13972cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13972d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13972d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13972de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13972e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13972e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13972ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13972f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13972f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13972fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139730130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139730630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139730b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139731030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139731530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139731a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139731f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139732430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139732930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139732e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139733330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139733830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139733d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139734230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139734730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139734c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139735130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139735630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139735b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139736030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139736530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139736a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139736f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139737430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139737e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139738330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139738830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139738d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139739230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139739730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139739c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13973a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13973a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13973ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13973b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13973b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13973ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13973bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13973c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13973c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13973ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13973d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13973d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13973dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13973e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13973e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13973ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13973f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13973f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13973fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139740030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139740530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139740a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139740f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139741430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139741930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139741e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139742330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139742830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139742d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139743230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139743730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139743c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139744130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139744630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139744b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139745030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139745530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139745a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1397464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139746a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139747040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1397475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139747af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139747ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1397484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139748bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139749070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139749330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1397498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139749de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13974a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13974a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13974ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13974b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13974baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13974bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13974c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13974c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13974cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13974d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13974da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13974dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13974e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13974eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13974f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13974f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13974fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1397501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1397507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139750d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139751300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1397518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139751e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139752410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1397529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139752f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139753520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139753ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139754080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139754630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139754be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139755190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139755740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139755cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1397562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139756850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139756e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1397573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139757960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139757f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1397584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139758a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139759020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1397595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139759b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13975a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13975a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13975ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13975b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13975b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13975bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13975c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13975c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13975ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13975d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13975da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13975dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13975e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13975eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13975f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13975f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13975fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139760130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139760630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139760b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139761030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139761530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139761a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139761f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139762430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139762930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139762e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139763330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139763830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139763d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139764230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x139764730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x139764c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x139765130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x139765630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x139765b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x139766030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x139766530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x139766a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x139766f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x139767430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139767930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139768340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139768a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139769180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1397698a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139769b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13976a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13976a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13976ac30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.950s
user	0m0.231s
sys	0m0.186s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
