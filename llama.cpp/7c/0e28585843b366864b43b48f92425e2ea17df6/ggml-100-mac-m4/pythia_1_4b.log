Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.588s
user	0m0.877s
sys	0m1.250s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Built target sha1
[  6%] Built target sha256
[  6%] Built target xxhash
[  6%] Built target build_info
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Built target llava
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking CXX static library libcommon.a
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target test-c
[ 32%] Built target llama-simple-chat
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-grammar-parser
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 44%] Linking CXX executable ../bin/test-llama-grammar
[ 44%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Built target test-grammar-parser
[ 46%] Built target test-arg-parser
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-log
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Linking CXX executable ../bin/test-chat-template
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-gguf
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 57%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Built target test-chat-template
[ 58%] Built target test-gguf
[ 58%] Built target test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../../bin/llama-batched-bench
[ 59%] Built target test-model-load-cancel
[ 59%] Built target test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Built target test-barrier
[ 61%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Built target llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 67%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Built target test-rope
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-batched
[ 70%] Built target llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gguf-split
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Built target llama-gritlm
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 74%] Built target llama-infill
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Built target llama-imatrix
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-cli
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookup
[ 79%] Built target llama-lookup-create
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-cli
[ 83%] Generating index.html.gz.hpp
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-lookup-stats
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-save-load-state
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-run
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-speculative
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-gen-docs
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-tts
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.835s
user	0m5.977s
sys	0m9.242s

main: quantize time =  5497.98 ms
main:    total time =  5497.98 ms

main: quantize time =  1750.91 ms
main:    total time =  1750.91 ms

main: quantize time =  1804.90 ms
main:    total time =  1804.90 ms

main: quantize time =  2586.19 ms
main:    total time =  2586.19 ms

main: quantize time =  2635.02 ms
main:    total time =  2635.02 ms

main: quantize time =  5367.59 ms
main:    total time =  5367.59 ms

main: quantize time =  5667.65 ms
main:    total time =  5667.65 ms

main: quantize time =  6850.48 ms
main:    total time =  6850.48 ms

main: quantize time =  5778.00 ms
main:    total time =  5778.00 ms

main: quantize time =  4617.27 ms
main:    total time =  4617.27 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.108 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.221 I main: llama backend init
0.00.000.228 I main: load the model and apply lora adapter, if any
0.00.035.027 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.046.039 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.058 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.063 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.064 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.064 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.065 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.065.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.408 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.409 I llama_model_loader: - type  f32:  194 tensors
0.00.065.410 I llama_model_loader: - type  f16:   98 tensors
0.00.095.768 I llm_load_vocab: special tokens cache size = 25
0.00.102.300 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.102.302 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.102.303 I llm_load_print_meta: arch             = gptneox
0.00.102.303 I llm_load_print_meta: vocab type       = BPE
0.00.102.303 I llm_load_print_meta: n_vocab          = 50304
0.00.102.303 I llm_load_print_meta: n_merges         = 50009
0.00.102.304 I llm_load_print_meta: vocab_only       = 0
0.00.102.304 I llm_load_print_meta: n_ctx_train      = 2048
0.00.102.304 I llm_load_print_meta: n_embd           = 2048
0.00.102.304 I llm_load_print_meta: n_layer          = 24
0.00.102.307 I llm_load_print_meta: n_head           = 16
0.00.102.308 I llm_load_print_meta: n_head_kv        = 16
0.00.102.308 I llm_load_print_meta: n_rot            = 32
0.00.102.308 I llm_load_print_meta: n_swa            = 0
0.00.102.309 I llm_load_print_meta: n_embd_head_k    = 128
0.00.102.311 I llm_load_print_meta: n_embd_head_v    = 128
0.00.102.311 I llm_load_print_meta: n_gqa            = 1
0.00.102.312 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.102.313 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.102.313 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.102.315 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.102.315 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.102.316 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.102.316 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.102.316 I llm_load_print_meta: n_ff             = 8192
0.00.102.316 I llm_load_print_meta: n_expert         = 0
0.00.102.317 I llm_load_print_meta: n_expert_used    = 0
0.00.102.317 I llm_load_print_meta: causal attn      = 1
0.00.102.317 I llm_load_print_meta: pooling type     = 0
0.00.102.317 I llm_load_print_meta: rope type        = 2
0.00.102.317 I llm_load_print_meta: rope scaling     = linear
0.00.102.318 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.102.318 I llm_load_print_meta: freq_scale_train = 1
0.00.102.318 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.102.318 I llm_load_print_meta: rope_finetuned   = unknown
0.00.102.319 I llm_load_print_meta: ssm_d_conv       = 0
0.00.102.319 I llm_load_print_meta: ssm_d_inner      = 0
0.00.102.319 I llm_load_print_meta: ssm_d_state      = 0
0.00.102.320 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.102.320 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.102.320 I llm_load_print_meta: model type       = 1.4B
0.00.102.321 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.102.321 I llm_load_print_meta: model params     = 1.41 B
0.00.102.322 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.102.322 I llm_load_print_meta: general.name     = 1.4B
0.00.102.322 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.102.322 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.102.323 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.102.323 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.102.323 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.102.323 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.102.324 I llm_load_print_meta: max token length = 1024
0.00.104.833 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.104.834 I llm_load_tensors: offloading output layer to GPU
0.00.104.834 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.104.853 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.104.854 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.105.794 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.795 I llama_new_context_with_model: n_ctx         = 2048
0.00.105.795 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.105.795 I llama_new_context_with_model: n_batch       = 2048
0.00.105.795 I llama_new_context_with_model: n_ubatch      = 512
0.00.105.795 I llama_new_context_with_model: flash_attn    = 0
0.00.105.796 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.796 I llama_new_context_with_model: freq_scale    = 1
0.00.105.797 I ggml_metal_init: allocating
0.00.105.800 I ggml_metal_init: found device: Apple M4
0.00.105.802 I ggml_metal_init: picking default device: Apple M4
0.00.106.462 I ggml_metal_init: using embedded metal library
0.00.117.787 I ggml_metal_init: GPU name:   Apple M4
0.00.117.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.117.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.117.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.117.790 I ggml_metal_init: simdgroup reduction   = true
0.00.117.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.117.790 I ggml_metal_init: has bfloat            = true
0.00.117.790 I ggml_metal_init: use bfloat            = true
0.00.117.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.117.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.141.424 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.162.880 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.162.889 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.162.909 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.163.894 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.163.896 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.163.896 I llama_new_context_with_model: graph nodes  = 967
0.00.163.896 I llama_new_context_with_model: graph splits = 2
0.00.163.922 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.164.065 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.164.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.241.421 I main: llama threadpool init, n_threads = 4
0.00.241.460 I 
0.00.241.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.241.499 I 
0.00.241.570 I sampler seed: 1234
0.00.241.575 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.241.612 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.241.614 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.241.614 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.085.985 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.02.085.985 I llama_perf_context_print:        load time =     206.38 ms
0.02.085.986 I llama_perf_context_print: prompt eval time =      43.78 ms /     7 tokens (    6.25 ms per token,   159.89 tokens per second)
0.02.085.987 I llama_perf_context_print:        eval time =    1797.60 ms /    63 runs   (   28.53 ms per token,    35.05 tokens per second)
0.02.085.987 I llama_perf_context_print:       total time =    1844.57 ms /    70 tokens
0.02.086.155 I ggml_metal_free: deallocating

real	0m2.393s
user	0m0.143s
sys	0m0.099s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.845 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.886 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.892 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.893 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.894 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.894 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.895 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.895 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.896 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.896 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.897 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.897 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.897 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.933 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.984 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.985 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.985 I llama_model_loader: - type  f32:  194 tensors
0.00.033.986 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.825 I llm_load_vocab: special tokens cache size = 25
0.00.062.787 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.791 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.792 I llm_load_print_meta: arch             = gptneox
0.00.062.792 I llm_load_print_meta: vocab type       = BPE
0.00.062.792 I llm_load_print_meta: n_vocab          = 50304
0.00.062.792 I llm_load_print_meta: n_merges         = 50009
0.00.062.793 I llm_load_print_meta: vocab_only       = 0
0.00.062.793 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.793 I llm_load_print_meta: n_embd           = 2048
0.00.062.793 I llm_load_print_meta: n_layer          = 24
0.00.062.798 I llm_load_print_meta: n_head           = 16
0.00.062.799 I llm_load_print_meta: n_head_kv        = 16
0.00.062.799 I llm_load_print_meta: n_rot            = 32
0.00.062.800 I llm_load_print_meta: n_swa            = 0
0.00.062.800 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.800 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.801 I llm_load_print_meta: n_gqa            = 1
0.00.062.802 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.802 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.803 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.803 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.804 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.804 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.805 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.806 I llm_load_print_meta: n_ff             = 8192
0.00.062.806 I llm_load_print_meta: n_expert         = 0
0.00.062.806 I llm_load_print_meta: n_expert_used    = 0
0.00.062.807 I llm_load_print_meta: causal attn      = 1
0.00.062.807 I llm_load_print_meta: pooling type     = 0
0.00.062.807 I llm_load_print_meta: rope type        = 2
0.00.062.807 I llm_load_print_meta: rope scaling     = linear
0.00.062.809 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.810 I llm_load_print_meta: freq_scale_train = 1
0.00.062.810 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.810 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.810 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.810 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.810 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.811 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.811 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.811 I llm_load_print_meta: model type       = 1.4B
0.00.062.811 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.812 I llm_load_print_meta: model params     = 1.41 B
0.00.062.812 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.812 I llm_load_print_meta: general.name     = 1.4B
0.00.062.813 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.813 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.813 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.814 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.814 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.815 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.815 I llm_load_print_meta: max token length = 1024
0.00.065.254 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.255 I llm_load_tensors: offloading output layer to GPU
0.00.065.255 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.266 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.267 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.229 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.229 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.230 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.230 I llama_new_context_with_model: n_batch       = 2048
0.00.066.230 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.230 I llama_new_context_with_model: flash_attn    = 0
0.00.066.231 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.231 I llama_new_context_with_model: freq_scale    = 1
0.00.066.232 I ggml_metal_init: allocating
0.00.066.235 I ggml_metal_init: found device: Apple M4
0.00.066.237 I ggml_metal_init: picking default device: Apple M4
0.00.066.976 I ggml_metal_init: using embedded metal library
0.00.069.550 I ggml_metal_init: GPU name:   Apple M4
0.00.069.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.553 I ggml_metal_init: simdgroup reduction   = true
0.00.069.553 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.553 I ggml_metal_init: has bfloat            = true
0.00.069.553 I ggml_metal_init: use bfloat            = true
0.00.069.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.030 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.001 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.009 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.031 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.220 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.222 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.222 I llama_new_context_with_model: graph nodes  = 967
0.00.106.223 I llama_new_context_with_model: graph splits = 2
0.00.106.241 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.368 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.336.734 I main: llama threadpool init, n_threads = 4
0.01.336.820 I 
0.01.336.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.336.883 I 
0.01.337.157 I sampler seed: 1234
0.01.337.165 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.337.256 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.337.279 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.337.279 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.439.393 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.02.439.394 I llama_perf_context_print:        load time =    1325.88 ms
0.02.439.394 I llama_perf_context_print: prompt eval time =      49.73 ms /     7 tokens (    7.10 ms per token,   140.75 tokens per second)
0.02.439.395 I llama_perf_context_print:        eval time =    1049.64 ms /    63 runs   (   16.66 ms per token,    60.02 tokens per second)
0.02.439.396 I llama_perf_context_print:       total time =    1102.67 ms /    70 tokens
0.02.439.610 I ggml_metal_free: deallocating

real	0m2.462s
user	0m0.123s
sys	0m0.251s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.016.354 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.214 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.221 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.222 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.222 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.223 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.223 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.229 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.230 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.173 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.215 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.217 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.217 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.218 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.218 I llama_model_loader: - type  f32:  194 tensors
0.00.042.218 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.219 I llama_model_loader: - type q6_K:    1 tensors
0.00.071.681 I llm_load_vocab: special tokens cache size = 25
0.00.082.639 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.643 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.644 I llm_load_print_meta: arch             = gptneox
0.00.082.644 I llm_load_print_meta: vocab type       = BPE
0.00.082.645 I llm_load_print_meta: n_vocab          = 50304
0.00.082.645 I llm_load_print_meta: n_merges         = 50009
0.00.082.645 I llm_load_print_meta: vocab_only       = 0
0.00.082.645 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.646 I llm_load_print_meta: n_embd           = 2048
0.00.082.646 I llm_load_print_meta: n_layer          = 24
0.00.082.654 I llm_load_print_meta: n_head           = 16
0.00.082.655 I llm_load_print_meta: n_head_kv        = 16
0.00.082.655 I llm_load_print_meta: n_rot            = 32
0.00.082.656 I llm_load_print_meta: n_swa            = 0
0.00.082.656 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.656 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.657 I llm_load_print_meta: n_gqa            = 1
0.00.082.659 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.660 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.661 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.661 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.661 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.661 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.662 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.663 I llm_load_print_meta: n_ff             = 8192
0.00.082.663 I llm_load_print_meta: n_expert         = 0
0.00.082.663 I llm_load_print_meta: n_expert_used    = 0
0.00.082.663 I llm_load_print_meta: causal attn      = 1
0.00.082.663 I llm_load_print_meta: pooling type     = 0
0.00.082.664 I llm_load_print_meta: rope type        = 2
0.00.082.664 I llm_load_print_meta: rope scaling     = linear
0.00.082.664 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.665 I llm_load_print_meta: freq_scale_train = 1
0.00.082.665 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.665 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.666 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.666 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.667 I llm_load_print_meta: model type       = 1.4B
0.00.082.667 I llm_load_print_meta: model ftype      = Q4_0
0.00.082.667 I llm_load_print_meta: model params     = 1.41 B
0.00.082.668 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.082.668 I llm_load_print_meta: general.name     = 1.4B
0.00.082.669 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.669 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.669 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.670 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.670 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.082.671 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.673 I llm_load_print_meta: max token length = 1024
0.00.085.690 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.691 I llm_load_tensors: offloading output layer to GPU
0.00.085.692 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.704 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.085.705 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.087.104 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.105 I llama_new_context_with_model: n_ctx         = 2048
0.00.087.105 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.087.106 I llama_new_context_with_model: n_batch       = 2048
0.00.087.106 I llama_new_context_with_model: n_ubatch      = 512
0.00.087.106 I llama_new_context_with_model: flash_attn    = 0
0.00.087.107 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.107 I llama_new_context_with_model: freq_scale    = 1
0.00.087.108 I ggml_metal_init: allocating
0.00.087.113 I ggml_metal_init: found device: Apple M4
0.00.087.115 I ggml_metal_init: picking default device: Apple M4
0.00.088.050 I ggml_metal_init: using embedded metal library
0.00.091.699 I ggml_metal_init: GPU name:   Apple M4
0.00.091.701 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.701 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.702 I ggml_metal_init: simdgroup reduction   = true
0.00.091.702 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.702 I ggml_metal_init: has bfloat            = true
0.00.091.702 I ggml_metal_init: use bfloat            = true
0.00.091.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.704 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.997 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.131.188 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.131.196 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.131.216 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.132.289 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.132.291 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.132.291 I llama_new_context_with_model: graph nodes  = 967
0.00.132.292 I llama_new_context_with_model: graph splits = 2
0.00.132.309 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.132.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.132.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.432 I main: llama threadpool init, n_threads = 4
0.00.717.481 I 
0.00.717.529 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.531 I 
0.00.717.812 I sampler seed: 1234
0.00.717.818 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.889 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.894 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.894 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.403.534 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.403.535 I llama_perf_context_print:        load time =     701.07 ms
0.01.403.536 I llama_perf_context_print: prompt eval time =      47.34 ms /     7 tokens (    6.76 ms per token,   147.86 tokens per second)
0.01.403.536 I llama_perf_context_print:        eval time =     635.28 ms /    63 runs   (   10.08 ms per token,    99.17 tokens per second)
0.01.403.537 I llama_perf_context_print:       total time =     686.11 ms /    70 tokens
0.01.403.689 I ggml_metal_free: deallocating

real	0m1.429s
user	0m0.137s
sys	0m0.175s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.808 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.114 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.126 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.129 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.130 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.130 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.130 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.136 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.065 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.196 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.017 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.018 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.019 I llama_model_loader: - type  f32:  194 tensors
0.00.025.019 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.019 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.377 I llm_load_vocab: special tokens cache size = 25
0.00.051.086 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.089 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.089 I llm_load_print_meta: arch             = gptneox
0.00.051.089 I llm_load_print_meta: vocab type       = BPE
0.00.051.090 I llm_load_print_meta: n_vocab          = 50304
0.00.051.090 I llm_load_print_meta: n_merges         = 50009
0.00.051.090 I llm_load_print_meta: vocab_only       = 0
0.00.051.090 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.090 I llm_load_print_meta: n_embd           = 2048
0.00.051.091 I llm_load_print_meta: n_layer          = 24
0.00.051.094 I llm_load_print_meta: n_head           = 16
0.00.051.095 I llm_load_print_meta: n_head_kv        = 16
0.00.051.095 I llm_load_print_meta: n_rot            = 32
0.00.051.095 I llm_load_print_meta: n_swa            = 0
0.00.051.097 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.097 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.098 I llm_load_print_meta: n_gqa            = 1
0.00.051.099 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.100 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.100 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.101 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.101 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.101 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.101 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.102 I llm_load_print_meta: n_ff             = 8192
0.00.051.102 I llm_load_print_meta: n_expert         = 0
0.00.051.102 I llm_load_print_meta: n_expert_used    = 0
0.00.051.102 I llm_load_print_meta: causal attn      = 1
0.00.051.103 I llm_load_print_meta: pooling type     = 0
0.00.051.103 I llm_load_print_meta: rope type        = 2
0.00.051.103 I llm_load_print_meta: rope scaling     = linear
0.00.051.104 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.104 I llm_load_print_meta: freq_scale_train = 1
0.00.051.104 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.104 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.104 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.105 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.106 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.106 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.106 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.107 I llm_load_print_meta: model type       = 1.4B
0.00.051.107 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.107 I llm_load_print_meta: model params     = 1.41 B
0.00.051.108 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.108 I llm_load_print_meta: general.name     = 1.4B
0.00.051.108 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.109 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.109 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.109 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.109 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.110 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.110 I llm_load_print_meta: max token length = 1024
0.00.052.891 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.892 I llm_load_tensors: offloading output layer to GPU
0.00.052.892 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.898 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.899 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.787 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.788 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.788 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.788 I llama_new_context_with_model: n_batch       = 2048
0.00.053.788 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.788 I llama_new_context_with_model: flash_attn    = 0
0.00.053.789 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.789 I llama_new_context_with_model: freq_scale    = 1
0.00.053.790 I ggml_metal_init: allocating
0.00.053.793 I ggml_metal_init: found device: Apple M4
0.00.053.795 I ggml_metal_init: picking default device: Apple M4
0.00.054.400 I ggml_metal_init: using embedded metal library
0.00.056.740 I ggml_metal_init: GPU name:   Apple M4
0.00.056.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.742 I ggml_metal_init: simdgroup reduction   = true
0.00.056.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.742 I ggml_metal_init: has bfloat            = true
0.00.056.742 I ggml_metal_init: use bfloat            = true
0.00.056.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.562 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.521 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.525 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.545 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.578 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.580 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.580 I llama_new_context_with_model: graph nodes  = 967
0.00.086.580 I llama_new_context_with_model: graph splits = 2
0.00.086.596 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.737 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.736 I main: llama threadpool init, n_threads = 4
0.00.680.772 I 
0.00.680.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.818 I 
0.00.681.041 I sampler seed: 1234
0.00.681.045 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.060 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.061 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.061 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.406.980 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63167.26 tokens per second)
0.01.406.980 I llama_perf_context_print:        load time =     671.92 ms
0.01.406.981 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.72 tokens per second)
0.01.406.982 I llama_perf_context_print:        eval time =     683.38 ms /    63 runs   (   10.85 ms per token,    92.19 tokens per second)
0.01.406.983 I llama_perf_context_print:       total time =     726.25 ms /    70 tokens
0.01.407.173 I ggml_metal_free: deallocating

real	0m1.424s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.011.382 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.219 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.229 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.230 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.231 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.232 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.232 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.232 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.233 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.238 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.239 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.239 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.237 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.103 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.103 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.103 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.104 I llama_model_loader: - type  f32:  194 tensors
0.00.027.104 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.104 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.636 I llm_load_vocab: special tokens cache size = 25
0.00.053.764 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.766 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.767 I llm_load_print_meta: arch             = gptneox
0.00.053.767 I llm_load_print_meta: vocab type       = BPE
0.00.053.767 I llm_load_print_meta: n_vocab          = 50304
0.00.053.768 I llm_load_print_meta: n_merges         = 50009
0.00.053.768 I llm_load_print_meta: vocab_only       = 0
0.00.053.768 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.768 I llm_load_print_meta: n_embd           = 2048
0.00.053.768 I llm_load_print_meta: n_layer          = 24
0.00.053.771 I llm_load_print_meta: n_head           = 16
0.00.053.772 I llm_load_print_meta: n_head_kv        = 16
0.00.053.772 I llm_load_print_meta: n_rot            = 32
0.00.053.772 I llm_load_print_meta: n_swa            = 0
0.00.053.772 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.772 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.773 I llm_load_print_meta: n_gqa            = 1
0.00.053.774 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.775 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.775 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.776 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.776 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.776 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.776 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.777 I llm_load_print_meta: n_ff             = 8192
0.00.053.777 I llm_load_print_meta: n_expert         = 0
0.00.053.777 I llm_load_print_meta: n_expert_used    = 0
0.00.053.778 I llm_load_print_meta: causal attn      = 1
0.00.053.778 I llm_load_print_meta: pooling type     = 0
0.00.053.778 I llm_load_print_meta: rope type        = 2
0.00.053.778 I llm_load_print_meta: rope scaling     = linear
0.00.053.779 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.781 I llm_load_print_meta: freq_scale_train = 1
0.00.053.782 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.782 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.782 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.782 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.782 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.782 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.782 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.783 I llm_load_print_meta: model type       = 1.4B
0.00.053.783 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.783 I llm_load_print_meta: model params     = 1.41 B
0.00.053.784 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.784 I llm_load_print_meta: general.name     = 1.4B
0.00.053.784 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.784 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.785 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.785 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.785 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.785 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.786 I llm_load_print_meta: max token length = 1024
0.00.055.800 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.801 I llm_load_tensors: offloading output layer to GPU
0.00.055.801 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.812 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.813 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.762 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.763 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.763 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.763 I llama_new_context_with_model: n_batch       = 2048
0.00.056.763 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.764 I llama_new_context_with_model: flash_attn    = 0
0.00.056.764 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.764 I llama_new_context_with_model: freq_scale    = 1
0.00.056.765 I ggml_metal_init: allocating
0.00.056.768 I ggml_metal_init: found device: Apple M4
0.00.056.770 I ggml_metal_init: picking default device: Apple M4
0.00.057.385 I ggml_metal_init: using embedded metal library
0.00.059.706 I ggml_metal_init: GPU name:   Apple M4
0.00.059.708 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.709 I ggml_metal_init: simdgroup reduction   = true
0.00.059.709 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.709 I ggml_metal_init: has bfloat            = true
0.00.059.709 I ggml_metal_init: use bfloat            = true
0.00.059.710 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.488 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.298 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.306 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.326 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.440 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.441 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.442 I llama_new_context_with_model: graph nodes  = 967
0.00.090.442 I llama_new_context_with_model: graph splits = 2
0.00.090.458 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.576 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.393 I main: llama threadpool init, n_threads = 4
0.00.795.437 I 
0.00.795.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.468 I 
0.00.795.681 I sampler seed: 1234
0.00.795.687 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.703 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.703 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.580.760 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.580.761 I llama_perf_context_print:        load time =     784.01 ms
0.01.580.761 I llama_perf_context_print: prompt eval time =      43.05 ms /     7 tokens (    6.15 ms per token,   162.60 tokens per second)
0.01.580.762 I llama_perf_context_print:        eval time =     738.99 ms /    63 runs   (   11.73 ms per token,    85.25 tokens per second)
0.01.580.763 I llama_perf_context_print:       total time =     785.37 ms /    70 tokens
0.01.580.974 I ggml_metal_free: deallocating

real	0m1.599s
user	0m0.110s
sys	0m0.169s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.767 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.986 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.992 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.993 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.994 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.994 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.995 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.995 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.995 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.997 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.000 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.001 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.106 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.107 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.107 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.108 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.108 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.108 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.109 I llama_model_loader: - type  f32:  194 tensors
0.00.026.109 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.109 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.362 I llm_load_vocab: special tokens cache size = 25
0.00.053.510 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.513 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.513 I llm_load_print_meta: arch             = gptneox
0.00.053.513 I llm_load_print_meta: vocab type       = BPE
0.00.053.514 I llm_load_print_meta: n_vocab          = 50304
0.00.053.514 I llm_load_print_meta: n_merges         = 50009
0.00.053.514 I llm_load_print_meta: vocab_only       = 0
0.00.053.514 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.515 I llm_load_print_meta: n_embd           = 2048
0.00.053.515 I llm_load_print_meta: n_layer          = 24
0.00.053.517 I llm_load_print_meta: n_head           = 16
0.00.053.518 I llm_load_print_meta: n_head_kv        = 16
0.00.053.518 I llm_load_print_meta: n_rot            = 32
0.00.053.518 I llm_load_print_meta: n_swa            = 0
0.00.053.519 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.519 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.520 I llm_load_print_meta: n_gqa            = 1
0.00.053.521 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.521 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.522 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.524 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.524 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.525 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.525 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.526 I llm_load_print_meta: n_ff             = 8192
0.00.053.526 I llm_load_print_meta: n_expert         = 0
0.00.053.526 I llm_load_print_meta: n_expert_used    = 0
0.00.053.529 I llm_load_print_meta: causal attn      = 1
0.00.053.529 I llm_load_print_meta: pooling type     = 0
0.00.053.529 I llm_load_print_meta: rope type        = 2
0.00.053.529 I llm_load_print_meta: rope scaling     = linear
0.00.053.530 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.530 I llm_load_print_meta: freq_scale_train = 1
0.00.053.530 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.531 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.531 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.531 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.531 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.531 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.531 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.531 I llm_load_print_meta: model type       = 1.4B
0.00.053.532 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.532 I llm_load_print_meta: model params     = 1.41 B
0.00.053.533 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.533 I llm_load_print_meta: general.name     = 1.4B
0.00.053.533 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.533 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.533 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.534 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.534 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.534 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.534 I llm_load_print_meta: max token length = 1024
0.00.055.615 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.616 I llm_load_tensors: offloading output layer to GPU
0.00.055.616 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.627 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.628 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.595 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.596 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.596 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.596 I llama_new_context_with_model: n_batch       = 2048
0.00.056.596 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.596 I llama_new_context_with_model: flash_attn    = 0
0.00.056.597 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.597 I llama_new_context_with_model: freq_scale    = 1
0.00.056.597 I ggml_metal_init: allocating
0.00.056.601 I ggml_metal_init: found device: Apple M4
0.00.056.603 I ggml_metal_init: picking default device: Apple M4
0.00.057.217 I ggml_metal_init: using embedded metal library
0.00.059.630 I ggml_metal_init: GPU name:   Apple M4
0.00.059.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.632 I ggml_metal_init: simdgroup reduction   = true
0.00.059.634 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.634 I ggml_metal_init: has bfloat            = true
0.00.059.634 I ggml_metal_init: use bfloat            = true
0.00.059.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.717 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.937 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.942 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.960 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.958 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.960 I llama_new_context_with_model: graph nodes  = 967
0.00.090.961 I llama_new_context_with_model: graph splits = 2
0.00.090.971 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.099 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.099 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.163 I main: llama threadpool init, n_threads = 4
0.00.798.205 I 
0.00.798.248 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.250 I 
0.00.798.472 I sampler seed: 1234
0.00.798.477 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.519 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.536 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.536 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.637.634 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.637.635 I llama_perf_context_print:        load time =     789.39 ms
0.01.637.636 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.71 tokens per second)
0.01.637.636 I llama_perf_context_print:        eval time =     793.84 ms /    63 runs   (   12.60 ms per token,    79.36 tokens per second)
0.01.637.637 I llama_perf_context_print:       total time =     839.47 ms /    70 tokens
0.01.637.831 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.113s
sys	0m0.166s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.423 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.433 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.433 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.434 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.436 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.305 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.342 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.343 I llama_model_loader: - type  f32:  194 tensors
0.00.024.344 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.344 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.344 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.647 I llm_load_vocab: special tokens cache size = 25
0.00.051.550 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.553 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.554 I llm_load_print_meta: arch             = gptneox
0.00.051.554 I llm_load_print_meta: vocab type       = BPE
0.00.051.554 I llm_load_print_meta: n_vocab          = 50304
0.00.051.554 I llm_load_print_meta: n_merges         = 50009
0.00.051.555 I llm_load_print_meta: vocab_only       = 0
0.00.051.555 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.555 I llm_load_print_meta: n_embd           = 2048
0.00.051.555 I llm_load_print_meta: n_layer          = 24
0.00.051.558 I llm_load_print_meta: n_head           = 16
0.00.051.561 I llm_load_print_meta: n_head_kv        = 16
0.00.051.561 I llm_load_print_meta: n_rot            = 32
0.00.051.562 I llm_load_print_meta: n_swa            = 0
0.00.051.562 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.562 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.563 I llm_load_print_meta: n_gqa            = 1
0.00.051.564 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.564 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.565 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.565 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.565 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.566 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.566 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.567 I llm_load_print_meta: n_ff             = 8192
0.00.051.567 I llm_load_print_meta: n_expert         = 0
0.00.051.567 I llm_load_print_meta: n_expert_used    = 0
0.00.051.567 I llm_load_print_meta: causal attn      = 1
0.00.051.567 I llm_load_print_meta: pooling type     = 0
0.00.051.568 I llm_load_print_meta: rope type        = 2
0.00.051.570 I llm_load_print_meta: rope scaling     = linear
0.00.051.570 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.571 I llm_load_print_meta: freq_scale_train = 1
0.00.051.571 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.571 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.571 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.571 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.571 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.572 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.572 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.572 I llm_load_print_meta: model type       = 1.4B
0.00.051.572 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.573 I llm_load_print_meta: model params     = 1.41 B
0.00.051.573 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.574 I llm_load_print_meta: general.name     = 1.4B
0.00.051.574 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.574 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.575 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.575 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.575 I llm_load_print_meta: max token length = 1024
0.00.053.486 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.487 I llm_load_tensors: offloading output layer to GPU
0.00.053.487 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.498 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.499 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.399 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.400 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.400 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.400 I llama_new_context_with_model: n_batch       = 2048
0.00.054.400 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.400 I llama_new_context_with_model: flash_attn    = 0
0.00.054.401 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.401 I llama_new_context_with_model: freq_scale    = 1
0.00.054.401 I ggml_metal_init: allocating
0.00.054.405 I ggml_metal_init: found device: Apple M4
0.00.054.407 I ggml_metal_init: picking default device: Apple M4
0.00.054.996 I ggml_metal_init: using embedded metal library
0.00.057.354 I ggml_metal_init: GPU name:   Apple M4
0.00.057.355 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.356 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.356 I ggml_metal_init: simdgroup reduction   = true
0.00.057.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.357 I ggml_metal_init: has bfloat            = true
0.00.057.357 I ggml_metal_init: use bfloat            = true
0.00.057.357 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.358 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.340 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.803 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.810 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.827 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.811 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.812 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.812 I llama_new_context_with_model: graph nodes  = 967
0.00.087.812 I llama_new_context_with_model: graph splits = 2
0.00.087.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.514.595 I main: llama threadpool init, n_threads = 4
0.00.514.638 I 
0.00.514.676 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.514.677 I 
0.00.514.909 I sampler seed: 1234
0.00.514.918 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.514.956 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.514.957 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.514.957 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.196.099 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.196.100 I llama_perf_context_print:        load time =     504.76 ms
0.01.196.101 I llama_perf_context_print: prompt eval time =      39.85 ms /     7 tokens (    5.69 ms per token,   175.67 tokens per second)
0.01.196.101 I llama_perf_context_print:        eval time =     638.29 ms /    63 runs   (   10.13 ms per token,    98.70 tokens per second)
0.01.196.102 I llama_perf_context_print:       total time =     681.51 ms /    70 tokens
0.01.196.321 I ggml_metal_free: deallocating

real	0m1.215s
user	0m0.111s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.221 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.492 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.497 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.509 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.475 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.476 I llama_model_loader: - type  f32:  194 tensors
0.00.025.476 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.476 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.477 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.477 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.184 I llm_load_vocab: special tokens cache size = 25
0.00.052.169 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.172 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.172 I llm_load_print_meta: arch             = gptneox
0.00.052.172 I llm_load_print_meta: vocab type       = BPE
0.00.052.173 I llm_load_print_meta: n_vocab          = 50304
0.00.052.173 I llm_load_print_meta: n_merges         = 50009
0.00.052.173 I llm_load_print_meta: vocab_only       = 0
0.00.052.173 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.174 I llm_load_print_meta: n_embd           = 2048
0.00.052.174 I llm_load_print_meta: n_layer          = 24
0.00.052.176 I llm_load_print_meta: n_head           = 16
0.00.052.177 I llm_load_print_meta: n_head_kv        = 16
0.00.052.177 I llm_load_print_meta: n_rot            = 32
0.00.052.178 I llm_load_print_meta: n_swa            = 0
0.00.052.178 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.178 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.179 I llm_load_print_meta: n_gqa            = 1
0.00.052.180 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.180 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.182 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.182 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.183 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.183 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.183 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.184 I llm_load_print_meta: n_ff             = 8192
0.00.052.184 I llm_load_print_meta: n_expert         = 0
0.00.052.184 I llm_load_print_meta: n_expert_used    = 0
0.00.052.184 I llm_load_print_meta: causal attn      = 1
0.00.052.184 I llm_load_print_meta: pooling type     = 0
0.00.052.184 I llm_load_print_meta: rope type        = 2
0.00.052.185 I llm_load_print_meta: rope scaling     = linear
0.00.052.185 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.185 I llm_load_print_meta: freq_scale_train = 1
0.00.052.186 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.186 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.186 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.186 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.186 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.186 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.187 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.187 I llm_load_print_meta: model type       = 1.4B
0.00.052.187 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.188 I llm_load_print_meta: model params     = 1.41 B
0.00.052.188 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.188 I llm_load_print_meta: general.name     = 1.4B
0.00.052.189 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.189 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.189 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.190 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.190 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.190 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.190 I llm_load_print_meta: max token length = 1024
0.00.054.198 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.198 I llm_load_tensors: offloading output layer to GPU
0.00.054.199 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.209 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.210 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.162 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.163 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.163 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.163 I llama_new_context_with_model: n_batch       = 2048
0.00.055.164 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.164 I llama_new_context_with_model: flash_attn    = 0
0.00.055.164 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.165 I llama_new_context_with_model: freq_scale    = 1
0.00.055.165 I ggml_metal_init: allocating
0.00.055.171 I ggml_metal_init: found device: Apple M4
0.00.055.173 I ggml_metal_init: picking default device: Apple M4
0.00.055.758 I ggml_metal_init: using embedded metal library
0.00.058.122 I ggml_metal_init: GPU name:   Apple M4
0.00.058.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.124 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.125 I ggml_metal_init: simdgroup reduction   = true
0.00.058.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.125 I ggml_metal_init: has bfloat            = true
0.00.058.125 I ggml_metal_init: use bfloat            = true
0.00.058.125 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.126 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.908 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.528 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.535 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.554 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.578 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.579 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.579 I llama_new_context_with_model: graph nodes  = 967
0.00.088.579 I llama_new_context_with_model: graph splits = 2
0.00.088.594 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.739 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.739 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.823 I main: llama threadpool init, n_threads = 4
0.00.570.865 I 
0.00.570.911 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.570.913 I 
0.00.571.143 I sampler seed: 1234
0.00.571.149 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.571.164 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.571.165 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.571.165 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.321.830 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56483.69 tokens per second)
0.01.321.830 I llama_perf_context_print:        load time =     560.60 ms
0.01.321.831 I llama_perf_context_print: prompt eval time =      44.31 ms /     7 tokens (    6.33 ms per token,   157.99 tokens per second)
0.01.321.832 I llama_perf_context_print:        eval time =     703.35 ms /    63 runs   (   11.16 ms per token,    89.57 tokens per second)
0.01.321.832 I llama_perf_context_print:       total time =     751.01 ms /    70 tokens
0.01.321.999 I ggml_metal_free: deallocating

real	0m1.338s
user	0m0.110s
sys	0m0.126s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.728 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.124 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.131 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.131 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.132 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.132 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.133 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.133 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.133 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.134 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.134 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.135 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.137 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.954 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.955 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.955 I llama_model_loader: - type  f32:  194 tensors
0.00.024.956 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.956 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.956 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.597 I llm_load_vocab: special tokens cache size = 25
0.00.051.339 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.342 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.342 I llm_load_print_meta: arch             = gptneox
0.00.051.342 I llm_load_print_meta: vocab type       = BPE
0.00.051.343 I llm_load_print_meta: n_vocab          = 50304
0.00.051.343 I llm_load_print_meta: n_merges         = 50009
0.00.051.343 I llm_load_print_meta: vocab_only       = 0
0.00.051.343 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.344 I llm_load_print_meta: n_embd           = 2048
0.00.051.344 I llm_load_print_meta: n_layer          = 24
0.00.051.347 I llm_load_print_meta: n_head           = 16
0.00.051.347 I llm_load_print_meta: n_head_kv        = 16
0.00.051.348 I llm_load_print_meta: n_rot            = 32
0.00.051.348 I llm_load_print_meta: n_swa            = 0
0.00.051.348 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.348 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.349 I llm_load_print_meta: n_gqa            = 1
0.00.051.350 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.350 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.351 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.351 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.352 I llm_load_print_meta: n_ff             = 8192
0.00.051.352 I llm_load_print_meta: n_expert         = 0
0.00.051.352 I llm_load_print_meta: n_expert_used    = 0
0.00.051.353 I llm_load_print_meta: causal attn      = 1
0.00.051.353 I llm_load_print_meta: pooling type     = 0
0.00.051.353 I llm_load_print_meta: rope type        = 2
0.00.051.353 I llm_load_print_meta: rope scaling     = linear
0.00.051.354 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.354 I llm_load_print_meta: freq_scale_train = 1
0.00.051.354 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.354 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.354 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.355 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.355 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.357 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.357 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.358 I llm_load_print_meta: model type       = 1.4B
0.00.051.358 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.358 I llm_load_print_meta: model params     = 1.41 B
0.00.051.359 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.359 I llm_load_print_meta: general.name     = 1.4B
0.00.051.360 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.360 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.361 I llm_load_print_meta: max token length = 1024
0.00.053.332 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.333 I llm_load_tensors: offloading output layer to GPU
0.00.053.333 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.343 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.344 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.276 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.277 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.277 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.277 I llama_new_context_with_model: n_batch       = 2048
0.00.054.277 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.278 I llama_new_context_with_model: flash_attn    = 0
0.00.054.278 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.278 I llama_new_context_with_model: freq_scale    = 1
0.00.054.279 I ggml_metal_init: allocating
0.00.054.284 I ggml_metal_init: found device: Apple M4
0.00.054.286 I ggml_metal_init: picking default device: Apple M4
0.00.054.882 I ggml_metal_init: using embedded metal library
0.00.057.195 I ggml_metal_init: GPU name:   Apple M4
0.00.057.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.197 I ggml_metal_init: simdgroup reduction   = true
0.00.057.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.197 I ggml_metal_init: has bfloat            = true
0.00.057.198 I ggml_metal_init: use bfloat            = true
0.00.057.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.838 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.455 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.462 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.479 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.527 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.528 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.529 I llama_new_context_with_model: graph nodes  = 967
0.00.087.529 I llama_new_context_with_model: graph splits = 2
0.00.087.544 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.296 I main: llama threadpool init, n_threads = 4
0.00.602.342 I 
0.00.602.368 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.369 I 
0.00.602.520 I sampler seed: 1234
0.00.602.525 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.545 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.545 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.545 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.361.647 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.01.361.647 I llama_perf_context_print:        load time =     591.56 ms
0.01.361.648 I llama_perf_context_print: prompt eval time =      47.01 ms /     7 tokens (    6.72 ms per token,   148.92 tokens per second)
0.01.361.649 I llama_perf_context_print:        eval time =     709.01 ms /    63 runs   (   11.25 ms per token,    88.86 tokens per second)
0.01.361.649 I llama_perf_context_print:       total time =     759.35 ms /    70 tokens
0.01.361.838 I ggml_metal_free: deallocating

real	0m1.381s
user	0m0.110s
sys	0m0.128s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.677 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.118 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.123 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.125 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.126 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.127 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.127 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.127 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.128 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.128 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.131 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.928 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.929 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.930 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.930 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.022.931 I llama_model_loader: - type  f32:  194 tensors
0.00.022.931 I llama_model_loader: - type q5_K:   61 tensors
0.00.022.931 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.466 I llm_load_vocab: special tokens cache size = 25
0.00.049.320 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.322 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.323 I llm_load_print_meta: arch             = gptneox
0.00.049.323 I llm_load_print_meta: vocab type       = BPE
0.00.049.323 I llm_load_print_meta: n_vocab          = 50304
0.00.049.324 I llm_load_print_meta: n_merges         = 50009
0.00.049.324 I llm_load_print_meta: vocab_only       = 0
0.00.049.324 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.324 I llm_load_print_meta: n_embd           = 2048
0.00.049.325 I llm_load_print_meta: n_layer          = 24
0.00.049.327 I llm_load_print_meta: n_head           = 16
0.00.049.328 I llm_load_print_meta: n_head_kv        = 16
0.00.049.328 I llm_load_print_meta: n_rot            = 32
0.00.049.328 I llm_load_print_meta: n_swa            = 0
0.00.049.328 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.328 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.329 I llm_load_print_meta: n_gqa            = 1
0.00.049.330 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.333 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.333 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.334 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.334 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.334 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.334 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.335 I llm_load_print_meta: n_ff             = 8192
0.00.049.335 I llm_load_print_meta: n_expert         = 0
0.00.049.337 I llm_load_print_meta: n_expert_used    = 0
0.00.049.337 I llm_load_print_meta: causal attn      = 1
0.00.049.337 I llm_load_print_meta: pooling type     = 0
0.00.049.337 I llm_load_print_meta: rope type        = 2
0.00.049.338 I llm_load_print_meta: rope scaling     = linear
0.00.049.338 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.338 I llm_load_print_meta: freq_scale_train = 1
0.00.049.338 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.339 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.339 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.339 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.339 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.339 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.339 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.340 I llm_load_print_meta: model type       = 1.4B
0.00.049.340 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.340 I llm_load_print_meta: model params     = 1.41 B
0.00.049.341 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.341 I llm_load_print_meta: general.name     = 1.4B
0.00.049.345 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.345 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.345 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.351 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.353 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.353 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.354 I llm_load_print_meta: max token length = 1024
0.00.051.002 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.002 I llm_load_tensors: offloading output layer to GPU
0.00.051.003 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.012 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.014 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.838 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.839 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.839 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.839 I llama_new_context_with_model: n_batch       = 2048
0.00.051.840 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.840 I llama_new_context_with_model: flash_attn    = 0
0.00.051.840 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.841 I llama_new_context_with_model: freq_scale    = 1
0.00.051.841 I ggml_metal_init: allocating
0.00.051.846 I ggml_metal_init: found device: Apple M4
0.00.051.848 I ggml_metal_init: picking default device: Apple M4
0.00.052.419 I ggml_metal_init: using embedded metal library
0.00.054.727 I ggml_metal_init: GPU name:   Apple M4
0.00.054.728 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.729 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.729 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.729 I ggml_metal_init: simdgroup reduction   = true
0.00.054.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.730 I ggml_metal_init: has bfloat            = true
0.00.054.730 I ggml_metal_init: use bfloat            = true
0.00.054.730 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.731 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.588 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.310 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.318 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.339 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.315 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.317 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.317 I llama_new_context_with_model: graph nodes  = 967
0.00.085.317 I llama_new_context_with_model: graph splits = 2
0.00.085.332 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.719 I main: llama threadpool init, n_threads = 4
0.00.697.769 I 
0.00.697.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.806 I 
0.00.698.032 I sampler seed: 1234
0.00.698.036 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.698.086 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.698.089 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.698.089 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.549.257 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49067.04 tokens per second)
0.01.549.258 I llama_perf_context_print:        load time =     689.03 ms
0.01.549.259 I llama_perf_context_print: prompt eval time =      55.50 ms /     7 tokens (    7.93 ms per token,   126.14 tokens per second)
0.01.549.260 I llama_perf_context_print:        eval time =     793.06 ms /    63 runs   (   12.59 ms per token,    79.44 tokens per second)
0.01.549.261 I llama_perf_context_print:       total time =     851.54 ms /    70 tokens
0.01.549.491 I ggml_metal_free: deallocating

real	0m1.568s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.305 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.892 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.899 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.899 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.900 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.900 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.900 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.901 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.902 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.902 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.902 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.903 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.903 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.906 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.906 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.865 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.742 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.743 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.744 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.744 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.744 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.745 I llama_model_loader: - type  f32:  194 tensors
0.00.023.745 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.194 I llm_load_vocab: special tokens cache size = 25
0.00.050.968 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.970 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.971 I llm_load_print_meta: arch             = gptneox
0.00.050.971 I llm_load_print_meta: vocab type       = BPE
0.00.050.971 I llm_load_print_meta: n_vocab          = 50304
0.00.050.972 I llm_load_print_meta: n_merges         = 50009
0.00.050.972 I llm_load_print_meta: vocab_only       = 0
0.00.050.972 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.972 I llm_load_print_meta: n_embd           = 2048
0.00.050.972 I llm_load_print_meta: n_layer          = 24
0.00.050.976 I llm_load_print_meta: n_head           = 16
0.00.050.977 I llm_load_print_meta: n_head_kv        = 16
0.00.050.977 I llm_load_print_meta: n_rot            = 32
0.00.050.977 I llm_load_print_meta: n_swa            = 0
0.00.050.977 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.978 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.978 I llm_load_print_meta: n_gqa            = 1
0.00.050.979 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.980 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.981 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.981 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.981 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.981 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.982 I llm_load_print_meta: n_ff             = 8192
0.00.050.982 I llm_load_print_meta: n_expert         = 0
0.00.050.982 I llm_load_print_meta: n_expert_used    = 0
0.00.050.982 I llm_load_print_meta: causal attn      = 1
0.00.050.982 I llm_load_print_meta: pooling type     = 0
0.00.050.985 I llm_load_print_meta: rope type        = 2
0.00.050.985 I llm_load_print_meta: rope scaling     = linear
0.00.050.985 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.986 I llm_load_print_meta: freq_scale_train = 1
0.00.050.986 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.986 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.986 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.986 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.986 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.987 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.987 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.987 I llm_load_print_meta: model type       = 1.4B
0.00.050.987 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.988 I llm_load_print_meta: model params     = 1.41 B
0.00.050.988 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.988 I llm_load_print_meta: general.name     = 1.4B
0.00.050.989 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.990 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.990 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.991 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.991 I llm_load_print_meta: max token length = 1024
0.00.052.970 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.970 I llm_load_tensors: offloading output layer to GPU
0.00.052.971 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.981 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.982 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.864 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.865 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.865 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.865 I llama_new_context_with_model: n_batch       = 2048
0.00.053.865 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.866 I llama_new_context_with_model: flash_attn    = 0
0.00.053.866 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.866 I llama_new_context_with_model: freq_scale    = 1
0.00.053.867 I ggml_metal_init: allocating
0.00.053.870 I ggml_metal_init: found device: Apple M4
0.00.053.873 I ggml_metal_init: picking default device: Apple M4
0.00.054.449 I ggml_metal_init: using embedded metal library
0.00.056.775 I ggml_metal_init: GPU name:   Apple M4
0.00.056.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.778 I ggml_metal_init: simdgroup reduction   = true
0.00.056.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.778 I ggml_metal_init: has bfloat            = true
0.00.056.778 I ggml_metal_init: use bfloat            = true
0.00.056.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.779 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.488 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.173 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.179 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.200 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.139 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.141 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.142 I llama_new_context_with_model: graph nodes  = 967
0.00.087.142 I llama_new_context_with_model: graph splits = 2
0.00.087.157 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.139 I main: llama threadpool init, n_threads = 4
0.00.754.178 I 
0.00.754.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.215 I 
0.00.754.462 I sampler seed: 1234
0.00.754.467 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.511 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.516 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.516 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.632.684 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.632.685 I llama_perf_context_print:        load time =     744.83 ms
0.01.632.685 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.46 tokens per second)
0.01.632.686 I llama_perf_context_print:        eval time =     820.76 ms /    63 runs   (   13.03 ms per token,    76.76 tokens per second)
0.01.632.686 I llama_perf_context_print:       total time =     878.55 ms /    70 tokens
0.01.632.879 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.111s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.704 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.687 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.045 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.072 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.131 I llama_model_loader: - type  f32:  194 tensors
0.00.057.132 I llama_model_loader: - type  f16:   98 tensors
0.00.085.523 I llm_load_vocab: special tokens cache size = 25
0.00.091.876 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.879 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.880 I llm_load_print_meta: arch             = gptneox
0.00.091.880 I llm_load_print_meta: vocab type       = BPE
0.00.091.880 I llm_load_print_meta: n_vocab          = 50304
0.00.091.880 I llm_load_print_meta: n_merges         = 50009
0.00.091.881 I llm_load_print_meta: vocab_only       = 0
0.00.091.881 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.881 I llm_load_print_meta: n_embd           = 2048
0.00.091.881 I llm_load_print_meta: n_layer          = 24
0.00.091.883 I llm_load_print_meta: n_head           = 16
0.00.091.885 I llm_load_print_meta: n_head_kv        = 16
0.00.091.886 I llm_load_print_meta: n_rot            = 32
0.00.091.886 I llm_load_print_meta: n_swa            = 0
0.00.091.886 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.886 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.887 I llm_load_print_meta: n_gqa            = 1
0.00.091.888 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.888 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.889 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.889 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.889 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.889 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.889 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.890 I llm_load_print_meta: n_ff             = 8192
0.00.091.890 I llm_load_print_meta: n_expert         = 0
0.00.091.890 I llm_load_print_meta: n_expert_used    = 0
0.00.091.891 I llm_load_print_meta: causal attn      = 1
0.00.091.891 I llm_load_print_meta: pooling type     = 0
0.00.091.891 I llm_load_print_meta: rope type        = 2
0.00.091.891 I llm_load_print_meta: rope scaling     = linear
0.00.091.891 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.892 I llm_load_print_meta: freq_scale_train = 1
0.00.091.892 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.892 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.892 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.892 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.892 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.892 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.893 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.893 I llm_load_print_meta: model type       = 1.4B
0.00.091.893 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.894 I llm_load_print_meta: model params     = 1.41 B
0.00.091.894 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.894 I llm_load_print_meta: general.name     = 1.4B
0.00.091.894 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.895 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.895 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.895 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.895 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.895 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.896 I llm_load_print_meta: max token length = 1024
0.00.094.493 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.494 I llm_load_tensors: offloading output layer to GPU
0.00.094.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.505 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.506 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.394 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.395 I llama_new_context_with_model: n_ctx         = 128
0.00.095.395 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.395 I llama_new_context_with_model: n_batch       = 128
0.00.095.396 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.396 I llama_new_context_with_model: flash_attn    = 0
0.00.095.396 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.396 I llama_new_context_with_model: freq_scale    = 1
0.00.095.397 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.397 I ggml_metal_init: allocating
0.00.095.400 I ggml_metal_init: found device: Apple M4
0.00.095.403 I ggml_metal_init: picking default device: Apple M4
0.00.095.998 I ggml_metal_init: using embedded metal library
0.00.098.513 I ggml_metal_init: GPU name:   Apple M4
0.00.098.515 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.516 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.516 I ggml_metal_init: simdgroup reduction   = true
0.00.098.516 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.516 I ggml_metal_init: has bfloat            = true
0.00.098.517 I ggml_metal_init: use bfloat            = true
0.00.098.517 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.518 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.108 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.401 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.404 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.417 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.306 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.307 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.308 I llama_new_context_with_model: graph nodes  = 967
0.00.110.308 I llama_new_context_with_model: graph splits = 2
0.00.110.320 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.557 I 
0.00.855.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.713 I perplexity: tokenizing the input ..
0.00.868.466 I perplexity: tokenization took 12.749 ms
0.00.868.472 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.989.413 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.991.303 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.991.333 I llama_perf_context_print:        load time =     828.86 ms
0.00.991.334 I llama_perf_context_print: prompt eval time =     120.64 ms /   128 tokens (    0.94 ms per token,  1061.04 tokens per second)
0.00.991.335 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.991.336 I llama_perf_context_print:       total time =     135.78 ms /   129 tokens
0.00.992.120 I ggml_metal_free: deallocating

real	0m1.183s
user	0m0.123s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.134 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.744 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.508 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.513 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.515 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.515 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.516 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.516 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.516 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.517 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.518 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.518 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.520 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.521 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.522 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.523 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.583 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.933 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.909 I llama_model_loader: - type  f32:  194 tensors
0.00.030.910 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.766 I llm_load_vocab: special tokens cache size = 25
0.00.062.124 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.127 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.128 I llm_load_print_meta: arch             = gptneox
0.00.062.128 I llm_load_print_meta: vocab type       = BPE
0.00.062.128 I llm_load_print_meta: n_vocab          = 50304
0.00.062.128 I llm_load_print_meta: n_merges         = 50009
0.00.062.128 I llm_load_print_meta: vocab_only       = 0
0.00.062.129 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.129 I llm_load_print_meta: n_embd           = 2048
0.00.062.129 I llm_load_print_meta: n_layer          = 24
0.00.062.133 I llm_load_print_meta: n_head           = 16
0.00.062.133 I llm_load_print_meta: n_head_kv        = 16
0.00.062.134 I llm_load_print_meta: n_rot            = 32
0.00.062.134 I llm_load_print_meta: n_swa            = 0
0.00.062.134 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.134 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.135 I llm_load_print_meta: n_gqa            = 1
0.00.062.137 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.147 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.150 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.151 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.151 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.151 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.151 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.159 I llm_load_print_meta: n_ff             = 8192
0.00.062.159 I llm_load_print_meta: n_expert         = 0
0.00.062.159 I llm_load_print_meta: n_expert_used    = 0
0.00.062.159 I llm_load_print_meta: causal attn      = 1
0.00.062.159 I llm_load_print_meta: pooling type     = 0
0.00.062.160 I llm_load_print_meta: rope type        = 2
0.00.062.160 I llm_load_print_meta: rope scaling     = linear
0.00.062.160 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.160 I llm_load_print_meta: freq_scale_train = 1
0.00.062.161 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.161 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.162 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.162 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.162 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.162 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.162 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.162 I llm_load_print_meta: model type       = 1.4B
0.00.062.163 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.163 I llm_load_print_meta: model params     = 1.41 B
0.00.062.164 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.164 I llm_load_print_meta: general.name     = 1.4B
0.00.062.165 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.166 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.166 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.166 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.166 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.166 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.167 I llm_load_print_meta: max token length = 1024
0.00.064.266 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.267 I llm_load_tensors: offloading output layer to GPU
0.00.064.267 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.278 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.279 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.174 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.175 I llama_new_context_with_model: n_ctx         = 128
0.00.065.175 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.175 I llama_new_context_with_model: n_batch       = 128
0.00.065.175 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.175 I llama_new_context_with_model: flash_attn    = 0
0.00.065.176 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.176 I llama_new_context_with_model: freq_scale    = 1
0.00.065.176 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.177 I ggml_metal_init: allocating
0.00.065.182 I ggml_metal_init: found device: Apple M4
0.00.065.188 I ggml_metal_init: picking default device: Apple M4
0.00.065.772 I ggml_metal_init: using embedded metal library
0.00.068.109 I ggml_metal_init: GPU name:   Apple M4
0.00.068.110 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.111 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.111 I ggml_metal_init: simdgroup reduction   = true
0.00.068.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.112 I ggml_metal_init: has bfloat            = true
0.00.068.112 I ggml_metal_init: use bfloat            = true
0.00.068.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.113 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.027 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.078.463 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.467 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.481 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.464 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.466 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.466 I llama_new_context_with_model: graph nodes  = 967
0.00.079.466 I llama_new_context_with_model: graph splits = 2
0.00.079.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.741 I 
0.00.847.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.810 I perplexity: tokenizing the input ..
0.00.855.968 I perplexity: tokenization took 8.157 ms
0.00.855.976 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.980.364 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.981.534 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.981.550 I llama_perf_context_print:        load time =     835.99 ms
0.00.981.551 I llama_perf_context_print: prompt eval time =     124.16 ms /   128 tokens (    0.97 ms per token,  1030.91 tokens per second)
0.00.981.551 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.981.552 I llama_perf_context_print:       total time =     133.81 ms /   129 tokens
0.00.981.990 I ggml_metal_free: deallocating

real	0m0.999s
user	0m0.090s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.389 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.250 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.251 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.253 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.255 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.994 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.995 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.996 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.996 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.997 I llama_model_loader: - type  f32:  194 tensors
0.00.023.997 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.997 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.226 I llm_load_vocab: special tokens cache size = 25
0.00.051.047 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.049 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.049 I llm_load_print_meta: arch             = gptneox
0.00.051.050 I llm_load_print_meta: vocab type       = BPE
0.00.051.050 I llm_load_print_meta: n_vocab          = 50304
0.00.051.050 I llm_load_print_meta: n_merges         = 50009
0.00.051.050 I llm_load_print_meta: vocab_only       = 0
0.00.051.051 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.051 I llm_load_print_meta: n_embd           = 2048
0.00.051.051 I llm_load_print_meta: n_layer          = 24
0.00.051.054 I llm_load_print_meta: n_head           = 16
0.00.051.055 I llm_load_print_meta: n_head_kv        = 16
0.00.051.055 I llm_load_print_meta: n_rot            = 32
0.00.051.055 I llm_load_print_meta: n_swa            = 0
0.00.051.055 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.058 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.059 I llm_load_print_meta: n_gqa            = 1
0.00.051.060 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.061 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.061 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.063 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.063 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.063 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.063 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.064 I llm_load_print_meta: n_ff             = 8192
0.00.051.064 I llm_load_print_meta: n_expert         = 0
0.00.051.064 I llm_load_print_meta: n_expert_used    = 0
0.00.051.064 I llm_load_print_meta: causal attn      = 1
0.00.051.064 I llm_load_print_meta: pooling type     = 0
0.00.051.064 I llm_load_print_meta: rope type        = 2
0.00.051.065 I llm_load_print_meta: rope scaling     = linear
0.00.051.065 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.065 I llm_load_print_meta: freq_scale_train = 1
0.00.051.066 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.066 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.066 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.066 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.066 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.066 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.066 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.067 I llm_load_print_meta: model type       = 1.4B
0.00.051.067 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.071 I llm_load_print_meta: model params     = 1.41 B
0.00.051.072 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.072 I llm_load_print_meta: general.name     = 1.4B
0.00.051.072 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.072 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.072 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.073 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.073 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.073 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.073 I llm_load_print_meta: max token length = 1024
0.00.053.070 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.071 I llm_load_tensors: offloading output layer to GPU
0.00.053.071 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.081 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.083 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.026 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.027 I llama_new_context_with_model: n_ctx         = 128
0.00.054.027 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.027 I llama_new_context_with_model: n_batch       = 128
0.00.054.027 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.027 I llama_new_context_with_model: flash_attn    = 0
0.00.054.028 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.028 I llama_new_context_with_model: freq_scale    = 1
0.00.054.029 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.029 I ggml_metal_init: allocating
0.00.054.035 I ggml_metal_init: found device: Apple M4
0.00.054.037 I ggml_metal_init: picking default device: Apple M4
0.00.054.586 I ggml_metal_init: using embedded metal library
0.00.056.899 I ggml_metal_init: GPU name:   Apple M4
0.00.056.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.901 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.901 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.901 I ggml_metal_init: simdgroup reduction   = true
0.00.056.902 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.902 I ggml_metal_init: has bfloat            = true
0.00.056.902 I ggml_metal_init: use bfloat            = true
0.00.056.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.687 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.977 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.982 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.998 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.865 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.866 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.866 I llama_new_context_with_model: graph nodes  = 967
0.00.068.867 I llama_new_context_with_model: graph splits = 2
0.00.068.879 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.153 I 
0.00.587.197 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.209 I perplexity: tokenizing the input ..
0.00.594.929 I perplexity: tokenization took 7.718 ms
0.00.594.933 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.717.368 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.718.537 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.718.553 I llama_perf_context_print:        load time =     577.76 ms
0.00.718.554 I llama_perf_context_print: prompt eval time =     122.21 ms /   128 tokens (    0.95 ms per token,  1047.39 tokens per second)
0.00.718.555 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.555 I llama_perf_context_print:       total time =     131.40 ms /   129 tokens
0.00.719.042 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.079s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.913 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.967 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.974 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.975 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.976 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.977 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.981 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.022 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.963 I llama_model_loader: - type  f32:  194 tensors
0.00.023.964 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.964 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.216 I llm_load_vocab: special tokens cache size = 25
0.00.051.039 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.042 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.042 I llm_load_print_meta: arch             = gptneox
0.00.051.042 I llm_load_print_meta: vocab type       = BPE
0.00.051.043 I llm_load_print_meta: n_vocab          = 50304
0.00.051.043 I llm_load_print_meta: n_merges         = 50009
0.00.051.043 I llm_load_print_meta: vocab_only       = 0
0.00.051.043 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.043 I llm_load_print_meta: n_embd           = 2048
0.00.051.043 I llm_load_print_meta: n_layer          = 24
0.00.051.046 I llm_load_print_meta: n_head           = 16
0.00.051.047 I llm_load_print_meta: n_head_kv        = 16
0.00.051.047 I llm_load_print_meta: n_rot            = 32
0.00.051.047 I llm_load_print_meta: n_swa            = 0
0.00.051.047 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.048 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.048 I llm_load_print_meta: n_gqa            = 1
0.00.051.049 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.050 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.050 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.051 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.051 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.051 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.051 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.052 I llm_load_print_meta: n_ff             = 8192
0.00.051.052 I llm_load_print_meta: n_expert         = 0
0.00.051.052 I llm_load_print_meta: n_expert_used    = 0
0.00.051.053 I llm_load_print_meta: causal attn      = 1
0.00.051.053 I llm_load_print_meta: pooling type     = 0
0.00.051.053 I llm_load_print_meta: rope type        = 2
0.00.051.053 I llm_load_print_meta: rope scaling     = linear
0.00.051.054 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.054 I llm_load_print_meta: freq_scale_train = 1
0.00.051.054 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.054 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.055 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.055 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.055 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.058 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.058 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.058 I llm_load_print_meta: model type       = 1.4B
0.00.051.058 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.059 I llm_load_print_meta: model params     = 1.41 B
0.00.051.059 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.059 I llm_load_print_meta: general.name     = 1.4B
0.00.051.060 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.064 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.064 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.065 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.065 I llm_load_print_meta: max token length = 1024
0.00.053.134 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.134 I llm_load_tensors: offloading output layer to GPU
0.00.053.135 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.145 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.146 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.038 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.039 I llama_new_context_with_model: n_ctx         = 128
0.00.054.039 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.039 I llama_new_context_with_model: n_batch       = 128
0.00.054.039 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.039 I llama_new_context_with_model: flash_attn    = 0
0.00.054.040 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.040 I llama_new_context_with_model: freq_scale    = 1
0.00.054.040 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.041 I ggml_metal_init: allocating
0.00.054.047 I ggml_metal_init: found device: Apple M4
0.00.054.051 I ggml_metal_init: picking default device: Apple M4
0.00.054.635 I ggml_metal_init: using embedded metal library
0.00.056.991 I ggml_metal_init: GPU name:   Apple M4
0.00.056.992 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.993 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.993 I ggml_metal_init: simdgroup reduction   = true
0.00.056.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.993 I ggml_metal_init: has bfloat            = true
0.00.056.994 I ggml_metal_init: use bfloat            = true
0.00.056.994 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.995 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.448 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.659 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.661 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.674 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.512 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.513 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.513 I llama_new_context_with_model: graph nodes  = 967
0.00.068.514 I llama_new_context_with_model: graph splits = 2
0.00.068.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.635 I 
0.00.620.677 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.688 I perplexity: tokenizing the input ..
0.00.628.782 I perplexity: tokenization took 8.092 ms
0.00.628.786 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.877 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.753.125 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.753.139 I llama_perf_context_print:        load time =     611.72 ms
0.00.753.140 I llama_perf_context_print: prompt eval time =     122.87 ms /   128 tokens (    0.96 ms per token,  1041.79 tokens per second)
0.00.753.140 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.753.141 I llama_perf_context_print:       total time =     132.51 ms /   129 tokens
0.00.753.527 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.078s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.056 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.818 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.820 I llama_model_loader: - type  f32:  194 tensors
0.00.024.820 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.821 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.955 I llm_load_vocab: special tokens cache size = 25
0.00.051.853 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.856 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.856 I llm_load_print_meta: arch             = gptneox
0.00.051.857 I llm_load_print_meta: vocab type       = BPE
0.00.051.857 I llm_load_print_meta: n_vocab          = 50304
0.00.051.857 I llm_load_print_meta: n_merges         = 50009
0.00.051.857 I llm_load_print_meta: vocab_only       = 0
0.00.051.857 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.857 I llm_load_print_meta: n_embd           = 2048
0.00.051.858 I llm_load_print_meta: n_layer          = 24
0.00.051.860 I llm_load_print_meta: n_head           = 16
0.00.051.861 I llm_load_print_meta: n_head_kv        = 16
0.00.051.861 I llm_load_print_meta: n_rot            = 32
0.00.051.862 I llm_load_print_meta: n_swa            = 0
0.00.051.862 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.862 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.865 I llm_load_print_meta: n_gqa            = 1
0.00.051.866 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.867 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.867 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.867 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.868 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.868 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.868 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.869 I llm_load_print_meta: n_ff             = 8192
0.00.051.869 I llm_load_print_meta: n_expert         = 0
0.00.051.871 I llm_load_print_meta: n_expert_used    = 0
0.00.051.871 I llm_load_print_meta: causal attn      = 1
0.00.051.871 I llm_load_print_meta: pooling type     = 0
0.00.051.871 I llm_load_print_meta: rope type        = 2
0.00.051.871 I llm_load_print_meta: rope scaling     = linear
0.00.051.873 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.874 I llm_load_print_meta: freq_scale_train = 1
0.00.051.878 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.878 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.879 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.879 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.879 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.879 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.879 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.879 I llm_load_print_meta: model type       = 1.4B
0.00.051.879 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.880 I llm_load_print_meta: model params     = 1.41 B
0.00.051.880 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.880 I llm_load_print_meta: general.name     = 1.4B
0.00.051.881 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.881 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.881 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.882 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.882 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.882 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.882 I llm_load_print_meta: max token length = 1024
0.00.053.985 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.985 I llm_load_tensors: offloading output layer to GPU
0.00.053.986 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.996 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.997 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.934 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.934 I llama_new_context_with_model: n_ctx         = 128
0.00.054.935 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.935 I llama_new_context_with_model: n_batch       = 128
0.00.054.935 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.935 I llama_new_context_with_model: flash_attn    = 0
0.00.054.936 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.936 I llama_new_context_with_model: freq_scale    = 1
0.00.054.936 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.937 I ggml_metal_init: allocating
0.00.054.940 I ggml_metal_init: found device: Apple M4
0.00.054.942 I ggml_metal_init: picking default device: Apple M4
0.00.055.524 I ggml_metal_init: using embedded metal library
0.00.057.885 I ggml_metal_init: GPU name:   Apple M4
0.00.057.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.887 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.887 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.887 I ggml_metal_init: simdgroup reduction   = true
0.00.057.888 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.888 I ggml_metal_init: has bfloat            = true
0.00.057.888 I ggml_metal_init: use bfloat            = true
0.00.057.888 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.889 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.879 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.233 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.237 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.254 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.135 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.136 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.136 I llama_new_context_with_model: graph nodes  = 967
0.00.070.136 I llama_new_context_with_model: graph splits = 2
0.00.070.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.257 I 
0.00.720.294 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.306 I perplexity: tokenizing the input ..
0.00.727.791 I perplexity: tokenization took 7.484 ms
0.00.727.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.838 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.863.159 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.863.171 I llama_perf_context_print:        load time =     710.20 ms
0.00.863.172 I llama_perf_context_print: prompt eval time =     133.81 ms /   128 tokens (    1.05 ms per token,   956.59 tokens per second)
0.00.863.173 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.863.173 I llama_perf_context_print:       total time =     142.91 ms /   129 tokens
0.00.863.510 I ggml_metal_free: deallocating

real	0m0.879s
user	0m0.080s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.687 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.696 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.697 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.697 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.698 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.701 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.701 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.519 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.521 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.523 I llama_model_loader: - type  f32:  194 tensors
0.00.023.523 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.523 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.116 I llm_load_vocab: special tokens cache size = 25
0.00.052.153 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.158 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.159 I llm_load_print_meta: arch             = gptneox
0.00.052.159 I llm_load_print_meta: vocab type       = BPE
0.00.052.159 I llm_load_print_meta: n_vocab          = 50304
0.00.052.160 I llm_load_print_meta: n_merges         = 50009
0.00.052.160 I llm_load_print_meta: vocab_only       = 0
0.00.052.160 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.160 I llm_load_print_meta: n_embd           = 2048
0.00.052.160 I llm_load_print_meta: n_layer          = 24
0.00.052.165 I llm_load_print_meta: n_head           = 16
0.00.052.165 I llm_load_print_meta: n_head_kv        = 16
0.00.052.168 I llm_load_print_meta: n_rot            = 32
0.00.052.168 I llm_load_print_meta: n_swa            = 0
0.00.052.168 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.169 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.169 I llm_load_print_meta: n_gqa            = 1
0.00.052.170 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.171 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.171 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.172 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.172 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.172 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.172 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.173 I llm_load_print_meta: n_ff             = 8192
0.00.052.173 I llm_load_print_meta: n_expert         = 0
0.00.052.173 I llm_load_print_meta: n_expert_used    = 0
0.00.052.173 I llm_load_print_meta: causal attn      = 1
0.00.052.173 I llm_load_print_meta: pooling type     = 0
0.00.052.173 I llm_load_print_meta: rope type        = 2
0.00.052.173 I llm_load_print_meta: rope scaling     = linear
0.00.052.174 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.175 I llm_load_print_meta: freq_scale_train = 1
0.00.052.175 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.175 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.175 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.175 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.175 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.176 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.176 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.176 I llm_load_print_meta: model type       = 1.4B
0.00.052.176 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.177 I llm_load_print_meta: model params     = 1.41 B
0.00.052.215 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.216 I llm_load_print_meta: general.name     = 1.4B
0.00.052.217 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.217 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.217 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.217 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.217 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.218 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.218 I llm_load_print_meta: max token length = 1024
0.00.054.232 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.233 I llm_load_tensors: offloading output layer to GPU
0.00.054.233 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.244 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.245 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.109 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.110 I llama_new_context_with_model: n_ctx         = 128
0.00.055.110 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.110 I llama_new_context_with_model: n_batch       = 128
0.00.055.110 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.111 I llama_new_context_with_model: flash_attn    = 0
0.00.055.111 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.111 I llama_new_context_with_model: freq_scale    = 1
0.00.055.112 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.113 I ggml_metal_init: allocating
0.00.055.119 I ggml_metal_init: found device: Apple M4
0.00.055.121 I ggml_metal_init: picking default device: Apple M4
0.00.055.700 I ggml_metal_init: using embedded metal library
0.00.058.001 I ggml_metal_init: GPU name:   Apple M4
0.00.058.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.004 I ggml_metal_init: simdgroup reduction   = true
0.00.058.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.004 I ggml_metal_init: has bfloat            = true
0.00.058.004 I ggml_metal_init: use bfloat            = true
0.00.058.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.129 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.492 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.496 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.516 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.408 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.409 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.409 I llama_new_context_with_model: graph nodes  = 967
0.00.070.410 I llama_new_context_with_model: graph splits = 2
0.00.070.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.553 I 
0.00.759.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.605 I perplexity: tokenizing the input ..
0.00.767.420 I perplexity: tokenization took 7.813 ms
0.00.767.423 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.902.469 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.903.657 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.903.669 I llama_perf_context_print:        load time =     750.91 ms
0.00.903.670 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.42 tokens per second)
0.00.903.671 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.903.671 I llama_perf_context_print:       total time =     144.12 ms /   129 tokens
0.00.904.101 I ggml_metal_free: deallocating

real	0m0.919s
user	0m0.081s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.998 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.754 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.760 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.761 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.761 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.761 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.762 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.763 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.765 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.766 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.766 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.768 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.651 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.653 I llama_model_loader: - type  f32:  194 tensors
0.00.024.654 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.654 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.654 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.094 I llm_load_vocab: special tokens cache size = 25
0.00.052.061 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.064 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.064 I llm_load_print_meta: arch             = gptneox
0.00.052.064 I llm_load_print_meta: vocab type       = BPE
0.00.052.065 I llm_load_print_meta: n_vocab          = 50304
0.00.052.065 I llm_load_print_meta: n_merges         = 50009
0.00.052.065 I llm_load_print_meta: vocab_only       = 0
0.00.052.065 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.066 I llm_load_print_meta: n_embd           = 2048
0.00.052.066 I llm_load_print_meta: n_layer          = 24
0.00.052.069 I llm_load_print_meta: n_head           = 16
0.00.052.069 I llm_load_print_meta: n_head_kv        = 16
0.00.052.069 I llm_load_print_meta: n_rot            = 32
0.00.052.070 I llm_load_print_meta: n_swa            = 0
0.00.052.070 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.070 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.071 I llm_load_print_meta: n_gqa            = 1
0.00.052.072 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.072 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.073 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.073 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.073 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.077 I llm_load_print_meta: n_ff             = 8192
0.00.052.077 I llm_load_print_meta: n_expert         = 0
0.00.052.078 I llm_load_print_meta: n_expert_used    = 0
0.00.052.078 I llm_load_print_meta: causal attn      = 1
0.00.052.079 I llm_load_print_meta: pooling type     = 0
0.00.052.079 I llm_load_print_meta: rope type        = 2
0.00.052.079 I llm_load_print_meta: rope scaling     = linear
0.00.052.080 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.080 I llm_load_print_meta: freq_scale_train = 1
0.00.052.081 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.081 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.082 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.082 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.082 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.082 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.082 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.082 I llm_load_print_meta: model type       = 1.4B
0.00.052.084 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.084 I llm_load_print_meta: model params     = 1.41 B
0.00.052.085 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.085 I llm_load_print_meta: general.name     = 1.4B
0.00.052.085 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.086 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.086 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.086 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.086 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.087 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.088 I llm_load_print_meta: max token length = 1024
0.00.054.032 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.032 I llm_load_tensors: offloading output layer to GPU
0.00.054.033 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.043 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.044 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.940 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.941 I llama_new_context_with_model: n_ctx         = 128
0.00.054.941 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.941 I llama_new_context_with_model: n_batch       = 128
0.00.054.941 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.941 I llama_new_context_with_model: flash_attn    = 0
0.00.054.942 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.942 I llama_new_context_with_model: freq_scale    = 1
0.00.054.943 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.943 I ggml_metal_init: allocating
0.00.054.950 I ggml_metal_init: found device: Apple M4
0.00.054.952 I ggml_metal_init: picking default device: Apple M4
0.00.055.519 I ggml_metal_init: using embedded metal library
0.00.057.840 I ggml_metal_init: GPU name:   Apple M4
0.00.057.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.842 I ggml_metal_init: simdgroup reduction   = true
0.00.057.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.843 I ggml_metal_init: has bfloat            = true
0.00.057.843 I ggml_metal_init: use bfloat            = true
0.00.057.843 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.845 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.270 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.622 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.625 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.639 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.516 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.517 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.517 I llama_new_context_with_model: graph nodes  = 967
0.00.069.517 I llama_new_context_with_model: graph splits = 2
0.00.069.530 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.531 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.839 I 
0.00.445.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.893 I perplexity: tokenizing the input ..
0.00.453.571 I perplexity: tokenization took 7.676 ms
0.00.453.575 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.586.371 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.587.630 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.587.651 I llama_perf_context_print:        load time =     435.84 ms
0.00.587.652 I llama_perf_context_print: prompt eval time =     132.57 ms /   128 tokens (    1.04 ms per token,   965.52 tokens per second)
0.00.587.652 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.587.653 I llama_perf_context_print:       total time =     141.81 ms /   129 tokens
0.00.588.223 I ggml_metal_free: deallocating

real	0m0.604s
user	0m0.079s
sys	0m0.071s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.769 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.673 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.674 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.676 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.677 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.678 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.678 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.678 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.680 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.680 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.680 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.640 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.641 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.641 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.642 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.643 I llama_model_loader: - type  f32:  194 tensors
0.00.023.643 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.643 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.643 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.644 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.986 I llm_load_vocab: special tokens cache size = 25
0.00.051.017 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.020 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.020 I llm_load_print_meta: arch             = gptneox
0.00.051.020 I llm_load_print_meta: vocab type       = BPE
0.00.051.021 I llm_load_print_meta: n_vocab          = 50304
0.00.051.021 I llm_load_print_meta: n_merges         = 50009
0.00.051.021 I llm_load_print_meta: vocab_only       = 0
0.00.051.021 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.021 I llm_load_print_meta: n_embd           = 2048
0.00.051.022 I llm_load_print_meta: n_layer          = 24
0.00.051.025 I llm_load_print_meta: n_head           = 16
0.00.051.025 I llm_load_print_meta: n_head_kv        = 16
0.00.051.025 I llm_load_print_meta: n_rot            = 32
0.00.051.026 I llm_load_print_meta: n_swa            = 0
0.00.051.026 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.026 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.027 I llm_load_print_meta: n_gqa            = 1
0.00.051.028 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.028 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.029 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.029 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.029 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.029 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.029 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.030 I llm_load_print_meta: n_ff             = 8192
0.00.051.030 I llm_load_print_meta: n_expert         = 0
0.00.051.030 I llm_load_print_meta: n_expert_used    = 0
0.00.051.031 I llm_load_print_meta: causal attn      = 1
0.00.051.031 I llm_load_print_meta: pooling type     = 0
0.00.051.031 I llm_load_print_meta: rope type        = 2
0.00.051.031 I llm_load_print_meta: rope scaling     = linear
0.00.051.032 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.032 I llm_load_print_meta: freq_scale_train = 1
0.00.051.032 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.032 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.032 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.035 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.035 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.035 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.035 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.035 I llm_load_print_meta: model type       = 1.4B
0.00.051.036 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.036 I llm_load_print_meta: model params     = 1.41 B
0.00.051.037 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.037 I llm_load_print_meta: general.name     = 1.4B
0.00.051.037 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.038 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.038 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.040 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.040 I llm_load_print_meta: max token length = 1024
0.00.053.025 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.026 I llm_load_tensors: offloading output layer to GPU
0.00.053.026 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.037 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.038 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.915 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.915 I llama_new_context_with_model: n_ctx         = 128
0.00.053.915 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.916 I llama_new_context_with_model: n_batch       = 128
0.00.053.916 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.916 I llama_new_context_with_model: flash_attn    = 0
0.00.053.916 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.917 I llama_new_context_with_model: freq_scale    = 1
0.00.053.917 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.918 I ggml_metal_init: allocating
0.00.053.921 I ggml_metal_init: found device: Apple M4
0.00.053.923 I ggml_metal_init: picking default device: Apple M4
0.00.054.485 I ggml_metal_init: using embedded metal library
0.00.056.822 I ggml_metal_init: GPU name:   Apple M4
0.00.056.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.824 I ggml_metal_init: simdgroup reduction   = true
0.00.056.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.824 I ggml_metal_init: has bfloat            = true
0.00.056.825 I ggml_metal_init: use bfloat            = true
0.00.056.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.385 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.660 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.664 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.530 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.531 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.531 I llama_new_context_with_model: graph nodes  = 967
0.00.068.531 I llama_new_context_with_model: graph splits = 2
0.00.068.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.723 I 
0.00.513.759 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.771 I perplexity: tokenizing the input ..
0.00.521.473 I perplexity: tokenization took 7.701 ms
0.00.521.477 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.937 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.655.089 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.655.109 I llama_perf_context_print:        load time =     504.95 ms
0.00.655.111 I llama_perf_context_print: prompt eval time =     132.22 ms /   128 tokens (    1.03 ms per token,   968.05 tokens per second)
0.00.655.114 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.114 I llama_perf_context_print:       total time =     141.39 ms /   129 tokens
0.00.655.621 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.079s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.859 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.532 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.538 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.539 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.539 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.540 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.540 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.541 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.541 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.541 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.544 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.545 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.546 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.547 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.453 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.520 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.373 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.374 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.374 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.374 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.375 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.375 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.376 I llama_model_loader: - type  f32:  194 tensors
0.00.023.376 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.376 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.376 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.529 I llm_load_vocab: special tokens cache size = 25
0.00.050.432 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.434 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.435 I llm_load_print_meta: arch             = gptneox
0.00.050.435 I llm_load_print_meta: vocab type       = BPE
0.00.050.435 I llm_load_print_meta: n_vocab          = 50304
0.00.050.436 I llm_load_print_meta: n_merges         = 50009
0.00.050.436 I llm_load_print_meta: vocab_only       = 0
0.00.050.436 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.436 I llm_load_print_meta: n_embd           = 2048
0.00.050.436 I llm_load_print_meta: n_layer          = 24
0.00.050.439 I llm_load_print_meta: n_head           = 16
0.00.050.440 I llm_load_print_meta: n_head_kv        = 16
0.00.050.440 I llm_load_print_meta: n_rot            = 32
0.00.050.440 I llm_load_print_meta: n_swa            = 0
0.00.050.440 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.440 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.441 I llm_load_print_meta: n_gqa            = 1
0.00.050.442 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.443 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.443 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.444 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.444 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.444 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.444 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.445 I llm_load_print_meta: n_ff             = 8192
0.00.050.445 I llm_load_print_meta: n_expert         = 0
0.00.050.447 I llm_load_print_meta: n_expert_used    = 0
0.00.050.447 I llm_load_print_meta: causal attn      = 1
0.00.050.448 I llm_load_print_meta: pooling type     = 0
0.00.050.448 I llm_load_print_meta: rope type        = 2
0.00.050.448 I llm_load_print_meta: rope scaling     = linear
0.00.050.448 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.448 I llm_load_print_meta: freq_scale_train = 1
0.00.050.450 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.450 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.450 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.450 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.451 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.451 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.451 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.451 I llm_load_print_meta: model type       = 1.4B
0.00.050.451 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.452 I llm_load_print_meta: model params     = 1.41 B
0.00.050.453 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.453 I llm_load_print_meta: general.name     = 1.4B
0.00.050.453 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.453 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.453 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.453 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.454 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.454 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.454 I llm_load_print_meta: max token length = 1024
0.00.052.424 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.425 I llm_load_tensors: offloading output layer to GPU
0.00.052.425 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.435 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.436 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.316 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.317 I llama_new_context_with_model: n_ctx         = 128
0.00.053.317 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.317 I llama_new_context_with_model: n_batch       = 128
0.00.053.317 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.318 I llama_new_context_with_model: flash_attn    = 0
0.00.053.318 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.318 I llama_new_context_with_model: freq_scale    = 1
0.00.053.319 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.319 I ggml_metal_init: allocating
0.00.053.322 I ggml_metal_init: found device: Apple M4
0.00.053.324 I ggml_metal_init: picking default device: Apple M4
0.00.053.901 I ggml_metal_init: using embedded metal library
0.00.056.236 I ggml_metal_init: GPU name:   Apple M4
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.239 I ggml_metal_init: simdgroup reduction   = true
0.00.056.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.239 I ggml_metal_init: has bfloat            = true
0.00.056.239 I ggml_metal_init: use bfloat            = true
0.00.056.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.240 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.127 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.451 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.454 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.471 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.392 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.393 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.394 I llama_new_context_with_model: graph nodes  = 967
0.00.068.394 I llama_new_context_with_model: graph splits = 2
0.00.068.407 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.408 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.566.383 I 
0.00.566.417 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.566.428 I perplexity: tokenizing the input ..
0.00.574.631 I perplexity: tokenization took 8.201 ms
0.00.574.634 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.709.115 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.710.284 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.710.303 I llama_perf_context_print:        load time =     557.52 ms
0.00.710.305 I llama_perf_context_print: prompt eval time =     134.23 ms /   128 tokens (    1.05 ms per token,   953.57 tokens per second)
0.00.710.305 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.710.306 I llama_perf_context_print:       total time =     143.92 ms /   129 tokens
0.00.710.834 I ggml_metal_free: deallocating

real	0m0.725s
user	0m0.080s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.360 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.886 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.887 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.889 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.890 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.892 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.893 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.896 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.896 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.750 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.848 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.692 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.693 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.694 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.694 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.694 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.695 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.695 I llama_model_loader: - type  f32:  194 tensors
0.00.023.696 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.696 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.074 I llm_load_vocab: special tokens cache size = 25
0.00.050.058 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.061 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.061 I llm_load_print_meta: arch             = gptneox
0.00.050.062 I llm_load_print_meta: vocab type       = BPE
0.00.050.062 I llm_load_print_meta: n_vocab          = 50304
0.00.050.062 I llm_load_print_meta: n_merges         = 50009
0.00.050.062 I llm_load_print_meta: vocab_only       = 0
0.00.050.062 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.062 I llm_load_print_meta: n_embd           = 2048
0.00.050.063 I llm_load_print_meta: n_layer          = 24
0.00.050.065 I llm_load_print_meta: n_head           = 16
0.00.050.066 I llm_load_print_meta: n_head_kv        = 16
0.00.050.066 I llm_load_print_meta: n_rot            = 32
0.00.050.066 I llm_load_print_meta: n_swa            = 0
0.00.050.066 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.067 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.067 I llm_load_print_meta: n_gqa            = 1
0.00.050.068 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.069 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.070 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.070 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.070 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.070 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.071 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.071 I llm_load_print_meta: n_ff             = 8192
0.00.050.072 I llm_load_print_meta: n_expert         = 0
0.00.050.072 I llm_load_print_meta: n_expert_used    = 0
0.00.050.072 I llm_load_print_meta: causal attn      = 1
0.00.050.072 I llm_load_print_meta: pooling type     = 0
0.00.050.074 I llm_load_print_meta: rope type        = 2
0.00.050.074 I llm_load_print_meta: rope scaling     = linear
0.00.050.074 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.074 I llm_load_print_meta: freq_scale_train = 1
0.00.050.075 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.075 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.075 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.075 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.075 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.075 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.075 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.076 I llm_load_print_meta: model type       = 1.4B
0.00.050.076 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.076 I llm_load_print_meta: model params     = 1.41 B
0.00.050.077 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.077 I llm_load_print_meta: general.name     = 1.4B
0.00.050.077 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.078 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.078 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.078 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.078 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.078 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.079 I llm_load_print_meta: max token length = 1024
0.00.052.093 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.094 I llm_load_tensors: offloading output layer to GPU
0.00.052.094 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.105 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.106 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.991 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.992 I llama_new_context_with_model: n_ctx         = 128
0.00.052.992 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.992 I llama_new_context_with_model: n_batch       = 128
0.00.052.992 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.992 I llama_new_context_with_model: flash_attn    = 0
0.00.052.993 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.993 I llama_new_context_with_model: freq_scale    = 1
0.00.052.993 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.994 I ggml_metal_init: allocating
0.00.052.997 I ggml_metal_init: found device: Apple M4
0.00.052.999 I ggml_metal_init: picking default device: Apple M4
0.00.053.553 I ggml_metal_init: using embedded metal library
0.00.055.887 I ggml_metal_init: GPU name:   Apple M4
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.889 I ggml_metal_init: simdgroup reduction   = true
0.00.055.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.890 I ggml_metal_init: has bfloat            = true
0.00.055.890 I ggml_metal_init: use bfloat            = true
0.00.055.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.531 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.807 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.811 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.826 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.703 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.704 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.704 I llama_new_context_with_model: graph nodes  = 967
0.00.067.705 I llama_new_context_with_model: graph splits = 2
0.00.067.717 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.261 I 
0.00.638.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.309 I perplexity: tokenizing the input ..
0.00.646.200 I perplexity: tokenization took 7.889 ms
0.00.646.204 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.158 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.788.411 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.788.436 I llama_perf_context_print:        load time =     628.89 ms
0.00.788.438 I llama_perf_context_print: prompt eval time =     140.73 ms /   128 tokens (    1.10 ms per token,   909.57 tokens per second)
0.00.788.439 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.439 I llama_perf_context_print:       total time =     150.18 ms /   129 tokens
0.00.788.929 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.078s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.256 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.773 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.779 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.783 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.786 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.786 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.786 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.825 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.729 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.730 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.730 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.731 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.731 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.732 I llama_model_loader: - type  f32:  194 tensors
0.00.023.732 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.001 I llm_load_vocab: special tokens cache size = 25
0.00.050.919 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.921 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.922 I llm_load_print_meta: arch             = gptneox
0.00.050.922 I llm_load_print_meta: vocab type       = BPE
0.00.050.922 I llm_load_print_meta: n_vocab          = 50304
0.00.050.923 I llm_load_print_meta: n_merges         = 50009
0.00.050.923 I llm_load_print_meta: vocab_only       = 0
0.00.050.923 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.923 I llm_load_print_meta: n_embd           = 2048
0.00.050.923 I llm_load_print_meta: n_layer          = 24
0.00.050.926 I llm_load_print_meta: n_head           = 16
0.00.050.927 I llm_load_print_meta: n_head_kv        = 16
0.00.050.927 I llm_load_print_meta: n_rot            = 32
0.00.050.927 I llm_load_print_meta: n_swa            = 0
0.00.050.927 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.927 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.928 I llm_load_print_meta: n_gqa            = 1
0.00.050.929 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.930 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.930 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.931 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.931 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.931 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.931 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.932 I llm_load_print_meta: n_ff             = 8192
0.00.050.932 I llm_load_print_meta: n_expert         = 0
0.00.050.932 I llm_load_print_meta: n_expert_used    = 0
0.00.050.932 I llm_load_print_meta: causal attn      = 1
0.00.050.932 I llm_load_print_meta: pooling type     = 0
0.00.050.932 I llm_load_print_meta: rope type        = 2
0.00.050.933 I llm_load_print_meta: rope scaling     = linear
0.00.050.933 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.935 I llm_load_print_meta: freq_scale_train = 1
0.00.050.935 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.936 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.936 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.936 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.936 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.936 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.938 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.938 I llm_load_print_meta: model type       = 1.4B
0.00.050.938 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.939 I llm_load_print_meta: model params     = 1.41 B
0.00.050.939 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.939 I llm_load_print_meta: general.name     = 1.4B
0.00.050.940 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.941 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.941 I llm_load_print_meta: max token length = 1024
0.00.052.997 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.998 I llm_load_tensors: offloading output layer to GPU
0.00.052.998 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.009 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.010 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.897 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.898 I llama_new_context_with_model: n_ctx         = 128
0.00.053.898 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.898 I llama_new_context_with_model: n_batch       = 128
0.00.053.898 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.898 I llama_new_context_with_model: flash_attn    = 0
0.00.053.899 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.899 I llama_new_context_with_model: freq_scale    = 1
0.00.053.899 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.900 I ggml_metal_init: allocating
0.00.053.905 I ggml_metal_init: found device: Apple M4
0.00.053.908 I ggml_metal_init: picking default device: Apple M4
0.00.054.446 I ggml_metal_init: using embedded metal library
0.00.056.791 I ggml_metal_init: GPU name:   Apple M4
0.00.056.793 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.794 I ggml_metal_init: simdgroup reduction   = true
0.00.056.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.794 I ggml_metal_init: has bfloat            = true
0.00.056.794 I ggml_metal_init: use bfloat            = true
0.00.056.794 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.795 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.301 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.573 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.579 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.594 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.449 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.450 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.450 I llama_new_context_with_model: graph nodes  = 967
0.00.068.451 I llama_new_context_with_model: graph splits = 2
0.00.068.463 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.252.797 I 
0.00.252.837 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.252.850 I perplexity: tokenizing the input ..
0.00.260.218 I perplexity: tokenization took 7.366 ms
0.00.260.225 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.399.506 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.400.705 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.400.721 I llama_perf_context_print:        load time =     243.54 ms
0.00.400.722 I llama_perf_context_print: prompt eval time =     139.06 ms /   128 tokens (    1.09 ms per token,   920.49 tokens per second)
0.00.400.723 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.400.723 I llama_perf_context_print:       total time =     147.93 ms /   129 tokens
0.00.401.126 I ggml_metal_free: deallocating

real	0m0.415s
user	0m0.079s
sys	0m0.050s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.231 I build: 4377 (7c0e2858) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.884 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.582 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.076 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.411 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.413 I llama_model_loader: - type  f32:  194 tensors
0.00.051.413 I llama_model_loader: - type  f16:   98 tensors
0.00.081.275 I llm_load_vocab: special tokens cache size = 25
0.00.087.933 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.936 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.936 I llm_load_print_meta: arch             = gptneox
0.00.087.936 I llm_load_print_meta: vocab type       = BPE
0.00.087.937 I llm_load_print_meta: n_vocab          = 50304
0.00.087.937 I llm_load_print_meta: n_merges         = 50009
0.00.087.937 I llm_load_print_meta: vocab_only       = 0
0.00.087.937 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.937 I llm_load_print_meta: n_embd           = 2048
0.00.087.937 I llm_load_print_meta: n_layer          = 24
0.00.087.941 I llm_load_print_meta: n_head           = 16
0.00.087.941 I llm_load_print_meta: n_head_kv        = 16
0.00.087.942 I llm_load_print_meta: n_rot            = 32
0.00.087.942 I llm_load_print_meta: n_swa            = 0
0.00.087.942 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.942 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.943 I llm_load_print_meta: n_gqa            = 1
0.00.087.944 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.944 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.945 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.945 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.945 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.946 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.946 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.946 I llm_load_print_meta: n_ff             = 8192
0.00.087.946 I llm_load_print_meta: n_expert         = 0
0.00.087.947 I llm_load_print_meta: n_expert_used    = 0
0.00.087.949 I llm_load_print_meta: causal attn      = 1
0.00.087.949 I llm_load_print_meta: pooling type     = 0
0.00.087.949 I llm_load_print_meta: rope type        = 2
0.00.087.949 I llm_load_print_meta: rope scaling     = linear
0.00.087.950 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.950 I llm_load_print_meta: freq_scale_train = 1
0.00.087.950 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.950 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.950 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.950 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.951 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.951 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.951 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.951 I llm_load_print_meta: model type       = 1.4B
0.00.087.952 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.952 I llm_load_print_meta: model params     = 1.41 B
0.00.087.952 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.953 I llm_load_print_meta: general.name     = 1.4B
0.00.087.953 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.953 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.953 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.953 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.954 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.954 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.954 I llm_load_print_meta: max token length = 1024
0.00.090.507 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.508 I llm_load_tensors: offloading output layer to GPU
0.00.090.508 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.518 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.519 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.447 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.448 I llama_new_context_with_model: n_ctx         = 128
0.00.091.448 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.448 I llama_new_context_with_model: n_batch       = 128
0.00.091.448 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.448 I llama_new_context_with_model: flash_attn    = 0
0.00.091.449 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.449 I llama_new_context_with_model: freq_scale    = 1
0.00.091.449 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.450 I ggml_metal_init: allocating
0.00.091.453 I ggml_metal_init: found device: Apple M4
0.00.091.457 I ggml_metal_init: picking default device: Apple M4
0.00.092.075 I ggml_metal_init: using embedded metal library
0.00.094.594 I ggml_metal_init: GPU name:   Apple M4
0.00.094.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.596 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.597 I ggml_metal_init: simdgroup reduction   = true
0.00.094.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.597 I ggml_metal_init: has bfloat            = true
0.00.094.597 I ggml_metal_init: use bfloat            = true
0.00.094.598 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.053 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.308 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.310 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.323 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.181 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.182 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.182 I llama_new_context_with_model: graph nodes  = 967
0.00.106.182 I llama_new_context_with_model: graph splits = 2
0.00.106.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.196 I 
0.00.106.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.230 I compute_imatrix: tokenizing the input ..
0.00.113.373 I compute_imatrix: tokenization took 7.143 ms
0.00.113.375 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.558.619 I compute_imatrix: 1.45 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.561.130 I llama_perf_context_print:        load time =    1536.73 ms
0.01.561.131 I llama_perf_context_print: prompt eval time =    1444.61 ms /   128 tokens (   11.29 ms per token,    88.61 tokens per second)
0.01.561.132 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.561.133 I llama_perf_context_print:       total time =    1539.23 ms /   129 tokens
0.01.561.752 I ggml_metal_free: deallocating

real	0m1.746s
user	0m0.166s
sys	0m0.254s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4377 (7c0e2858)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b60a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b60a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b60aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b60b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b60ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b60bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b60c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b60cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b60d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b60d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b60daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b60dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b60eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b60f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b60fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b6101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b611030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b611750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b611f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b612640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b612d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b613480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b615ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b616180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b6168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b617170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b6176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b617e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b6182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b618750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b619090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b6199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b619e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b61a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b61a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b61abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b61b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b61bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b61c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b61c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b61cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b61d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b61d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b61df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b61e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b61ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b61f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b61f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b61f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b620160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b6208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b620d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b6216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b621fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b622920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b623260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b623ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b6240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b6250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b625b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b6260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b626620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b6270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b627610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b6280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b6290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b6295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b629b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b62a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b62a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b62ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b62b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b62b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b62bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b61b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b62bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b62c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b62cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b62d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b62d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b62dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b62e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b62e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b62ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b62f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b62f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b62fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b6301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b630700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b6310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b6335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b6343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b6351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b6368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b6376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b63a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b63a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b63a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b63ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b63b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b63b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b63bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b63c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b63c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b63c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b63ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b63d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b63d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b63dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b63e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b63e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b63ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b63eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b63f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b63f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b63fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b640f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b6413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b641d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b6421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b642fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b6438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b6446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b644b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b645010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b6454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b645950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b645df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b646bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b647070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b647510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b6479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b647e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b6483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b6488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b648e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b64a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b64a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b64b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b64b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b64b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b64bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b64c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b64cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b64d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b64d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b64d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b64e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b64e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b64ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b64f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b64f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b64fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b650150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b6506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b650bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b651690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b651be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b652130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b652680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b652bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b653120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b653670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b653bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b654110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b654660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b654bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b655100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b655650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b655ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b6560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b656b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b6570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b657630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b6580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b658620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b658b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b6590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b659610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b65a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b65a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b65ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b65b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b65b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b65bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b65c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b65c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b65cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b65d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b65d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b65db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b65e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b65e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b65eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b65f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b65f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b65fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b660050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b6605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b6618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b661d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b662210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b6626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b662b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b662ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b663490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b663930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b663dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b664270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b664710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b664bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b6655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b665cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b6663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b666b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b667220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b6674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b6685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.151.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.151.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b625360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b6257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b6260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b626520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b626990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b626e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b627270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b6276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b627b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b6285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b628e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b629610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b629df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b62a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b62abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b62b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b62b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b62c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b62ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b62d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b62d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b62def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b62e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b62ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b62eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b62f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b62f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b62fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b630080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b6304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b630960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b630c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b631090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b631500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12eb04430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12eb048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12eb04d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12eb05180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12eb055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12eb05a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12eb05ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12eb06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12eb067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12eb06c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12eb07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12eb07500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12eb07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12eb07de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12eb08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12eb086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12eb08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12eb08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12eb09410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12eb09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12eb09de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12eb0a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12eb0a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12eb0abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12eb0b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12eb0b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12eb0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12eb0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12eb0c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12eb0c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12eb0cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12eb0cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12eb0d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12eb0d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12eb0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12eb0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12eb0e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12eb0e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12eb0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12eb0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12eb0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12eb0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12eb10010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12eb10480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12eb108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12eb10d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12eb111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12eb11640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12eb11ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12eb11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12eb12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12eb12800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12eb12c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12eb130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12eb13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12eb139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12eb13e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12eb142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12eb14710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12eb14b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12eb14ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12eb15460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12eb158d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12eb15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12eb161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12eb16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12eb16a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12eb16f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12eb17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12eb177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12eb17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12eb180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12eb18530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12eb189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12eb18e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12eb19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12eb196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12eb19b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12eb19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12eb1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12eb1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12eb1ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12eb1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12eb1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12eb1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12eb1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12eb1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12eb1c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12eb1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12eb1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12eb1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12eb1d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12eb1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12eb1e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12eb1e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12eb1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12eb1efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12eb1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12eb1f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12eb1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12eb20170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12eb205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12eb20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12eb20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12eb21330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12eb217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12eb21c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12eb22080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12eb224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12eb22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12eb22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12eb23240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12eb236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12eb23b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12eb23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12eb24400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12eb24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12eb24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12eb25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12eb255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12eb25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12eb25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12eb26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12eb26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12eb26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12eb27060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12eb274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12eb27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12eb27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12eb28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12eb28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12eb28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12eb28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12eb293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12eb29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12eb29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12eb2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12eb2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12eb2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12eb2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12eb2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12eb2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12eb2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12eb2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12eb2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12eb2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12eb2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12eb2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12eb2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12eb2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12eb2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12eb2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12eb2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12eb2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12eb2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12eb2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12eb2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12eb2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12eb302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12eb30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12eb30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12eb31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12eb31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12eb31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12eb31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12eb321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12eb32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12eb32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12eb32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12eb333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12eb33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12eb33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12eb34210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12eb34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12eb351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12eb35490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12eb35750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12eb35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12eb36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12eb364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12eb36910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12eb36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12eb371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12eb37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12eb37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12eb37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12eb383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12eb38820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12eb38c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12eb39100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12eb39570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12eb399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12eb39e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12eb3a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12eb3a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12eb3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12eb3b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12eb3b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12eb3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12eb3bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12eb3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12eb3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12eb3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12eb3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12eb3d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12eb3d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12eb3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12eb3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12eb3e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12eb3e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12eb3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12eb3f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12eb3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12eb3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12eb3fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12eb40460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12eb408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12eb40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12eb411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12eb41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12eb41a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12eb41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12eb42370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12eb427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12eb42c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12eb430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12eb43530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12eb439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12eb43e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12eb44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12eb446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12eb44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12eb44fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12eb45440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12eb458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12eb45d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12eb46190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12eb46600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12eb46a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12eb46ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12eb47350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12eb477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12eb47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12eb480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12eb48510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12eb48980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12eb48df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12eb49860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12eb49f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12eb4a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12eb4adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12eb4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12eb4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12eb4baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12eb4c100 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c9044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c904950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c904dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c905230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c9056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c905b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c905f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c9063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c906860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c906d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c9071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c907860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c908380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c908b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c909340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c909a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c90a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c90a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c90afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c90b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c90beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c90c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c90ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c90d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c90db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c90ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c90e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c90e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c90e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c90ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c90f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c90f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c90fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c90fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c910340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c9107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c910c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c911090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c911500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c911970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c911de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c912250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c9126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c912b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c912fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12eb04430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12eb048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12eb04d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12eb05180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12eb055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12eb05a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12eb05ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12eb06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12eb067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12eb06c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12eb07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12eb07500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12eb07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12eb07de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12eb08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12eb086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12eb08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12eb08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12eb09410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12eb09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12eb09cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12eb0a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12eb0a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12eb0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12eb0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12eb0b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12eb0b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12eb0bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12eb0c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12eb0c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12eb0c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12eb0cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12eb0d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12eb0d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12eb0db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12eb0df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12eb0e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12eb0e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12eb0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12eb0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12eb0f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12eb0fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12eb0fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12eb10300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12eb10770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12eb10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12eb11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12eb114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12eb11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12eb11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12eb12210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12eb12680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12eb12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12eb12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12eb133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12eb13840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12eb13cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12eb14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12eb14590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12eb14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12eb14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12eb152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12eb15750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12eb15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12eb16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12eb164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12eb16910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12eb16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12eb171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12eb17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12eb17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12eb17f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12eb183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12eb18820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12eb18c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12eb19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12eb19570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12eb199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12eb19e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12eb1a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12eb1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12eb1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12eb1b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12eb1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12eb1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12eb1bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12eb1c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12eb1c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12eb1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12eb1cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12eb1d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12eb1d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12eb1dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12eb1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12eb1e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12eb1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12eb1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12eb1f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12eb1f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12eb1fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12eb1fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12eb20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12eb208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12eb20d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12eb211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12eb21620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12eb21a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12eb21f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12eb22370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12eb227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12eb22c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12eb230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12eb23530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12eb239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12eb23e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12eb24280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12eb246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12eb24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12eb24fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12eb25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12eb258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12eb25d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12eb26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12eb26600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12eb26a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12eb26ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12eb27350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12eb277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12eb27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12eb280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12eb28510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12eb28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12eb28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12eb29260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12eb296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12eb29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12eb29fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12eb2a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12eb2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12eb2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12eb2b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12eb2b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12eb2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12eb2bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12eb2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12eb2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12eb2cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12eb2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12eb2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12eb2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12eb2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12eb2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12eb2e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12eb2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12eb2ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12eb2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12eb2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12eb2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12eb30150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12eb305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12eb30a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12eb30ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12eb31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12eb31780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12eb31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12eb324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12eb32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12eb32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12eb33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12eb336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12eb33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12eb33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12eb343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12eb34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12eb34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12eb35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12eb355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12eb35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12eb35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12eb36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12eb36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12eb36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12eb37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12eb374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12eb37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12eb37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12eb38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12eb38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12eb38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12eb38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12eb393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12eb39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12eb39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12eb3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12eb3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12eb3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12eb3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12eb3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12eb3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12eb3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12eb3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12eb3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12eb3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12eb3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12eb3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12eb3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12eb3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12eb3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12eb3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12eb3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12eb3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12eb3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12eb3f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12eb3fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12eb40100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12eb40570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12eb409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12eb40e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12eb412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12eb41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12eb41ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12eb42010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12eb42480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12eb428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12eb42d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12eb431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12eb43640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12eb43ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12eb43f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12eb44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12eb44800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12eb44c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12eb450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12eb45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12eb459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12eb45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12eb462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12eb46710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12eb46e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12eb474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12eb47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12eb482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12eb48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12eb48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12eb49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12eb49490 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.761s
user	0m0.289s
sys	0m0.308s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4377 (7c0e2858)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ff0ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ff0f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ff0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ff0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ff10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ff10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ff110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ff11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ff11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ff12100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ff12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ff12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ff13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ff13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ff145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ff14d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ff15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ff15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ff16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ff16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ff17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ff17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ff17f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ff18830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ff18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ff19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ff19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ff1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ff1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ff1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ff1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ff1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ff1bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ff1c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ff1c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ff1c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ff1cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ff1d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ff1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ff1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ff1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ff1e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ff1e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ff1ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ff1f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ff1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ff1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ff20620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ff20c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ff21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ff21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ff21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ff22470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ff22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ff23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ff23710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ff23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ff23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ff24480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ff24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ff24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ff253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ff25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ff25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ff261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ff26650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ff26af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ff26f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ff27430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ff278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ff27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ff28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ff286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ff28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ff29150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ff296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ff29bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ff2a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ff2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ff2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ff2b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ff2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ff2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ff2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ff2c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ff2cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ff2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ff2d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ff2dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ff2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ff2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ff2eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ff2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ff2f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ff2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ff300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ff30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ff20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ff30aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ff31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ff317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ff31cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ff32240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ff32790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ff32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ff33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ff33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ff33cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ff34220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ff34770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ff34cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ff35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ff35760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ff35c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ff360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ff36540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ff369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ff36e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ff37320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ff377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ff37c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ff38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ff385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ff38a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ff38ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ff39380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ff39820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ff39cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ff3a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ff3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ff3aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ff3af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ff3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ff3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ff3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ff3c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ff3c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ff3cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ff3cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ff3d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ff3d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ff3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ff3e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ff3e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ff3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ff3f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ff3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ff3f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ff3fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ff40280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ff40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ff40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ff41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ff41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ff419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ff41e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ff422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ff42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ff42c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ff430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ff43560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ff43a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ff43ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ff44340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ff447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ff44c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ff45120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ff455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ff45a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ff45f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ff463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ff46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ff46ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ff47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ff47620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ff47ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ff47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ff48400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ff488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ff48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ff491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ff49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ff49b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ff49fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ff4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ff4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ff4ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ff4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ff4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ff4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ff4c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ff4c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ff4c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ff4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ff4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ff4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ff4dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ff4e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ff4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ff4ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ff4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ff4fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ff50020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ff502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ff508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ff50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ff516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ff51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ff52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ff524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ff52c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ff531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ff53720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ff53c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ff541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ff54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ff54c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ff551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ff55700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ff55c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ff561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ff566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ff56c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ff57190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ff576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ff57c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ff58180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ff586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ff58c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ff59170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ff596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ff59c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ff5a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ff5a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ff5ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ff5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ff5b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ff5bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ff5c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ff5c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ff5cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ff5d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ff5d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ff5dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ff5e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ff5e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ff5ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ff5f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ff5f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ff5fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ff60100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ff60650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ff60ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ff610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ff61640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ff61b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ff620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ff62630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ff62b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ff630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ff63620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ff63b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ff640c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ff64610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ff64b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ff650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ff65600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ff65aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ff65f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ff663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ff66880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ff66d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ff671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ff67660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ff67b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ff67fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ff68440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ff688e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ff68d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ff69220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ff696c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ff69b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ff6a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ff6a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ff6aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ff6b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ff6bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ff6bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ff6c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ff6caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ff6d0b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.888 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.892 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fe05bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fe06020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fe06490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fe06900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fe06d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fe071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fe07650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fe07ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fe07f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fe083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fe08810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fe08e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fe09990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fe0a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fe0a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fe0b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fe0b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fe0beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fe0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fe0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fe0d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fe0dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fe0e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fe0ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fe0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fe0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fe0f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fe0fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fe0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fe10410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fe10880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fe10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fe11220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fe114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fe11950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fe11dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fe12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fe126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fe12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fe12f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fe133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fe13860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fe13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fe14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fe145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fe14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fe14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fe15300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fe15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fe15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fe16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fe164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fe16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fe16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fe17210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fe17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fe17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fe180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fe18560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fe189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fe18e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fe192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fe19720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fe19b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fe1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fe1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fe1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fe1ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fe1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fe1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fe1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fe1bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fe1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fe1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fe1cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fe1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fe1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fe1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fe1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fe1e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fe1e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fe1eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fe1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fe1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fe1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fe1fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fe201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fe20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fe20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fe20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fe21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fe217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fe21c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fe220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fe22520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fe22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fe22e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fe23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fe236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fe23b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fe23fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fe24430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fe248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fe24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fe25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fe255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fe25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fe25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fe26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fe267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fe26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fe27090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fe27500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fe27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fe27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fe28250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fe286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fe28b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fe28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fe29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fe29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fe29cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fe2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fe2a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fe2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fe2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fe2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fe2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fe2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fe2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fe2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fe2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fe2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fe2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fe2d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fe2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fe2df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fe2e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fe2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fe2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fe2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fe2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fe2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fe2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fe30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fe30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fe30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fe31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fe314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fe31930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fe31da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fe32210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fe32680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fe32af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fe32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fe333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fe33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fe33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fe34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fe34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fe34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fe34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fe352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fe35750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fe35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fe36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fe364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fe36910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fe36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fe371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fe37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fe37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fe37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fe383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fe38820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fe38c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fe39100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fe39570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fe399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fe39e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fe3a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fe3a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fe3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fe3b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fe3b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fe3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fe3bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fe3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fe3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fe3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fe3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fe3d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fe3d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fe3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fe3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fe3e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fe3e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fe3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fe3f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fe3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fe3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fe3fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fe40460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fe408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fe40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fe411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fe41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fe41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fe42020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fe42490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fe42fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fe432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fe43560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fe439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fe43e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fe442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fe44720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fe44b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fe45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fe45470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fe458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fe45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fe461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fe46630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fe46aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fe46f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fe47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fe477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fe47c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fe480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fe48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fe489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fe48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fe49290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fe49700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fe49b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fe49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fe4a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fe4a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fe4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fe4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fe4b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fe4ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fe4bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fe4c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fe4c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fe4cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fe4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fe4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fe4d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12fe4de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12fe4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12fe4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12fe4eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12fe4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12fe4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12fe4f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12fe4fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12fe50180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12fe505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12fe50a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12fe50ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12fe51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12fe517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12fe51c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12fe52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12fe52500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12fe52970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12fe52de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12fe53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12fe536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12fe53b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12fe53fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12fe54410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12fe54880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12fe54cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12fe55160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12fe555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12fe55a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12fe55eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12fe56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12fe56790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12fe56c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12fe57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12fe57d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12fe584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12fe58bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12fe58e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12fe59300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12fe59900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12fe59f10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ff044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ff04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ff04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ff05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ff056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ff05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ff05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ff063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ff06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ff06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ff07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ff07720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ff082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ff08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ff092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ff099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ff0a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ff0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ff0af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ff0b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ff0c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ff0c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ff0ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ff0d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ff0dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ff0df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ff0e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ff0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ff0f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ff0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ff0fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ff100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ff10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ff10e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ff11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ff115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ff11a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ff11f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ff123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ff12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ff12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ff131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ff13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ff13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ff13db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ff143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ff149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ff14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ff155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ff15c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ff16210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ff16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ff16e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ff17440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ff17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ff180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ff18570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ff18830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ff18e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ff19630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ff19ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ff19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ff1a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ff1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ff1ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ff1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ff1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ff1bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ff1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ff1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ff1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ff1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ff1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ff1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ff1dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ff1e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ff1e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ff1ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ff1f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ff1f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ff1fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ff20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ff20770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ff20cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ff21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ff21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ff21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ff22200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ff22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ff22ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ff231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ff23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ff23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ff241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ff24730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ff24c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ff251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ff25720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ff25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ff261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ff26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ff26c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ff271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ff27700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ff27c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ff281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ff286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ff28c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ff29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ff296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ff29c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ff2a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ff2a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ff2ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ff2b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ff2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ff2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ff2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ff2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ff2c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ff2cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ff2d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ff2d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ff2d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ff2de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ff2e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ff2e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ff2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ff2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ff2f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ff2fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ff2feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ff30350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ff307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ff30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ff31130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ff315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ff31a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ff31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ff323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ff32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ff32cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ff33190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ff33630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ff33ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ff33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ff34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ff348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ff34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ff351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ff35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ff35b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ff35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ff36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ff36910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ff36db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ff37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ff376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ff37b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ff38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ff384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ff38970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ff38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ff392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ff39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ff39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ff3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ff3a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ff3a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ff3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ff3b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ff3b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ff3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ff3c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ff3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ff3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ff3ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ff3d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ff3d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ff3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ff3e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ff3e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ff3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ff3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ff3f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ff3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ff3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ff401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ff40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ff40af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ff40f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ff41430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ff418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ff41e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ff42370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ff428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ff42e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ff430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ff436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ff43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ff44300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ff44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ff44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ff45250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ff45860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ff45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ff46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ff46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ff46fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ff47440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ff47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ff48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ff48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ff48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ff49130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ff49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ff49bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ff4a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ff4a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ff4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ff4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ff4b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ff4bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ff4c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ff4c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ff4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ff4d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ff4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ff4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ff4e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ff4e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ff4eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ff4f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ff4f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ff4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ff500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ff50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ff50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ff510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ff51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ff51b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ff520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ff525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ff52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ff53090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ff535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ff53b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ff54080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ff545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ff54b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ff55070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ff555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ff55b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ff56060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ff565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ff56b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ff57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ff575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ff57af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ff58040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ff58590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ff58ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ff59030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ff59580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ff59ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ff5a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ff5a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ff5aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ff5aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ff5b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ff5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ff5bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ff5c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ff5c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ff5ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ff5cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ff5d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ff5d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ff5dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ff5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ff5e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ff5ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ff5f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ff5f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ff5fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ff60580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ff60ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ff60f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ff61750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ff61a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ff62020 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.244s
sys	0m0.147s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
