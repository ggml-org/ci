### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.46 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.62 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.26 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.76 sec*proc (28 tests)

Total Test time (real) = 222.77 sec

real	3m42.848s
user	7m40.537s
sys	0m6.219s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.21 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.29 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.32 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.32 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.19 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.26 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.24 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.82 sec*proc (28 tests)

Total Test time (real) =  51.83 sec

real	0m51.843s
user	1m11.253s
sys	0m5.643s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.132 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.654 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.922 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.932 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.933 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.934 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.935 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.936 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.937 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.938 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.938 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.939 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.942 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.943 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.944 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.944 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.945 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.945 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.946 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.517 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.520 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.520 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.521 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.521 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.030.522 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.522 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.030.523 I llama_model_loader: - type  f32:  124 tensors
0.00.030.524 I llama_model_loader: - type  f16:   73 tensors
0.00.035.255 I llm_load_vocab: special tokens cache size = 5
0.00.037.678 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.037.682 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.037.682 I llm_load_print_meta: arch             = bert
0.00.037.683 I llm_load_print_meta: vocab type       = WPM
0.00.037.683 I llm_load_print_meta: n_vocab          = 30522
0.00.037.683 I llm_load_print_meta: n_merges         = 0
0.00.037.684 I llm_load_print_meta: vocab_only       = 0
0.00.037.684 I llm_load_print_meta: n_ctx_train      = 512
0.00.037.684 I llm_load_print_meta: n_embd           = 384
0.00.037.684 I llm_load_print_meta: n_layer          = 12
0.00.037.688 I llm_load_print_meta: n_head           = 12
0.00.037.689 I llm_load_print_meta: n_head_kv        = 12
0.00.037.689 I llm_load_print_meta: n_rot            = 32
0.00.037.690 I llm_load_print_meta: n_swa            = 0
0.00.037.690 I llm_load_print_meta: n_embd_head_k    = 32
0.00.037.690 I llm_load_print_meta: n_embd_head_v    = 32
0.00.037.691 I llm_load_print_meta: n_gqa            = 1
0.00.037.692 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.037.692 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.037.693 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.037.694 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.037.694 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.037.694 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.037.694 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.037.695 I llm_load_print_meta: n_ff             = 1536
0.00.037.698 I llm_load_print_meta: n_expert         = 0
0.00.037.699 I llm_load_print_meta: n_expert_used    = 0
0.00.037.699 I llm_load_print_meta: causal attn      = 0
0.00.037.699 I llm_load_print_meta: pooling type     = 2
0.00.037.699 I llm_load_print_meta: rope type        = 2
0.00.037.700 I llm_load_print_meta: rope scaling     = linear
0.00.037.700 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.037.701 I llm_load_print_meta: freq_scale_train = 1
0.00.037.701 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.037.701 I llm_load_print_meta: rope_finetuned   = unknown
0.00.037.701 I llm_load_print_meta: ssm_d_conv       = 0
0.00.037.702 I llm_load_print_meta: ssm_d_inner      = 0
0.00.037.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.037.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.037.702 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.037.703 I llm_load_print_meta: model type       = 33M
0.00.037.703 I llm_load_print_meta: model ftype      = F16
0.00.037.704 I llm_load_print_meta: model params     = 33.21 M
0.00.037.704 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.037.704 I llm_load_print_meta: general.name     = Bge Small
0.00.037.705 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.037.705 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.037.706 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.037.711 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.037.711 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.037.711 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.037.712 I llm_load_print_meta: max token length = 21
0.00.039.747 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.747 I llm_load_tensors: offloading output layer to GPU
0.00.039.748 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.774 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.776 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.385 I llama_new_context_with_model: n_seq_max     = 1
0.00.040.387 I llama_new_context_with_model: n_ctx         = 512
0.00.040.387 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.040.388 I llama_new_context_with_model: n_batch       = 2048
0.00.040.388 I llama_new_context_with_model: n_ubatch      = 2048
0.00.040.388 I llama_new_context_with_model: flash_attn    = 0
0.00.040.389 I llama_new_context_with_model: freq_base     = 10000.0
0.00.040.389 I llama_new_context_with_model: freq_scale    = 1
0.00.040.390 I ggml_metal_init: allocating
0.00.040.400 I ggml_metal_init: found device: Apple M4
0.00.040.404 I ggml_metal_init: picking default device: Apple M4
0.00.041.273 I ggml_metal_init: using embedded metal library
0.00.045.735 I ggml_metal_init: GPU name:   Apple M4
0.00.045.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.740 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.740 I ggml_metal_init: simdgroup reduction   = true
0.00.045.740 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.740 I ggml_metal_init: has bfloat            = true
0.00.045.741 I ggml_metal_init: use bfloat            = true
0.00.045.741 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.742 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.442 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.123 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.126 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.127 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.059.991 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.059.993 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.059.993 I llama_new_context_with_model: graph nodes  = 429
0.00.059.993 I llama_new_context_with_model: graph splits = 2
0.00.059.995 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.059.995 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.441 I 
0.00.066.461 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.067.148 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.905 I llama_perf_context_print:        load time =      46.78 ms
0.00.071.906 I llama_perf_context_print: prompt eval time =       4.63 ms /     9 tokens (    0.51 ms per token,  1945.10 tokens per second)
0.00.071.907 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.907 I llama_perf_context_print:       total time =       5.47 ms /    10 tokens
0.00.072.073 I ggml_metal_free: deallocating

real	0m0.253s
user	0m0.051s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.328 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.589 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.594 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.595 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.595 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.596 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.596 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.597 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.597 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.598 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.598 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.600 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.601 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.602 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.602 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.602 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.604 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.605 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.832 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.833 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.833 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.834 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.834 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.834 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.834 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.835 I llama_model_loader: - type  f32:  124 tensors
0.00.014.835 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.475 I llm_load_vocab: special tokens cache size = 5
0.00.018.804 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.807 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.807 I llm_load_print_meta: arch             = bert
0.00.018.807 I llm_load_print_meta: vocab type       = WPM
0.00.018.808 I llm_load_print_meta: n_vocab          = 30522
0.00.018.808 I llm_load_print_meta: n_merges         = 0
0.00.018.808 I llm_load_print_meta: vocab_only       = 0
0.00.018.808 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.808 I llm_load_print_meta: n_embd           = 384
0.00.018.808 I llm_load_print_meta: n_layer          = 12
0.00.018.811 I llm_load_print_meta: n_head           = 12
0.00.018.811 I llm_load_print_meta: n_head_kv        = 12
0.00.018.812 I llm_load_print_meta: n_rot            = 32
0.00.018.812 I llm_load_print_meta: n_swa            = 0
0.00.018.812 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.814 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.815 I llm_load_print_meta: n_gqa            = 1
0.00.018.815 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.817 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.817 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.818 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.818 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.818 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.818 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.819 I llm_load_print_meta: n_ff             = 1536
0.00.018.819 I llm_load_print_meta: n_expert         = 0
0.00.018.819 I llm_load_print_meta: n_expert_used    = 0
0.00.018.819 I llm_load_print_meta: causal attn      = 0
0.00.018.819 I llm_load_print_meta: pooling type     = 2
0.00.018.819 I llm_load_print_meta: rope type        = 2
0.00.018.821 I llm_load_print_meta: rope scaling     = linear
0.00.018.821 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.822 I llm_load_print_meta: freq_scale_train = 1
0.00.018.822 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.822 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.822 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.822 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.822 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.822 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.822 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.823 I llm_load_print_meta: model type       = 33M
0.00.018.823 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.823 I llm_load_print_meta: model params     = 33.21 M
0.00.018.824 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.824 I llm_load_print_meta: general.name     = Bge Small
0.00.018.824 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.829 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.829 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.829 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.830 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.830 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.830 I llm_load_print_meta: max token length = 21
0.00.020.166 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.166 I llm_load_tensors: offloading output layer to GPU
0.00.020.166 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.174 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.175 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.544 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.545 I llama_new_context_with_model: n_ctx         = 512
0.00.020.545 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.545 I llama_new_context_with_model: n_batch       = 2048
0.00.020.545 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.546 I llama_new_context_with_model: flash_attn    = 0
0.00.020.546 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.546 I llama_new_context_with_model: freq_scale    = 1
0.00.020.547 I ggml_metal_init: allocating
0.00.020.550 I ggml_metal_init: found device: Apple M4
0.00.020.553 I ggml_metal_init: picking default device: Apple M4
0.00.021.192 I ggml_metal_init: using embedded metal library
0.00.023.715 I ggml_metal_init: GPU name:   Apple M4
0.00.023.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.718 I ggml_metal_init: simdgroup reduction   = true
0.00.023.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.718 I ggml_metal_init: has bfloat            = true
0.00.023.719 I ggml_metal_init: use bfloat            = true
0.00.023.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.720 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.118 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.605 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.608 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.609 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.266 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.267 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.268 I llama_new_context_with_model: graph nodes  = 429
0.00.035.268 I llama_new_context_with_model: graph splits = 2
0.00.035.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.515 I 
0.00.040.534 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.124 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.608 I llama_perf_context_print:        load time =      31.18 ms
0.00.045.609 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2067.06 tokens per second)
0.00.045.610 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.610 I llama_perf_context_print:       total time =       5.09 ms /    10 tokens
0.00.045.791 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.179 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.001 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.750 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.755 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.757 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.758 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.758 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.760 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.767 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.768 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.769 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.770 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.770 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.771 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.774 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.775 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.775 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.776 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.777 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.362 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.363 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.363 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.363 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.364 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.364 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.364 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.365 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.365 I llama_model_loader: - type  f32:   40 tensors
0.00.048.366 I llama_model_loader: - type  f16:   30 tensors
0.00.066.576 W llm_load_vocab: empty token at index 5
0.00.071.018 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.278 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.308 I llm_load_vocab: special tokens cache size = 5
0.00.340.603 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.340.612 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.340.613 I llm_load_print_meta: arch             = jina-bert-v2
0.00.340.618 I llm_load_print_meta: vocab type       = BPE
0.00.340.619 I llm_load_print_meta: n_vocab          = 61056
0.00.340.619 I llm_load_print_meta: n_merges         = 39382
0.00.340.619 I llm_load_print_meta: vocab_only       = 0
0.00.340.619 I llm_load_print_meta: n_ctx_train      = 8192
0.00.340.619 I llm_load_print_meta: n_embd           = 384
0.00.340.619 I llm_load_print_meta: n_layer          = 4
0.00.340.623 I llm_load_print_meta: n_head           = 12
0.00.340.624 I llm_load_print_meta: n_head_kv        = 12
0.00.340.624 I llm_load_print_meta: n_rot            = 32
0.00.340.624 I llm_load_print_meta: n_swa            = 0
0.00.340.624 I llm_load_print_meta: n_embd_head_k    = 32
0.00.340.624 I llm_load_print_meta: n_embd_head_v    = 32
0.00.340.625 I llm_load_print_meta: n_gqa            = 1
0.00.340.625 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.340.626 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.340.626 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.340.626 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.340.626 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.340.627 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.340.627 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.340.627 I llm_load_print_meta: n_ff             = 1536
0.00.340.627 I llm_load_print_meta: n_expert         = 0
0.00.340.628 I llm_load_print_meta: n_expert_used    = 0
0.00.340.628 I llm_load_print_meta: causal attn      = 0
0.00.340.628 I llm_load_print_meta: pooling type     = -1
0.00.340.628 I llm_load_print_meta: rope type        = -1
0.00.340.628 I llm_load_print_meta: rope scaling     = linear
0.00.340.628 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.340.629 I llm_load_print_meta: freq_scale_train = 1
0.00.340.629 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.340.629 I llm_load_print_meta: rope_finetuned   = unknown
0.00.340.629 I llm_load_print_meta: ssm_d_conv       = 0
0.00.340.629 I llm_load_print_meta: ssm_d_inner      = 0
0.00.340.630 I llm_load_print_meta: ssm_d_state      = 0
0.00.340.630 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.340.630 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.340.630 I llm_load_print_meta: model type       = 33M
0.00.340.630 I llm_load_print_meta: model ftype      = F16
0.00.340.631 I llm_load_print_meta: model params     = 32.90 M
0.00.340.631 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.340.632 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.340.635 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.340.635 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.340.635 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.340.636 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.340.636 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.340.636 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.340.636 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.340.637 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.340.637 I llm_load_print_meta: max token length = 45
0.00.341.385 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.341.386 I llm_load_tensors: offloading output layer to GPU
0.00.341.386 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.341.406 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.341.407 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.342.119 I llama_new_context_with_model: n_seq_max     = 1
0.00.342.120 I llama_new_context_with_model: n_ctx         = 8192
0.00.342.120 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.342.120 I llama_new_context_with_model: n_batch       = 2048
0.00.342.120 I llama_new_context_with_model: n_ubatch      = 2048
0.00.342.121 I llama_new_context_with_model: flash_attn    = 0
0.00.342.121 I llama_new_context_with_model: freq_base     = 10000.0
0.00.342.121 I llama_new_context_with_model: freq_scale    = 1
0.00.342.122 I ggml_metal_init: allocating
0.00.342.126 I ggml_metal_init: found device: Apple M4
0.00.342.128 I ggml_metal_init: picking default device: Apple M4
0.00.342.799 I ggml_metal_init: using embedded metal library
0.00.345.407 I ggml_metal_init: GPU name:   Apple M4
0.00.345.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.409 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.410 I ggml_metal_init: simdgroup reduction   = true
0.00.345.410 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.410 I ggml_metal_init: has bfloat            = true
0.00.345.411 I ggml_metal_init: use bfloat            = true
0.00.345.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.355.289 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.357.750 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.357.752 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.357.754 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.358.361 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.358.362 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.358.363 I llama_new_context_with_model: graph nodes  = 154
0.00.358.363 I llama_new_context_with_model: graph splits = 2
0.00.358.364 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.358.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.578 I 
0.00.368.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.368.759 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.368.760 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.368.763 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.368.763 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.368.766 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.368.768 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.369.273 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.372.785 I llama_perf_context_print:        load time =     345.57 ms
0.00.372.786 I llama_perf_context_print: prompt eval time =       3.50 ms /    62 tokens (    0.06 ms per token, 17699.12 tokens per second)
0.00.372.787 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.372.787 I llama_perf_context_print:       total time =       4.21 ms /    63 tokens
0.00.373.035 I ggml_metal_free: deallocating

real	0m1.102s
user	0m0.348s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.189 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.318 I main: llama backend init
0.00.000.327 I main: load the model and apply lora adapter, if any
0.00.032.022 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.301 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.317 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.320 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.325 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.326 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.327 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.327 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.329 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.330 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.330 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.331 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.332 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.333 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.334 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.337 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.178 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.488 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.768 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.773 I llama_model_loader: - type  f32:  194 tensors
0.00.061.774 I llama_model_loader: - type  f16:   98 tensors
0.00.092.559 I llm_load_vocab: special tokens cache size = 25
0.00.099.260 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.264 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.264 I llm_load_print_meta: arch             = gptneox
0.00.099.264 I llm_load_print_meta: vocab type       = BPE
0.00.099.264 I llm_load_print_meta: n_vocab          = 50304
0.00.099.265 I llm_load_print_meta: n_merges         = 50009
0.00.099.265 I llm_load_print_meta: vocab_only       = 0
0.00.099.265 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.265 I llm_load_print_meta: n_embd           = 2048
0.00.099.265 I llm_load_print_meta: n_layer          = 24
0.00.099.268 I llm_load_print_meta: n_head           = 16
0.00.099.269 I llm_load_print_meta: n_head_kv        = 16
0.00.099.269 I llm_load_print_meta: n_rot            = 32
0.00.099.269 I llm_load_print_meta: n_swa            = 0
0.00.099.270 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.270 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.270 I llm_load_print_meta: n_gqa            = 1
0.00.099.271 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.272 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.272 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.273 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.273 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.273 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.273 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.274 I llm_load_print_meta: n_ff             = 8192
0.00.099.274 I llm_load_print_meta: n_expert         = 0
0.00.099.274 I llm_load_print_meta: n_expert_used    = 0
0.00.099.274 I llm_load_print_meta: causal attn      = 1
0.00.099.274 I llm_load_print_meta: pooling type     = 0
0.00.099.276 I llm_load_print_meta: rope type        = 2
0.00.099.276 I llm_load_print_meta: rope scaling     = linear
0.00.099.276 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.277 I llm_load_print_meta: freq_scale_train = 1
0.00.099.277 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.277 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.279 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.279 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.279 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.279 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.279 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.279 I llm_load_print_meta: model type       = 1.4B
0.00.099.280 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.280 I llm_load_print_meta: model params     = 1.41 B
0.00.099.281 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.281 I llm_load_print_meta: general.name     = 1.4B
0.00.099.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.285 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.285 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.285 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.286 I llm_load_print_meta: LF token         = 128 ''
0.00.099.286 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.286 I llm_load_print_meta: max token length = 1024
0.00.101.955 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.955 I llm_load_tensors: offloading output layer to GPU
0.00.101.955 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.973 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.975 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.930 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.931 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.931 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.932 I llama_new_context_with_model: n_batch       = 2048
0.00.102.932 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.932 I llama_new_context_with_model: flash_attn    = 0
0.00.102.932 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.933 I llama_new_context_with_model: freq_scale    = 1
0.00.102.933 I ggml_metal_init: allocating
0.00.102.942 I ggml_metal_init: found device: Apple M4
0.00.102.944 I ggml_metal_init: picking default device: Apple M4
0.00.103.622 I ggml_metal_init: using embedded metal library
0.00.106.389 I ggml_metal_init: GPU name:   Apple M4
0.00.106.391 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.106.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.106.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.106.392 I ggml_metal_init: simdgroup reduction   = true
0.00.106.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.106.392 I ggml_metal_init: has bfloat            = true
0.00.106.392 I ggml_metal_init: use bfloat            = true
0.00.106.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.106.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.116.860 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.135.540 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.545 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.564 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.136.545 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.136.546 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.136.547 I llama_new_context_with_model: graph nodes  = 967
0.00.136.547 I llama_new_context_with_model: graph splits = 2
0.00.136.550 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.136.693 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.136.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.217.714 I main: llama threadpool init, n_threads = 4
0.00.217.752 I 
0.00.217.772 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.217.772 I 
0.00.217.851 I sampler seed: 1234
0.00.217.856 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.217.880 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.217.882 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.217.882 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.050.222 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.02.050.223 I llama_perf_context_print:        load time =     185.68 ms
0.02.050.223 I llama_perf_context_print: prompt eval time =      43.51 ms /     7 tokens (    6.22 ms per token,   160.87 tokens per second)
0.02.050.225 I llama_perf_context_print:        eval time =    1786.04 ms /    63 runs   (   28.35 ms per token,    35.27 tokens per second)
0.02.050.225 I llama_perf_context_print:       total time =    1832.51 ms /    70 tokens
0.02.050.486 I ggml_metal_free: deallocating

real	0m2.362s
user	0m0.145s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.345 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.996 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.014 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.025 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.031 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.032 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.033 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.034 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.039 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.040 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.040 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.507 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.336 I llama_model_loader: - type  f32:  194 tensors
0.00.052.337 I llama_model_loader: - type  f16:   98 tensors
0.00.081.746 I llm_load_vocab: special tokens cache size = 25
0.00.088.190 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.193 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.193 I llm_load_print_meta: arch             = gptneox
0.00.088.193 I llm_load_print_meta: vocab type       = BPE
0.00.088.193 I llm_load_print_meta: n_vocab          = 50304
0.00.088.193 I llm_load_print_meta: n_merges         = 50009
0.00.088.194 I llm_load_print_meta: vocab_only       = 0
0.00.088.194 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.194 I llm_load_print_meta: n_embd           = 2048
0.00.088.194 I llm_load_print_meta: n_layer          = 24
0.00.088.197 I llm_load_print_meta: n_head           = 16
0.00.088.197 I llm_load_print_meta: n_head_kv        = 16
0.00.088.197 I llm_load_print_meta: n_rot            = 32
0.00.088.198 I llm_load_print_meta: n_swa            = 0
0.00.088.198 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.198 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.199 I llm_load_print_meta: n_gqa            = 1
0.00.088.199 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.200 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.200 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.201 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.201 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.201 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.201 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.202 I llm_load_print_meta: n_ff             = 8192
0.00.088.202 I llm_load_print_meta: n_expert         = 0
0.00.088.202 I llm_load_print_meta: n_expert_used    = 0
0.00.088.202 I llm_load_print_meta: causal attn      = 1
0.00.088.202 I llm_load_print_meta: pooling type     = 0
0.00.088.202 I llm_load_print_meta: rope type        = 2
0.00.088.202 I llm_load_print_meta: rope scaling     = linear
0.00.088.203 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.203 I llm_load_print_meta: freq_scale_train = 1
0.00.088.203 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.204 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.204 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.204 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.204 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.204 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.204 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.204 I llm_load_print_meta: model type       = 1.4B
0.00.088.205 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.205 I llm_load_print_meta: model params     = 1.41 B
0.00.088.206 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.206 I llm_load_print_meta: general.name     = 1.4B
0.00.088.206 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.206 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.206 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.206 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.207 I llm_load_print_meta: LF token         = 128 ''
0.00.088.207 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.207 I llm_load_print_meta: max token length = 1024
0.00.090.729 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.729 I llm_load_tensors: offloading output layer to GPU
0.00.090.730 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.740 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.741 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.692 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.693 I llama_new_context_with_model: n_ctx         = 128
0.00.091.694 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.694 I llama_new_context_with_model: n_batch       = 128
0.00.091.694 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.694 I llama_new_context_with_model: flash_attn    = 0
0.00.091.695 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.695 I llama_new_context_with_model: freq_scale    = 1
0.00.091.695 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.696 I ggml_metal_init: allocating
0.00.091.704 I ggml_metal_init: found device: Apple M4
0.00.091.706 I ggml_metal_init: picking default device: Apple M4
0.00.092.334 I ggml_metal_init: using embedded metal library
0.00.094.890 I ggml_metal_init: GPU name:   Apple M4
0.00.094.892 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.893 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.893 I ggml_metal_init: simdgroup reduction   = true
0.00.094.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.893 I ggml_metal_init: has bfloat            = true
0.00.094.893 I ggml_metal_init: use bfloat            = true
0.00.094.894 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.979 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.225 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.226 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.240 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.171 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.172 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.173 I llama_new_context_with_model: graph nodes  = 967
0.00.107.173 I llama_new_context_with_model: graph splits = 2
0.00.107.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.819.010 I 
0.00.819.072 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.819.121 I perplexity: tokenizing the input ..
0.00.833.266 I perplexity: tokenization took 14.141 ms
0.00.833.275 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.954.089 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.955.789 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.955.818 I llama_perf_context_print:        load time =     796.00 ms
0.00.955.819 I llama_perf_context_print: prompt eval time =     119.96 ms /   128 tokens (    0.94 ms per token,  1067.06 tokens per second)
0.00.955.821 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.955.821 I llama_perf_context_print:       total time =     136.81 ms /   129 tokens
0.00.956.581 I ggml_metal_free: deallocating

real	0m1.145s
user	0m0.124s
sys	0m0.185s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.807 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.102 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.797 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.879 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.923 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.925 I llama_model_loader: - type  f32:  194 tensors
0.00.032.925 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.380 I llm_load_vocab: special tokens cache size = 25
0.00.060.302 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.307 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.307 I llm_load_print_meta: arch             = gptneox
0.00.060.311 I llm_load_print_meta: vocab type       = BPE
0.00.060.311 I llm_load_print_meta: n_vocab          = 50304
0.00.060.311 I llm_load_print_meta: n_merges         = 50009
0.00.060.311 I llm_load_print_meta: vocab_only       = 0
0.00.060.312 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.312 I llm_load_print_meta: n_embd           = 2048
0.00.060.312 I llm_load_print_meta: n_layer          = 24
0.00.060.319 I llm_load_print_meta: n_head           = 16
0.00.060.320 I llm_load_print_meta: n_head_kv        = 16
0.00.060.320 I llm_load_print_meta: n_rot            = 32
0.00.060.321 I llm_load_print_meta: n_swa            = 0
0.00.060.321 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.321 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.322 I llm_load_print_meta: n_gqa            = 1
0.00.060.323 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.323 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.324 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.325 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.325 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.325 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.325 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.326 I llm_load_print_meta: n_ff             = 8192
0.00.060.326 I llm_load_print_meta: n_expert         = 0
0.00.060.327 I llm_load_print_meta: n_expert_used    = 0
0.00.060.327 I llm_load_print_meta: causal attn      = 1
0.00.060.327 I llm_load_print_meta: pooling type     = 0
0.00.060.327 I llm_load_print_meta: rope type        = 2
0.00.060.327 I llm_load_print_meta: rope scaling     = linear
0.00.060.328 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.328 I llm_load_print_meta: freq_scale_train = 1
0.00.060.328 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.329 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.329 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.329 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.329 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.329 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.329 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.330 I llm_load_print_meta: model type       = 1.4B
0.00.060.330 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.331 I llm_load_print_meta: model params     = 1.41 B
0.00.060.331 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.332 I llm_load_print_meta: general.name     = 1.4B
0.00.060.332 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.332 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.332 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.332 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.333 I llm_load_print_meta: LF token         = 128 ''
0.00.060.333 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.335 I llm_load_print_meta: max token length = 1024
0.00.062.488 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.489 I llm_load_tensors: offloading output layer to GPU
0.00.062.489 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.495 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.496 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.453 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.454 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.454 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.455 I llama_new_context_with_model: n_batch       = 2048
0.00.063.455 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.455 I llama_new_context_with_model: flash_attn    = 0
0.00.063.455 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.456 I llama_new_context_with_model: freq_scale    = 1
0.00.063.456 I ggml_metal_init: allocating
0.00.063.460 I ggml_metal_init: found device: Apple M4
0.00.063.462 I ggml_metal_init: picking default device: Apple M4
0.00.064.189 I ggml_metal_init: using embedded metal library
0.00.066.768 I ggml_metal_init: GPU name:   Apple M4
0.00.066.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.771 I ggml_metal_init: simdgroup reduction   = true
0.00.066.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.771 I ggml_metal_init: has bfloat            = true
0.00.066.771 I ggml_metal_init: use bfloat            = true
0.00.066.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.188 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.166 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.175 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.211 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.278 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.280 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.281 I llama_new_context_with_model: graph nodes  = 967
0.00.103.281 I llama_new_context_with_model: graph splits = 2
0.00.103.286 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.770.382 I main: llama threadpool init, n_threads = 4
0.01.770.461 I 
0.01.770.514 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.770.515 I 
0.01.771.165 I sampler seed: 1234
0.01.771.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.771.240 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.771.246 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.771.246 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.871.704 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50569.80 tokens per second)
0.02.871.704 I llama_perf_context_print:        load time =    1760.56 ms
0.02.871.705 I llama_perf_context_print: prompt eval time =      50.20 ms /     7 tokens (    7.17 ms per token,   139.45 tokens per second)
0.02.871.706 I llama_perf_context_print:        eval time =    1047.44 ms /    63 runs   (   16.63 ms per token,    60.15 tokens per second)
0.02.871.706 I llama_perf_context_print:       total time =    1101.33 ms /    70 tokens
0.02.871.936 I ggml_metal_free: deallocating

real	0m2.893s
user	0m0.122s
sys	0m0.235s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.138 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.217 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.255 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.261 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.263 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.264 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.265 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.265 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.266 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.267 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.267 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.268 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.268 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.269 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.271 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.272 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.567 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.569 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.569 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.570 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.571 I llama_model_loader: - type  f32:  194 tensors
0.00.033.572 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.628 I llm_load_vocab: special tokens cache size = 25
0.00.064.636 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.639 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.639 I llm_load_print_meta: arch             = gptneox
0.00.064.639 I llm_load_print_meta: vocab type       = BPE
0.00.064.640 I llm_load_print_meta: n_vocab          = 50304
0.00.064.640 I llm_load_print_meta: n_merges         = 50009
0.00.064.640 I llm_load_print_meta: vocab_only       = 0
0.00.064.640 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.640 I llm_load_print_meta: n_embd           = 2048
0.00.064.640 I llm_load_print_meta: n_layer          = 24
0.00.064.645 I llm_load_print_meta: n_head           = 16
0.00.064.645 I llm_load_print_meta: n_head_kv        = 16
0.00.064.645 I llm_load_print_meta: n_rot            = 32
0.00.064.646 I llm_load_print_meta: n_swa            = 0
0.00.064.646 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.646 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.649 I llm_load_print_meta: n_gqa            = 1
0.00.064.649 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.651 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.651 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.652 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.652 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.652 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.652 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.653 I llm_load_print_meta: n_ff             = 8192
0.00.064.653 I llm_load_print_meta: n_expert         = 0
0.00.064.653 I llm_load_print_meta: n_expert_used    = 0
0.00.064.653 I llm_load_print_meta: causal attn      = 1
0.00.064.653 I llm_load_print_meta: pooling type     = 0
0.00.064.653 I llm_load_print_meta: rope type        = 2
0.00.064.654 I llm_load_print_meta: rope scaling     = linear
0.00.064.654 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.654 I llm_load_print_meta: freq_scale_train = 1
0.00.064.654 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.655 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.655 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.655 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.655 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.655 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.655 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.656 I llm_load_print_meta: model type       = 1.4B
0.00.064.656 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.656 I llm_load_print_meta: model params     = 1.41 B
0.00.064.657 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.657 I llm_load_print_meta: general.name     = 1.4B
0.00.064.657 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.657 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.657 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.658 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.658 I llm_load_print_meta: LF token         = 128 ''
0.00.064.658 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.658 I llm_load_print_meta: max token length = 1024
0.00.066.944 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.944 I llm_load_tensors: offloading output layer to GPU
0.00.066.944 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.955 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.956 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.903 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.903 I llama_new_context_with_model: n_ctx         = 128
0.00.067.904 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.904 I llama_new_context_with_model: n_batch       = 128
0.00.067.904 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.904 I llama_new_context_with_model: flash_attn    = 0
0.00.067.905 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.905 I llama_new_context_with_model: freq_scale    = 1
0.00.067.905 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.906 I ggml_metal_init: allocating
0.00.067.912 I ggml_metal_init: found device: Apple M4
0.00.067.914 I ggml_metal_init: picking default device: Apple M4
0.00.068.534 I ggml_metal_init: using embedded metal library
0.00.070.938 I ggml_metal_init: GPU name:   Apple M4
0.00.070.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.940 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.940 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.940 I ggml_metal_init: simdgroup reduction   = true
0.00.070.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.941 I ggml_metal_init: has bfloat            = true
0.00.070.941 I ggml_metal_init: use bfloat            = true
0.00.070.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.942 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.679 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.054 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.059 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.075 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.069 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.082.070 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.082.070 I llama_new_context_with_model: graph nodes  = 967
0.00.082.070 I llama_new_context_with_model: graph splits = 2
0.00.082.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.072 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.483 I 
0.00.817.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.544 I perplexity: tokenizing the input ..
0.00.825.203 I perplexity: tokenization took 7.657 ms
0.00.825.206 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.948.814 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.950.068 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.950.081 I llama_perf_context_print:        load time =     805.26 ms
0.00.950.081 I llama_perf_context_print: prompt eval time =     123.39 ms /   128 tokens (    0.96 ms per token,  1037.39 tokens per second)
0.00.950.082 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.950.083 I llama_perf_context_print:       total time =     132.60 ms /   129 tokens
0.00.950.474 I ggml_metal_free: deallocating

real	0m0.968s
user	0m0.091s
sys	0m0.145s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.014.581 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.743 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.756 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.757 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.757 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.758 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.756 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.053 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.047.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.498 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.499 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.499 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.499 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.500 I llama_model_loader: - type  f32:  194 tensors
0.00.047.500 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.501 I llama_model_loader: - type q6_K:    1 tensors
0.00.086.891 I llm_load_vocab: special tokens cache size = 25
0.00.096.911 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.096.916 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.096.916 I llm_load_print_meta: arch             = gptneox
0.00.096.917 I llm_load_print_meta: vocab type       = BPE
0.00.096.917 I llm_load_print_meta: n_vocab          = 50304
0.00.096.917 I llm_load_print_meta: n_merges         = 50009
0.00.096.917 I llm_load_print_meta: vocab_only       = 0
0.00.096.918 I llm_load_print_meta: n_ctx_train      = 2048
0.00.096.919 I llm_load_print_meta: n_embd           = 2048
0.00.096.919 I llm_load_print_meta: n_layer          = 24
0.00.096.925 I llm_load_print_meta: n_head           = 16
0.00.096.926 I llm_load_print_meta: n_head_kv        = 16
0.00.096.926 I llm_load_print_meta: n_rot            = 32
0.00.096.926 I llm_load_print_meta: n_swa            = 0
0.00.096.927 I llm_load_print_meta: n_embd_head_k    = 128
0.00.096.927 I llm_load_print_meta: n_embd_head_v    = 128
0.00.096.928 I llm_load_print_meta: n_gqa            = 1
0.00.096.928 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.096.929 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.096.930 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.096.932 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.096.933 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.096.933 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.096.933 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.096.934 I llm_load_print_meta: n_ff             = 8192
0.00.096.934 I llm_load_print_meta: n_expert         = 0
0.00.096.934 I llm_load_print_meta: n_expert_used    = 0
0.00.096.934 I llm_load_print_meta: causal attn      = 1
0.00.096.935 I llm_load_print_meta: pooling type     = 0
0.00.096.935 I llm_load_print_meta: rope type        = 2
0.00.096.935 I llm_load_print_meta: rope scaling     = linear
0.00.096.937 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.096.938 I llm_load_print_meta: freq_scale_train = 1
0.00.096.938 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.096.938 I llm_load_print_meta: rope_finetuned   = unknown
0.00.096.938 I llm_load_print_meta: ssm_d_conv       = 0
0.00.096.938 I llm_load_print_meta: ssm_d_inner      = 0
0.00.096.939 I llm_load_print_meta: ssm_d_state      = 0
0.00.096.939 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.096.939 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.096.939 I llm_load_print_meta: model type       = 1.4B
0.00.096.940 I llm_load_print_meta: model ftype      = Q4_0
0.00.096.940 I llm_load_print_meta: model params     = 1.41 B
0.00.096.941 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.096.941 I llm_load_print_meta: general.name     = 1.4B
0.00.096.942 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.096.942 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.096.947 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.096.947 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.096.947 I llm_load_print_meta: LF token         = 128 ''
0.00.096.948 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.096.948 I llm_load_print_meta: max token length = 1024
0.00.099.701 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.099.702 I llm_load_tensors: offloading output layer to GPU
0.00.099.702 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.099.714 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.099.716 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.100.995 I llama_new_context_with_model: n_seq_max     = 1
0.00.100.996 I llama_new_context_with_model: n_ctx         = 2048
0.00.100.997 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.100.997 I llama_new_context_with_model: n_batch       = 2048
0.00.100.997 I llama_new_context_with_model: n_ubatch      = 512
0.00.100.997 I llama_new_context_with_model: flash_attn    = 0
0.00.100.998 I llama_new_context_with_model: freq_base     = 10000.0
0.00.100.998 I llama_new_context_with_model: freq_scale    = 1
0.00.100.999 I ggml_metal_init: allocating
0.00.101.003 I ggml_metal_init: found device: Apple M4
0.00.101.006 I ggml_metal_init: picking default device: Apple M4
0.00.101.928 I ggml_metal_init: using embedded metal library
0.00.105.358 I ggml_metal_init: GPU name:   Apple M4
0.00.105.360 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.105.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.105.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.105.361 I ggml_metal_init: simdgroup reduction   = true
0.00.105.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.105.361 I ggml_metal_init: has bfloat            = true
0.00.105.361 I ggml_metal_init: use bfloat            = true
0.00.105.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.105.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.118.561 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.143.312 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.143.322 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.143.348 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.144.419 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.144.421 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.144.421 I llama_new_context_with_model: graph nodes  = 967
0.00.144.422 I llama_new_context_with_model: graph splits = 2
0.00.144.425 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.144.556 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.791.710 I main: llama threadpool init, n_threads = 4
0.00.791.787 I 
0.00.791.835 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.791.837 I 
0.00.792.106 I sampler seed: 1234
0.00.792.115 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.792.172 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.792.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.792.177 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.485.683 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51636.36 tokens per second)
0.01.485.683 I llama_perf_context_print:        load time =     777.12 ms
0.01.485.684 I llama_perf_context_print: prompt eval time =      50.80 ms /     7 tokens (    7.26 ms per token,   137.80 tokens per second)
0.01.485.684 I llama_perf_context_print:        eval time =     639.54 ms /    63 runs   (   10.15 ms per token,    98.51 tokens per second)
0.01.485.689 I llama_perf_context_print:       total time =     693.98 ms /    70 tokens
0.01.485.921 I ggml_metal_free: deallocating

real	0m1.522s
user	0m0.160s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.737 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.806 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.809 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.811 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.811 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.812 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.601 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.597 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.598 I llama_model_loader: - type  f32:  194 tensors
0.00.024.598 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.598 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.639 I llm_load_vocab: special tokens cache size = 25
0.00.051.704 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.707 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.707 I llm_load_print_meta: arch             = gptneox
0.00.051.707 I llm_load_print_meta: vocab type       = BPE
0.00.051.708 I llm_load_print_meta: n_vocab          = 50304
0.00.051.708 I llm_load_print_meta: n_merges         = 50009
0.00.051.708 I llm_load_print_meta: vocab_only       = 0
0.00.051.708 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.708 I llm_load_print_meta: n_embd           = 2048
0.00.051.709 I llm_load_print_meta: n_layer          = 24
0.00.051.711 I llm_load_print_meta: n_head           = 16
0.00.051.712 I llm_load_print_meta: n_head_kv        = 16
0.00.051.712 I llm_load_print_meta: n_rot            = 32
0.00.051.712 I llm_load_print_meta: n_swa            = 0
0.00.051.713 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.715 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.716 I llm_load_print_meta: n_gqa            = 1
0.00.051.717 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.717 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.718 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.718 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.718 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.719 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.719 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.719 I llm_load_print_meta: n_ff             = 8192
0.00.051.720 I llm_load_print_meta: n_expert         = 0
0.00.051.720 I llm_load_print_meta: n_expert_used    = 0
0.00.051.720 I llm_load_print_meta: causal attn      = 1
0.00.051.720 I llm_load_print_meta: pooling type     = 0
0.00.051.720 I llm_load_print_meta: rope type        = 2
0.00.051.720 I llm_load_print_meta: rope scaling     = linear
0.00.051.721 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.721 I llm_load_print_meta: freq_scale_train = 1
0.00.051.721 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.721 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.722 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.722 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.722 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.722 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.722 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.722 I llm_load_print_meta: model type       = 1.4B
0.00.051.723 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.723 I llm_load_print_meta: model params     = 1.41 B
0.00.051.724 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.724 I llm_load_print_meta: general.name     = 1.4B
0.00.051.724 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.724 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.724 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.725 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.725 I llm_load_print_meta: LF token         = 128 ''
0.00.051.725 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.725 I llm_load_print_meta: max token length = 1024
0.00.053.702 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.702 I llm_load_tensors: offloading output layer to GPU
0.00.053.702 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.713 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.714 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.610 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.611 I llama_new_context_with_model: n_ctx         = 128
0.00.054.612 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.612 I llama_new_context_with_model: n_batch       = 128
0.00.054.612 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.612 I llama_new_context_with_model: flash_attn    = 0
0.00.054.612 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.613 I llama_new_context_with_model: freq_scale    = 1
0.00.054.613 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.614 I ggml_metal_init: allocating
0.00.054.617 I ggml_metal_init: found device: Apple M4
0.00.054.620 I ggml_metal_init: picking default device: Apple M4
0.00.055.168 I ggml_metal_init: using embedded metal library
0.00.057.445 I ggml_metal_init: GPU name:   Apple M4
0.00.057.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.447 I ggml_metal_init: simdgroup reduction   = true
0.00.057.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.447 I ggml_metal_init: has bfloat            = true
0.00.057.448 I ggml_metal_init: use bfloat            = true
0.00.057.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.449 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.966 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.245 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.247 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.260 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.113 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.114 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.115 I llama_new_context_with_model: graph nodes  = 967
0.00.069.115 I llama_new_context_with_model: graph splits = 2
0.00.069.116 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.116 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.595.575 I 
0.00.595.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.595.663 I perplexity: tokenizing the input ..
0.00.603.445 I perplexity: tokenization took 7.779 ms
0.00.603.448 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.726.059 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.727.231 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.727.243 I llama_perf_context_print:        load time =     585.82 ms
0.00.727.245 I llama_perf_context_print: prompt eval time =     122.39 ms /   128 tokens (    0.96 ms per token,  1045.88 tokens per second)
0.00.727.245 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.246 I llama_perf_context_print:       total time =     131.68 ms /   129 tokens
0.00.727.605 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.079s
sys	0m0.100s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.293 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.220 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.224 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.226 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.227 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.228 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.229 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.230 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.202 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.235 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.236 I llama_model_loader: - type  f32:  194 tensors
0.00.026.236 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.236 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.774 I llm_load_vocab: special tokens cache size = 25
0.00.052.801 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.804 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.804 I llm_load_print_meta: arch             = gptneox
0.00.052.805 I llm_load_print_meta: vocab type       = BPE
0.00.052.805 I llm_load_print_meta: n_vocab          = 50304
0.00.052.805 I llm_load_print_meta: n_merges         = 50009
0.00.052.805 I llm_load_print_meta: vocab_only       = 0
0.00.052.805 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.806 I llm_load_print_meta: n_embd           = 2048
0.00.052.806 I llm_load_print_meta: n_layer          = 24
0.00.052.809 I llm_load_print_meta: n_head           = 16
0.00.052.809 I llm_load_print_meta: n_head_kv        = 16
0.00.052.809 I llm_load_print_meta: n_rot            = 32
0.00.052.810 I llm_load_print_meta: n_swa            = 0
0.00.052.810 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.810 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.811 I llm_load_print_meta: n_gqa            = 1
0.00.052.811 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.812 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.813 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.815 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.815 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.816 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.816 I llm_load_print_meta: n_ff             = 8192
0.00.052.816 I llm_load_print_meta: n_expert         = 0
0.00.052.817 I llm_load_print_meta: n_expert_used    = 0
0.00.052.818 I llm_load_print_meta: causal attn      = 1
0.00.052.820 I llm_load_print_meta: pooling type     = 0
0.00.052.820 I llm_load_print_meta: rope type        = 2
0.00.052.820 I llm_load_print_meta: rope scaling     = linear
0.00.052.820 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.821 I llm_load_print_meta: freq_scale_train = 1
0.00.052.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.821 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.822 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.822 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.822 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.822 I llm_load_print_meta: model type       = 1.4B
0.00.052.822 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.827 I llm_load_print_meta: model params     = 1.41 B
0.00.052.827 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.827 I llm_load_print_meta: general.name     = 1.4B
0.00.052.829 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.829 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.829 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.829 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.830 I llm_load_print_meta: LF token         = 128 ''
0.00.052.830 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.830 I llm_load_print_meta: max token length = 1024
0.00.054.768 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.768 I llm_load_tensors: offloading output layer to GPU
0.00.054.768 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.779 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.780 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.675 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.676 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.676 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.676 I llama_new_context_with_model: n_batch       = 2048
0.00.055.676 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.676 I llama_new_context_with_model: flash_attn    = 0
0.00.055.677 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.677 I llama_new_context_with_model: freq_scale    = 1
0.00.055.678 I ggml_metal_init: allocating
0.00.055.685 I ggml_metal_init: found device: Apple M4
0.00.055.687 I ggml_metal_init: picking default device: Apple M4
0.00.056.264 I ggml_metal_init: using embedded metal library
0.00.058.589 I ggml_metal_init: GPU name:   Apple M4
0.00.058.590 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.590 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.591 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.591 I ggml_metal_init: simdgroup reduction   = true
0.00.058.591 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.591 I ggml_metal_init: has bfloat            = true
0.00.058.593 I ggml_metal_init: use bfloat            = true
0.00.058.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.249 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.594 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.599 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.615 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.613 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.614 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.615 I llama_new_context_with_model: graph nodes  = 967
0.00.088.615 I llama_new_context_with_model: graph splits = 2
0.00.088.618 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.752 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.753 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.802 I main: llama threadpool init, n_threads = 4
0.00.698.839 I 
0.00.698.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.862 I 
0.00.699.090 I sampler seed: 1234
0.00.699.095 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.106 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.108 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.108 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.418.399 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64369.90 tokens per second)
0.01.418.400 I llama_perf_context_print:        load time =     688.50 ms
0.01.418.401 I llama_perf_context_print: prompt eval time =      42.55 ms /     7 tokens (    6.08 ms per token,   164.52 tokens per second)
0.01.418.401 I llama_perf_context_print:        eval time =     673.88 ms /    63 runs   (   10.70 ms per token,    93.49 tokens per second)
0.01.418.402 I llama_perf_context_print:       total time =     719.60 ms /    70 tokens
0.01.418.597 I ggml_metal_free: deallocating

real	0m1.438s
user	0m0.110s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.729 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.743 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.534 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.425 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.426 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.426 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.427 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.427 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.427 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.428 I llama_model_loader: - type  f32:  194 tensors
0.00.023.428 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.428 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.761 I llm_load_vocab: special tokens cache size = 25
0.00.049.723 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.725 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.726 I llm_load_print_meta: arch             = gptneox
0.00.049.726 I llm_load_print_meta: vocab type       = BPE
0.00.049.726 I llm_load_print_meta: n_vocab          = 50304
0.00.049.727 I llm_load_print_meta: n_merges         = 50009
0.00.049.727 I llm_load_print_meta: vocab_only       = 0
0.00.049.727 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.727 I llm_load_print_meta: n_embd           = 2048
0.00.049.727 I llm_load_print_meta: n_layer          = 24
0.00.049.730 I llm_load_print_meta: n_head           = 16
0.00.049.731 I llm_load_print_meta: n_head_kv        = 16
0.00.049.731 I llm_load_print_meta: n_rot            = 32
0.00.049.732 I llm_load_print_meta: n_swa            = 0
0.00.049.732 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.732 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.733 I llm_load_print_meta: n_gqa            = 1
0.00.049.733 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.734 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.735 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.735 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.737 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.737 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.738 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.738 I llm_load_print_meta: n_ff             = 8192
0.00.049.738 I llm_load_print_meta: n_expert         = 0
0.00.049.739 I llm_load_print_meta: n_expert_used    = 0
0.00.049.741 I llm_load_print_meta: causal attn      = 1
0.00.049.741 I llm_load_print_meta: pooling type     = 0
0.00.049.741 I llm_load_print_meta: rope type        = 2
0.00.049.741 I llm_load_print_meta: rope scaling     = linear
0.00.049.742 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.742 I llm_load_print_meta: freq_scale_train = 1
0.00.049.742 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.742 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.743 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.743 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.743 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.743 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.743 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.743 I llm_load_print_meta: model type       = 1.4B
0.00.049.748 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.748 I llm_load_print_meta: model params     = 1.41 B
0.00.049.749 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.749 I llm_load_print_meta: general.name     = 1.4B
0.00.049.749 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.749 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: LF token         = 128 ''
0.00.049.750 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: max token length = 1024
0.00.051.735 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.735 I llm_load_tensors: offloading output layer to GPU
0.00.051.736 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.746 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.747 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.645 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.646 I llama_new_context_with_model: n_ctx         = 128
0.00.052.646 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.647 I llama_new_context_with_model: n_batch       = 128
0.00.052.647 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.647 I llama_new_context_with_model: flash_attn    = 0
0.00.052.647 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.648 I llama_new_context_with_model: freq_scale    = 1
0.00.052.648 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.649 I ggml_metal_init: allocating
0.00.052.652 I ggml_metal_init: found device: Apple M4
0.00.052.654 I ggml_metal_init: picking default device: Apple M4
0.00.053.201 I ggml_metal_init: using embedded metal library
0.00.055.521 I ggml_metal_init: GPU name:   Apple M4
0.00.055.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.523 I ggml_metal_init: simdgroup reduction   = true
0.00.055.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.524 I ggml_metal_init: has bfloat            = true
0.00.055.524 I ggml_metal_init: use bfloat            = true
0.00.055.524 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.525 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.154 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.438 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.441 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.454 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.361 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.362 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.362 I llama_new_context_with_model: graph nodes  = 967
0.00.067.362 I llama_new_context_with_model: graph splits = 2
0.00.067.364 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.312 I 
0.00.618.352 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.366 I perplexity: tokenizing the input ..
0.00.625.753 I perplexity: tokenization took 7.386 ms
0.00.625.756 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.461 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.749.705 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.749.724 I llama_perf_context_print:        load time =     609.45 ms
0.00.749.725 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.31 tokens per second)
0.00.749.726 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.749.726 I llama_perf_context_print:       total time =     131.42 ms /   129 tokens
0.00.750.231 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.078s
sys	0m0.093s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.587 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.209 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.214 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.215 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.216 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.218 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.221 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.221 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.221 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.102 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.939 I llama_model_loader: - type  f32:  194 tensors
0.00.024.939 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.173 I llm_load_vocab: special tokens cache size = 25
0.00.051.171 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.174 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.174 I llm_load_print_meta: arch             = gptneox
0.00.051.174 I llm_load_print_meta: vocab type       = BPE
0.00.051.175 I llm_load_print_meta: n_vocab          = 50304
0.00.051.175 I llm_load_print_meta: n_merges         = 50009
0.00.051.175 I llm_load_print_meta: vocab_only       = 0
0.00.051.175 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.175 I llm_load_print_meta: n_embd           = 2048
0.00.051.176 I llm_load_print_meta: n_layer          = 24
0.00.051.178 I llm_load_print_meta: n_head           = 16
0.00.051.179 I llm_load_print_meta: n_head_kv        = 16
0.00.051.179 I llm_load_print_meta: n_rot            = 32
0.00.051.179 I llm_load_print_meta: n_swa            = 0
0.00.051.180 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.180 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.180 I llm_load_print_meta: n_gqa            = 1
0.00.051.181 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.182 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.182 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.183 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.183 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.183 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.183 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.186 I llm_load_print_meta: n_ff             = 8192
0.00.051.186 I llm_load_print_meta: n_expert         = 0
0.00.051.186 I llm_load_print_meta: n_expert_used    = 0
0.00.051.187 I llm_load_print_meta: causal attn      = 1
0.00.051.189 I llm_load_print_meta: pooling type     = 0
0.00.051.189 I llm_load_print_meta: rope type        = 2
0.00.051.189 I llm_load_print_meta: rope scaling     = linear
0.00.051.189 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.190 I llm_load_print_meta: freq_scale_train = 1
0.00.051.190 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.190 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.190 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.190 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.191 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.191 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.191 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.191 I llm_load_print_meta: model type       = 1.4B
0.00.051.191 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.192 I llm_load_print_meta: model params     = 1.41 B
0.00.051.193 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.193 I llm_load_print_meta: general.name     = 1.4B
0.00.051.197 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.197 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.197 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.198 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: LF token         = 128 ''
0.00.051.199 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: max token length = 1024
0.00.052.768 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.768 I llm_load_tensors: offloading output layer to GPU
0.00.052.769 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.778 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.779 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.667 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.667 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.668 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.668 I llama_new_context_with_model: n_batch       = 2048
0.00.053.668 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.668 I llama_new_context_with_model: flash_attn    = 0
0.00.053.669 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.669 I llama_new_context_with_model: freq_scale    = 1
0.00.053.669 I ggml_metal_init: allocating
0.00.053.673 I ggml_metal_init: found device: Apple M4
0.00.053.675 I ggml_metal_init: picking default device: Apple M4
0.00.054.247 I ggml_metal_init: using embedded metal library
0.00.056.538 I ggml_metal_init: GPU name:   Apple M4
0.00.056.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.540 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.541 I ggml_metal_init: simdgroup reduction   = true
0.00.056.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.541 I ggml_metal_init: has bfloat            = true
0.00.056.541 I ggml_metal_init: use bfloat            = true
0.00.056.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.480 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.645 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.653 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.673 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.746 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.748 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.749 I llama_new_context_with_model: graph nodes  = 967
0.00.086.749 I llama_new_context_with_model: graph splits = 2
0.00.086.751 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.900 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.756 I main: llama threadpool init, n_threads = 4
0.00.780.795 I 
0.00.780.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.830 I 
0.00.781.076 I sampler seed: 1234
0.00.781.081 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.092 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.094 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.094 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.566.562 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.566.562 I llama_perf_context_print:        load time =     771.16 ms
0.01.566.563 I llama_perf_context_print: prompt eval time =      43.07 ms /     7 tokens (    6.15 ms per token,   162.53 tokens per second)
0.01.566.564 I llama_perf_context_print:        eval time =     739.38 ms /    63 runs   (   11.74 ms per token,    85.21 tokens per second)
0.01.566.565 I llama_perf_context_print:       total time =     785.81 ms /    70 tokens
0.01.566.767 I ggml_metal_free: deallocating

real	0m1.586s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.088 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.021.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.651 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.651 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.651 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.652 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.653 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.653 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.654 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.654 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.657 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.364 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.364 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.030.365 I llama_model_loader: - type  f32:  194 tensors
0.00.030.365 I llama_model_loader: - type q5_0:   97 tensors
0.00.030.366 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.454 I llm_load_vocab: special tokens cache size = 25
0.00.057.416 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.419 I llm_load_print_meta: arch             = gptneox
0.00.057.419 I llm_load_print_meta: vocab type       = BPE
0.00.057.420 I llm_load_print_meta: n_vocab          = 50304
0.00.057.420 I llm_load_print_meta: n_merges         = 50009
0.00.057.420 I llm_load_print_meta: vocab_only       = 0
0.00.057.420 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.420 I llm_load_print_meta: n_embd           = 2048
0.00.057.420 I llm_load_print_meta: n_layer          = 24
0.00.057.424 I llm_load_print_meta: n_head           = 16
0.00.057.424 I llm_load_print_meta: n_head_kv        = 16
0.00.057.425 I llm_load_print_meta: n_rot            = 32
0.00.057.425 I llm_load_print_meta: n_swa            = 0
0.00.057.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.425 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.426 I llm_load_print_meta: n_gqa            = 1
0.00.057.427 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.427 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.428 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.428 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.428 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.429 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.429 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.429 I llm_load_print_meta: n_ff             = 8192
0.00.057.430 I llm_load_print_meta: n_expert         = 0
0.00.057.430 I llm_load_print_meta: n_expert_used    = 0
0.00.057.430 I llm_load_print_meta: causal attn      = 1
0.00.057.430 I llm_load_print_meta: pooling type     = 0
0.00.057.432 I llm_load_print_meta: rope type        = 2
0.00.057.432 I llm_load_print_meta: rope scaling     = linear
0.00.057.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.433 I llm_load_print_meta: freq_scale_train = 1
0.00.057.433 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.434 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.435 I llm_load_print_meta: model type       = 1.4B
0.00.057.435 I llm_load_print_meta: model ftype      = Q5_0
0.00.057.435 I llm_load_print_meta: model params     = 1.41 B
0.00.057.442 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.057.443 I llm_load_print_meta: general.name     = 1.4B
0.00.057.443 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.444 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.444 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.444 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.444 I llm_load_print_meta: LF token         = 128 ''
0.00.057.445 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.445 I llm_load_print_meta: max token length = 1024
0.00.059.497 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.497 I llm_load_tensors: offloading output layer to GPU
0.00.059.498 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.508 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.059.509 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.060.486 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.487 I llama_new_context_with_model: n_ctx         = 128
0.00.060.487 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.060.487 I llama_new_context_with_model: n_batch       = 128
0.00.060.488 I llama_new_context_with_model: n_ubatch      = 128
0.00.060.488 I llama_new_context_with_model: flash_attn    = 0
0.00.060.488 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.489 I llama_new_context_with_model: freq_scale    = 1
0.00.060.489 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.060.490 I ggml_metal_init: allocating
0.00.060.493 I ggml_metal_init: found device: Apple M4
0.00.060.495 I ggml_metal_init: picking default device: Apple M4
0.00.061.052 I ggml_metal_init: using embedded metal library
0.00.063.390 I ggml_metal_init: GPU name:   Apple M4
0.00.063.391 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.392 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.392 I ggml_metal_init: simdgroup reduction   = true
0.00.063.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.393 I ggml_metal_init: has bfloat            = true
0.00.063.393 I ggml_metal_init: use bfloat            = true
0.00.063.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.346 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.074.624 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.626 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.641 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.075.562 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.075.563 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.075.564 I llama_new_context_with_model: graph nodes  = 967
0.00.075.564 I llama_new_context_with_model: graph splits = 2
0.00.075.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.075.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.788 I 
0.00.719.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.862 I perplexity: tokenizing the input ..
0.00.727.407 I perplexity: tokenization took 7.543 ms
0.00.727.411 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.862.519 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.863.700 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.863.718 I llama_perf_context_print:        load time =     706.69 ms
0.00.863.719 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   949.01 tokens per second)
0.00.863.719 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.863.720 I llama_perf_context_print:       total time =     143.93 ms /   129 tokens
0.00.864.102 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.079s
sys	0m0.105s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.634 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.386 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.393 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.396 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.396 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.396 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.397 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.400 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.305 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.346 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.269 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.271 I llama_model_loader: - type  f32:  194 tensors
0.00.025.271 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.272 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.532 I llm_load_vocab: special tokens cache size = 25
0.00.052.397 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.401 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.401 I llm_load_print_meta: arch             = gptneox
0.00.052.402 I llm_load_print_meta: vocab type       = BPE
0.00.052.402 I llm_load_print_meta: n_vocab          = 50304
0.00.052.402 I llm_load_print_meta: n_merges         = 50009
0.00.052.402 I llm_load_print_meta: vocab_only       = 0
0.00.052.402 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.402 I llm_load_print_meta: n_embd           = 2048
0.00.052.403 I llm_load_print_meta: n_layer          = 24
0.00.052.406 I llm_load_print_meta: n_head           = 16
0.00.052.406 I llm_load_print_meta: n_head_kv        = 16
0.00.052.407 I llm_load_print_meta: n_rot            = 32
0.00.052.407 I llm_load_print_meta: n_swa            = 0
0.00.052.408 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.408 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.409 I llm_load_print_meta: n_gqa            = 1
0.00.052.409 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.410 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.412 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.412 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.413 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.413 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.413 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.414 I llm_load_print_meta: n_ff             = 8192
0.00.052.414 I llm_load_print_meta: n_expert         = 0
0.00.052.414 I llm_load_print_meta: n_expert_used    = 0
0.00.052.414 I llm_load_print_meta: causal attn      = 1
0.00.052.414 I llm_load_print_meta: pooling type     = 0
0.00.052.416 I llm_load_print_meta: rope type        = 2
0.00.052.416 I llm_load_print_meta: rope scaling     = linear
0.00.052.417 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.417 I llm_load_print_meta: freq_scale_train = 1
0.00.052.417 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.417 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.417 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.418 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.418 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.418 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.418 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.418 I llm_load_print_meta: model type       = 1.4B
0.00.052.419 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.419 I llm_load_print_meta: model params     = 1.41 B
0.00.052.419 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.421 I llm_load_print_meta: general.name     = 1.4B
0.00.052.421 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.421 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.421 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.422 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.422 I llm_load_print_meta: LF token         = 128 ''
0.00.052.422 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.424 I llm_load_print_meta: max token length = 1024
0.00.054.433 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.433 I llm_load_tensors: offloading output layer to GPU
0.00.054.434 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.444 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.445 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.368 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.368 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.369 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.369 I llama_new_context_with_model: n_batch       = 2048
0.00.055.369 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.369 I llama_new_context_with_model: flash_attn    = 0
0.00.055.370 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.370 I llama_new_context_with_model: freq_scale    = 1
0.00.055.371 I ggml_metal_init: allocating
0.00.055.377 I ggml_metal_init: found device: Apple M4
0.00.055.379 I ggml_metal_init: picking default device: Apple M4
0.00.055.976 I ggml_metal_init: using embedded metal library
0.00.058.327 I ggml_metal_init: GPU name:   Apple M4
0.00.058.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.329 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.329 I ggml_metal_init: simdgroup reduction   = true
0.00.058.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.329 I ggml_metal_init: has bfloat            = true
0.00.058.330 I ggml_metal_init: use bfloat            = true
0.00.058.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.911 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.146 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.151 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.169 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.188 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.189 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.190 I llama_new_context_with_model: graph nodes  = 967
0.00.088.190 I llama_new_context_with_model: graph splits = 2
0.00.088.193 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.352 I main: llama threadpool init, n_threads = 4
0.00.757.393 I 
0.00.757.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.434 I 
0.00.757.686 I sampler seed: 1234
0.00.757.691 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.757.732 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.757.734 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.757.734 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.597.170 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.597.171 I llama_perf_context_print:        load time =     747.71 ms
0.01.597.172 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.68 tokens per second)
0.01.597.173 I llama_perf_context_print:        eval time =     794.11 ms /    63 runs   (   12.60 ms per token,    79.33 tokens per second)
0.01.597.174 I llama_perf_context_print:       total time =     839.82 ms /    70 tokens
0.01.597.420 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.399 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.316 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.325 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.326 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.328 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.329 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.331 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.182 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.122 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.123 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.123 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.124 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.124 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.124 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.125 I llama_model_loader: - type  f32:  194 tensors
0.00.023.125 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.125 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.320 I llm_load_vocab: special tokens cache size = 25
0.00.050.239 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.242 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.242 I llm_load_print_meta: arch             = gptneox
0.00.050.243 I llm_load_print_meta: vocab type       = BPE
0.00.050.243 I llm_load_print_meta: n_vocab          = 50304
0.00.050.243 I llm_load_print_meta: n_merges         = 50009
0.00.050.243 I llm_load_print_meta: vocab_only       = 0
0.00.050.243 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.244 I llm_load_print_meta: n_embd           = 2048
0.00.050.244 I llm_load_print_meta: n_layer          = 24
0.00.050.247 I llm_load_print_meta: n_head           = 16
0.00.050.248 I llm_load_print_meta: n_head_kv        = 16
0.00.050.248 I llm_load_print_meta: n_rot            = 32
0.00.050.248 I llm_load_print_meta: n_swa            = 0
0.00.050.248 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.248 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.249 I llm_load_print_meta: n_gqa            = 1
0.00.050.250 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.250 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.251 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.251 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.251 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.251 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.252 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.252 I llm_load_print_meta: n_ff             = 8192
0.00.050.254 I llm_load_print_meta: n_expert         = 0
0.00.050.254 I llm_load_print_meta: n_expert_used    = 0
0.00.050.254 I llm_load_print_meta: causal attn      = 1
0.00.050.254 I llm_load_print_meta: pooling type     = 0
0.00.050.254 I llm_load_print_meta: rope type        = 2
0.00.050.255 I llm_load_print_meta: rope scaling     = linear
0.00.050.255 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.255 I llm_load_print_meta: freq_scale_train = 1
0.00.050.256 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.256 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.256 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.263 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.263 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.264 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.264 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.264 I llm_load_print_meta: model type       = 1.4B
0.00.050.266 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.266 I llm_load_print_meta: model params     = 1.41 B
0.00.050.267 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.267 I llm_load_print_meta: general.name     = 1.4B
0.00.050.267 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.267 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.268 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.268 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.268 I llm_load_print_meta: LF token         = 128 ''
0.00.050.268 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.268 I llm_load_print_meta: max token length = 1024
0.00.052.291 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.291 I llm_load_tensors: offloading output layer to GPU
0.00.052.291 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.302 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.303 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.175 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.176 I llama_new_context_with_model: n_ctx         = 128
0.00.053.176 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.176 I llama_new_context_with_model: n_batch       = 128
0.00.053.176 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.177 I llama_new_context_with_model: flash_attn    = 0
0.00.053.177 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.177 I llama_new_context_with_model: freq_scale    = 1
0.00.053.178 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.178 I ggml_metal_init: allocating
0.00.053.184 I ggml_metal_init: found device: Apple M4
0.00.053.186 I ggml_metal_init: picking default device: Apple M4
0.00.053.725 I ggml_metal_init: using embedded metal library
0.00.056.055 I ggml_metal_init: GPU name:   Apple M4
0.00.056.056 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.057 I ggml_metal_init: simdgroup reduction   = true
0.00.056.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.057 I ggml_metal_init: has bfloat            = true
0.00.056.057 I ggml_metal_init: use bfloat            = true
0.00.056.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.058 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.394 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.651 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.655 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.668 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.507 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.508 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.508 I llama_new_context_with_model: graph nodes  = 967
0.00.067.509 I llama_new_context_with_model: graph splits = 2
0.00.067.510 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.006 I 
0.00.726.047 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.063 I perplexity: tokenizing the input ..
0.00.733.526 I perplexity: tokenization took 7.462 ms
0.00.733.529 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.868.018 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.869.419 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.869.441 I llama_perf_context_print:        load time =     717.60 ms
0.00.869.443 I llama_perf_context_print: prompt eval time =     134.23 ms /   128 tokens (    1.05 ms per token,   953.59 tokens per second)
0.00.869.444 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.869.444 I llama_perf_context_print:       total time =     143.44 ms /   129 tokens
0.00.869.863 I ggml_metal_free: deallocating

real	0m0.884s
user	0m0.078s
sys	0m0.126s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.422 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.427 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.428 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.429 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.429 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.430 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.430 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.433 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.434 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.295 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.093 I llama_model_loader: - type  f32:  194 tensors
0.00.024.094 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.094 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.094 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.172 I llm_load_vocab: special tokens cache size = 25
0.00.051.147 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.150 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.150 I llm_load_print_meta: arch             = gptneox
0.00.051.151 I llm_load_print_meta: vocab type       = BPE
0.00.051.151 I llm_load_print_meta: n_vocab          = 50304
0.00.051.151 I llm_load_print_meta: n_merges         = 50009
0.00.051.151 I llm_load_print_meta: vocab_only       = 0
0.00.051.152 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.152 I llm_load_print_meta: n_embd           = 2048
0.00.051.152 I llm_load_print_meta: n_layer          = 24
0.00.051.155 I llm_load_print_meta: n_head           = 16
0.00.051.156 I llm_load_print_meta: n_head_kv        = 16
0.00.051.156 I llm_load_print_meta: n_rot            = 32
0.00.051.156 I llm_load_print_meta: n_swa            = 0
0.00.051.156 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.156 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.157 I llm_load_print_meta: n_gqa            = 1
0.00.051.158 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.159 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.159 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.160 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.160 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.160 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.160 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.161 I llm_load_print_meta: n_ff             = 8192
0.00.051.161 I llm_load_print_meta: n_expert         = 0
0.00.051.161 I llm_load_print_meta: n_expert_used    = 0
0.00.051.162 I llm_load_print_meta: causal attn      = 1
0.00.051.162 I llm_load_print_meta: pooling type     = 0
0.00.051.162 I llm_load_print_meta: rope type        = 2
0.00.051.162 I llm_load_print_meta: rope scaling     = linear
0.00.051.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.165 I llm_load_print_meta: freq_scale_train = 1
0.00.051.165 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.165 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.166 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.166 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.166 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.166 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.166 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.166 I llm_load_print_meta: model type       = 1.4B
0.00.051.167 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.167 I llm_load_print_meta: model params     = 1.41 B
0.00.051.168 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.168 I llm_load_print_meta: general.name     = 1.4B
0.00.051.168 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.168 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.168 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.168 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.169 I llm_load_print_meta: LF token         = 128 ''
0.00.051.169 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.169 I llm_load_print_meta: max token length = 1024
0.00.053.086 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.087 I llm_load_tensors: offloading output layer to GPU
0.00.053.087 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.098 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.099 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.028 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.029 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.029 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.029 I llama_new_context_with_model: n_batch       = 2048
0.00.054.029 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.030 I llama_new_context_with_model: flash_attn    = 0
0.00.054.030 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.030 I llama_new_context_with_model: freq_scale    = 1
0.00.054.031 I ggml_metal_init: allocating
0.00.054.034 I ggml_metal_init: found device: Apple M4
0.00.054.036 I ggml_metal_init: picking default device: Apple M4
0.00.054.623 I ggml_metal_init: using embedded metal library
0.00.057.045 I ggml_metal_init: GPU name:   Apple M4
0.00.057.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.047 I ggml_metal_init: simdgroup reduction   = true
0.00.057.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.047 I ggml_metal_init: has bfloat            = true
0.00.057.048 I ggml_metal_init: use bfloat            = true
0.00.057.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.177 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.512 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.518 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.537 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.627 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.629 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.629 I llama_new_context_with_model: graph nodes  = 967
0.00.087.629 I llama_new_context_with_model: graph splits = 2
0.00.087.632 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.762 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.492.428 I main: llama threadpool init, n_threads = 4
0.00.492.468 I 
0.00.492.488 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.492.488 I 
0.00.492.650 I sampler seed: 1234
0.00.492.655 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.492.673 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.492.673 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.492.673 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.167.671 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.167.672 I llama_perf_context_print:        load time =     482.67 ms
0.01.167.675 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.82 tokens per second)
0.01.167.675 I llama_perf_context_print:        eval time =     636.50 ms /    63 runs   (   10.10 ms per token,    98.98 tokens per second)
0.01.167.676 I llama_perf_context_print:       total time =     675.25 ms /    70 tokens
0.01.167.924 I ggml_metal_free: deallocating

real	0m1.186s
user	0m0.111s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.125 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.824 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.829 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.835 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.836 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.836 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.836 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.837 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.838 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.838 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.839 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.840 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.840 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.843 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.681 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.777 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.779 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.779 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.779 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.780 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.780 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.781 I llama_model_loader: - type  f32:  194 tensors
0.00.024.781 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.781 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.781 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.265 I llm_load_vocab: special tokens cache size = 25
0.00.052.387 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.392 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.392 I llm_load_print_meta: arch             = gptneox
0.00.052.393 I llm_load_print_meta: vocab type       = BPE
0.00.052.393 I llm_load_print_meta: n_vocab          = 50304
0.00.052.395 I llm_load_print_meta: n_merges         = 50009
0.00.052.396 I llm_load_print_meta: vocab_only       = 0
0.00.052.396 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.396 I llm_load_print_meta: n_embd           = 2048
0.00.052.396 I llm_load_print_meta: n_layer          = 24
0.00.052.400 I llm_load_print_meta: n_head           = 16
0.00.052.401 I llm_load_print_meta: n_head_kv        = 16
0.00.052.401 I llm_load_print_meta: n_rot            = 32
0.00.052.402 I llm_load_print_meta: n_swa            = 0
0.00.052.402 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.403 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.404 I llm_load_print_meta: n_gqa            = 1
0.00.052.405 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.410 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.410 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.410 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.410 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.411 I llm_load_print_meta: n_ff             = 8192
0.00.052.411 I llm_load_print_meta: n_expert         = 0
0.00.052.412 I llm_load_print_meta: n_expert_used    = 0
0.00.052.412 I llm_load_print_meta: causal attn      = 1
0.00.052.412 I llm_load_print_meta: pooling type     = 0
0.00.052.412 I llm_load_print_meta: rope type        = 2
0.00.052.412 I llm_load_print_meta: rope scaling     = linear
0.00.052.414 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.417 I llm_load_print_meta: freq_scale_train = 1
0.00.052.417 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.418 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.418 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.418 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.418 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.418 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.418 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.419 I llm_load_print_meta: model type       = 1.4B
0.00.052.419 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.420 I llm_load_print_meta: model params     = 1.41 B
0.00.052.420 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.420 I llm_load_print_meta: general.name     = 1.4B
0.00.052.421 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.423 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.423 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.423 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.423 I llm_load_print_meta: LF token         = 128 ''
0.00.052.423 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.424 I llm_load_print_meta: max token length = 1024
0.00.054.199 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.199 I llm_load_tensors: offloading output layer to GPU
0.00.054.199 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.210 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.211 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.129 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.130 I llama_new_context_with_model: n_ctx         = 128
0.00.055.130 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.131 I llama_new_context_with_model: n_batch       = 128
0.00.055.131 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.131 I llama_new_context_with_model: flash_attn    = 0
0.00.055.131 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.132 I llama_new_context_with_model: freq_scale    = 1
0.00.055.132 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.133 I ggml_metal_init: allocating
0.00.055.139 I ggml_metal_init: found device: Apple M4
0.00.055.142 I ggml_metal_init: picking default device: Apple M4
0.00.055.765 I ggml_metal_init: using embedded metal library
0.00.058.108 I ggml_metal_init: GPU name:   Apple M4
0.00.058.110 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.110 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.111 I ggml_metal_init: simdgroup reduction   = true
0.00.058.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.111 I ggml_metal_init: has bfloat            = true
0.00.058.111 I ggml_metal_init: use bfloat            = true
0.00.058.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.284 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.542 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.544 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.571 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.512 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.513 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.514 I llama_new_context_with_model: graph nodes  = 967
0.00.069.514 I llama_new_context_with_model: graph splits = 2
0.00.069.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.456.170 I 
0.00.456.226 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.456.247 I perplexity: tokenizing the input ..
0.00.471.140 I perplexity: tokenization took 14.886 ms
0.00.471.164 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.222 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.623.080 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.623.104 I llama_perf_context_print:        load time =     446.04 ms
0.00.623.111 I llama_perf_context_print: prompt eval time =     147.03 ms /   128 tokens (    1.15 ms per token,   870.60 tokens per second)
0.00.623.113 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.623.113 I llama_perf_context_print:       total time =     166.94 ms /   129 tokens
0.00.623.846 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.102s
sys	0m0.066s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.806 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.313 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.327 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.328 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.328 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.328 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.329 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.330 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.330 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.330 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.331 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.331 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.333 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.335 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.336 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.336 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.182 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.281 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.203 I llama_model_loader: - type  f32:  194 tensors
0.00.024.203 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.203 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.204 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.204 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.938 I llm_load_vocab: special tokens cache size = 25
0.00.051.937 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.941 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.942 I llm_load_print_meta: arch             = gptneox
0.00.051.942 I llm_load_print_meta: vocab type       = BPE
0.00.051.942 I llm_load_print_meta: n_vocab          = 50304
0.00.051.942 I llm_load_print_meta: n_merges         = 50009
0.00.051.943 I llm_load_print_meta: vocab_only       = 0
0.00.051.943 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.943 I llm_load_print_meta: n_embd           = 2048
0.00.051.943 I llm_load_print_meta: n_layer          = 24
0.00.051.947 I llm_load_print_meta: n_head           = 16
0.00.051.948 I llm_load_print_meta: n_head_kv        = 16
0.00.051.948 I llm_load_print_meta: n_rot            = 32
0.00.051.949 I llm_load_print_meta: n_swa            = 0
0.00.051.949 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.949 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.950 I llm_load_print_meta: n_gqa            = 1
0.00.051.950 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.952 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.953 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.953 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.953 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.953 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.954 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.954 I llm_load_print_meta: n_ff             = 8192
0.00.051.954 I llm_load_print_meta: n_expert         = 0
0.00.051.954 I llm_load_print_meta: n_expert_used    = 0
0.00.051.955 I llm_load_print_meta: causal attn      = 1
0.00.051.955 I llm_load_print_meta: pooling type     = 0
0.00.051.955 I llm_load_print_meta: rope type        = 2
0.00.051.956 I llm_load_print_meta: rope scaling     = linear
0.00.051.957 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.957 I llm_load_print_meta: freq_scale_train = 1
0.00.051.957 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.957 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.958 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.958 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.958 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.958 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.958 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.958 I llm_load_print_meta: model type       = 1.4B
0.00.051.959 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.959 I llm_load_print_meta: model params     = 1.41 B
0.00.051.960 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.961 I llm_load_print_meta: general.name     = 1.4B
0.00.051.961 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.961 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.961 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.961 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.962 I llm_load_print_meta: LF token         = 128 ''
0.00.051.962 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.962 I llm_load_print_meta: max token length = 1024
0.00.053.916 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.916 I llm_load_tensors: offloading output layer to GPU
0.00.053.916 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.927 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.928 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.840 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.840 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.841 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.841 I llama_new_context_with_model: n_batch       = 2048
0.00.054.841 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.841 I llama_new_context_with_model: flash_attn    = 0
0.00.054.842 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.842 I llama_new_context_with_model: freq_scale    = 1
0.00.054.843 I ggml_metal_init: allocating
0.00.054.846 I ggml_metal_init: found device: Apple M4
0.00.054.848 I ggml_metal_init: picking default device: Apple M4
0.00.055.466 I ggml_metal_init: using embedded metal library
0.00.057.874 I ggml_metal_init: GPU name:   Apple M4
0.00.057.876 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.876 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.877 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.877 I ggml_metal_init: simdgroup reduction   = true
0.00.057.877 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.877 I ggml_metal_init: has bfloat            = true
0.00.057.877 I ggml_metal_init: use bfloat            = true
0.00.057.878 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.879 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.069 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.475 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.481 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.501 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.475 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.477 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.477 I llama_new_context_with_model: graph nodes  = 967
0.00.088.478 I llama_new_context_with_model: graph splits = 2
0.00.088.481 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.621 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.623 I main: llama threadpool init, n_threads = 4
0.00.565.661 I 
0.00.565.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.565.699 I 
0.00.565.934 I sampler seed: 1234
0.00.565.939 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.565.949 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.565.950 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.565.951 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.305.289 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62500.00 tokens per second)
0.01.305.290 I llama_perf_context_print:        load time =     556.81 ms
0.01.305.290 I llama_perf_context_print: prompt eval time =      40.65 ms /     7 tokens (    5.81 ms per token,   172.18 tokens per second)
0.01.305.291 I llama_perf_context_print:        eval time =     695.80 ms /    63 runs   (   11.04 ms per token,    90.54 tokens per second)
0.01.305.291 I llama_perf_context_print:       total time =     739.67 ms /    70 tokens
0.01.305.532 I ggml_metal_free: deallocating

real	0m1.323s
user	0m0.112s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.190 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.249 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.260 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.261 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.262 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.262 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.262 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.263 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.263 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.263 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.264 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.266 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.266 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.290 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.291 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.292 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.292 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.292 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.293 I llama_model_loader: - type  f32:  194 tensors
0.00.024.293 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.293 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.294 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.294 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.850 I llm_load_vocab: special tokens cache size = 25
0.00.052.058 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.064 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.064 I llm_load_print_meta: arch             = gptneox
0.00.052.064 I llm_load_print_meta: vocab type       = BPE
0.00.052.065 I llm_load_print_meta: n_vocab          = 50304
0.00.052.067 I llm_load_print_meta: n_merges         = 50009
0.00.052.067 I llm_load_print_meta: vocab_only       = 0
0.00.052.067 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.067 I llm_load_print_meta: n_embd           = 2048
0.00.052.067 I llm_load_print_meta: n_layer          = 24
0.00.052.071 I llm_load_print_meta: n_head           = 16
0.00.052.072 I llm_load_print_meta: n_head_kv        = 16
0.00.052.072 I llm_load_print_meta: n_rot            = 32
0.00.052.073 I llm_load_print_meta: n_swa            = 0
0.00.052.073 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.073 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.073 I llm_load_print_meta: n_gqa            = 1
0.00.052.074 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.075 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.075 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.075 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.076 I llm_load_print_meta: n_ff             = 8192
0.00.052.076 I llm_load_print_meta: n_expert         = 0
0.00.052.077 I llm_load_print_meta: n_expert_used    = 0
0.00.052.077 I llm_load_print_meta: causal attn      = 1
0.00.052.077 I llm_load_print_meta: pooling type     = 0
0.00.052.077 I llm_load_print_meta: rope type        = 2
0.00.052.079 I llm_load_print_meta: rope scaling     = linear
0.00.052.080 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.080 I llm_load_print_meta: freq_scale_train = 1
0.00.052.080 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.080 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.081 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.081 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.081 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.081 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.082 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.082 I llm_load_print_meta: model type       = 1.4B
0.00.052.083 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.083 I llm_load_print_meta: model params     = 1.41 B
0.00.052.084 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.084 I llm_load_print_meta: general.name     = 1.4B
0.00.052.084 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.086 I llm_load_print_meta: LF token         = 128 ''
0.00.052.086 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.086 I llm_load_print_meta: max token length = 1024
0.00.053.941 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.942 I llm_load_tensors: offloading output layer to GPU
0.00.053.942 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.953 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.954 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.966 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.967 I llama_new_context_with_model: n_ctx         = 128
0.00.054.967 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.967 I llama_new_context_with_model: n_batch       = 128
0.00.054.967 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.968 I llama_new_context_with_model: flash_attn    = 0
0.00.054.968 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.968 I llama_new_context_with_model: freq_scale    = 1
0.00.054.969 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.969 I ggml_metal_init: allocating
0.00.054.973 I ggml_metal_init: found device: Apple M4
0.00.054.975 I ggml_metal_init: picking default device: Apple M4
0.00.055.604 I ggml_metal_init: using embedded metal library
0.00.058.073 I ggml_metal_init: GPU name:   Apple M4
0.00.058.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.076 I ggml_metal_init: simdgroup reduction   = true
0.00.058.077 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.078 I ggml_metal_init: has bfloat            = true
0.00.058.078 I ggml_metal_init: use bfloat            = true
0.00.058.078 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.079 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.085 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.434 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.437 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.452 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.368 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.369 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.369 I llama_new_context_with_model: graph nodes  = 967
0.00.069.370 I llama_new_context_with_model: graph splits = 2
0.00.069.371 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.901 I 
0.00.506.936 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.958 I perplexity: tokenizing the input ..
0.00.515.352 I perplexity: tokenization took 8.392 ms
0.00.515.355 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.649.284 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.650.497 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.650.513 I llama_perf_context_print:        load time =     497.71 ms
0.00.650.514 I llama_perf_context_print: prompt eval time =     133.71 ms /   128 tokens (    1.04 ms per token,   957.30 tokens per second)
0.00.650.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.650.515 I llama_perf_context_print:       total time =     143.61 ms /   129 tokens
0.00.650.837 I ggml_metal_free: deallocating

real	0m0.671s
user	0m0.082s
sys	0m0.082s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.236 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.704 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.710 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.711 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.715 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.720 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.721 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.721 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.662 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.458 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.459 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.459 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.460 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.460 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.460 I llama_model_loader: - type  f32:  194 tensors
0.00.025.461 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.461 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.461 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.548 I llm_load_vocab: special tokens cache size = 25
0.00.052.458 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.461 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.461 I llm_load_print_meta: arch             = gptneox
0.00.052.462 I llm_load_print_meta: vocab type       = BPE
0.00.052.462 I llm_load_print_meta: n_vocab          = 50304
0.00.052.462 I llm_load_print_meta: n_merges         = 50009
0.00.052.462 I llm_load_print_meta: vocab_only       = 0
0.00.052.463 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.463 I llm_load_print_meta: n_embd           = 2048
0.00.052.463 I llm_load_print_meta: n_layer          = 24
0.00.052.466 I llm_load_print_meta: n_head           = 16
0.00.052.467 I llm_load_print_meta: n_head_kv        = 16
0.00.052.467 I llm_load_print_meta: n_rot            = 32
0.00.052.467 I llm_load_print_meta: n_swa            = 0
0.00.052.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.468 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.468 I llm_load_print_meta: n_gqa            = 1
0.00.052.469 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.470 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.470 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.473 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.473 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.473 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.473 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.474 I llm_load_print_meta: n_ff             = 8192
0.00.052.474 I llm_load_print_meta: n_expert         = 0
0.00.052.475 I llm_load_print_meta: n_expert_used    = 0
0.00.052.475 I llm_load_print_meta: causal attn      = 1
0.00.052.475 I llm_load_print_meta: pooling type     = 0
0.00.052.475 I llm_load_print_meta: rope type        = 2
0.00.052.475 I llm_load_print_meta: rope scaling     = linear
0.00.052.476 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.476 I llm_load_print_meta: freq_scale_train = 1
0.00.052.478 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.478 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.478 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.478 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.478 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.478 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.479 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.479 I llm_load_print_meta: model type       = 1.4B
0.00.052.479 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.480 I llm_load_print_meta: model params     = 1.41 B
0.00.052.480 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.480 I llm_load_print_meta: general.name     = 1.4B
0.00.052.485 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.485 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.486 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.486 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.487 I llm_load_print_meta: LF token         = 128 ''
0.00.052.487 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.487 I llm_load_print_meta: max token length = 1024
0.00.054.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.151 I llm_load_tensors: offloading output layer to GPU
0.00.054.151 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.161 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.162 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.030 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.030 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.031 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.031 I llama_new_context_with_model: n_batch       = 2048
0.00.055.031 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.031 I llama_new_context_with_model: flash_attn    = 0
0.00.055.032 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.032 I llama_new_context_with_model: freq_scale    = 1
0.00.055.033 I ggml_metal_init: allocating
0.00.055.039 I ggml_metal_init: found device: Apple M4
0.00.055.042 I ggml_metal_init: picking default device: Apple M4
0.00.055.634 I ggml_metal_init: using embedded metal library
0.00.057.946 I ggml_metal_init: GPU name:   Apple M4
0.00.057.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.948 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.949 I ggml_metal_init: simdgroup reduction   = true
0.00.057.949 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.949 I ggml_metal_init: has bfloat            = true
0.00.057.949 I ggml_metal_init: use bfloat            = true
0.00.057.949 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.454 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.947 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.953 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.970 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.004 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.005 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.006 I llama_new_context_with_model: graph nodes  = 967
0.00.088.006 I llama_new_context_with_model: graph splits = 2
0.00.088.009 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.231 I main: llama threadpool init, n_threads = 4
0.00.783.270 I 
0.00.783.309 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.310 I 
0.00.783.472 I sampler seed: 1234
0.00.783.478 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.490 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.490 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.577.575 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.577.576 I llama_perf_context_print:        load time =     772.99 ms
0.01.577.576 I llama_perf_context_print: prompt eval time =      47.15 ms /     7 tokens (    6.74 ms per token,   148.46 tokens per second)
0.01.577.577 I llama_perf_context_print:        eval time =     743.75 ms /    63 runs   (   11.81 ms per token,    84.71 tokens per second)
0.01.577.577 I llama_perf_context_print:       total time =     794.35 ms /    70 tokens
0.01.577.820 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.111s
sys	0m0.295s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.898 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.795 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.031.800 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.801 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.802 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.811 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.956 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.040.956 I llama_model_loader: - type  f32:  194 tensors
0.00.040.956 I llama_model_loader: - type q4_K:   61 tensors
0.00.040.957 I llama_model_loader: - type q5_K:   24 tensors
0.00.040.957 I llama_model_loader: - type q6_K:   13 tensors
0.00.065.309 I llm_load_vocab: special tokens cache size = 25
0.00.072.777 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.780 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.780 I llm_load_print_meta: arch             = gptneox
0.00.072.780 I llm_load_print_meta: vocab type       = BPE
0.00.072.780 I llm_load_print_meta: n_vocab          = 50304
0.00.072.781 I llm_load_print_meta: n_merges         = 50009
0.00.072.781 I llm_load_print_meta: vocab_only       = 0
0.00.072.781 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.781 I llm_load_print_meta: n_embd           = 2048
0.00.072.781 I llm_load_print_meta: n_layer          = 24
0.00.072.784 I llm_load_print_meta: n_head           = 16
0.00.072.785 I llm_load_print_meta: n_head_kv        = 16
0.00.072.785 I llm_load_print_meta: n_rot            = 32
0.00.072.785 I llm_load_print_meta: n_swa            = 0
0.00.072.787 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.787 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.789 I llm_load_print_meta: n_gqa            = 1
0.00.072.790 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.790 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.791 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.791 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.792 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.792 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.792 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.793 I llm_load_print_meta: n_ff             = 8192
0.00.072.793 I llm_load_print_meta: n_expert         = 0
0.00.072.793 I llm_load_print_meta: n_expert_used    = 0
0.00.072.793 I llm_load_print_meta: causal attn      = 1
0.00.072.793 I llm_load_print_meta: pooling type     = 0
0.00.072.793 I llm_load_print_meta: rope type        = 2
0.00.072.795 I llm_load_print_meta: rope scaling     = linear
0.00.072.797 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.797 I llm_load_print_meta: freq_scale_train = 1
0.00.072.797 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.797 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.798 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.798 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.798 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.798 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.798 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.798 I llm_load_print_meta: model type       = 1.4B
0.00.072.799 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.072.799 I llm_load_print_meta: model params     = 1.41 B
0.00.072.800 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.072.800 I llm_load_print_meta: general.name     = 1.4B
0.00.072.800 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.800 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.800 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.801 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.801 I llm_load_print_meta: LF token         = 128 ''
0.00.072.804 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.805 I llm_load_print_meta: max token length = 1024
0.00.074.994 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.994 I llm_load_tensors: offloading output layer to GPU
0.00.074.994 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.004 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.075.006 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.076.013 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.014 I llama_new_context_with_model: n_ctx         = 128
0.00.076.014 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.076.014 I llama_new_context_with_model: n_batch       = 128
0.00.076.014 I llama_new_context_with_model: n_ubatch      = 128
0.00.076.015 I llama_new_context_with_model: flash_attn    = 0
0.00.076.015 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.015 I llama_new_context_with_model: freq_scale    = 1
0.00.076.016 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.076.016 I ggml_metal_init: allocating
0.00.076.019 I ggml_metal_init: found device: Apple M4
0.00.076.021 I ggml_metal_init: picking default device: Apple M4
0.00.076.672 I ggml_metal_init: using embedded metal library
0.00.079.510 I ggml_metal_init: GPU name:   Apple M4
0.00.079.512 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.513 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.513 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.513 I ggml_metal_init: simdgroup reduction   = true
0.00.079.513 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.514 I ggml_metal_init: has bfloat            = true
0.00.079.515 I ggml_metal_init: use bfloat            = true
0.00.079.515 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.516 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.153 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.789 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.093.793 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.093.811 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.978 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.094.979 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.094.980 I llama_new_context_with_model: graph nodes  = 967
0.00.094.980 I llama_new_context_with_model: graph splits = 2
0.00.094.981 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.094.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.299 I 
0.00.609.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.340 I perplexity: tokenizing the input ..
0.00.616.805 I perplexity: tokenization took 7.465 ms
0.00.616.809 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.071 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.752.374 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.752.392 I llama_perf_context_print:        load time =     597.40 ms
0.00.752.393 I llama_perf_context_print: prompt eval time =     134.02 ms /   128 tokens (    1.05 ms per token,   955.11 tokens per second)
0.00.752.393 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.752.394 I llama_perf_context_print:       total time =     143.09 ms /   129 tokens
0.00.752.960 I ggml_metal_free: deallocating

real	0m0.769s
user	0m0.088s
sys	0m0.097s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.987 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.812 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.601 I llama_model_loader: - type  f32:  194 tensors
0.00.025.601 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.601 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.932 I llm_load_vocab: special tokens cache size = 25
0.00.051.871 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.874 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.875 I llm_load_print_meta: arch             = gptneox
0.00.051.875 I llm_load_print_meta: vocab type       = BPE
0.00.051.875 I llm_load_print_meta: n_vocab          = 50304
0.00.051.875 I llm_load_print_meta: n_merges         = 50009
0.00.051.876 I llm_load_print_meta: vocab_only       = 0
0.00.051.876 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.876 I llm_load_print_meta: n_embd           = 2048
0.00.051.876 I llm_load_print_meta: n_layer          = 24
0.00.051.879 I llm_load_print_meta: n_head           = 16
0.00.051.881 I llm_load_print_meta: n_head_kv        = 16
0.00.051.881 I llm_load_print_meta: n_rot            = 32
0.00.051.881 I llm_load_print_meta: n_swa            = 0
0.00.051.881 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.881 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.882 I llm_load_print_meta: n_gqa            = 1
0.00.051.883 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.883 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.884 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.885 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.886 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.886 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.887 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.887 I llm_load_print_meta: n_ff             = 8192
0.00.051.887 I llm_load_print_meta: n_expert         = 0
0.00.051.887 I llm_load_print_meta: n_expert_used    = 0
0.00.051.888 I llm_load_print_meta: causal attn      = 1
0.00.051.888 I llm_load_print_meta: pooling type     = 0
0.00.051.888 I llm_load_print_meta: rope type        = 2
0.00.051.888 I llm_load_print_meta: rope scaling     = linear
0.00.051.889 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.889 I llm_load_print_meta: freq_scale_train = 1
0.00.051.889 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.889 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.890 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.890 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.890 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.890 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.890 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.890 I llm_load_print_meta: model type       = 1.4B
0.00.051.891 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.891 I llm_load_print_meta: model params     = 1.41 B
0.00.051.892 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.893 I llm_load_print_meta: general.name     = 1.4B
0.00.051.893 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.893 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: LF token         = 128 ''
0.00.051.894 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: max token length = 1024
0.00.053.567 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.567 I llm_load_tensors: offloading output layer to GPU
0.00.053.567 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.577 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.578 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.467 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.468 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.468 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.468 I llama_new_context_with_model: n_batch       = 2048
0.00.054.468 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.469 I llama_new_context_with_model: flash_attn    = 0
0.00.054.469 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.469 I llama_new_context_with_model: freq_scale    = 1
0.00.054.470 I ggml_metal_init: allocating
0.00.054.473 I ggml_metal_init: found device: Apple M4
0.00.054.475 I ggml_metal_init: picking default device: Apple M4
0.00.055.062 I ggml_metal_init: using embedded metal library
0.00.057.376 I ggml_metal_init: GPU name:   Apple M4
0.00.057.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.378 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.378 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.378 I ggml_metal_init: simdgroup reduction   = true
0.00.057.379 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.379 I ggml_metal_init: has bfloat            = true
0.00.057.379 I ggml_metal_init: use bfloat            = true
0.00.057.379 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.380 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.339 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.757 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.763 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.782 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.802 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.804 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.804 I llama_new_context_with_model: graph nodes  = 967
0.00.087.804 I llama_new_context_with_model: graph splits = 2
0.00.087.807 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.468 I main: llama threadpool init, n_threads = 4
0.00.694.518 I 
0.00.694.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.557 I 
0.00.694.718 I sampler seed: 1234
0.00.694.723 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.758 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.760 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.760 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.570.341 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62445.03 tokens per second)
0.01.570.342 I llama_perf_context_print:        load time =     684.48 ms
0.01.570.343 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.63 tokens per second)
0.01.570.343 I llama_perf_context_print:        eval time =     821.08 ms /    63 runs   (   13.03 ms per token,    76.73 tokens per second)
0.01.570.344 I llama_perf_context_print:       total time =     875.88 ms /    70 tokens
0.01.570.561 I ggml_metal_free: deallocating

real	0m1.589s
user	0m0.110s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.147 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.595 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.601 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.602 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.602 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.602 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.607 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.607 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.608 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.609 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.609 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.610 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.424 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.501 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.304 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.305 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.305 I llama_model_loader: - type  f32:  194 tensors
0.00.028.306 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.306 I llama_model_loader: - type q6_K:   37 tensors
0.00.048.615 I llm_load_vocab: special tokens cache size = 25
0.00.054.432 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.435 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.435 I llm_load_print_meta: arch             = gptneox
0.00.054.435 I llm_load_print_meta: vocab type       = BPE
0.00.054.436 I llm_load_print_meta: n_vocab          = 50304
0.00.054.436 I llm_load_print_meta: n_merges         = 50009
0.00.054.436 I llm_load_print_meta: vocab_only       = 0
0.00.054.436 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.436 I llm_load_print_meta: n_embd           = 2048
0.00.054.436 I llm_load_print_meta: n_layer          = 24
0.00.054.440 I llm_load_print_meta: n_head           = 16
0.00.054.441 I llm_load_print_meta: n_head_kv        = 16
0.00.054.441 I llm_load_print_meta: n_rot            = 32
0.00.054.441 I llm_load_print_meta: n_swa            = 0
0.00.054.441 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.441 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.442 I llm_load_print_meta: n_gqa            = 1
0.00.054.443 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.444 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.444 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.444 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.445 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.447 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.447 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.448 I llm_load_print_meta: n_ff             = 8192
0.00.054.448 I llm_load_print_meta: n_expert         = 0
0.00.054.448 I llm_load_print_meta: n_expert_used    = 0
0.00.054.448 I llm_load_print_meta: causal attn      = 1
0.00.054.448 I llm_load_print_meta: pooling type     = 0
0.00.054.448 I llm_load_print_meta: rope type        = 2
0.00.054.449 I llm_load_print_meta: rope scaling     = linear
0.00.054.449 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.449 I llm_load_print_meta: freq_scale_train = 1
0.00.054.449 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.450 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.451 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.452 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.452 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.452 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.452 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.452 I llm_load_print_meta: model type       = 1.4B
0.00.054.453 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.054.453 I llm_load_print_meta: model params     = 1.41 B
0.00.054.454 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.054.454 I llm_load_print_meta: general.name     = 1.4B
0.00.054.454 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.454 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.454 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.455 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.455 I llm_load_print_meta: LF token         = 128 ''
0.00.054.460 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.460 I llm_load_print_meta: max token length = 1024
0.00.056.468 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.468 I llm_load_tensors: offloading output layer to GPU
0.00.056.468 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.479 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.056.480 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.057.398 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.399 I llama_new_context_with_model: n_ctx         = 128
0.00.057.399 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.057.399 I llama_new_context_with_model: n_batch       = 128
0.00.057.399 I llama_new_context_with_model: n_ubatch      = 128
0.00.057.399 I llama_new_context_with_model: flash_attn    = 0
0.00.057.400 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.400 I llama_new_context_with_model: freq_scale    = 1
0.00.057.400 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.401 I ggml_metal_init: allocating
0.00.057.404 I ggml_metal_init: found device: Apple M4
0.00.057.406 I ggml_metal_init: picking default device: Apple M4
0.00.057.956 I ggml_metal_init: using embedded metal library
0.00.060.239 I ggml_metal_init: GPU name:   Apple M4
0.00.060.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.242 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.242 I ggml_metal_init: simdgroup reduction   = true
0.00.060.242 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.242 I ggml_metal_init: has bfloat            = true
0.00.060.242 I ggml_metal_init: use bfloat            = true
0.00.060.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.882 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.159 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.162 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.178 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.090 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.091 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.091 I llama_new_context_with_model: graph nodes  = 967
0.00.072.092 I llama_new_context_with_model: graph splits = 2
0.00.072.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.890 I 
0.00.581.923 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.938 I perplexity: tokenizing the input ..
0.00.589.455 I perplexity: tokenization took 7.516 ms
0.00.589.458 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.288 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.731.458 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.731.483 I llama_perf_context_print:        load time =     572.74 ms
0.00.731.485 I llama_perf_context_print: prompt eval time =     140.60 ms /   128 tokens (    1.10 ms per token,   910.38 tokens per second)
0.00.731.485 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.486 I llama_perf_context_print:       total time =     149.59 ms /   129 tokens
0.00.731.980 I ggml_metal_free: deallocating

real	0m0.748s
user	0m0.078s
sys	0m0.098s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.762 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.625 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.635 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.636 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.637 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.637 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.637 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.570 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.462 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.463 I llama_model_loader: - type  f32:  194 tensors
0.00.025.464 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.800 I llm_load_vocab: special tokens cache size = 25
0.00.052.842 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.845 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.845 I llm_load_print_meta: arch             = gptneox
0.00.052.845 I llm_load_print_meta: vocab type       = BPE
0.00.052.846 I llm_load_print_meta: n_vocab          = 50304
0.00.052.846 I llm_load_print_meta: n_merges         = 50009
0.00.052.846 I llm_load_print_meta: vocab_only       = 0
0.00.052.846 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.846 I llm_load_print_meta: n_embd           = 2048
0.00.052.847 I llm_load_print_meta: n_layer          = 24
0.00.052.849 I llm_load_print_meta: n_head           = 16
0.00.052.851 I llm_load_print_meta: n_head_kv        = 16
0.00.052.851 I llm_load_print_meta: n_rot            = 32
0.00.052.851 I llm_load_print_meta: n_swa            = 0
0.00.052.851 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.854 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.854 I llm_load_print_meta: n_gqa            = 1
0.00.052.855 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.856 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.856 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.857 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.858 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.858 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.858 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.859 I llm_load_print_meta: n_ff             = 8192
0.00.052.861 I llm_load_print_meta: n_expert         = 0
0.00.052.861 I llm_load_print_meta: n_expert_used    = 0
0.00.052.861 I llm_load_print_meta: causal attn      = 1
0.00.052.861 I llm_load_print_meta: pooling type     = 0
0.00.052.861 I llm_load_print_meta: rope type        = 2
0.00.052.861 I llm_load_print_meta: rope scaling     = linear
0.00.052.862 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.862 I llm_load_print_meta: freq_scale_train = 1
0.00.052.862 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.862 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.863 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.863 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.863 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.863 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.863 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.863 I llm_load_print_meta: model type       = 1.4B
0.00.052.864 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.868 I llm_load_print_meta: model params     = 1.41 B
0.00.052.868 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.869 I llm_load_print_meta: general.name     = 1.4B
0.00.052.869 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.869 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.869 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.869 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.869 I llm_load_print_meta: LF token         = 128 ''
0.00.052.870 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.870 I llm_load_print_meta: max token length = 1024
0.00.054.939 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.940 I llm_load_tensors: offloading output layer to GPU
0.00.054.940 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.950 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.952 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.895 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.896 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.896 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.896 I llama_new_context_with_model: n_batch       = 2048
0.00.055.896 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.896 I llama_new_context_with_model: flash_attn    = 0
0.00.055.897 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.897 I llama_new_context_with_model: freq_scale    = 1
0.00.055.897 I ggml_metal_init: allocating
0.00.055.900 I ggml_metal_init: found device: Apple M4
0.00.055.902 I ggml_metal_init: picking default device: Apple M4
0.00.056.529 I ggml_metal_init: using embedded metal library
0.00.058.958 I ggml_metal_init: GPU name:   Apple M4
0.00.058.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.961 I ggml_metal_init: simdgroup reduction   = true
0.00.058.961 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.961 I ggml_metal_init: has bfloat            = true
0.00.058.961 I ggml_metal_init: use bfloat            = true
0.00.058.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.306 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.465 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.474 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.495 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.527 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.529 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.529 I llama_new_context_with_model: graph nodes  = 967
0.00.089.529 I llama_new_context_with_model: graph splits = 2
0.00.089.532 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.675 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.003 I main: llama threadpool init, n_threads = 4
0.00.744.043 I 
0.00.744.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.064 I 
0.00.744.310 I sampler seed: 1234
0.00.744.316 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.370 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.370 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.370 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.625.974 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.01.625.975 I llama_perf_context_print:        load time =     734.24 ms
0.01.625.976 I llama_perf_context_print: prompt eval time =      54.56 ms /     7 tokens (    7.79 ms per token,   128.30 tokens per second)
0.01.625.976 I llama_perf_context_print:        eval time =     824.12 ms /    63 runs   (   13.08 ms per token,    76.44 tokens per second)
0.01.625.977 I llama_perf_context_print:       total time =     881.97 ms /    70 tokens
0.01.626.178 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.112s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4411 (c7b006fc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.945 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.613 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.624 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.624 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.625 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.625 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.627 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.375 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.377 I llama_model_loader: - type  f32:  194 tensors
0.00.025.378 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.351 I llm_load_vocab: special tokens cache size = 25
0.00.052.418 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.421 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.421 I llm_load_print_meta: arch             = gptneox
0.00.052.421 I llm_load_print_meta: vocab type       = BPE
0.00.052.422 I llm_load_print_meta: n_vocab          = 50304
0.00.052.422 I llm_load_print_meta: n_merges         = 50009
0.00.052.422 I llm_load_print_meta: vocab_only       = 0
0.00.052.422 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.422 I llm_load_print_meta: n_embd           = 2048
0.00.052.423 I llm_load_print_meta: n_layer          = 24
0.00.052.425 I llm_load_print_meta: n_head           = 16
0.00.052.426 I llm_load_print_meta: n_head_kv        = 16
0.00.052.426 I llm_load_print_meta: n_rot            = 32
0.00.052.427 I llm_load_print_meta: n_swa            = 0
0.00.052.427 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.427 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.428 I llm_load_print_meta: n_gqa            = 1
0.00.052.428 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.429 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.430 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.430 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.430 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.430 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.431 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.431 I llm_load_print_meta: n_ff             = 8192
0.00.052.432 I llm_load_print_meta: n_expert         = 0
0.00.052.432 I llm_load_print_meta: n_expert_used    = 0
0.00.052.432 I llm_load_print_meta: causal attn      = 1
0.00.052.432 I llm_load_print_meta: pooling type     = 0
0.00.052.432 I llm_load_print_meta: rope type        = 2
0.00.052.432 I llm_load_print_meta: rope scaling     = linear
0.00.052.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.433 I llm_load_print_meta: freq_scale_train = 1
0.00.052.433 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.434 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.435 I llm_load_print_meta: model type       = 1.4B
0.00.052.435 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.435 I llm_load_print_meta: model params     = 1.41 B
0.00.052.436 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.436 I llm_load_print_meta: general.name     = 1.4B
0.00.052.436 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.436 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.437 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.437 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.437 I llm_load_print_meta: LF token         = 128 ''
0.00.052.437 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: max token length = 1024
0.00.054.506 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.506 I llm_load_tensors: offloading output layer to GPU
0.00.054.507 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.517 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.519 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.410 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.411 I llama_new_context_with_model: n_ctx         = 128
0.00.055.411 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.412 I llama_new_context_with_model: n_batch       = 128
0.00.055.412 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.412 I llama_new_context_with_model: flash_attn    = 0
0.00.055.412 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.413 I llama_new_context_with_model: freq_scale    = 1
0.00.055.413 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.414 I ggml_metal_init: allocating
0.00.055.417 I ggml_metal_init: found device: Apple M4
0.00.055.419 I ggml_metal_init: picking default device: Apple M4
0.00.055.990 I ggml_metal_init: using embedded metal library
0.00.058.421 I ggml_metal_init: GPU name:   Apple M4
0.00.058.423 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.424 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.424 I ggml_metal_init: simdgroup reduction   = true
0.00.058.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.424 I ggml_metal_init: has bfloat            = true
0.00.058.424 I ggml_metal_init: use bfloat            = true
0.00.058.425 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.302 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.600 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.602 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.617 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.535 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.536 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.536 I llama_new_context_with_model: graph nodes  = 967
0.00.070.536 I llama_new_context_with_model: graph splits = 2
0.00.070.538 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.132.991 I 
0.00.133.028 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.133.044 I perplexity: tokenizing the input ..
0.00.140.415 I perplexity: tokenization took 7.369 ms
0.00.140.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.279.139 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.280.374 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.280.383 I llama_perf_context_print:        load time =     123.04 ms
0.00.280.386 I llama_perf_context_print: prompt eval time =     138.49 ms /   128 tokens (    1.08 ms per token,   924.25 tokens per second)
0.00.280.386 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.280.388 I llama_perf_context_print:       total time =     147.40 ms /   129 tokens
0.00.280.811 I ggml_metal_free: deallocating

real	0m0.296s
user	0m0.079s
sys	0m0.039s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4411 (c7b006fc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14460a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14460a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14460af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14460b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14460bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14460c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14460c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14460cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14460d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14460d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14460db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14460e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14460eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14460f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14460fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144610270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144610990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1446110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1446117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144611fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1446126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144612de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144613500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144613da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1446144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144614780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144614d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144615a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144615f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1446166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1446171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144617730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1446179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144617e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144618330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1446187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144618c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144619110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1446195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144619a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144619ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14461a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14461a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14461ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14461b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14461bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14461c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14461c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14461cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14461d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14461d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14461dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14461e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14461ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14461f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14461f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14461f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1446201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1446204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144620940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144621280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144622060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144622500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1446229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1446232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144623780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144623c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144624170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1446246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144624c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144625160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1446256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144625c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144626150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1446266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144626bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144627140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144627690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144627be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144628130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144628680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144628bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144629120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144629670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144629bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14462a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14462a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14462abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14462b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14462b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14462bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14461b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14462c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14462c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14462cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14462d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14462d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14462dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14462e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14462e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14462ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14462f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14462f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14462fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144630230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144630780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144630cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144631170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144631610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144631ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144631f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1446323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144632890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144632d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1446331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144633670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144633b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144633fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144634450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1446348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144634d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1446356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144635b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144636010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1446364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144636950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144636df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144637290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144637730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144637bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144638070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144638510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1446389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144638e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1446392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144639790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144639c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14463a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14463a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14463aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14463aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14463b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14463b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14463bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14463c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14463c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14463ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14463cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14463d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14463d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14463dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14463e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14463e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14463ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14463ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14463f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14463f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14463fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1446401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144640690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144640b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144640fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144641470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144641910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144641db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144642250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1446426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144642b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144643030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1446434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144643970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144643e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1446442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144644750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144644bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144645090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144645530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1446459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144645e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144646310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1446467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144646c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1446470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144647590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144647a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144647ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144648420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144648970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144648ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144649410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1446496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144649ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14464a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14464a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14464b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14464b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14464b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14464be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14464c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14464cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14464d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14464d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14464da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14464e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14464e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14464ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14464f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14464f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14464fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1446501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144650720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144650c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1446511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144651710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144651c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1446521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144652700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144652c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1446531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1446536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144653c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144654190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1446546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144654c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144655180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1446556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144655c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144656170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1446566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144656c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144657160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1446576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144657c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144658150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1446586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144658bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144659140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144659690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144659be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14465a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14465a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14465abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14465b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14465b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14465bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14465c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14465c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14465cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14465d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14465d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14465dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14465e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14465e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14465eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14465f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14465f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14465fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1446600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144660620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144660b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144661010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1446614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144661950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144661df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144662290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144662730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144662bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144663070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144663510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1446639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144663e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1446642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144664790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144664c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1446650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144665620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144665d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144666460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144666b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1446672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144667560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144667d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144668010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144668620 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.146.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.146.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1446682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144649fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144649990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14464a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14461d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14461d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14461f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14464c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144614a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14461b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14461be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14461c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14461a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14461ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144613a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14461fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14462c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144667820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144616ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14464c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14464abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144615310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1446155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144668a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144668d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144669000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1446692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144669580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144669840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144669b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144669dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14466a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14466a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14466a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14466a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14466ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14466ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14466b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14466b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14466b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14466b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14466bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14466bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14466c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14466c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14466c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14466c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14466cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14466cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14466d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14466d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14466d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14466da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14466dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14466dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14466e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14466e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14466e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14466eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14466ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14466f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14466f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14466f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14466f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14466fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14466fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1446700c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144670380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144670640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144670900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144670bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144670e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144671140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144671400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1446716c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144671980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144671c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144671f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1446721c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144672480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144672740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144672a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144672cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144672f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144673240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144673500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1446737c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144673a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144673d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144674000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1446742c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144674580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144674840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144674b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144674dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144675080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144675340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144675600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1446758c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144675b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144675e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144676100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1446763c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144676680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144676940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144676c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144676ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144677180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144677440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144677700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1446779c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144677c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144677f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144678200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1446784c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144678780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144678a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144678d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144678fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144679280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144679540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144679800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144679ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144679d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14467a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14467a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14467a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14467a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14467ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14467ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14467b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14467b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14467b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14467b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14467bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14467be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14467c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14467c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14467c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14467c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14467cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14467cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14467d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14467d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14467d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14467da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14467dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14467df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14467e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14467e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14467e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14467ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14467ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14467f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14467f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14467f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14467f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14467fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14467fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144680080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144680340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144680600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1446808c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144680b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144680e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144681100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1446813c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144681680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144681940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144681c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144681ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144682180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144682440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144682700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1446829c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144682c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144682f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144683200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1446834c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144683780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144683a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144683d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144683fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144684280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144684540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144684800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144684ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144684d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144685040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144685300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1446855c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144685880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144685b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144685e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1446860c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144686380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144686640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144686900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144686bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144686e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144687140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144687400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1446876c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144687980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144687c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144687f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1446881c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144688660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144688e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1446890d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144689390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144689800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144689c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14468a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14468a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14468a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14468ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14468b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14468b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14468bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14468bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14468c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14468c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14468cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14468d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14468d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14468da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14468df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14468e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14468e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14468ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14468f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14468f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14468f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14468fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144690280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1446906f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144690b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144690fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144691440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1446918b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144691d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144692190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144692600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144692a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144692ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144693350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1446937c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144693c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1446940a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144694510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144694980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144694df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144695260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1446956d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144695b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144695fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144696420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144696890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144696d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144697170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1446975e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144697a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144697ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144698330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1446987a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144698c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144699080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1446994f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144699960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144699dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14469a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14469a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14469ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14469af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14469b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14469b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14469bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14469c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14469c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14469ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14469d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14469dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14469e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14469ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14469ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14469f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14469f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14469fd80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14469ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14469fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14469ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1446a01e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1446a04a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1446a0760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1446a0a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1446a0ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1446a0fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1446a1260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1446a1520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1446a17e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1446a1db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1446a2380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1446a29b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1446a2c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1446a2f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1446a31f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1446a34b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1446a3770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1446a3a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1446a3cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1446a3fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1446a4270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1446a4530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1446a47f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1446a4ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1446a4d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1446a5030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1446a52f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1446a55b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1446a5870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1446a5b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1446a5df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1446a60b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1446a6370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1446a6630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1446a68f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1446a6bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1446a6e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1446a7130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1446a73f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1446a76b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1446a7970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1446a7c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1446a7ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1446a81b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1446a8470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1446a8730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1446a89f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1446a8cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1446a8f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1446a9230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1446a94f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1446a97b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1446a9a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1446a9d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1446a9ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1446aa2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1446aa570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1446aa830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1446aaaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1446aadb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1446ab070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1446ab330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1446ab5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1446ab8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1446abb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1446abe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1446ac0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1446ac3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1446ac670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1446ac930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1446acbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1446aceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1446ad170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1446ad430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1446ad6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1446ad9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1446adc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1446adf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1446ae1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1446ae4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1446ae770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1446aea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1446aecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1446aefb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1446af270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1446af530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1446af7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1446afab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1446afd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1446b0030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1446b02f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1446b05b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1446b0870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1446b0b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1446b0df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1446b10b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1446b1370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1446b1630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1446b18f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1446b1bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1446b1e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1446b2130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1446b23f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1446b26b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1446b2970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1446b2c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1446b2ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1446b31b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1446b3470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1446b3730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1446b39f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1446b3cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1446b3f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1446b4230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1446b44f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1446b47b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1446b4a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1446b4d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1446b4ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1446b52b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1446b5570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1446b5830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1446b5af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1446b5db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1446b6070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1446b6330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1446b65f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1446b68b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1446b6b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1446b6e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1446b70f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1446b73b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1446b7670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1446b7930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1446b7bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1446b7eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1446b8170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1446b8430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1446b86f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1446b89b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1446b8c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1446b8f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1446b91f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1446b94b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1446b9770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1446b9a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1446b9cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1446b9fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1446ba270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1446ba530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1446ba7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1446baab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1446bad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1446bb030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1446bb2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1446bb5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1446bb870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1446bbb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1446bbdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1446bc0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1446bc370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1446bc630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1446bc8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1446bcbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1446bce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1446bd130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1446bd3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1446bd6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1446bd970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1446bdc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1446bdef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1446be1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1446be470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1446be730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1446be9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1446becb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1446bef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1446bf230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1446bf4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1446bf7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1446bfa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1446bfd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1446bfff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1446c02b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1446c0570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1446c0830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1446c0af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1446c0db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1446c1070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1446c1330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1446c15f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1446c18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1446c1b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1446c1e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1446c20f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1446c23b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1446c2670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1446c2930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1446c2bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1446c2eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1446c3170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1446c3430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1446c36f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1446c39b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1446c3c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1446c3f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1446c41f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1446c47c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1446c4a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1446c4d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1446c5000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1446c52c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1446c5580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1446c5840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1446c5b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1446c5dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1446c6080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1446c6340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1446c6600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1446c68c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1446c6b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1446c6e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1446c7100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1446c73c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1446c7680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1446c7940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1446c7c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1446c7ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1446c8180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1446c8440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1446c8700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1446c89c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1446c8c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1446c8f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1446c9200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1446c94c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1446c9780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1446c9a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1446c9d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1446c9fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1446ca280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1446ca540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1446ca800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1446caac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1446cad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1446cb040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1446cb300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1446cb5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1446cb880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1446cbb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1446cbe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1446cc0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1446cc380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1446cc640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1446cc900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1446ccbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1446cce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1446cd140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1446cd400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1446cd6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1446cd980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1446cdc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1446cdf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1446ce1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1446ce480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1446ce740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1446cea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1446cecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1446cef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1446cf380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1446cf640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1446cf900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1446cfd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1446d01e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1446d0650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1446d0ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1446d0f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1446d13a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1446d1810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1446d1c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1446d27f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1446d2f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1446d3630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1446d3d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1446d4010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1446d42d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1446d4800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1446d4c70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.758s
user	0m0.300s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4411 (c7b006fc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137f102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137f109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137f10fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137f11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137f11b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137f120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137f12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137f12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137f131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137f136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137f13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137f140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137f14be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137f15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137f15ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137f162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137f169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137f17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137f17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137f17ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137f18710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137f18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137f19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137f19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137f1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137f1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137f1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137f1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137f1c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137f1c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137f1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137f1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137f1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137f1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137f1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137f1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137f1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137f1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137f1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137f1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137f1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137f203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137f206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137f20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137f21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137f221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137f22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137f22e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137f23420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137f23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137f24040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137f24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137f24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137f25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137f25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137f25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137f26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137f264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137f26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137f26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137f272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137f27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137f27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137f28550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137f289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137f29330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137f297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137f29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137f2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137f2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137f2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137f2b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137f2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137f2bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137f2c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137f2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137f2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137f2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137f2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137f2e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137f2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137f2ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137f2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137f2f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137f2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137f30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137f306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137f30c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137f31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137f316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137f31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137f218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137f32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137f32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137f32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137f332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137f33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137f33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137f342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137f347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137f34d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137f35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137f357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137f35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137f36280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137f367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137f371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137f37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137f37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137f38440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137f388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137f38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137f39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137f396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137f39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137f3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f3a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137f3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137f3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137f3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137f3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137f3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137f3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137f3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137f3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137f3e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137f3e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137f3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137f3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f3f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137f3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137f40120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137f405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137f40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137f40f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137f413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137f41840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137f41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137f42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137f42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137f42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137f43400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137f438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137f43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137f441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137f44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137f44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137f44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137f45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137f45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137f45da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137f46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137f466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137f46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137f47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137f474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137f47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137f47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137f482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137f48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137f48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137f49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137f49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137f4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137f4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137f4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137f4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137f4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137f4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137f4c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137f4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137f4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137f4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137f4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137f4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137f4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137f4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137f4ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137f4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137f4f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137f4fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137f50340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137f50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137f51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137f515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137f518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137f51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137f524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137f52cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137f53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137f535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137f53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137f54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137f54790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137f54ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137f55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137f55780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137f55cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137f56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137f56770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137f56cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137f57210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137f57760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137f57cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137f58200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137f58750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137f58ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137f591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137f59c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137f5a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137f5a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137f5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137f5b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137f5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137f5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137f5c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137f5c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137f5cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137f5d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137f5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137f5dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137f5e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137f5e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137f5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137f5f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137f5f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137f5fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137f60180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137f606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137f60c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137f61170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137f616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137f61c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137f62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137f626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137f62c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137f63150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137f636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137f63bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137f64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137f64690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137f64be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137f65130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137f65680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137f65bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137f66120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137f66670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137f66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137f67060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137f67500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137f679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137f67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137f682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137f68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137f68c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137f690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137f69560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137f69a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137f69ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137f6a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137f6a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137f6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137f6b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137f6b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137f6bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137f6c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137f6cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137f6d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137f6d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137f6dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137f6e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137f6e670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.676 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.680 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137e059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137e05e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137e06290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137e06700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137e06b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137e06fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137e07450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137e078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137e07d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137e081a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137e08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137e08c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137e09790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137e09f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137e0a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137e0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137e0b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137e0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137e0c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137e0cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137e0d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137e0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137e0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137e0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137e0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137e0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137e0f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137e0f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137e0fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137e10210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137e10680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137e10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137e11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137e112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137e11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137e11bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137e12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137e124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137e12910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137e12d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137e131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137e13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137e13ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137e13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137e143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137e14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137e14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137e15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137e15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137e159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137e15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137e162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137e16730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137e16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137e17010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137e17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137e179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137e17ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137e18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137e187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137e18c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137e190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137e19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137e19990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137e19e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137e1a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137e1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137e1ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137e1afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137e1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137e1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137e1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137e1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137e1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137e1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137e1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137e1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137e1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137e1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137e1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137e1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137e1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137e1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137e1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137e1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137e1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137e1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137e20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137e20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137e20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137e21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137e215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137e21a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137e21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137e22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137e22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137e22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137e23070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137e234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137e23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137e23dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137e24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137e246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137e24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137e24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137e253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137e25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137e25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137e26140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137e265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137e26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137e26e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137e27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137e27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137e27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137e28050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137e284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137e28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137e28da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137e29210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137e29680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137e29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137e29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137e2a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137e2a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137e2acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137e2b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137e2b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137e2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137e2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137e2c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137e2c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137e2cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137e2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137e2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137e2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137e2dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137e2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137e2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137e2ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137e2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137e2f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137e2f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137e2fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137e30100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137e30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137e309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137e30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137e312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137e31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137e31ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137e32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137e32480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137e328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137e32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137e331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137e33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137e33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137e33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137e34390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137e34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137e34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137e350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137e35550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137e359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137e35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137e365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137e36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137e37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137e375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137e37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137e37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137e38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137e387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137e38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137e39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137e39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137e39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137e3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137e3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137e3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137e3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137e3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137e3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137e3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137e3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137e3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137e3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137e3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137e3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137e3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137e3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137e3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137e3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137e3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137e3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f04430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f04d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f05180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f05a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f05ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f06c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f07c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f07ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f08180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f08a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f08ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f09c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f0a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f0a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f0ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f0b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f0b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f0bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f0bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f0c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f0c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f0ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f0d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f0d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f0da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f0deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f0e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f0e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f0ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f0f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f0f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f0f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f10b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f10f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f11860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f11cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f12140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f12e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f13770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f13be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f14050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f14930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f14da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f15210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f15680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f15af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f15f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f16840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f16cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f17120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f17590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f17a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f17e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f1af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f1c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f1d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f1dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f1df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f1eb30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137e08f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137e088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137e360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137e05550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137e3f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137e3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137e3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137e3ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137e401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137e40490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137e40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137e40a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137e40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137e415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137e41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137e41ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137e423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137e42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137e42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137e43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137e43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137e440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137e445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137e44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137e45070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137e455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137e45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137e45b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137e45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137e460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137e46370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137e46630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137e468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137e46bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137e46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137e47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137e473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137e476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137e47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137e47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137e47ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137e481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137e48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137e48730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137e489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137e48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137e48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137e49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137e494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137e497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137e49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137e49d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137e49ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137e4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137e4a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137e4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137e4aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137e4adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137e4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137e4b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137e4b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137e4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137e4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137e4be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137e4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137e4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137e4c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137e4c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137e4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137e4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137e4d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137e4d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137e4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137e4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137e4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137e4df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137e4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137e4e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137e4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137e4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137e4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137e4efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137e4f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137e4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137e4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137e4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137e4fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137e50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137e502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137e505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137e50870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137e50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137e50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137e510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137e51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137e51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137e518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137e51bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137e51e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137e52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137e523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137e526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137e52970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137e52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137e52ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137e531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137e53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137e53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137e539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137e53cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137e53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137e54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137e544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137e547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137e54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137e54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137e54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137e552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137e55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137e55830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137e55af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137e55db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137e56070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137e56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137e565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137e568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137e56b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137e56e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137e570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137e573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137e57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137e57930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137e57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137e57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137e58170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137e58430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137e586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137e589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137e58c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137e58f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137e591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137e594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137e59770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137e59a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137e59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137e59fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137e5a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137e5a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137e5a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137e5aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137e5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137e5b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137e5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137e5b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137e5b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137e5bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137e5bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137e5c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137e5c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137e5c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137e5c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137e5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137e5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137e5d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137e5d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137e5d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137e5d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137e5dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137e5def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137e5e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137e5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137e5e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137e5e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137e5ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137e5ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137e5f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137e5f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137e5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137e5fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137e5fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137e5fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137e602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137e60570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137e60830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137e60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137e60db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137e61070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137e61330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137e615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137e619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137e61cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137e621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137e626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137e62bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137e63160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137e63710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137e63cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137e64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137e64880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137e64e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137e654a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137e65c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137e66130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137e663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137e66a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137e67010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137e67800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137e67ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137e68140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137e685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137e68d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137e692e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137e69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137e69d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137e6a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137e6a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137e6ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137e6b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137e6b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137e6bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137e6c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137e6c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137e6cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137e6d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137e6d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137e6dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137e6e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137e6e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137e6ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137e6f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137e6f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137e6fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137e70270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137e707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137e70d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137e71260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137e717b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137e71d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137e72250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137e727a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137e72cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137e73240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137e73790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137e73ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137e74230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137e74780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137e74cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137e75220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137e75770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137e75cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137e76210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137e76760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137e76cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137e77200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137e77750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137e77ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137e781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137e78740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137e78c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137e791e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137e79730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137e79c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137e7a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137e7a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137e7ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137e7b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137e7b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137e7bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137e7c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137e7c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137e7c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137e7ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137e7d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137e7d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137e7dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137e7e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137e7e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137e7e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137e7ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137e7f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137e7f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137e7fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137e801c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137e808e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137e81000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137e81720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137e81e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137e82100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137e828f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137e82bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137e831c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.916s
user	0m0.243s
sys	0m0.138s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.26 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
