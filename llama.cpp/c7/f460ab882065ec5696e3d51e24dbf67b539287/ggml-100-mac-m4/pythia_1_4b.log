Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.568s
user	0m0.833s
sys	0m1.262s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Built target build_info
[  4%] Built target sha256
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple-chat
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-sampling
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-gguf
[ 62%] Built target test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-arg-parser
[ 62%] Built target test-backend-ops
[ 62%] Built target test-barrier
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target test-rope
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-embedding
[ 71%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-batched
[ 72%] Built target llama-imatrix
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Built target llama-bench
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup
[ 81%] Built target llama-cli
[ 81%] Built target llama-passkey
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookahead
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-parallel
[ 82%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Built target llama-quantize
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-perplexity
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Built target llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-run
[ 91%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Built target llama-llava-clip-quantize-cli
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.150s
user	0m6.523s
sys	0m10.073s

main: quantize time =  3638.05 ms
main:    total time =  3638.05 ms

main: quantize time =  6283.69 ms
main:    total time =  6283.69 ms

main: quantize time =  3442.26 ms
main:    total time =  3442.26 ms

main: quantize time =  3837.27 ms
main:    total time =  3837.27 ms

main: quantize time =  2335.25 ms
main:    total time =  2335.25 ms

main: quantize time =  5466.82 ms
main:    total time =  5466.82 ms

main: quantize time =  5970.31 ms
main:    total time =  5970.31 ms

main: quantize time =  7210.80 ms
main:    total time =  7210.80 ms

main: quantize time =  5831.88 ms
main:    total time =  5831.88 ms

main: quantize time =  4368.70 ms
main:    total time =  4368.70 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.121 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.278 I main: llama backend init
0.00.000.284 I main: load the model and apply lora adapter, if any
0.00.058.589 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.070.632 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.070.641 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.070.656 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.070.658 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.070.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.070.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.070.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.070.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.070.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.070.665 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.070.666 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.070.666 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.070.667 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.070.668 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.070.671 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.070.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.070.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.077.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.079.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.086.801 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.801 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.802 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.802 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.804 I llama_model_loader: - type  f32:  194 tensors
0.00.086.804 I llama_model_loader: - type  f16:   98 tensors
0.00.086.805 I print_info: file format = GGUF V3 (latest)
0.00.086.806 I print_info: file type   = all F32 (guessed)
0.00.086.808 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.018 I load: special tokens cache size = 25
0.00.106.613 I load: token to piece cache size = 0.2984 MB
0.00.106.618 I print_info: arch             = gptneox
0.00.106.619 I print_info: vocab_only       = 0
0.00.106.619 I print_info: n_ctx_train      = 2048
0.00.106.619 I print_info: n_embd           = 2048
0.00.106.620 I print_info: n_layer          = 24
0.00.106.625 I print_info: n_head           = 16
0.00.106.627 I print_info: n_head_kv        = 16
0.00.106.627 I print_info: n_rot            = 32
0.00.106.627 I print_info: n_swa            = 0
0.00.106.627 I print_info: n_embd_head_k    = 128
0.00.106.628 I print_info: n_embd_head_v    = 128
0.00.106.629 I print_info: n_gqa            = 1
0.00.106.630 I print_info: n_embd_k_gqa     = 2048
0.00.106.631 I print_info: n_embd_v_gqa     = 2048
0.00.106.632 I print_info: f_norm_eps       = 1.0e-05
0.00.106.632 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.106.633 I print_info: f_clamp_kqv      = 0.0e+00
0.00.106.633 I print_info: f_max_alibi_bias = 0.0e+00
0.00.106.633 I print_info: f_logit_scale    = 0.0e+00
0.00.106.634 I print_info: n_ff             = 8192
0.00.106.635 I print_info: n_expert         = 0
0.00.106.635 I print_info: n_expert_used    = 0
0.00.106.635 I print_info: causal attn      = 1
0.00.106.635 I print_info: pooling type     = 0
0.00.106.635 I print_info: rope type        = 2
0.00.106.636 I print_info: rope scaling     = linear
0.00.106.640 I print_info: freq_base_train  = 10000.0
0.00.106.640 I print_info: freq_scale_train = 1
0.00.106.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.106.641 I print_info: rope_finetuned   = unknown
0.00.106.641 I print_info: ssm_d_conv       = 0
0.00.106.642 I print_info: ssm_d_inner      = 0
0.00.106.642 I print_info: ssm_d_state      = 0
0.00.106.642 I print_info: ssm_dt_rank      = 0
0.00.106.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.106.643 I print_info: model type       = 1.4B
0.00.106.643 I print_info: model params     = 1.41 B
0.00.106.644 I print_info: general.name     = 1.4B
0.00.106.644 I print_info: vocab type       = BPE
0.00.106.644 I print_info: n_vocab          = 50304
0.00.106.645 I print_info: n_merges         = 50009
0.00.106.645 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.106.646 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.106.646 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.106.646 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.106.647 I print_info: LF token         = 187 'Ċ'
0.00.106.647 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.106.647 I print_info: max token length = 1024
0.00.106.648 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.151.546 I load_tensors: offloading 24 repeating layers to GPU
0.00.151.548 I load_tensors: offloading output layer to GPU
0.00.151.549 I load_tensors: offloaded 25/25 layers to GPU
0.00.151.574 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.151.576 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.152.049 I llama_init_from_model: n_seq_max     = 1
0.00.152.050 I llama_init_from_model: n_ctx         = 2048
0.00.152.050 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.152.050 I llama_init_from_model: n_batch       = 2048
0.00.152.050 I llama_init_from_model: n_ubatch      = 512
0.00.152.050 I llama_init_from_model: flash_attn    = 0
0.00.152.051 I llama_init_from_model: freq_base     = 10000.0
0.00.152.051 I llama_init_from_model: freq_scale    = 1
0.00.152.053 I ggml_metal_init: allocating
0.00.152.097 I ggml_metal_init: found device: Apple M4
0.00.152.103 I ggml_metal_init: picking default device: Apple M4
0.00.152.779 I ggml_metal_init: using embedded metal library
0.00.173.109 I ggml_metal_init: GPU name:   Apple M4
0.00.173.113 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.173.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.173.114 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.173.114 I ggml_metal_init: simdgroup reduction   = true
0.00.173.115 I ggml_metal_init: simdgroup matrix mul. = true
0.00.173.115 I ggml_metal_init: has residency sets    = true
0.00.173.115 I ggml_metal_init: has bfloat            = true
0.00.173.115 I ggml_metal_init: use bfloat            = true
0.00.173.115 I ggml_metal_init: hasUnifiedMemory      = true
0.00.173.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.224.118 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.252.614 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.252.618 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.252.660 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.256.406 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.256.407 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.256.408 I llama_init_from_model: graph nodes  = 967
0.00.256.408 I llama_init_from_model: graph splits = 2
0.00.256.414 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.256.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.256.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.320.669 I main: llama threadpool init, n_threads = 4
0.00.320.708 I 
0.00.320.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.320.721 I 
0.00.320.875 I sampler seed: 1234
0.00.320.880 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.320.904 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.320.905 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.320.905 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.190.037 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49168.98 tokens per second)
0.02.190.037 I llama_perf_context_print:        load time =     261.26 ms
0.02.190.038 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.24 tokens per second)
0.02.190.039 I llama_perf_context_print:        eval time =    1822.95 ms /    63 runs   (   28.94 ms per token,    34.56 tokens per second)
0.02.190.039 I llama_perf_context_print:       total time =    1870.18 ms /    70 tokens
0.02.190.301 I ggml_metal_free: deallocating

real	0m2.536s
user	0m0.124s
sys	0m0.130s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.106 I main: llama backend init
0.00.000.108 I main: load the model and apply lora adapter, if any
0.00.009.833 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.708 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.720 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.721 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.721 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.723 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.723 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.724 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.724 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.725 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.896 I llama_model_loader: - type  f32:  194 tensors
0.00.037.896 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.896 I print_info: file format = GGUF V3 (latest)
0.00.037.897 I print_info: file type   = Q8_0
0.00.037.899 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.221 I load: special tokens cache size = 25
0.00.052.432 I load: token to piece cache size = 0.2984 MB
0.00.052.436 I print_info: arch             = gptneox
0.00.052.436 I print_info: vocab_only       = 0
0.00.052.436 I print_info: n_ctx_train      = 2048
0.00.052.437 I print_info: n_embd           = 2048
0.00.052.437 I print_info: n_layer          = 24
0.00.052.441 I print_info: n_head           = 16
0.00.052.442 I print_info: n_head_kv        = 16
0.00.052.442 I print_info: n_rot            = 32
0.00.052.442 I print_info: n_swa            = 0
0.00.052.442 I print_info: n_embd_head_k    = 128
0.00.052.442 I print_info: n_embd_head_v    = 128
0.00.052.445 I print_info: n_gqa            = 1
0.00.052.446 I print_info: n_embd_k_gqa     = 2048
0.00.052.447 I print_info: n_embd_v_gqa     = 2048
0.00.052.447 I print_info: f_norm_eps       = 1.0e-05
0.00.052.448 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.448 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.448 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.448 I print_info: f_logit_scale    = 0.0e+00
0.00.052.449 I print_info: n_ff             = 8192
0.00.052.449 I print_info: n_expert         = 0
0.00.052.449 I print_info: n_expert_used    = 0
0.00.052.450 I print_info: causal attn      = 1
0.00.052.450 I print_info: pooling type     = 0
0.00.052.450 I print_info: rope type        = 2
0.00.052.450 I print_info: rope scaling     = linear
0.00.052.451 I print_info: freq_base_train  = 10000.0
0.00.052.452 I print_info: freq_scale_train = 1
0.00.052.452 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.452 I print_info: rope_finetuned   = unknown
0.00.052.452 I print_info: ssm_d_conv       = 0
0.00.052.452 I print_info: ssm_d_inner      = 0
0.00.052.452 I print_info: ssm_d_state      = 0
0.00.052.452 I print_info: ssm_dt_rank      = 0
0.00.052.453 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.453 I print_info: model type       = 1.4B
0.00.052.453 I print_info: model params     = 1.41 B
0.00.052.453 I print_info: general.name     = 1.4B
0.00.052.454 I print_info: vocab type       = BPE
0.00.052.454 I print_info: n_vocab          = 50304
0.00.052.455 I print_info: n_merges         = 50009
0.00.052.455 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.455 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.455 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.455 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.456 I print_info: LF token         = 187 'Ċ'
0.00.052.456 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.456 I print_info: max token length = 1024
0.00.052.457 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.768.006 I load_tensors: offloading 24 repeating layers to GPU
0.01.768.012 I load_tensors: offloading output layer to GPU
0.01.768.013 I load_tensors: offloaded 25/25 layers to GPU
0.01.768.042 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.768.045 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.768.862 I llama_init_from_model: n_seq_max     = 1
0.01.768.864 I llama_init_from_model: n_ctx         = 2048
0.01.768.864 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.768.864 I llama_init_from_model: n_batch       = 2048
0.01.768.865 I llama_init_from_model: n_ubatch      = 512
0.01.768.865 I llama_init_from_model: flash_attn    = 0
0.01.768.866 I llama_init_from_model: freq_base     = 10000.0
0.01.768.866 I llama_init_from_model: freq_scale    = 1
0.01.768.867 I ggml_metal_init: allocating
0.01.768.929 I ggml_metal_init: found device: Apple M4
0.01.768.941 I ggml_metal_init: picking default device: Apple M4
0.01.770.297 I ggml_metal_init: using embedded metal library
0.01.775.569 I ggml_metal_init: GPU name:   Apple M4
0.01.775.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.775.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.775.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.775.574 I ggml_metal_init: simdgroup reduction   = true
0.01.775.574 I ggml_metal_init: simdgroup matrix mul. = true
0.01.775.574 I ggml_metal_init: has residency sets    = true
0.01.775.574 I ggml_metal_init: has bfloat            = true
0.01.775.574 I ggml_metal_init: use bfloat            = true
0.01.775.575 I ggml_metal_init: hasUnifiedMemory      = true
0.01.775.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.790.410 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.831.103 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.831.110 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.831.148 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.835.574 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.835.576 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.835.576 I llama_init_from_model: graph nodes  = 967
0.01.835.577 I llama_init_from_model: graph splits = 2
0.01.835.586 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.835.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.835.719 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.890.627 I main: llama threadpool init, n_threads = 4
0.01.890.665 I 
0.01.890.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.890.685 I 
0.01.890.837 I sampler seed: 1234
0.01.890.842 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.890.884 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.890.886 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.890.886 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.988.436 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.02.988.436 I llama_perf_context_print:        load time =    1880.07 ms
0.02.988.437 I llama_perf_context_print: prompt eval time =      49.16 ms /     7 tokens (    7.02 ms per token,   142.39 tokens per second)
0.02.988.439 I llama_perf_context_print:        eval time =    1045.44 ms /    63 runs   (   16.59 ms per token,    60.26 tokens per second)
0.02.988.439 I llama_perf_context_print:       total time =    1098.52 ms /    70 tokens
0.02.988.666 I ggml_metal_free: deallocating

real	0m3.009s
user	0m0.108s
sys	0m0.251s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.016.059 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.040.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.614 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.614 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.619 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.619 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.619 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.539 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.968 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.052.969 I llama_model_loader: - type  f32:  194 tensors
0.00.052.970 I llama_model_loader: - type q4_0:   97 tensors
0.00.052.970 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.971 I print_info: file format = GGUF V3 (latest)
0.00.052.973 I print_info: file type   = Q4_0
0.00.052.975 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.065.081 I load: special tokens cache size = 25
0.00.077.954 I load: token to piece cache size = 0.2984 MB
0.00.077.961 I print_info: arch             = gptneox
0.00.077.961 I print_info: vocab_only       = 0
0.00.077.962 I print_info: n_ctx_train      = 2048
0.00.077.962 I print_info: n_embd           = 2048
0.00.077.969 I print_info: n_layer          = 24
0.00.077.975 I print_info: n_head           = 16
0.00.077.976 I print_info: n_head_kv        = 16
0.00.077.977 I print_info: n_rot            = 32
0.00.077.977 I print_info: n_swa            = 0
0.00.077.978 I print_info: n_embd_head_k    = 128
0.00.077.978 I print_info: n_embd_head_v    = 128
0.00.077.979 I print_info: n_gqa            = 1
0.00.077.981 I print_info: n_embd_k_gqa     = 2048
0.00.077.982 I print_info: n_embd_v_gqa     = 2048
0.00.077.983 I print_info: f_norm_eps       = 1.0e-05
0.00.077.983 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.984 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.984 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.984 I print_info: f_logit_scale    = 0.0e+00
0.00.077.986 I print_info: n_ff             = 8192
0.00.077.986 I print_info: n_expert         = 0
0.00.077.986 I print_info: n_expert_used    = 0
0.00.077.986 I print_info: causal attn      = 1
0.00.077.987 I print_info: pooling type     = 0
0.00.077.987 I print_info: rope type        = 2
0.00.077.987 I print_info: rope scaling     = linear
0.00.077.988 I print_info: freq_base_train  = 10000.0
0.00.077.989 I print_info: freq_scale_train = 1
0.00.077.989 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.990 I print_info: rope_finetuned   = unknown
0.00.077.990 I print_info: ssm_d_conv       = 0
0.00.077.990 I print_info: ssm_d_inner      = 0
0.00.077.990 I print_info: ssm_d_state      = 0
0.00.077.991 I print_info: ssm_dt_rank      = 0
0.00.077.991 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.991 I print_info: model type       = 1.4B
0.00.077.992 I print_info: model params     = 1.41 B
0.00.077.992 I print_info: general.name     = 1.4B
0.00.077.993 I print_info: vocab type       = BPE
0.00.077.993 I print_info: n_vocab          = 50304
0.00.077.994 I print_info: n_merges         = 50009
0.00.077.994 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.994 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.995 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.995 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.996 I print_info: LF token         = 187 'Ċ'
0.00.077.996 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.997 I print_info: max token length = 1024
0.00.077.997 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.654.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.451 I load_tensors: offloading output layer to GPU
0.00.654.452 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.492 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.654.493 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.655.985 I llama_init_from_model: n_seq_max     = 1
0.00.655.988 I llama_init_from_model: n_ctx         = 2048
0.00.655.988 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.655.989 I llama_init_from_model: n_batch       = 2048
0.00.655.989 I llama_init_from_model: n_ubatch      = 512
0.00.655.990 I llama_init_from_model: flash_attn    = 0
0.00.655.992 I llama_init_from_model: freq_base     = 10000.0
0.00.655.992 I llama_init_from_model: freq_scale    = 1
0.00.655.994 I ggml_metal_init: allocating
0.00.656.114 I ggml_metal_init: found device: Apple M4
0.00.656.129 I ggml_metal_init: picking default device: Apple M4
0.00.658.047 I ggml_metal_init: using embedded metal library
0.00.664.663 I ggml_metal_init: GPU name:   Apple M4
0.00.664.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.673 I ggml_metal_init: simdgroup reduction   = true
0.00.664.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.674 I ggml_metal_init: has residency sets    = true
0.00.664.674 I ggml_metal_init: has bfloat            = true
0.00.664.676 I ggml_metal_init: use bfloat            = true
0.00.664.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.529 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.891 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.898 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.935 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.479 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.742.482 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.742.482 I llama_init_from_model: graph nodes  = 967
0.00.742.483 I llama_init_from_model: graph splits = 2
0.00.742.488 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.742.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.811 I main: llama threadpool init, n_threads = 4
0.00.797.856 I 
0.00.797.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.872 I 
0.00.798.044 I sampler seed: 1234
0.00.798.049 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.068 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.069 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.069 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.492.355 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50354.61 tokens per second)
0.01.492.355 I llama_perf_context_print:        load time =     781.04 ms
0.01.492.356 I llama_perf_context_print: prompt eval time =      48.98 ms /     7 tokens (    7.00 ms per token,   142.92 tokens per second)
0.01.492.357 I llama_perf_context_print:        eval time =     642.40 ms /    63 runs   (   10.20 ms per token,    98.07 tokens per second)
0.01.492.357 I llama_perf_context_print:       total time =     695.24 ms /    70 tokens
0.01.492.574 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.128s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.341 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.351 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.356 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.357 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.357 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.357 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.358 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.360 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.360 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.361 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.096 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.098 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.098 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.098 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.099 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.099 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.099 I llama_model_loader: - type  f32:  194 tensors
0.00.028.100 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.100 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.101 I print_info: file format = GGUF V3 (latest)
0.00.028.101 I print_info: file type   = Q4_1
0.00.028.102 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.235 I load: special tokens cache size = 25
0.00.042.268 I load: token to piece cache size = 0.2984 MB
0.00.042.271 I print_info: arch             = gptneox
0.00.042.271 I print_info: vocab_only       = 0
0.00.042.272 I print_info: n_ctx_train      = 2048
0.00.042.272 I print_info: n_embd           = 2048
0.00.042.272 I print_info: n_layer          = 24
0.00.042.275 I print_info: n_head           = 16
0.00.042.275 I print_info: n_head_kv        = 16
0.00.042.275 I print_info: n_rot            = 32
0.00.042.276 I print_info: n_swa            = 0
0.00.042.276 I print_info: n_embd_head_k    = 128
0.00.042.276 I print_info: n_embd_head_v    = 128
0.00.042.277 I print_info: n_gqa            = 1
0.00.042.278 I print_info: n_embd_k_gqa     = 2048
0.00.042.278 I print_info: n_embd_v_gqa     = 2048
0.00.042.279 I print_info: f_norm_eps       = 1.0e-05
0.00.042.279 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.279 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.280 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.280 I print_info: f_logit_scale    = 0.0e+00
0.00.042.280 I print_info: n_ff             = 8192
0.00.042.281 I print_info: n_expert         = 0
0.00.042.281 I print_info: n_expert_used    = 0
0.00.042.281 I print_info: causal attn      = 1
0.00.042.281 I print_info: pooling type     = 0
0.00.042.281 I print_info: rope type        = 2
0.00.042.281 I print_info: rope scaling     = linear
0.00.042.282 I print_info: freq_base_train  = 10000.0
0.00.042.282 I print_info: freq_scale_train = 1
0.00.042.282 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.283 I print_info: rope_finetuned   = unknown
0.00.042.283 I print_info: ssm_d_conv       = 0
0.00.042.283 I print_info: ssm_d_inner      = 0
0.00.042.283 I print_info: ssm_d_state      = 0
0.00.042.283 I print_info: ssm_dt_rank      = 0
0.00.042.283 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.284 I print_info: model type       = 1.4B
0.00.042.284 I print_info: model params     = 1.41 B
0.00.042.284 I print_info: general.name     = 1.4B
0.00.042.285 I print_info: vocab type       = BPE
0.00.042.285 I print_info: n_vocab          = 50304
0.00.042.285 I print_info: n_merges         = 50009
0.00.042.285 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.286 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.286 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.286 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.286 I print_info: LF token         = 187 'Ċ'
0.00.042.287 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.287 I print_info: max token length = 1024
0.00.042.287 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.685.384 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.400 I load_tensors: offloading output layer to GPU
0.00.685.401 I load_tensors: offloaded 25/25 layers to GPU
0.00.685.434 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.685.435 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.686.960 I llama_init_from_model: n_seq_max     = 1
0.00.686.964 I llama_init_from_model: n_ctx         = 2048
0.00.686.964 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.686.965 I llama_init_from_model: n_batch       = 2048
0.00.686.965 I llama_init_from_model: n_ubatch      = 512
0.00.686.966 I llama_init_from_model: flash_attn    = 0
0.00.686.968 I llama_init_from_model: freq_base     = 10000.0
0.00.686.969 I llama_init_from_model: freq_scale    = 1
0.00.686.972 I ggml_metal_init: allocating
0.00.687.047 I ggml_metal_init: found device: Apple M4
0.00.687.060 I ggml_metal_init: picking default device: Apple M4
0.00.688.902 I ggml_metal_init: using embedded metal library
0.00.695.577 I ggml_metal_init: GPU name:   Apple M4
0.00.695.582 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.584 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.584 I ggml_metal_init: simdgroup reduction   = true
0.00.695.584 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.585 I ggml_metal_init: has residency sets    = true
0.00.695.585 I ggml_metal_init: has bfloat            = true
0.00.695.585 I ggml_metal_init: use bfloat            = true
0.00.695.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.713.706 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.772.800 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.772.807 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.772.843 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.777.747 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.777.749 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.777.750 I llama_init_from_model: graph nodes  = 967
0.00.777.750 I llama_init_from_model: graph splits = 2
0.00.777.756 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.777.889 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.777.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.834.722 I main: llama threadpool init, n_threads = 4
0.00.834.765 I 
0.00.834.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.834.781 I 
0.00.834.935 I sampler seed: 1234
0.00.834.940 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.834.951 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.834.951 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.834.951 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.576.600 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.576.600 I llama_perf_context_print:        load time =     825.11 ms
0.01.576.602 I llama_perf_context_print: prompt eval time =      48.95 ms /     7 tokens (    6.99 ms per token,   143.02 tokens per second)
0.01.576.602 I llama_perf_context_print:        eval time =     690.16 ms /    63 runs   (   10.95 ms per token,    91.28 tokens per second)
0.01.576.603 I llama_perf_context_print:       total time =     742.60 ms /    70 tokens
0.01.576.842 I ggml_metal_free: deallocating

real	0m1.592s
user	0m0.111s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.831 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.960 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.965 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.971 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.972 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.972 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.974 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.975 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.975 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.978 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.978 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.829 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.802 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.523 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.525 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.526 I llama_model_loader: - type  f32:  194 tensors
0.00.025.526 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.526 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.527 I print_info: file format = GGUF V3 (latest)
0.00.025.527 I print_info: file type   = Q5_0
0.00.025.528 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.350 I load: special tokens cache size = 25
0.00.039.418 I load: token to piece cache size = 0.2984 MB
0.00.039.421 I print_info: arch             = gptneox
0.00.039.421 I print_info: vocab_only       = 0
0.00.039.421 I print_info: n_ctx_train      = 2048
0.00.039.421 I print_info: n_embd           = 2048
0.00.039.422 I print_info: n_layer          = 24
0.00.039.424 I print_info: n_head           = 16
0.00.039.425 I print_info: n_head_kv        = 16
0.00.039.425 I print_info: n_rot            = 32
0.00.039.426 I print_info: n_swa            = 0
0.00.039.426 I print_info: n_embd_head_k    = 128
0.00.039.426 I print_info: n_embd_head_v    = 128
0.00.039.427 I print_info: n_gqa            = 1
0.00.039.428 I print_info: n_embd_k_gqa     = 2048
0.00.039.428 I print_info: n_embd_v_gqa     = 2048
0.00.039.429 I print_info: f_norm_eps       = 1.0e-05
0.00.039.429 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.429 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.429 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.430 I print_info: f_logit_scale    = 0.0e+00
0.00.039.430 I print_info: n_ff             = 8192
0.00.039.430 I print_info: n_expert         = 0
0.00.039.431 I print_info: n_expert_used    = 0
0.00.039.431 I print_info: causal attn      = 1
0.00.039.431 I print_info: pooling type     = 0
0.00.039.431 I print_info: rope type        = 2
0.00.039.433 I print_info: rope scaling     = linear
0.00.039.433 I print_info: freq_base_train  = 10000.0
0.00.039.433 I print_info: freq_scale_train = 1
0.00.039.434 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.434 I print_info: rope_finetuned   = unknown
0.00.039.434 I print_info: ssm_d_conv       = 0
0.00.039.434 I print_info: ssm_d_inner      = 0
0.00.039.434 I print_info: ssm_d_state      = 0
0.00.039.434 I print_info: ssm_dt_rank      = 0
0.00.039.434 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.435 I print_info: model type       = 1.4B
0.00.039.435 I print_info: model params     = 1.41 B
0.00.039.435 I print_info: general.name     = 1.4B
0.00.039.436 I print_info: vocab type       = BPE
0.00.039.436 I print_info: n_vocab          = 50304
0.00.039.436 I print_info: n_merges         = 50009
0.00.039.436 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: LF token         = 187 'Ċ'
0.00.039.439 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.439 I print_info: max token length = 1024
0.00.039.440 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.677.375 I load_tensors: offloading 24 repeating layers to GPU
0.00.677.388 I load_tensors: offloading output layer to GPU
0.00.677.389 I load_tensors: offloaded 25/25 layers to GPU
0.00.677.424 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.677.425 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.678.862 I llama_init_from_model: n_seq_max     = 1
0.00.678.865 I llama_init_from_model: n_ctx         = 2048
0.00.678.866 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.678.866 I llama_init_from_model: n_batch       = 2048
0.00.678.867 I llama_init_from_model: n_ubatch      = 512
0.00.678.867 I llama_init_from_model: flash_attn    = 0
0.00.678.869 I llama_init_from_model: freq_base     = 10000.0
0.00.678.869 I llama_init_from_model: freq_scale    = 1
0.00.678.871 I ggml_metal_init: allocating
0.00.678.951 I ggml_metal_init: found device: Apple M4
0.00.678.964 I ggml_metal_init: picking default device: Apple M4
0.00.680.502 I ggml_metal_init: using embedded metal library
0.00.686.943 I ggml_metal_init: GPU name:   Apple M4
0.00.686.947 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.948 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.949 I ggml_metal_init: simdgroup reduction   = true
0.00.686.949 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.950 I ggml_metal_init: has residency sets    = true
0.00.686.950 I ggml_metal_init: has bfloat            = true
0.00.686.950 I ggml_metal_init: use bfloat            = true
0.00.686.951 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.953 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.704.682 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.177 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.758.185 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.758.222 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.762.383 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.762.385 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.762.386 I llama_init_from_model: graph nodes  = 967
0.00.762.386 I llama_init_from_model: graph splits = 2
0.00.762.392 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.762.524 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.762.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.118 I main: llama threadpool init, n_threads = 4
0.00.818.164 I 
0.00.818.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.182 I 
0.00.818.334 I sampler seed: 1234
0.00.818.339 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.350 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.350 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.350 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.603.749 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.603.752 I llama_perf_context_print:        load time =     808.59 ms
0.01.603.753 I llama_perf_context_print: prompt eval time =      42.74 ms /     7 tokens (    6.11 ms per token,   163.78 tokens per second)
0.01.603.754 I llama_perf_context_print:        eval time =     739.78 ms /    63 runs   (   11.74 ms per token,    85.16 tokens per second)
0.01.603.754 I llama_perf_context_print:       total time =     786.33 ms /    70 tokens
0.01.604.018 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.108s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.978 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.494 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.500 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.513 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.395 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.364 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.076 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.077 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.077 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.078 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.078 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.078 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.079 I llama_model_loader: - type  f32:  194 tensors
0.00.026.079 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.079 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.080 I print_info: file format = GGUF V3 (latest)
0.00.026.081 I print_info: file type   = Q5_1
0.00.026.085 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.030 I load: special tokens cache size = 25
0.00.040.069 I load: token to piece cache size = 0.2984 MB
0.00.040.072 I print_info: arch             = gptneox
0.00.040.072 I print_info: vocab_only       = 0
0.00.040.072 I print_info: n_ctx_train      = 2048
0.00.040.072 I print_info: n_embd           = 2048
0.00.040.073 I print_info: n_layer          = 24
0.00.040.075 I print_info: n_head           = 16
0.00.040.076 I print_info: n_head_kv        = 16
0.00.040.076 I print_info: n_rot            = 32
0.00.040.076 I print_info: n_swa            = 0
0.00.040.077 I print_info: n_embd_head_k    = 128
0.00.040.077 I print_info: n_embd_head_v    = 128
0.00.040.077 I print_info: n_gqa            = 1
0.00.040.078 I print_info: n_embd_k_gqa     = 2048
0.00.040.079 I print_info: n_embd_v_gqa     = 2048
0.00.040.081 I print_info: f_norm_eps       = 1.0e-05
0.00.040.082 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.082 I print_info: f_logit_scale    = 0.0e+00
0.00.040.083 I print_info: n_ff             = 8192
0.00.040.083 I print_info: n_expert         = 0
0.00.040.083 I print_info: n_expert_used    = 0
0.00.040.083 I print_info: causal attn      = 1
0.00.040.083 I print_info: pooling type     = 0
0.00.040.085 I print_info: rope type        = 2
0.00.040.087 I print_info: rope scaling     = linear
0.00.040.087 I print_info: freq_base_train  = 10000.0
0.00.040.087 I print_info: freq_scale_train = 1
0.00.040.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.088 I print_info: rope_finetuned   = unknown
0.00.040.088 I print_info: ssm_d_conv       = 0
0.00.040.088 I print_info: ssm_d_inner      = 0
0.00.040.088 I print_info: ssm_d_state      = 0
0.00.040.088 I print_info: ssm_dt_rank      = 0
0.00.040.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.089 I print_info: model type       = 1.4B
0.00.040.089 I print_info: model params     = 1.41 B
0.00.040.089 I print_info: general.name     = 1.4B
0.00.040.089 I print_info: vocab type       = BPE
0.00.040.090 I print_info: n_vocab          = 50304
0.00.040.090 I print_info: n_merges         = 50009
0.00.040.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: LF token         = 187 'Ċ'
0.00.040.095 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.095 I print_info: max token length = 1024
0.00.040.096 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.839 I load_tensors: offloading output layer to GPU
0.00.604.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.865 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.867 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.606.335 I llama_init_from_model: n_seq_max     = 1
0.00.606.337 I llama_init_from_model: n_ctx         = 2048
0.00.606.338 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.606.339 I llama_init_from_model: n_batch       = 2048
0.00.606.339 I llama_init_from_model: n_ubatch      = 512
0.00.606.339 I llama_init_from_model: flash_attn    = 0
0.00.606.340 I llama_init_from_model: freq_base     = 10000.0
0.00.606.341 I llama_init_from_model: freq_scale    = 1
0.00.606.342 I ggml_metal_init: allocating
0.00.606.358 I ggml_metal_init: found device: Apple M4
0.00.606.368 I ggml_metal_init: picking default device: Apple M4
0.00.607.856 I ggml_metal_init: using embedded metal library
0.00.613.962 I ggml_metal_init: GPU name:   Apple M4
0.00.613.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.966 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.968 I ggml_metal_init: simdgroup reduction   = true
0.00.613.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.968 I ggml_metal_init: has residency sets    = true
0.00.613.968 I ggml_metal_init: has bfloat            = true
0.00.613.969 I ggml_metal_init: use bfloat            = true
0.00.613.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.161 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.610 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.685.616 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.685.654 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.690.192 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.690.193 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.690.194 I llama_init_from_model: graph nodes  = 967
0.00.690.194 I llama_init_from_model: graph splits = 2
0.00.690.199 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.690.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.690.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.124 I main: llama threadpool init, n_threads = 4
0.00.745.167 I 
0.00.745.180 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.180 I 
0.00.745.342 I sampler seed: 1234
0.00.745.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.357 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.358 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.359 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.576.099 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.01.576.100 I llama_perf_context_print:        load time =     734.46 ms
0.01.576.101 I llama_perf_context_print: prompt eval time =      41.91 ms /     7 tokens (    5.99 ms per token,   167.00 tokens per second)
0.01.576.102 I llama_perf_context_print:        eval time =     786.02 ms /    63 runs   (   12.48 ms per token,    80.15 tokens per second)
0.01.576.102 I llama_perf_context_print:       total time =     831.66 ms /    70 tokens
0.01.576.396 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.108s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.286 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.116 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.151 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.209 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.070 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.072 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.072 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.073 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.073 I llama_model_loader: - type  f32:  194 tensors
0.00.025.074 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.074 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.074 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.075 I print_info: file format = GGUF V3 (latest)
0.00.025.075 I print_info: file type   = Q2_K - Medium
0.00.025.076 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.242 I load: special tokens cache size = 25
0.00.039.213 I load: token to piece cache size = 0.2984 MB
0.00.039.217 I print_info: arch             = gptneox
0.00.039.218 I print_info: vocab_only       = 0
0.00.039.218 I print_info: n_ctx_train      = 2048
0.00.039.218 I print_info: n_embd           = 2048
0.00.039.218 I print_info: n_layer          = 24
0.00.039.221 I print_info: n_head           = 16
0.00.039.222 I print_info: n_head_kv        = 16
0.00.039.222 I print_info: n_rot            = 32
0.00.039.223 I print_info: n_swa            = 0
0.00.039.224 I print_info: n_embd_head_k    = 128
0.00.039.224 I print_info: n_embd_head_v    = 128
0.00.039.225 I print_info: n_gqa            = 1
0.00.039.225 I print_info: n_embd_k_gqa     = 2048
0.00.039.226 I print_info: n_embd_v_gqa     = 2048
0.00.039.227 I print_info: f_norm_eps       = 1.0e-05
0.00.039.227 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.227 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.228 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.228 I print_info: f_logit_scale    = 0.0e+00
0.00.039.229 I print_info: n_ff             = 8192
0.00.039.229 I print_info: n_expert         = 0
0.00.039.230 I print_info: n_expert_used    = 0
0.00.039.230 I print_info: causal attn      = 1
0.00.039.230 I print_info: pooling type     = 0
0.00.039.230 I print_info: rope type        = 2
0.00.039.230 I print_info: rope scaling     = linear
0.00.039.231 I print_info: freq_base_train  = 10000.0
0.00.039.231 I print_info: freq_scale_train = 1
0.00.039.231 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.231 I print_info: rope_finetuned   = unknown
0.00.039.232 I print_info: ssm_d_conv       = 0
0.00.039.232 I print_info: ssm_d_inner      = 0
0.00.039.233 I print_info: ssm_d_state      = 0
0.00.039.233 I print_info: ssm_dt_rank      = 0
0.00.039.234 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.234 I print_info: model type       = 1.4B
0.00.039.234 I print_info: model params     = 1.41 B
0.00.039.234 I print_info: general.name     = 1.4B
0.00.039.235 I print_info: vocab type       = BPE
0.00.039.236 I print_info: n_vocab          = 50304
0.00.039.237 I print_info: n_merges         = 50009
0.00.039.237 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.237 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.237 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.237 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.238 I print_info: LF token         = 187 'Ċ'
0.00.039.238 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.238 I print_info: max token length = 1024
0.00.039.238 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.348.632 I load_tensors: offloading 24 repeating layers to GPU
0.00.348.646 I load_tensors: offloading output layer to GPU
0.00.348.647 I load_tensors: offloaded 25/25 layers to GPU
0.00.348.679 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.348.697 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.350.132 I llama_init_from_model: n_seq_max     = 1
0.00.350.138 I llama_init_from_model: n_ctx         = 2048
0.00.350.139 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.350.139 I llama_init_from_model: n_batch       = 2048
0.00.350.139 I llama_init_from_model: n_ubatch      = 512
0.00.350.140 I llama_init_from_model: flash_attn    = 0
0.00.350.142 I llama_init_from_model: freq_base     = 10000.0
0.00.350.143 I llama_init_from_model: freq_scale    = 1
0.00.350.145 I ggml_metal_init: allocating
0.00.350.221 I ggml_metal_init: found device: Apple M4
0.00.350.234 I ggml_metal_init: picking default device: Apple M4
0.00.352.060 I ggml_metal_init: using embedded metal library
0.00.358.843 I ggml_metal_init: GPU name:   Apple M4
0.00.358.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.358.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.358.850 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.358.850 I ggml_metal_init: simdgroup reduction   = true
0.00.358.851 I ggml_metal_init: simdgroup matrix mul. = true
0.00.358.851 I ggml_metal_init: has residency sets    = true
0.00.358.851 I ggml_metal_init: has bfloat            = true
0.00.358.851 I ggml_metal_init: use bfloat            = true
0.00.358.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.358.854 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.605 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.435.991 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.435.997 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.436.032 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.440.273 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.440.274 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.440.275 I llama_init_from_model: graph nodes  = 967
0.00.440.275 I llama_init_from_model: graph splits = 2
0.00.440.281 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.440.397 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.440.397 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.664 I main: llama threadpool init, n_threads = 4
0.00.498.700 I 
0.00.498.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.766 I 
0.00.498.966 I sampler seed: 1234
0.00.498.973 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.498.987 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.498.989 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.498.989 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.163.748 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50212.16 tokens per second)
0.01.163.748 I llama_perf_context_print:        load time =     488.67 ms
0.01.163.749 I llama_perf_context_print: prompt eval time =      35.89 ms /     7 tokens (    5.13 ms per token,   195.04 tokens per second)
0.01.163.749 I llama_perf_context_print:        eval time =     626.33 ms /    63 runs   (    9.94 ms per token,   100.59 tokens per second)
0.01.163.750 I llama_perf_context_print:       total time =     665.79 ms /    70 tokens
0.01.164.018 I ggml_metal_free: deallocating

real	0m1.181s
user	0m0.112s
sys	0m0.168s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.173 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.174 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.176 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.176 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.176 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.039 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.076 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.903 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.903 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.904 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.905 I llama_model_loader: - type  f32:  194 tensors
0.00.024.905 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.905 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.906 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.906 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.906 I print_info: file format = GGUF V3 (latest)
0.00.024.907 I print_info: file type   = Q3_K - Medium
0.00.024.908 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.732 I load: special tokens cache size = 25
0.00.038.820 I load: token to piece cache size = 0.2984 MB
0.00.038.823 I print_info: arch             = gptneox
0.00.038.823 I print_info: vocab_only       = 0
0.00.038.823 I print_info: n_ctx_train      = 2048
0.00.038.823 I print_info: n_embd           = 2048
0.00.038.823 I print_info: n_layer          = 24
0.00.038.826 I print_info: n_head           = 16
0.00.038.827 I print_info: n_head_kv        = 16
0.00.038.827 I print_info: n_rot            = 32
0.00.038.827 I print_info: n_swa            = 0
0.00.038.827 I print_info: n_embd_head_k    = 128
0.00.038.828 I print_info: n_embd_head_v    = 128
0.00.038.828 I print_info: n_gqa            = 1
0.00.038.829 I print_info: n_embd_k_gqa     = 2048
0.00.038.830 I print_info: n_embd_v_gqa     = 2048
0.00.038.830 I print_info: f_norm_eps       = 1.0e-05
0.00.038.831 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.831 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.831 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.831 I print_info: f_logit_scale    = 0.0e+00
0.00.038.832 I print_info: n_ff             = 8192
0.00.038.832 I print_info: n_expert         = 0
0.00.038.832 I print_info: n_expert_used    = 0
0.00.038.834 I print_info: causal attn      = 1
0.00.038.834 I print_info: pooling type     = 0
0.00.038.834 I print_info: rope type        = 2
0.00.038.837 I print_info: rope scaling     = linear
0.00.038.838 I print_info: freq_base_train  = 10000.0
0.00.038.838 I print_info: freq_scale_train = 1
0.00.038.838 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.838 I print_info: rope_finetuned   = unknown
0.00.038.840 I print_info: ssm_d_conv       = 0
0.00.038.840 I print_info: ssm_d_inner      = 0
0.00.038.840 I print_info: ssm_d_state      = 0
0.00.038.840 I print_info: ssm_dt_rank      = 0
0.00.038.840 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.841 I print_info: model type       = 1.4B
0.00.038.841 I print_info: model params     = 1.41 B
0.00.038.841 I print_info: general.name     = 1.4B
0.00.038.842 I print_info: vocab type       = BPE
0.00.038.842 I print_info: n_vocab          = 50304
0.00.038.842 I print_info: n_merges         = 50009
0.00.038.842 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.843 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.843 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.843 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.843 I print_info: LF token         = 187 'Ċ'
0.00.038.843 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.845 I print_info: max token length = 1024
0.00.038.845 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.446.883 I load_tensors: offloading 24 repeating layers to GPU
0.00.446.898 I load_tensors: offloading output layer to GPU
0.00.446.899 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.931 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.446.932 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.617 I llama_init_from_model: n_seq_max     = 1
0.00.448.628 I llama_init_from_model: n_ctx         = 2048
0.00.448.629 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.448.629 I llama_init_from_model: n_batch       = 2048
0.00.448.629 I llama_init_from_model: n_ubatch      = 512
0.00.448.630 I llama_init_from_model: flash_attn    = 0
0.00.448.631 I llama_init_from_model: freq_base     = 10000.0
0.00.448.632 I llama_init_from_model: freq_scale    = 1
0.00.448.635 I ggml_metal_init: allocating
0.00.448.684 I ggml_metal_init: found device: Apple M4
0.00.448.696 I ggml_metal_init: picking default device: Apple M4
0.00.451.233 I ggml_metal_init: using embedded metal library
0.00.458.961 I ggml_metal_init: GPU name:   Apple M4
0.00.458.980 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.458.981 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.458.981 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.458.982 I ggml_metal_init: simdgroup reduction   = true
0.00.458.983 I ggml_metal_init: simdgroup matrix mul. = true
0.00.458.983 I ggml_metal_init: has residency sets    = true
0.00.458.983 I ggml_metal_init: has bfloat            = true
0.00.458.983 I ggml_metal_init: use bfloat            = true
0.00.458.988 I ggml_metal_init: hasUnifiedMemory      = true
0.00.458.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.481.219 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.543.304 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.543.380 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.547.612 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.547.614 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.547.614 I llama_init_from_model: graph nodes  = 967
0.00.547.614 I llama_init_from_model: graph splits = 2
0.00.547.620 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.547.741 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.547.742 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.146 I main: llama threadpool init, n_threads = 4
0.00.607.195 I 
0.00.607.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.210 I 
0.00.607.390 I sampler seed: 1234
0.00.607.395 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.607.406 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.607.406 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.607.408 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.347.942 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.347.943 I llama_perf_context_print:        load time =     597.75 ms
0.01.347.944 I llama_perf_context_print: prompt eval time =      45.86 ms /     7 tokens (    6.55 ms per token,   152.63 tokens per second)
0.01.347.944 I llama_perf_context_print:        eval time =     691.73 ms /    63 runs   (   10.98 ms per token,    91.08 tokens per second)
0.01.347.945 I llama_perf_context_print:       total time =     741.49 ms /    70 tokens
0.01.348.194 I ggml_metal_free: deallocating

real	0m1.367s
user	0m0.114s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.715 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.720 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.721 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.724 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.725 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.729 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.729 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.729 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.525 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.358 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.360 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.360 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.361 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.361 I llama_model_loader: - type  f32:  194 tensors
0.00.026.362 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.362 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.362 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.363 I print_info: file format = GGUF V3 (latest)
0.00.026.363 I print_info: file type   = Q4_K - Medium
0.00.026.364 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.132 I load: special tokens cache size = 25
0.00.040.229 I load: token to piece cache size = 0.2984 MB
0.00.040.232 I print_info: arch             = gptneox
0.00.040.232 I print_info: vocab_only       = 0
0.00.040.232 I print_info: n_ctx_train      = 2048
0.00.040.233 I print_info: n_embd           = 2048
0.00.040.233 I print_info: n_layer          = 24
0.00.040.236 I print_info: n_head           = 16
0.00.040.237 I print_info: n_head_kv        = 16
0.00.040.237 I print_info: n_rot            = 32
0.00.040.237 I print_info: n_swa            = 0
0.00.040.237 I print_info: n_embd_head_k    = 128
0.00.040.237 I print_info: n_embd_head_v    = 128
0.00.040.238 I print_info: n_gqa            = 1
0.00.040.239 I print_info: n_embd_k_gqa     = 2048
0.00.040.241 I print_info: n_embd_v_gqa     = 2048
0.00.040.242 I print_info: f_norm_eps       = 1.0e-05
0.00.040.243 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.244 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.244 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.244 I print_info: f_logit_scale    = 0.0e+00
0.00.040.245 I print_info: n_ff             = 8192
0.00.040.245 I print_info: n_expert         = 0
0.00.040.245 I print_info: n_expert_used    = 0
0.00.040.245 I print_info: causal attn      = 1
0.00.040.247 I print_info: pooling type     = 0
0.00.040.248 I print_info: rope type        = 2
0.00.040.248 I print_info: rope scaling     = linear
0.00.040.248 I print_info: freq_base_train  = 10000.0
0.00.040.249 I print_info: freq_scale_train = 1
0.00.040.249 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.249 I print_info: rope_finetuned   = unknown
0.00.040.249 I print_info: ssm_d_conv       = 0
0.00.040.249 I print_info: ssm_d_inner      = 0
0.00.040.249 I print_info: ssm_d_state      = 0
0.00.040.249 I print_info: ssm_dt_rank      = 0
0.00.040.250 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.250 I print_info: model type       = 1.4B
0.00.040.250 I print_info: model params     = 1.41 B
0.00.040.250 I print_info: general.name     = 1.4B
0.00.040.255 I print_info: vocab type       = BPE
0.00.040.255 I print_info: n_vocab          = 50304
0.00.040.255 I print_info: n_merges         = 50009
0.00.040.256 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.256 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.256 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.257 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.257 I print_info: LF token         = 187 'Ċ'
0.00.040.257 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.257 I print_info: max token length = 1024
0.00.040.260 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.536.582 I load_tensors: offloading 24 repeating layers to GPU
0.00.536.593 I load_tensors: offloading output layer to GPU
0.00.536.594 I load_tensors: offloaded 25/25 layers to GPU
0.00.536.625 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.536.627 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.538.348 I llama_init_from_model: n_seq_max     = 1
0.00.538.367 I llama_init_from_model: n_ctx         = 2048
0.00.538.368 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.538.368 I llama_init_from_model: n_batch       = 2048
0.00.538.372 I llama_init_from_model: n_ubatch      = 512
0.00.538.372 I llama_init_from_model: flash_attn    = 0
0.00.538.374 I llama_init_from_model: freq_base     = 10000.0
0.00.538.375 I llama_init_from_model: freq_scale    = 1
0.00.538.377 I ggml_metal_init: allocating
0.00.538.454 I ggml_metal_init: found device: Apple M4
0.00.538.468 I ggml_metal_init: picking default device: Apple M4
0.00.540.331 I ggml_metal_init: using embedded metal library
0.00.546.957 I ggml_metal_init: GPU name:   Apple M4
0.00.546.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.546.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.546.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.546.965 I ggml_metal_init: simdgroup reduction   = true
0.00.546.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.546.966 I ggml_metal_init: has residency sets    = true
0.00.546.966 I ggml_metal_init: has bfloat            = true
0.00.546.966 I ggml_metal_init: use bfloat            = true
0.00.546.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.546.976 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.307 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.252 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.625.261 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.625.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.629.377 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.629.379 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.629.379 I llama_init_from_model: graph nodes  = 967
0.00.629.379 I llama_init_from_model: graph splits = 2
0.00.629.384 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.629.513 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.629.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.097 I main: llama threadpool init, n_threads = 4
0.00.686.135 I 
0.00.686.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.149 I 
0.00.686.273 I sampler seed: 1234
0.00.686.278 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.288 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.288 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.288 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.456.961 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.01.456.962 I llama_perf_context_print:        load time =     675.49 ms
0.01.456.963 I llama_perf_context_print: prompt eval time =      57.49 ms /     7 tokens (    8.21 ms per token,   121.76 tokens per second)
0.01.456.963 I llama_perf_context_print:        eval time =     710.26 ms /    63 runs   (   11.27 ms per token,    88.70 tokens per second)
0.01.456.964 I llama_perf_context_print:       total time =     771.54 ms /    70 tokens
0.01.457.242 I ggml_metal_free: deallocating

real	0m1.475s
user	0m0.111s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.759 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.523 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.523 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.526 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.351 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.174 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.175 I llama_model_loader: - type  f32:  194 tensors
0.00.024.175 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.175 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.176 I print_info: file format = GGUF V3 (latest)
0.00.024.176 I print_info: file type   = Q5_K - Medium
0.00.024.182 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.938 I load: special tokens cache size = 25
0.00.037.920 I load: token to piece cache size = 0.2984 MB
0.00.037.923 I print_info: arch             = gptneox
0.00.037.923 I print_info: vocab_only       = 0
0.00.037.924 I print_info: n_ctx_train      = 2048
0.00.037.924 I print_info: n_embd           = 2048
0.00.037.924 I print_info: n_layer          = 24
0.00.037.927 I print_info: n_head           = 16
0.00.037.928 I print_info: n_head_kv        = 16
0.00.037.928 I print_info: n_rot            = 32
0.00.037.928 I print_info: n_swa            = 0
0.00.037.928 I print_info: n_embd_head_k    = 128
0.00.037.928 I print_info: n_embd_head_v    = 128
0.00.037.929 I print_info: n_gqa            = 1
0.00.037.930 I print_info: n_embd_k_gqa     = 2048
0.00.037.931 I print_info: n_embd_v_gqa     = 2048
0.00.037.931 I print_info: f_norm_eps       = 1.0e-05
0.00.037.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.934 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.934 I print_info: f_logit_scale    = 0.0e+00
0.00.037.935 I print_info: n_ff             = 8192
0.00.037.935 I print_info: n_expert         = 0
0.00.037.935 I print_info: n_expert_used    = 0
0.00.037.936 I print_info: causal attn      = 1
0.00.037.936 I print_info: pooling type     = 0
0.00.037.936 I print_info: rope type        = 2
0.00.037.936 I print_info: rope scaling     = linear
0.00.037.936 I print_info: freq_base_train  = 10000.0
0.00.037.937 I print_info: freq_scale_train = 1
0.00.037.937 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.937 I print_info: rope_finetuned   = unknown
0.00.037.937 I print_info: ssm_d_conv       = 0
0.00.037.938 I print_info: ssm_d_inner      = 0
0.00.037.938 I print_info: ssm_d_state      = 0
0.00.037.938 I print_info: ssm_dt_rank      = 0
0.00.037.938 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.942 I print_info: model type       = 1.4B
0.00.037.943 I print_info: model params     = 1.41 B
0.00.037.943 I print_info: general.name     = 1.4B
0.00.037.944 I print_info: vocab type       = BPE
0.00.037.944 I print_info: n_vocab          = 50304
0.00.037.945 I print_info: n_merges         = 50009
0.00.037.945 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.945 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.946 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.946 I print_info: LF token         = 187 'Ċ'
0.00.037.946 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.946 I print_info: max token length = 1024
0.00.037.947 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.863 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.876 I load_tensors: offloading output layer to GPU
0.00.591.877 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.909 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.910 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.593.592 I llama_init_from_model: n_seq_max     = 1
0.00.593.595 I llama_init_from_model: n_ctx         = 2048
0.00.593.596 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.593.596 I llama_init_from_model: n_batch       = 2048
0.00.593.597 I llama_init_from_model: n_ubatch      = 512
0.00.593.597 I llama_init_from_model: flash_attn    = 0
0.00.593.599 I llama_init_from_model: freq_base     = 10000.0
0.00.593.600 I llama_init_from_model: freq_scale    = 1
0.00.593.602 I ggml_metal_init: allocating
0.00.593.721 I ggml_metal_init: found device: Apple M4
0.00.593.734 I ggml_metal_init: picking default device: Apple M4
0.00.595.754 I ggml_metal_init: using embedded metal library
0.00.602.207 I ggml_metal_init: GPU name:   Apple M4
0.00.602.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.213 I ggml_metal_init: simdgroup reduction   = true
0.00.602.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.214 I ggml_metal_init: has residency sets    = true
0.00.602.214 I ggml_metal_init: has bfloat            = true
0.00.602.214 I ggml_metal_init: use bfloat            = true
0.00.602.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.610 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.885 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.678.892 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.678.926 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.683.943 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.683.946 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.683.946 I llama_init_from_model: graph nodes  = 967
0.00.683.946 I llama_init_from_model: graph splits = 2
0.00.683.951 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.445 I main: llama threadpool init, n_threads = 4
0.00.745.489 I 
0.00.745.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.505 I 
0.00.745.678 I sampler seed: 1234
0.00.745.682 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.703 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.703 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.703 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.596.168 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.596.169 I llama_perf_context_print:        load time =     736.00 ms
0.01.596.170 I llama_perf_context_print: prompt eval time =      51.24 ms /     7 tokens (    7.32 ms per token,   136.61 tokens per second)
0.01.596.170 I llama_perf_context_print:        eval time =     796.25 ms /    63 runs   (   12.64 ms per token,    79.12 tokens per second)
0.01.596.171 I llama_perf_context_print:       total time =     851.41 ms /    70 tokens
0.01.596.434 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.109s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.753 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.350 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.354 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.356 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.856 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.856 I llama_model_loader: - type  f32:  194 tensors
0.00.023.857 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.857 I print_info: file format = GGUF V3 (latest)
0.00.023.858 I print_info: file type   = Q6_K
0.00.023.858 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.704 I load: special tokens cache size = 25
0.00.037.641 I load: token to piece cache size = 0.2984 MB
0.00.037.644 I print_info: arch             = gptneox
0.00.037.644 I print_info: vocab_only       = 0
0.00.037.644 I print_info: n_ctx_train      = 2048
0.00.037.645 I print_info: n_embd           = 2048
0.00.037.645 I print_info: n_layer          = 24
0.00.037.647 I print_info: n_head           = 16
0.00.037.648 I print_info: n_head_kv        = 16
0.00.037.648 I print_info: n_rot            = 32
0.00.037.648 I print_info: n_swa            = 0
0.00.037.648 I print_info: n_embd_head_k    = 128
0.00.037.649 I print_info: n_embd_head_v    = 128
0.00.037.649 I print_info: n_gqa            = 1
0.00.037.650 I print_info: n_embd_k_gqa     = 2048
0.00.037.651 I print_info: n_embd_v_gqa     = 2048
0.00.037.651 I print_info: f_norm_eps       = 1.0e-05
0.00.037.652 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.652 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.652 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.652 I print_info: f_logit_scale    = 0.0e+00
0.00.037.653 I print_info: n_ff             = 8192
0.00.037.653 I print_info: n_expert         = 0
0.00.037.653 I print_info: n_expert_used    = 0
0.00.037.654 I print_info: causal attn      = 1
0.00.037.654 I print_info: pooling type     = 0
0.00.037.654 I print_info: rope type        = 2
0.00.037.654 I print_info: rope scaling     = linear
0.00.037.654 I print_info: freq_base_train  = 10000.0
0.00.037.656 I print_info: freq_scale_train = 1
0.00.037.658 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.658 I print_info: rope_finetuned   = unknown
0.00.037.659 I print_info: ssm_d_conv       = 0
0.00.037.659 I print_info: ssm_d_inner      = 0
0.00.037.659 I print_info: ssm_d_state      = 0
0.00.037.659 I print_info: ssm_dt_rank      = 0
0.00.037.659 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.659 I print_info: model type       = 1.4B
0.00.037.660 I print_info: model params     = 1.41 B
0.00.037.660 I print_info: general.name     = 1.4B
0.00.037.660 I print_info: vocab type       = BPE
0.00.037.661 I print_info: n_vocab          = 50304
0.00.037.661 I print_info: n_merges         = 50009
0.00.037.661 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.661 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.661 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.661 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.662 I print_info: LF token         = 187 'Ċ'
0.00.037.662 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.662 I print_info: max token length = 1024
0.00.037.663 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.830 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.832 I load_tensors: offloading output layer to GPU
0.00.641.833 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.857 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.641.859 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.643.510 I llama_init_from_model: n_seq_max     = 1
0.00.643.512 I llama_init_from_model: n_ctx         = 2048
0.00.643.512 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.513 I llama_init_from_model: n_batch       = 2048
0.00.643.513 I llama_init_from_model: n_ubatch      = 512
0.00.643.514 I llama_init_from_model: flash_attn    = 0
0.00.643.515 I llama_init_from_model: freq_base     = 10000.0
0.00.643.515 I llama_init_from_model: freq_scale    = 1
0.00.643.517 I ggml_metal_init: allocating
0.00.643.568 I ggml_metal_init: found device: Apple M4
0.00.643.580 I ggml_metal_init: picking default device: Apple M4
0.00.645.035 I ggml_metal_init: using embedded metal library
0.00.650.718 I ggml_metal_init: GPU name:   Apple M4
0.00.650.722 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.723 I ggml_metal_init: simdgroup reduction   = true
0.00.650.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.724 I ggml_metal_init: has residency sets    = true
0.00.650.724 I ggml_metal_init: has bfloat            = true
0.00.650.724 I ggml_metal_init: use bfloat            = true
0.00.650.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.726 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.317 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.281 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.291 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.721.329 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.712 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.725.714 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.725.714 I llama_init_from_model: graph nodes  = 967
0.00.725.714 I llama_init_from_model: graph splits = 2
0.00.725.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.725.852 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.530 I main: llama threadpool init, n_threads = 4
0.00.795.576 I 
0.00.795.591 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.592 I 
0.00.795.764 I sampler seed: 1234
0.00.795.769 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.778 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.778 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.778 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.679.306 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.01.679.307 I llama_perf_context_print:        load time =     786.07 ms
0.01.679.307 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.61 tokens per second)
0.01.679.308 I llama_perf_context_print:        eval time =     826.18 ms /    63 runs   (   13.11 ms per token,    76.25 tokens per second)
0.01.679.308 I llama_perf_context_print:       total time =     884.48 ms /    70 tokens
0.01.679.533 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.108s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.752 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.180 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.367 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.374 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.376 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.379 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.382 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.384 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.384 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.385 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.532 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.533 I llama_model_loader: - type  f32:  194 tensors
0.00.049.533 I llama_model_loader: - type  f16:   98 tensors
0.00.049.534 I print_info: file format = GGUF V3 (latest)
0.00.049.535 I print_info: file type   = all F32 (guessed)
0.00.049.537 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.061.752 I load: special tokens cache size = 25
0.00.069.357 I load: token to piece cache size = 0.2984 MB
0.00.069.360 I print_info: arch             = gptneox
0.00.069.360 I print_info: vocab_only       = 0
0.00.069.360 I print_info: n_ctx_train      = 2048
0.00.069.360 I print_info: n_embd           = 2048
0.00.069.361 I print_info: n_layer          = 24
0.00.069.364 I print_info: n_head           = 16
0.00.069.364 I print_info: n_head_kv        = 16
0.00.069.365 I print_info: n_rot            = 32
0.00.069.365 I print_info: n_swa            = 0
0.00.069.365 I print_info: n_embd_head_k    = 128
0.00.069.365 I print_info: n_embd_head_v    = 128
0.00.069.366 I print_info: n_gqa            = 1
0.00.069.367 I print_info: n_embd_k_gqa     = 2048
0.00.069.367 I print_info: n_embd_v_gqa     = 2048
0.00.069.368 I print_info: f_norm_eps       = 1.0e-05
0.00.069.368 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.368 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.368 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.370 I print_info: f_logit_scale    = 0.0e+00
0.00.069.371 I print_info: n_ff             = 8192
0.00.069.371 I print_info: n_expert         = 0
0.00.069.371 I print_info: n_expert_used    = 0
0.00.069.371 I print_info: causal attn      = 1
0.00.069.371 I print_info: pooling type     = 0
0.00.069.371 I print_info: rope type        = 2
0.00.069.372 I print_info: rope scaling     = linear
0.00.069.372 I print_info: freq_base_train  = 10000.0
0.00.069.374 I print_info: freq_scale_train = 1
0.00.069.374 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.374 I print_info: rope_finetuned   = unknown
0.00.069.375 I print_info: ssm_d_conv       = 0
0.00.069.375 I print_info: ssm_d_inner      = 0
0.00.069.375 I print_info: ssm_d_state      = 0
0.00.069.375 I print_info: ssm_dt_rank      = 0
0.00.069.375 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.375 I print_info: model type       = 1.4B
0.00.069.376 I print_info: model params     = 1.41 B
0.00.069.376 I print_info: general.name     = 1.4B
0.00.069.376 I print_info: vocab type       = BPE
0.00.069.376 I print_info: n_vocab          = 50304
0.00.069.377 I print_info: n_merges         = 50009
0.00.069.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.382 I print_info: LF token         = 187 'Ċ'
0.00.069.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.383 I print_info: max token length = 1024
0.00.069.383 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.410.302 I load_tensors: offloading 24 repeating layers to GPU
0.01.410.310 I load_tensors: offloading output layer to GPU
0.01.410.311 I load_tensors: offloaded 25/25 layers to GPU
0.01.410.336 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.410.337 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.411.320 I llama_init_from_model: n_seq_max     = 1
0.01.411.321 I llama_init_from_model: n_ctx         = 128
0.01.411.321 I llama_init_from_model: n_ctx_per_seq = 128
0.01.411.321 I llama_init_from_model: n_batch       = 128
0.01.411.322 I llama_init_from_model: n_ubatch      = 128
0.01.411.322 I llama_init_from_model: flash_attn    = 0
0.01.411.323 I llama_init_from_model: freq_base     = 10000.0
0.01.411.323 I llama_init_from_model: freq_scale    = 1
0.01.411.323 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.411.324 I ggml_metal_init: allocating
0.01.411.354 I ggml_metal_init: found device: Apple M4
0.01.411.361 I ggml_metal_init: picking default device: Apple M4
0.01.412.374 I ggml_metal_init: using embedded metal library
0.01.416.067 I ggml_metal_init: GPU name:   Apple M4
0.01.416.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.416.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.416.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.416.070 I ggml_metal_init: simdgroup reduction   = true
0.01.416.070 I ggml_metal_init: simdgroup matrix mul. = true
0.01.416.070 I ggml_metal_init: has residency sets    = true
0.01.416.070 I ggml_metal_init: has bfloat            = true
0.01.416.071 I ggml_metal_init: use bfloat            = true
0.01.416.071 I ggml_metal_init: hasUnifiedMemory      = true
0.01.416.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.426.141 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.427.857 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.427.859 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.427.883 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.429.598 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.429.600 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.429.600 I llama_init_from_model: graph nodes  = 967
0.01.429.600 I llama_init_from_model: graph splits = 2
0.01.429.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.429.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.463.201 I 
0.01.463.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.463.255 I perplexity: tokenizing the input ..
0.01.467.925 I perplexity: tokenization took 4.669 ms
0.01.467.929 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.587.416 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.590.309 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.590.334 I llama_perf_context_print:        load time =    1441.01 ms
0.01.590.336 I llama_perf_context_print: prompt eval time =     119.21 ms /   128 tokens (    0.93 ms per token,  1073.70 tokens per second)
0.01.590.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.590.338 I llama_perf_context_print:       total time =     127.13 ms /   129 tokens
0.01.591.140 I ggml_metal_free: deallocating

real	0m1.784s
user	0m0.100s
sys	0m0.247s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.780 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.224 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.231 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.231 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.232 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.232 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.232 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.233 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.235 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.235 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.236 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.236 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.236 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.237 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.238 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.239 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.239 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.194 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.186 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.056 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.057 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.057 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.057 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.057 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.058 I llama_model_loader: - type  f32:  194 tensors
0.00.026.059 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.059 I print_info: file format = GGUF V3 (latest)
0.00.026.060 I print_info: file type   = Q8_0
0.00.026.061 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.046 I load: special tokens cache size = 25
0.00.039.997 I load: token to piece cache size = 0.2984 MB
0.00.040.001 I print_info: arch             = gptneox
0.00.040.001 I print_info: vocab_only       = 0
0.00.040.001 I print_info: n_ctx_train      = 2048
0.00.040.002 I print_info: n_embd           = 2048
0.00.040.002 I print_info: n_layer          = 24
0.00.040.006 I print_info: n_head           = 16
0.00.040.007 I print_info: n_head_kv        = 16
0.00.040.007 I print_info: n_rot            = 32
0.00.040.007 I print_info: n_swa            = 0
0.00.040.007 I print_info: n_embd_head_k    = 128
0.00.040.008 I print_info: n_embd_head_v    = 128
0.00.040.008 I print_info: n_gqa            = 1
0.00.040.012 I print_info: n_embd_k_gqa     = 2048
0.00.040.012 I print_info: n_embd_v_gqa     = 2048
0.00.040.013 I print_info: f_norm_eps       = 1.0e-05
0.00.040.013 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.013 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.013 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.013 I print_info: f_logit_scale    = 0.0e+00
0.00.040.014 I print_info: n_ff             = 8192
0.00.040.014 I print_info: n_expert         = 0
0.00.040.014 I print_info: n_expert_used    = 0
0.00.040.015 I print_info: causal attn      = 1
0.00.040.015 I print_info: pooling type     = 0
0.00.040.015 I print_info: rope type        = 2
0.00.040.016 I print_info: rope scaling     = linear
0.00.040.016 I print_info: freq_base_train  = 10000.0
0.00.040.016 I print_info: freq_scale_train = 1
0.00.040.017 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.018 I print_info: rope_finetuned   = unknown
0.00.040.018 I print_info: ssm_d_conv       = 0
0.00.040.018 I print_info: ssm_d_inner      = 0
0.00.040.018 I print_info: ssm_d_state      = 0
0.00.040.018 I print_info: ssm_dt_rank      = 0
0.00.040.018 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.018 I print_info: model type       = 1.4B
0.00.040.047 I print_info: model params     = 1.41 B
0.00.040.049 I print_info: general.name     = 1.4B
0.00.040.049 I print_info: vocab type       = BPE
0.00.040.050 I print_info: n_vocab          = 50304
0.00.040.050 I print_info: n_merges         = 50009
0.00.040.050 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.050 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.050 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.051 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.051 I print_info: LF token         = 187 'Ċ'
0.00.040.053 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.054 I print_info: max token length = 1024
0.00.040.054 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.913.958 I load_tensors: offloading 24 repeating layers to GPU
0.00.913.965 I load_tensors: offloading output layer to GPU
0.00.913.966 I load_tensors: offloaded 25/25 layers to GPU
0.00.913.995 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.913.998 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.915.357 I llama_init_from_model: n_seq_max     = 1
0.00.915.359 I llama_init_from_model: n_ctx         = 128
0.00.915.359 I llama_init_from_model: n_ctx_per_seq = 128
0.00.915.360 I llama_init_from_model: n_batch       = 128
0.00.915.360 I llama_init_from_model: n_ubatch      = 128
0.00.915.360 I llama_init_from_model: flash_attn    = 0
0.00.915.361 I llama_init_from_model: freq_base     = 10000.0
0.00.915.362 I llama_init_from_model: freq_scale    = 1
0.00.915.363 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.915.364 I ggml_metal_init: allocating
0.00.915.443 I ggml_metal_init: found device: Apple M4
0.00.915.453 I ggml_metal_init: picking default device: Apple M4
0.00.916.808 I ggml_metal_init: using embedded metal library
0.00.922.010 I ggml_metal_init: GPU name:   Apple M4
0.00.922.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.922.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.922.015 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.922.015 I ggml_metal_init: simdgroup reduction   = true
0.00.922.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.922.015 I ggml_metal_init: has residency sets    = true
0.00.922.016 I ggml_metal_init: has bfloat            = true
0.00.922.016 I ggml_metal_init: use bfloat            = true
0.00.922.017 I ggml_metal_init: hasUnifiedMemory      = true
0.00.922.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.936.877 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.940.223 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.940.232 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.940.273 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.943.481 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.943.483 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.943.483 I llama_init_from_model: graph nodes  = 967
0.00.943.484 I llama_init_from_model: graph splits = 2
0.00.943.486 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.943.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.972.992 I 
0.00.973.053 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.973.088 I perplexity: tokenizing the input ..
0.00.980.020 I perplexity: tokenization took 6.928 ms
0.00.980.028 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.118.304 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.119.650 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.119.667 I llama_perf_context_print:        load time =     963.20 ms
0.01.119.669 I llama_perf_context_print: prompt eval time =     137.31 ms /   128 tokens (    1.07 ms per token,   932.23 tokens per second)
0.01.119.670 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.119.670 I llama_perf_context_print:       total time =     146.68 ms /   129 tokens
0.01.120.091 I ggml_metal_free: deallocating

real	0m1.135s
user	0m0.076s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.691 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.341 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.345 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.346 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.347 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.348 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.349 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.349 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.351 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.449 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.257 I llama_model_loader: - type  f32:  194 tensors
0.00.039.257 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.257 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.257 I print_info: file format = GGUF V3 (latest)
0.00.039.258 I print_info: file type   = Q4_0
0.00.039.261 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.617 I load: special tokens cache size = 25
0.00.056.132 I load: token to piece cache size = 0.2984 MB
0.00.056.135 I print_info: arch             = gptneox
0.00.056.135 I print_info: vocab_only       = 0
0.00.056.135 I print_info: n_ctx_train      = 2048
0.00.056.136 I print_info: n_embd           = 2048
0.00.056.136 I print_info: n_layer          = 24
0.00.056.139 I print_info: n_head           = 16
0.00.056.139 I print_info: n_head_kv        = 16
0.00.056.140 I print_info: n_rot            = 32
0.00.056.140 I print_info: n_swa            = 0
0.00.056.140 I print_info: n_embd_head_k    = 128
0.00.056.140 I print_info: n_embd_head_v    = 128
0.00.056.141 I print_info: n_gqa            = 1
0.00.056.141 I print_info: n_embd_k_gqa     = 2048
0.00.056.142 I print_info: n_embd_v_gqa     = 2048
0.00.056.143 I print_info: f_norm_eps       = 1.0e-05
0.00.056.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.143 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.145 I print_info: f_logit_scale    = 0.0e+00
0.00.056.145 I print_info: n_ff             = 8192
0.00.056.145 I print_info: n_expert         = 0
0.00.056.146 I print_info: n_expert_used    = 0
0.00.056.146 I print_info: causal attn      = 1
0.00.056.146 I print_info: pooling type     = 0
0.00.056.146 I print_info: rope type        = 2
0.00.056.147 I print_info: rope scaling     = linear
0.00.056.148 I print_info: freq_base_train  = 10000.0
0.00.056.148 I print_info: freq_scale_train = 1
0.00.056.148 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.148 I print_info: rope_finetuned   = unknown
0.00.056.149 I print_info: ssm_d_conv       = 0
0.00.056.149 I print_info: ssm_d_inner      = 0
0.00.056.149 I print_info: ssm_d_state      = 0
0.00.056.149 I print_info: ssm_dt_rank      = 0
0.00.056.149 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.149 I print_info: model type       = 1.4B
0.00.056.150 I print_info: model params     = 1.41 B
0.00.056.150 I print_info: general.name     = 1.4B
0.00.056.150 I print_info: vocab type       = BPE
0.00.056.150 I print_info: n_vocab          = 50304
0.00.056.151 I print_info: n_merges         = 50009
0.00.056.151 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.151 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.151 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.151 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.152 I print_info: LF token         = 187 'Ċ'
0.00.056.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.152 I print_info: max token length = 1024
0.00.056.152 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.574 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.588 I load_tensors: offloading output layer to GPU
0.00.634.589 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.625 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.634.626 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.636.365 I llama_init_from_model: n_seq_max     = 1
0.00.636.368 I llama_init_from_model: n_ctx         = 128
0.00.636.369 I llama_init_from_model: n_ctx_per_seq = 128
0.00.636.370 I llama_init_from_model: n_batch       = 128
0.00.636.370 I llama_init_from_model: n_ubatch      = 128
0.00.636.370 I llama_init_from_model: flash_attn    = 0
0.00.636.373 I llama_init_from_model: freq_base     = 10000.0
0.00.636.373 I llama_init_from_model: freq_scale    = 1
0.00.636.374 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.636.376 I ggml_metal_init: allocating
0.00.636.448 I ggml_metal_init: found device: Apple M4
0.00.636.461 I ggml_metal_init: picking default device: Apple M4
0.00.638.205 I ggml_metal_init: using embedded metal library
0.00.643.697 I ggml_metal_init: GPU name:   Apple M4
0.00.643.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.704 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.705 I ggml_metal_init: simdgroup reduction   = true
0.00.643.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.706 I ggml_metal_init: has residency sets    = true
0.00.643.706 I ggml_metal_init: has bfloat            = true
0.00.643.706 I ggml_metal_init: use bfloat            = true
0.00.643.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.044 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.618 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.666.633 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.666.681 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.833 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.669.834 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.669.835 I llama_init_from_model: graph nodes  = 967
0.00.669.835 I llama_init_from_model: graph splits = 2
0.00.669.838 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.669.839 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.645 I 
0.00.695.703 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.739 I perplexity: tokenizing the input ..
0.00.703.253 I perplexity: tokenization took 7.511 ms
0.00.703.267 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.621 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.840.974 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.840.990 I llama_perf_context_print:        load time =     679.94 ms
0.00.840.990 I llama_perf_context_print: prompt eval time =     135.47 ms /   128 tokens (    1.06 ms per token,   944.83 tokens per second)
0.00.840.991 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.991 I llama_perf_context_print:       total time =     145.35 ms /   129 tokens
0.00.841.378 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.084s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.933 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.388 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.396 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.397 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.397 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.975 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.975 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.975 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.976 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.976 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.977 I llama_model_loader: - type  f32:  194 tensors
0.00.032.977 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.977 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.978 I print_info: file format = GGUF V3 (latest)
0.00.032.979 I print_info: file type   = Q4_1
0.00.032.980 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.041.094 I load: special tokens cache size = 25
0.00.047.070 I load: token to piece cache size = 0.2984 MB
0.00.047.074 I print_info: arch             = gptneox
0.00.047.074 I print_info: vocab_only       = 0
0.00.047.074 I print_info: n_ctx_train      = 2048
0.00.047.074 I print_info: n_embd           = 2048
0.00.047.075 I print_info: n_layer          = 24
0.00.047.078 I print_info: n_head           = 16
0.00.047.080 I print_info: n_head_kv        = 16
0.00.047.081 I print_info: n_rot            = 32
0.00.047.081 I print_info: n_swa            = 0
0.00.047.081 I print_info: n_embd_head_k    = 128
0.00.047.081 I print_info: n_embd_head_v    = 128
0.00.047.082 I print_info: n_gqa            = 1
0.00.047.082 I print_info: n_embd_k_gqa     = 2048
0.00.047.083 I print_info: n_embd_v_gqa     = 2048
0.00.047.084 I print_info: f_norm_eps       = 1.0e-05
0.00.047.084 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.084 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.085 I print_info: f_logit_scale    = 0.0e+00
0.00.047.086 I print_info: n_ff             = 8192
0.00.047.086 I print_info: n_expert         = 0
0.00.047.086 I print_info: n_expert_used    = 0
0.00.047.086 I print_info: causal attn      = 1
0.00.047.087 I print_info: pooling type     = 0
0.00.047.087 I print_info: rope type        = 2
0.00.047.087 I print_info: rope scaling     = linear
0.00.047.087 I print_info: freq_base_train  = 10000.0
0.00.047.088 I print_info: freq_scale_train = 1
0.00.047.088 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.088 I print_info: rope_finetuned   = unknown
0.00.047.088 I print_info: ssm_d_conv       = 0
0.00.047.088 I print_info: ssm_d_inner      = 0
0.00.047.089 I print_info: ssm_d_state      = 0
0.00.047.089 I print_info: ssm_dt_rank      = 0
0.00.047.089 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.089 I print_info: model type       = 1.4B
0.00.047.089 I print_info: model params     = 1.41 B
0.00.047.090 I print_info: general.name     = 1.4B
0.00.047.090 I print_info: vocab type       = BPE
0.00.047.092 I print_info: n_vocab          = 50304
0.00.047.093 I print_info: n_merges         = 50009
0.00.047.093 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.093 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.093 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.093 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.094 I print_info: LF token         = 187 'Ċ'
0.00.047.094 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.094 I print_info: max token length = 1024
0.00.047.095 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.835.668 I load_tensors: offloading 24 repeating layers to GPU
0.00.835.677 I load_tensors: offloading output layer to GPU
0.00.835.677 I load_tensors: offloaded 25/25 layers to GPU
0.00.835.709 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.835.711 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.837.252 I llama_init_from_model: n_seq_max     = 1
0.00.837.264 I llama_init_from_model: n_ctx         = 128
0.00.837.265 I llama_init_from_model: n_ctx_per_seq = 128
0.00.837.265 I llama_init_from_model: n_batch       = 128
0.00.837.265 I llama_init_from_model: n_ubatch      = 128
0.00.837.266 I llama_init_from_model: flash_attn    = 0
0.00.837.269 I llama_init_from_model: freq_base     = 10000.0
0.00.837.269 I llama_init_from_model: freq_scale    = 1
0.00.837.270 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.837.272 I ggml_metal_init: allocating
0.00.837.349 I ggml_metal_init: found device: Apple M4
0.00.837.362 I ggml_metal_init: picking default device: Apple M4
0.00.839.051 I ggml_metal_init: using embedded metal library
0.00.844.799 I ggml_metal_init: GPU name:   Apple M4
0.00.844.818 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.844.819 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.844.819 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.844.820 I ggml_metal_init: simdgroup reduction   = true
0.00.844.820 I ggml_metal_init: simdgroup matrix mul. = true
0.00.844.821 I ggml_metal_init: has residency sets    = true
0.00.844.821 I ggml_metal_init: has bfloat            = true
0.00.844.821 I ggml_metal_init: use bfloat            = true
0.00.844.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.844.828 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.864.544 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.868.264 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.868.271 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.868.323 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.871.581 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.871.583 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.871.583 I llama_init_from_model: graph nodes  = 967
0.00.871.584 I llama_init_from_model: graph splits = 2
0.00.871.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.871.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.901.429 I 
0.00.901.489 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.901.526 I perplexity: tokenizing the input ..
0.00.906.907 I perplexity: tokenization took 5.38 ms
0.00.906.913 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.043.181 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.01.044.510 I Final estimate: PPL = 10.5507 +/- 3.34263

0.01.044.523 I llama_perf_context_print:        load time =     892.48 ms
0.01.044.524 I llama_perf_context_print: prompt eval time =     136.04 ms /   128 tokens (    1.06 ms per token,   940.90 tokens per second)
0.01.044.524 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.044.525 I llama_perf_context_print:       total time =     143.10 ms /   129 tokens
0.01.044.877 I ggml_metal_free: deallocating

real	0m1.061s
user	0m0.077s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.721 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.320 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.326 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.336 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.337 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.337 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.338 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.338 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.339 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.341 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.149 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.229 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.084 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.085 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.085 I llama_model_loader: - type  f32:  194 tensors
0.00.026.086 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.086 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.087 I print_info: file format = GGUF V3 (latest)
0.00.026.094 I print_info: file type   = Q5_0
0.00.026.095 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.512 I load: special tokens cache size = 25
0.00.040.603 I load: token to piece cache size = 0.2984 MB
0.00.040.609 I print_info: arch             = gptneox
0.00.040.612 I print_info: vocab_only       = 0
0.00.040.612 I print_info: n_ctx_train      = 2048
0.00.040.612 I print_info: n_embd           = 2048
0.00.040.612 I print_info: n_layer          = 24
0.00.040.616 I print_info: n_head           = 16
0.00.040.617 I print_info: n_head_kv        = 16
0.00.040.618 I print_info: n_rot            = 32
0.00.040.618 I print_info: n_swa            = 0
0.00.040.618 I print_info: n_embd_head_k    = 128
0.00.040.618 I print_info: n_embd_head_v    = 128
0.00.040.619 I print_info: n_gqa            = 1
0.00.040.619 I print_info: n_embd_k_gqa     = 2048
0.00.040.620 I print_info: n_embd_v_gqa     = 2048
0.00.040.621 I print_info: f_norm_eps       = 1.0e-05
0.00.040.621 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.621 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.623 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.623 I print_info: f_logit_scale    = 0.0e+00
0.00.040.623 I print_info: n_ff             = 8192
0.00.040.624 I print_info: n_expert         = 0
0.00.040.624 I print_info: n_expert_used    = 0
0.00.040.624 I print_info: causal attn      = 1
0.00.040.625 I print_info: pooling type     = 0
0.00.040.626 I print_info: rope type        = 2
0.00.040.626 I print_info: rope scaling     = linear
0.00.040.626 I print_info: freq_base_train  = 10000.0
0.00.040.627 I print_info: freq_scale_train = 1
0.00.040.627 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.627 I print_info: rope_finetuned   = unknown
0.00.040.627 I print_info: ssm_d_conv       = 0
0.00.040.627 I print_info: ssm_d_inner      = 0
0.00.040.628 I print_info: ssm_d_state      = 0
0.00.040.628 I print_info: ssm_dt_rank      = 0
0.00.040.628 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.628 I print_info: model type       = 1.4B
0.00.040.629 I print_info: model params     = 1.41 B
0.00.040.629 I print_info: general.name     = 1.4B
0.00.040.629 I print_info: vocab type       = BPE
0.00.040.630 I print_info: n_vocab          = 50304
0.00.040.630 I print_info: n_merges         = 50009
0.00.040.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.631 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.631 I print_info: LF token         = 187 'Ċ'
0.00.040.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.631 I print_info: max token length = 1024
0.00.040.632 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.417.242 I load_tensors: offloading 24 repeating layers to GPU
0.01.417.251 I load_tensors: offloading output layer to GPU
0.01.417.252 I load_tensors: offloaded 25/25 layers to GPU
0.01.417.287 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.01.417.289 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.01.418.499 I llama_init_from_model: n_seq_max     = 1
0.01.418.503 I llama_init_from_model: n_ctx         = 128
0.01.418.504 I llama_init_from_model: n_ctx_per_seq = 128
0.01.418.504 I llama_init_from_model: n_batch       = 128
0.01.418.504 I llama_init_from_model: n_ubatch      = 128
0.01.418.505 I llama_init_from_model: flash_attn    = 0
0.01.418.506 I llama_init_from_model: freq_base     = 10000.0
0.01.418.507 I llama_init_from_model: freq_scale    = 1
0.01.418.507 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.418.509 I ggml_metal_init: allocating
0.01.418.562 I ggml_metal_init: found device: Apple M4
0.01.418.573 I ggml_metal_init: picking default device: Apple M4
0.01.420.244 I ggml_metal_init: using embedded metal library
0.01.425.489 I ggml_metal_init: GPU name:   Apple M4
0.01.425.498 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.425.499 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.425.499 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.425.500 I ggml_metal_init: simdgroup reduction   = true
0.01.425.500 I ggml_metal_init: simdgroup matrix mul. = true
0.01.425.500 I ggml_metal_init: has residency sets    = true
0.01.425.501 I ggml_metal_init: has bfloat            = true
0.01.425.501 I ggml_metal_init: use bfloat            = true
0.01.425.503 I ggml_metal_init: hasUnifiedMemory      = true
0.01.425.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.439.317 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.441.498 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.441.503 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.441.535 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.443.337 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.443.339 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.443.339 I llama_init_from_model: graph nodes  = 967
0.01.443.339 I llama_init_from_model: graph splits = 2
0.01.443.341 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.443.341 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.473.081 I 
0.01.473.108 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.473.126 I perplexity: tokenizing the input ..
0.01.476.962 I perplexity: tokenization took 3.834 ms
0.01.476.966 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.625.629 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.01.628.701 I Final estimate: PPL = 10.0972 +/- 3.20136

0.01.628.752 I llama_perf_context_print:        load time =    1464.35 ms
0.01.628.753 I llama_perf_context_print: prompt eval time =     148.43 ms /   128 tokens (    1.16 ms per token,   862.36 tokens per second)
0.01.628.754 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.628.755 I llama_perf_context_print:       total time =     155.67 ms /   129 tokens
0.01.629.455 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.079s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.974 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.898 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.905 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.912 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.916 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.918 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.918 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.919 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.594 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.597 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.598 I llama_model_loader: - type  f32:  194 tensors
0.00.025.598 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.599 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.599 I print_info: file format = GGUF V3 (latest)
0.00.025.600 I print_info: file type   = Q5_1
0.00.025.602 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.587 I load: special tokens cache size = 25
0.00.039.588 I load: token to piece cache size = 0.2984 MB
0.00.039.592 I print_info: arch             = gptneox
0.00.039.592 I print_info: vocab_only       = 0
0.00.039.593 I print_info: n_ctx_train      = 2048
0.00.039.593 I print_info: n_embd           = 2048
0.00.039.593 I print_info: n_layer          = 24
0.00.039.597 I print_info: n_head           = 16
0.00.039.598 I print_info: n_head_kv        = 16
0.00.039.598 I print_info: n_rot            = 32
0.00.039.598 I print_info: n_swa            = 0
0.00.039.598 I print_info: n_embd_head_k    = 128
0.00.039.599 I print_info: n_embd_head_v    = 128
0.00.039.599 I print_info: n_gqa            = 1
0.00.039.600 I print_info: n_embd_k_gqa     = 2048
0.00.039.601 I print_info: n_embd_v_gqa     = 2048
0.00.039.601 I print_info: f_norm_eps       = 1.0e-05
0.00.039.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.606 I print_info: f_logit_scale    = 0.0e+00
0.00.039.606 I print_info: n_ff             = 8192
0.00.039.607 I print_info: n_expert         = 0
0.00.039.607 I print_info: n_expert_used    = 0
0.00.039.608 I print_info: causal attn      = 1
0.00.039.608 I print_info: pooling type     = 0
0.00.039.608 I print_info: rope type        = 2
0.00.039.608 I print_info: rope scaling     = linear
0.00.039.608 I print_info: freq_base_train  = 10000.0
0.00.039.610 I print_info: freq_scale_train = 1
0.00.039.611 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.612 I print_info: rope_finetuned   = unknown
0.00.039.612 I print_info: ssm_d_conv       = 0
0.00.039.612 I print_info: ssm_d_inner      = 0
0.00.039.612 I print_info: ssm_d_state      = 0
0.00.039.612 I print_info: ssm_dt_rank      = 0
0.00.039.612 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.612 I print_info: model type       = 1.4B
0.00.039.613 I print_info: model params     = 1.41 B
0.00.039.613 I print_info: general.name     = 1.4B
0.00.039.613 I print_info: vocab type       = BPE
0.00.039.614 I print_info: n_vocab          = 50304
0.00.039.614 I print_info: n_merges         = 50009
0.00.039.614 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.615 I print_info: LF token         = 187 'Ċ'
0.00.039.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.621 I print_info: max token length = 1024
0.00.039.622 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.595 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.609 I load_tensors: offloading output layer to GPU
0.00.615.610 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.653 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.615.656 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.617.182 I llama_init_from_model: n_seq_max     = 1
0.00.617.185 I llama_init_from_model: n_ctx         = 128
0.00.617.185 I llama_init_from_model: n_ctx_per_seq = 128
0.00.617.186 I llama_init_from_model: n_batch       = 128
0.00.617.186 I llama_init_from_model: n_ubatch      = 128
0.00.617.187 I llama_init_from_model: flash_attn    = 0
0.00.617.189 I llama_init_from_model: freq_base     = 10000.0
0.00.617.190 I llama_init_from_model: freq_scale    = 1
0.00.617.190 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.617.193 I ggml_metal_init: allocating
0.00.617.273 I ggml_metal_init: found device: Apple M4
0.00.617.286 I ggml_metal_init: picking default device: Apple M4
0.00.619.135 I ggml_metal_init: using embedded metal library
0.00.625.759 I ggml_metal_init: GPU name:   Apple M4
0.00.625.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.767 I ggml_metal_init: simdgroup reduction   = true
0.00.625.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.767 I ggml_metal_init: has residency sets    = true
0.00.625.768 I ggml_metal_init: has bfloat            = true
0.00.625.768 I ggml_metal_init: use bfloat            = true
0.00.625.769 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.728 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.253 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.257 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.297 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.630 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.632 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.633 I llama_init_from_model: graph nodes  = 967
0.00.650.633 I llama_init_from_model: graph splits = 2
0.00.650.636 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.636 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.326 I 
0.00.683.394 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.434 I perplexity: tokenizing the input ..
0.00.690.731 I perplexity: tokenization took 7.293 ms
0.00.690.739 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.654 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.838.011 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.838.032 I llama_perf_context_print:        load time =     673.34 ms
0.00.838.033 I llama_perf_context_print: prompt eval time =     145.02 ms /   128 tokens (    1.13 ms per token,   882.66 tokens per second)
0.00.838.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.034 I llama_perf_context_print:       total time =     154.71 ms /   129 tokens
0.00.838.434 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.083s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.005 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.995 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.003 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.005 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.006 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.006 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.007 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.009 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.729 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.701 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.391 I llama_model_loader: - type  f32:  194 tensors
0.00.024.391 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.393 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.393 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.394 I print_info: file format = GGUF V3 (latest)
0.00.024.394 I print_info: file type   = Q2_K - Medium
0.00.024.395 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.086 I load: special tokens cache size = 25
0.00.038.036 I load: token to piece cache size = 0.2984 MB
0.00.038.039 I print_info: arch             = gptneox
0.00.038.039 I print_info: vocab_only       = 0
0.00.038.039 I print_info: n_ctx_train      = 2048
0.00.038.039 I print_info: n_embd           = 2048
0.00.038.040 I print_info: n_layer          = 24
0.00.038.043 I print_info: n_head           = 16
0.00.038.044 I print_info: n_head_kv        = 16
0.00.038.046 I print_info: n_rot            = 32
0.00.038.047 I print_info: n_swa            = 0
0.00.038.047 I print_info: n_embd_head_k    = 128
0.00.038.047 I print_info: n_embd_head_v    = 128
0.00.038.048 I print_info: n_gqa            = 1
0.00.038.048 I print_info: n_embd_k_gqa     = 2048
0.00.038.049 I print_info: n_embd_v_gqa     = 2048
0.00.038.054 I print_info: f_norm_eps       = 1.0e-05
0.00.038.054 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.054 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.054 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.054 I print_info: f_logit_scale    = 0.0e+00
0.00.038.057 I print_info: n_ff             = 8192
0.00.038.057 I print_info: n_expert         = 0
0.00.038.057 I print_info: n_expert_used    = 0
0.00.038.057 I print_info: causal attn      = 1
0.00.038.058 I print_info: pooling type     = 0
0.00.038.058 I print_info: rope type        = 2
0.00.038.058 I print_info: rope scaling     = linear
0.00.038.059 I print_info: freq_base_train  = 10000.0
0.00.038.059 I print_info: freq_scale_train = 1
0.00.038.062 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.062 I print_info: rope_finetuned   = unknown
0.00.038.062 I print_info: ssm_d_conv       = 0
0.00.038.062 I print_info: ssm_d_inner      = 0
0.00.038.062 I print_info: ssm_d_state      = 0
0.00.038.063 I print_info: ssm_dt_rank      = 0
0.00.038.063 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.064 I print_info: model type       = 1.4B
0.00.038.064 I print_info: model params     = 1.41 B
0.00.038.064 I print_info: general.name     = 1.4B
0.00.038.065 I print_info: vocab type       = BPE
0.00.038.065 I print_info: n_vocab          = 50304
0.00.038.065 I print_info: n_merges         = 50009
0.00.038.066 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.066 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.066 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.066 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.066 I print_info: LF token         = 187 'Ċ'
0.00.038.067 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.067 I print_info: max token length = 1024
0.00.038.067 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.353.346 I load_tensors: offloading 24 repeating layers to GPU
0.00.353.359 I load_tensors: offloading output layer to GPU
0.00.353.360 I load_tensors: offloaded 25/25 layers to GPU
0.00.353.400 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.353.402 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.354.918 I llama_init_from_model: n_seq_max     = 1
0.00.354.924 I llama_init_from_model: n_ctx         = 128
0.00.354.924 I llama_init_from_model: n_ctx_per_seq = 128
0.00.354.925 I llama_init_from_model: n_batch       = 128
0.00.354.925 I llama_init_from_model: n_ubatch      = 128
0.00.354.925 I llama_init_from_model: flash_attn    = 0
0.00.354.927 I llama_init_from_model: freq_base     = 10000.0
0.00.354.927 I llama_init_from_model: freq_scale    = 1
0.00.354.928 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.354.931 I ggml_metal_init: allocating
0.00.355.022 I ggml_metal_init: found device: Apple M4
0.00.355.037 I ggml_metal_init: picking default device: Apple M4
0.00.357.364 I ggml_metal_init: using embedded metal library
0.00.363.678 I ggml_metal_init: GPU name:   Apple M4
0.00.363.688 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.363.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.363.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.363.691 I ggml_metal_init: simdgroup reduction   = true
0.00.363.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.363.691 I ggml_metal_init: has residency sets    = true
0.00.363.692 I ggml_metal_init: has bfloat            = true
0.00.363.692 I ggml_metal_init: use bfloat            = true
0.00.363.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.363.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.385.246 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.388.844 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.388.851 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.388.902 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.392.437 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.392.439 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.392.439 I llama_init_from_model: graph nodes  = 967
0.00.392.440 I llama_init_from_model: graph splits = 2
0.00.392.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.392.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.422.015 I 
0.00.422.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.422.121 I perplexity: tokenizing the input ..
0.00.428.850 I perplexity: tokenization took 6.728 ms
0.00.428.854 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.738 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.562.153 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.562.171 I llama_perf_context_print:        load time =     413.00 ms
0.00.562.172 I llama_perf_context_print: prompt eval time =     131.65 ms /   128 tokens (    1.03 ms per token,   972.28 tokens per second)
0.00.562.172 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.562.173 I llama_perf_context_print:       total time =     140.16 ms /   129 tokens
0.00.562.519 I ggml_metal_free: deallocating

real	0m0.576s
user	0m0.079s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.970 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.361 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.368 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.370 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.370 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.371 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.372 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.372 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.373 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.373 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.373 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.374 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.374 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.376 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.376 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.377 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.149 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.269 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.199 I llama_model_loader: - type  f32:  194 tensors
0.00.025.199 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.200 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.200 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.200 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.201 I print_info: file format = GGUF V3 (latest)
0.00.025.201 I print_info: file type   = Q3_K - Medium
0.00.025.202 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.258 I load: special tokens cache size = 25
0.00.039.271 I load: token to piece cache size = 0.2984 MB
0.00.039.275 I print_info: arch             = gptneox
0.00.039.276 I print_info: vocab_only       = 0
0.00.039.276 I print_info: n_ctx_train      = 2048
0.00.039.276 I print_info: n_embd           = 2048
0.00.039.276 I print_info: n_layer          = 24
0.00.039.280 I print_info: n_head           = 16
0.00.039.281 I print_info: n_head_kv        = 16
0.00.039.281 I print_info: n_rot            = 32
0.00.039.281 I print_info: n_swa            = 0
0.00.039.282 I print_info: n_embd_head_k    = 128
0.00.039.282 I print_info: n_embd_head_v    = 128
0.00.039.282 I print_info: n_gqa            = 1
0.00.039.283 I print_info: n_embd_k_gqa     = 2048
0.00.039.284 I print_info: n_embd_v_gqa     = 2048
0.00.039.284 I print_info: f_norm_eps       = 1.0e-05
0.00.039.285 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.287 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.288 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.288 I print_info: f_logit_scale    = 0.0e+00
0.00.039.288 I print_info: n_ff             = 8192
0.00.039.289 I print_info: n_expert         = 0
0.00.039.289 I print_info: n_expert_used    = 0
0.00.039.289 I print_info: causal attn      = 1
0.00.039.290 I print_info: pooling type     = 0
0.00.039.290 I print_info: rope type        = 2
0.00.039.291 I print_info: rope scaling     = linear
0.00.039.291 I print_info: freq_base_train  = 10000.0
0.00.039.292 I print_info: freq_scale_train = 1
0.00.039.292 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.292 I print_info: rope_finetuned   = unknown
0.00.039.292 I print_info: ssm_d_conv       = 0
0.00.039.292 I print_info: ssm_d_inner      = 0
0.00.039.292 I print_info: ssm_d_state      = 0
0.00.039.293 I print_info: ssm_dt_rank      = 0
0.00.039.293 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.293 I print_info: model type       = 1.4B
0.00.039.295 I print_info: model params     = 1.41 B
0.00.039.295 I print_info: general.name     = 1.4B
0.00.039.295 I print_info: vocab type       = BPE
0.00.039.296 I print_info: n_vocab          = 50304
0.00.039.296 I print_info: n_merges         = 50009
0.00.039.296 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.296 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.296 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.296 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.297 I print_info: LF token         = 187 'Ċ'
0.00.039.297 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.297 I print_info: max token length = 1024
0.00.039.297 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.439.351 I load_tensors: offloading 24 repeating layers to GPU
0.00.439.360 I load_tensors: offloading output layer to GPU
0.00.439.361 I load_tensors: offloaded 25/25 layers to GPU
0.00.439.389 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.439.391 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.440.657 I llama_init_from_model: n_seq_max     = 1
0.00.440.660 I llama_init_from_model: n_ctx         = 128
0.00.440.660 I llama_init_from_model: n_ctx_per_seq = 128
0.00.440.661 I llama_init_from_model: n_batch       = 128
0.00.440.661 I llama_init_from_model: n_ubatch      = 128
0.00.440.661 I llama_init_from_model: flash_attn    = 0
0.00.440.664 I llama_init_from_model: freq_base     = 10000.0
0.00.440.664 I llama_init_from_model: freq_scale    = 1
0.00.440.665 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.440.680 I ggml_metal_init: allocating
0.00.440.787 I ggml_metal_init: found device: Apple M4
0.00.440.824 I ggml_metal_init: picking default device: Apple M4
0.00.442.551 I ggml_metal_init: using embedded metal library
0.00.448.222 I ggml_metal_init: GPU name:   Apple M4
0.00.448.228 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.448.229 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.448.230 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.448.230 I ggml_metal_init: simdgroup reduction   = true
0.00.448.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.448.231 I ggml_metal_init: has residency sets    = true
0.00.448.231 I ggml_metal_init: has bfloat            = true
0.00.448.232 I ggml_metal_init: use bfloat            = true
0.00.448.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.448.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.853 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.471.447 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.471.454 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.471.522 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.474.649 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.474.651 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.474.651 I llama_init_from_model: graph nodes  = 967
0.00.474.652 I llama_init_from_model: graph splits = 2
0.00.474.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.474.655 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.435 I 
0.00.504.496 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.504.532 I perplexity: tokenizing the input ..
0.00.509.884 I perplexity: tokenization took 5.351 ms
0.00.509.888 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.642.963 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.644.302 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.644.323 I llama_perf_context_print:        load time =     495.45 ms
0.00.644.324 I llama_perf_context_print: prompt eval time =     132.85 ms /   128 tokens (    1.04 ms per token,   963.51 tokens per second)
0.00.644.325 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.644.325 I llama_perf_context_print:       total time =     139.89 ms /   129 tokens
0.00.644.737 I ggml_metal_free: deallocating

real	0m0.658s
user	0m0.077s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.875 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.881 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.885 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.885 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.886 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.886 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.886 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.887 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.888 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.888 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.888 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.889 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.889 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.893 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.796 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.617 I llama_model_loader: - type  f32:  194 tensors
0.00.025.617 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.617 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.617 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.618 I print_info: file format = GGUF V3 (latest)
0.00.025.619 I print_info: file type   = Q4_K - Medium
0.00.025.620 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.646 I load: special tokens cache size = 25
0.00.039.722 I load: token to piece cache size = 0.2984 MB
0.00.039.726 I print_info: arch             = gptneox
0.00.039.726 I print_info: vocab_only       = 0
0.00.039.726 I print_info: n_ctx_train      = 2048
0.00.039.726 I print_info: n_embd           = 2048
0.00.039.726 I print_info: n_layer          = 24
0.00.039.731 I print_info: n_head           = 16
0.00.039.731 I print_info: n_head_kv        = 16
0.00.039.731 I print_info: n_rot            = 32
0.00.039.732 I print_info: n_swa            = 0
0.00.039.732 I print_info: n_embd_head_k    = 128
0.00.039.732 I print_info: n_embd_head_v    = 128
0.00.039.736 I print_info: n_gqa            = 1
0.00.039.736 I print_info: n_embd_k_gqa     = 2048
0.00.039.737 I print_info: n_embd_v_gqa     = 2048
0.00.039.738 I print_info: f_norm_eps       = 1.0e-05
0.00.039.739 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.739 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.739 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.740 I print_info: f_logit_scale    = 0.0e+00
0.00.039.740 I print_info: n_ff             = 8192
0.00.039.740 I print_info: n_expert         = 0
0.00.039.741 I print_info: n_expert_used    = 0
0.00.039.741 I print_info: causal attn      = 1
0.00.039.741 I print_info: pooling type     = 0
0.00.039.741 I print_info: rope type        = 2
0.00.039.741 I print_info: rope scaling     = linear
0.00.039.742 I print_info: freq_base_train  = 10000.0
0.00.039.742 I print_info: freq_scale_train = 1
0.00.039.742 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.743 I print_info: rope_finetuned   = unknown
0.00.039.743 I print_info: ssm_d_conv       = 0
0.00.039.743 I print_info: ssm_d_inner      = 0
0.00.039.743 I print_info: ssm_d_state      = 0
0.00.039.744 I print_info: ssm_dt_rank      = 0
0.00.039.744 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.745 I print_info: model type       = 1.4B
0.00.039.745 I print_info: model params     = 1.41 B
0.00.039.745 I print_info: general.name     = 1.4B
0.00.039.746 I print_info: vocab type       = BPE
0.00.039.746 I print_info: n_vocab          = 50304
0.00.039.746 I print_info: n_merges         = 50009
0.00.039.746 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: LF token         = 187 'Ċ'
0.00.039.747 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.748 I print_info: max token length = 1024
0.00.039.748 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.532.867 I load_tensors: offloading 24 repeating layers to GPU
0.00.532.881 I load_tensors: offloading output layer to GPU
0.00.532.881 I load_tensors: offloaded 25/25 layers to GPU
0.00.532.913 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.532.914 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.534.735 I llama_init_from_model: n_seq_max     = 1
0.00.534.742 I llama_init_from_model: n_ctx         = 128
0.00.534.743 I llama_init_from_model: n_ctx_per_seq = 128
0.00.534.743 I llama_init_from_model: n_batch       = 128
0.00.534.744 I llama_init_from_model: n_ubatch      = 128
0.00.534.744 I llama_init_from_model: flash_attn    = 0
0.00.534.746 I llama_init_from_model: freq_base     = 10000.0
0.00.534.747 I llama_init_from_model: freq_scale    = 1
0.00.534.748 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.534.750 I ggml_metal_init: allocating
0.00.534.872 I ggml_metal_init: found device: Apple M4
0.00.534.885 I ggml_metal_init: picking default device: Apple M4
0.00.536.806 I ggml_metal_init: using embedded metal library
0.00.543.527 I ggml_metal_init: GPU name:   Apple M4
0.00.543.532 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.535 I ggml_metal_init: simdgroup reduction   = true
0.00.543.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.535 I ggml_metal_init: has residency sets    = true
0.00.543.535 I ggml_metal_init: has bfloat            = true
0.00.543.536 I ggml_metal_init: use bfloat            = true
0.00.543.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.447 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.565.929 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.565.933 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.565.984 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.569.168 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.569.169 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.569.170 I llama_init_from_model: graph nodes  = 967
0.00.569.170 I llama_init_from_model: graph splits = 2
0.00.569.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.569.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.410 I 
0.00.597.466 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.504 I perplexity: tokenizing the input ..
0.00.604.608 I perplexity: tokenization took 7.1 ms
0.00.604.621 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.636 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.749.976 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.749.992 I llama_perf_context_print:        load time =     587.58 ms
0.00.749.994 I llama_perf_context_print: prompt eval time =     143.09 ms /   128 tokens (    1.12 ms per token,   894.51 tokens per second)
0.00.749.995 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.749.995 I llama_perf_context_print:       total time =     152.59 ms /   129 tokens
0.00.750.364 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.081s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.746 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.579 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.587 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.589 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.592 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.593 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.594 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.489 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.390 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.392 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.392 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.393 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.393 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.394 I llama_model_loader: - type  f32:  194 tensors
0.00.024.394 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.394 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.395 I print_info: file format = GGUF V3 (latest)
0.00.024.396 I print_info: file type   = Q5_K - Medium
0.00.024.397 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.677 I load: special tokens cache size = 25
0.00.038.859 I load: token to piece cache size = 0.2984 MB
0.00.038.862 I print_info: arch             = gptneox
0.00.038.863 I print_info: vocab_only       = 0
0.00.038.863 I print_info: n_ctx_train      = 2048
0.00.038.863 I print_info: n_embd           = 2048
0.00.038.863 I print_info: n_layer          = 24
0.00.038.867 I print_info: n_head           = 16
0.00.038.868 I print_info: n_head_kv        = 16
0.00.038.868 I print_info: n_rot            = 32
0.00.038.868 I print_info: n_swa            = 0
0.00.038.868 I print_info: n_embd_head_k    = 128
0.00.038.869 I print_info: n_embd_head_v    = 128
0.00.038.869 I print_info: n_gqa            = 1
0.00.038.870 I print_info: n_embd_k_gqa     = 2048
0.00.038.871 I print_info: n_embd_v_gqa     = 2048
0.00.038.871 I print_info: f_norm_eps       = 1.0e-05
0.00.038.872 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.872 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.872 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.872 I print_info: f_logit_scale    = 0.0e+00
0.00.038.873 I print_info: n_ff             = 8192
0.00.038.873 I print_info: n_expert         = 0
0.00.038.874 I print_info: n_expert_used    = 0
0.00.038.874 I print_info: causal attn      = 1
0.00.038.874 I print_info: pooling type     = 0
0.00.038.874 I print_info: rope type        = 2
0.00.038.874 I print_info: rope scaling     = linear
0.00.038.875 I print_info: freq_base_train  = 10000.0
0.00.038.875 I print_info: freq_scale_train = 1
0.00.038.875 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.875 I print_info: rope_finetuned   = unknown
0.00.038.876 I print_info: ssm_d_conv       = 0
0.00.038.876 I print_info: ssm_d_inner      = 0
0.00.038.876 I print_info: ssm_d_state      = 0
0.00.038.878 I print_info: ssm_dt_rank      = 0
0.00.038.878 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.878 I print_info: model type       = 1.4B
0.00.038.878 I print_info: model params     = 1.41 B
0.00.038.879 I print_info: general.name     = 1.4B
0.00.038.879 I print_info: vocab type       = BPE
0.00.038.879 I print_info: n_vocab          = 50304
0.00.038.879 I print_info: n_merges         = 50009
0.00.038.880 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.880 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.880 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.880 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.880 I print_info: LF token         = 187 'Ċ'
0.00.038.881 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.881 I print_info: max token length = 1024
0.00.038.881 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.334 I load_tensors: offloading output layer to GPU
0.00.609.335 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.365 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.609.366 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.610.852 I llama_init_from_model: n_seq_max     = 1
0.00.610.857 I llama_init_from_model: n_ctx         = 128
0.00.610.857 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.858 I llama_init_from_model: n_batch       = 128
0.00.610.858 I llama_init_from_model: n_ubatch      = 128
0.00.610.859 I llama_init_from_model: flash_attn    = 0
0.00.610.860 I llama_init_from_model: freq_base     = 10000.0
0.00.610.861 I llama_init_from_model: freq_scale    = 1
0.00.610.861 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.867 I ggml_metal_init: allocating
0.00.610.917 I ggml_metal_init: found device: Apple M4
0.00.610.931 I ggml_metal_init: picking default device: Apple M4
0.00.612.698 I ggml_metal_init: using embedded metal library
0.00.619.241 I ggml_metal_init: GPU name:   Apple M4
0.00.619.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.245 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.246 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.247 I ggml_metal_init: simdgroup reduction   = true
0.00.619.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.247 I ggml_metal_init: has residency sets    = true
0.00.619.248 I ggml_metal_init: has bfloat            = true
0.00.619.248 I ggml_metal_init: use bfloat            = true
0.00.619.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.367 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.878 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.639.886 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.639.942 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.199 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.643.201 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.643.201 I llama_init_from_model: graph nodes  = 967
0.00.643.202 I llama_init_from_model: graph splits = 2
0.00.643.205 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.205 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.577 I 
0.00.682.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.677 I perplexity: tokenizing the input ..
0.00.689.695 I perplexity: tokenization took 7.015 ms
0.00.689.701 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.396 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.845.731 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.845.747 I llama_perf_context_print:        load time =     673.82 ms
0.00.845.748 I llama_perf_context_print: prompt eval time =     153.77 ms /   128 tokens (    1.20 ms per token,   832.41 tokens per second)
0.00.845.749 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.749 I llama_perf_context_print:       total time =     163.17 ms /   129 tokens
0.00.846.198 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.080s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.472 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.338 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.338 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.339 I llama_model_loader: - type  f32:  194 tensors
0.00.024.339 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.340 I print_info: file format = GGUF V3 (latest)
0.00.024.341 I print_info: file type   = Q6_K
0.00.024.342 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.146 I load: special tokens cache size = 25
0.00.038.259 I load: token to piece cache size = 0.2984 MB
0.00.038.262 I print_info: arch             = gptneox
0.00.038.262 I print_info: vocab_only       = 0
0.00.038.262 I print_info: n_ctx_train      = 2048
0.00.038.262 I print_info: n_embd           = 2048
0.00.038.262 I print_info: n_layer          = 24
0.00.038.266 I print_info: n_head           = 16
0.00.038.267 I print_info: n_head_kv        = 16
0.00.038.267 I print_info: n_rot            = 32
0.00.038.267 I print_info: n_swa            = 0
0.00.038.267 I print_info: n_embd_head_k    = 128
0.00.038.267 I print_info: n_embd_head_v    = 128
0.00.038.268 I print_info: n_gqa            = 1
0.00.038.269 I print_info: n_embd_k_gqa     = 2048
0.00.038.270 I print_info: n_embd_v_gqa     = 2048
0.00.038.270 I print_info: f_norm_eps       = 1.0e-05
0.00.038.271 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.271 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.271 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.271 I print_info: f_logit_scale    = 0.0e+00
0.00.038.272 I print_info: n_ff             = 8192
0.00.038.272 I print_info: n_expert         = 0
0.00.038.272 I print_info: n_expert_used    = 0
0.00.038.272 I print_info: causal attn      = 1
0.00.038.272 I print_info: pooling type     = 0
0.00.038.273 I print_info: rope type        = 2
0.00.038.273 I print_info: rope scaling     = linear
0.00.038.273 I print_info: freq_base_train  = 10000.0
0.00.038.273 I print_info: freq_scale_train = 1
0.00.038.274 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.274 I print_info: rope_finetuned   = unknown
0.00.038.274 I print_info: ssm_d_conv       = 0
0.00.038.274 I print_info: ssm_d_inner      = 0
0.00.038.274 I print_info: ssm_d_state      = 0
0.00.038.274 I print_info: ssm_dt_rank      = 0
0.00.038.275 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.275 I print_info: model type       = 1.4B
0.00.038.275 I print_info: model params     = 1.41 B
0.00.038.275 I print_info: general.name     = 1.4B
0.00.038.276 I print_info: vocab type       = BPE
0.00.038.276 I print_info: n_vocab          = 50304
0.00.038.276 I print_info: n_merges         = 50009
0.00.038.277 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.277 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.277 I print_info: LF token         = 187 'Ċ'
0.00.038.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.278 I print_info: max token length = 1024
0.00.038.278 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.722 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.736 I load_tensors: offloading output layer to GPU
0.00.613.737 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.772 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.613.779 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.615.302 I llama_init_from_model: n_seq_max     = 1
0.00.615.304 I llama_init_from_model: n_ctx         = 128
0.00.615.305 I llama_init_from_model: n_ctx_per_seq = 128
0.00.615.305 I llama_init_from_model: n_batch       = 128
0.00.615.306 I llama_init_from_model: n_ubatch      = 128
0.00.615.306 I llama_init_from_model: flash_attn    = 0
0.00.615.307 I llama_init_from_model: freq_base     = 10000.0
0.00.615.307 I llama_init_from_model: freq_scale    = 1
0.00.615.308 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.615.309 I ggml_metal_init: allocating
0.00.615.334 I ggml_metal_init: found device: Apple M4
0.00.615.346 I ggml_metal_init: picking default device: Apple M4
0.00.616.804 I ggml_metal_init: using embedded metal library
0.00.623.051 I ggml_metal_init: GPU name:   Apple M4
0.00.623.054 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.055 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.056 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.056 I ggml_metal_init: simdgroup reduction   = true
0.00.623.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.057 I ggml_metal_init: has residency sets    = true
0.00.623.057 I ggml_metal_init: has bfloat            = true
0.00.623.057 I ggml_metal_init: use bfloat            = true
0.00.623.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.523 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.930 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.934 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.973 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.315 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.646.317 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.646.317 I llama_init_from_model: graph nodes  = 967
0.00.646.317 I llama_init_from_model: graph splits = 2
0.00.646.321 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.684 I 
0.00.678.750 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.791 I perplexity: tokenizing the input ..
0.00.685.305 I perplexity: tokenization took 6.512 ms
0.00.685.309 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.395 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.825.731 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.825.752 I llama_perf_context_print:        load time =     669.73 ms
0.00.825.754 I llama_perf_context_print: prompt eval time =     138.85 ms /   128 tokens (    1.08 ms per token,   921.82 tokens per second)
0.00.825.756 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.756 I llama_perf_context_print:       total time =     147.07 ms /   129 tokens
0.00.826.173 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.076s
sys	0m0.142s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.278 I build: 4706 (c7f460ab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.297 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.304 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.305 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.306 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.306 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.307 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.307 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.309 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.318 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.322 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.325 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.326 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.327 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.518 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.531 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.643 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.643 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.644 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.644 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.645 I llama_model_loader: - type  f32:  194 tensors
0.00.055.646 I llama_model_loader: - type  f16:   98 tensors
0.00.055.646 I print_info: file format = GGUF V3 (latest)
0.00.055.648 I print_info: file type   = all F32 (guessed)
0.00.055.649 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.142 I load: special tokens cache size = 25
0.00.076.165 I load: token to piece cache size = 0.2984 MB
0.00.076.168 I print_info: arch             = gptneox
0.00.076.168 I print_info: vocab_only       = 0
0.00.076.169 I print_info: n_ctx_train      = 2048
0.00.076.169 I print_info: n_embd           = 2048
0.00.076.169 I print_info: n_layer          = 24
0.00.076.172 I print_info: n_head           = 16
0.00.076.173 I print_info: n_head_kv        = 16
0.00.076.173 I print_info: n_rot            = 32
0.00.076.173 I print_info: n_swa            = 0
0.00.076.174 I print_info: n_embd_head_k    = 128
0.00.076.174 I print_info: n_embd_head_v    = 128
0.00.076.174 I print_info: n_gqa            = 1
0.00.076.175 I print_info: n_embd_k_gqa     = 2048
0.00.076.176 I print_info: n_embd_v_gqa     = 2048
0.00.076.176 I print_info: f_norm_eps       = 1.0e-05
0.00.076.177 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.177 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.177 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.177 I print_info: f_logit_scale    = 0.0e+00
0.00.076.178 I print_info: n_ff             = 8192
0.00.076.178 I print_info: n_expert         = 0
0.00.076.178 I print_info: n_expert_used    = 0
0.00.076.178 I print_info: causal attn      = 1
0.00.076.179 I print_info: pooling type     = 0
0.00.076.179 I print_info: rope type        = 2
0.00.076.179 I print_info: rope scaling     = linear
0.00.076.179 I print_info: freq_base_train  = 10000.0
0.00.076.180 I print_info: freq_scale_train = 1
0.00.076.180 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.180 I print_info: rope_finetuned   = unknown
0.00.076.180 I print_info: ssm_d_conv       = 0
0.00.076.181 I print_info: ssm_d_inner      = 0
0.00.076.181 I print_info: ssm_d_state      = 0
0.00.076.181 I print_info: ssm_dt_rank      = 0
0.00.076.181 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.181 I print_info: model type       = 1.4B
0.00.076.182 I print_info: model params     = 1.41 B
0.00.076.182 I print_info: general.name     = 1.4B
0.00.076.182 I print_info: vocab type       = BPE
0.00.076.183 I print_info: n_vocab          = 50304
0.00.076.183 I print_info: n_merges         = 50009
0.00.076.183 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.183 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.183 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.184 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.185 I print_info: LF token         = 187 'Ċ'
0.00.076.185 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.186 I print_info: max token length = 1024
0.00.076.186 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.359.399 I load_tensors: offloading 24 repeating layers to GPU
0.01.359.402 I load_tensors: offloading output layer to GPU
0.01.359.403 I load_tensors: offloaded 25/25 layers to GPU
0.01.359.430 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.359.432 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.360.594 I llama_init_from_model: n_seq_max     = 1
0.01.360.595 I llama_init_from_model: n_ctx         = 128
0.01.360.595 I llama_init_from_model: n_ctx_per_seq = 128
0.01.360.595 I llama_init_from_model: n_batch       = 128
0.01.360.595 I llama_init_from_model: n_ubatch      = 128
0.01.360.596 I llama_init_from_model: flash_attn    = 0
0.01.360.596 I llama_init_from_model: freq_base     = 10000.0
0.01.360.597 I llama_init_from_model: freq_scale    = 1
0.01.360.597 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.360.601 I ggml_metal_init: allocating
0.01.360.686 I ggml_metal_init: found device: Apple M4
0.01.360.693 I ggml_metal_init: picking default device: Apple M4
0.01.361.892 I ggml_metal_init: using embedded metal library
0.01.365.565 I ggml_metal_init: GPU name:   Apple M4
0.01.365.567 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.365.567 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.365.568 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.365.568 I ggml_metal_init: simdgroup reduction   = true
0.01.365.568 I ggml_metal_init: simdgroup matrix mul. = true
0.01.365.568 I ggml_metal_init: has residency sets    = true
0.01.365.569 I ggml_metal_init: has bfloat            = true
0.01.365.569 I ggml_metal_init: use bfloat            = true
0.01.365.569 I ggml_metal_init: hasUnifiedMemory      = true
0.01.365.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.375.859 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.377.554 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.377.556 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.377.580 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.379.200 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.379.201 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.379.202 I llama_init_from_model: graph nodes  = 967
0.01.379.202 I llama_init_from_model: graph splits = 2
0.01.379.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.379.203 I 
0.01.379.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.379.231 I compute_imatrix: tokenizing the input ..
0.01.383.252 I compute_imatrix: tokenization took 4.021 ms
0.01.383.254 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.652.364 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.654.762 I llama_perf_context_print:        load time =    1628.55 ms
0.01.654.763 I llama_perf_context_print: prompt eval time =     267.38 ms /   128 tokens (    2.09 ms per token,   478.72 tokens per second)
0.01.654.763 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.654.764 I llama_perf_context_print:       total time =    1630.95 ms /   129 tokens
0.01.655.278 I ggml_metal_free: deallocating

real	0m1.853s
user	0m0.126s
sys	0m0.261s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4706 (c7f460ab)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a905260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a9085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a908a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a908ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a909340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a9097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a909c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a90a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a90a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a90a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a90ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a90b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a90bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a90c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a90cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a90d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a90dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a90e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a90ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a90f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a90fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a9101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a910910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a9111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a9118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a911b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a911e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a9122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a9129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a912e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a913410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a913920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a913d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a914050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a9144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a914930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a914e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a915390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a915890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a915d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a916290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a916790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a916c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a917190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a917690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a917b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a917f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a9183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a918b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a918fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a919450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a9198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a919d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a91a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a91a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a91ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a91b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a91b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a91ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a91c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a91c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a91c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a91ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a91d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a91d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a91dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a91e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a91e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a91ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a91eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a91f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a91f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a91fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a9201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a920740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a920c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a9211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a921730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a921c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a9221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a922720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a922c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a9231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a923710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a923c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a9241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a924700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a924c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a9251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a9256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a925c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a926190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a9266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a926c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a927180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a9276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a927c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a9186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a928090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a928840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a928d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a9292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a929830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a929d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a92a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a92a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a92ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a92b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a92b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a92bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a92c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a92c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a92cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a92d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a92d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a92db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a92dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a92e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a92e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a92edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a92f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a92f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a92fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a9302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a930560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a930a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a930f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a931460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a931960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a931e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a932360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a932860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a932d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a933260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a933760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a933c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a934160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a934660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a934b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a935060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a935560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a935a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a935f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a936460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a936960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a936e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a937360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a937860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a937d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a938260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a938760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a938c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a939160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a939660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a939b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a93a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a93a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a93aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a93af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a93b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a93b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a93be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a93c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a93c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a93cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a93d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a93d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a93dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a93e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a93e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a93eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a93f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a93f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a93fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a93ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a940460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a940960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a940e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a941360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a941860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a941d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a942260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a942760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a942c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a943160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a943660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a943b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a944060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a944560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a944a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a944f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a945460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a945960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a945e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a946410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a9469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a946f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a947520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a947b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a948140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a948750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a948f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a9493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a9496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a949cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a94a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a94aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a94af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a94b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a94b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a94c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a94c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a94cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a94d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a94d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a94dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a94e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a94e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a94eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a94f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a94f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a94fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a950000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a950550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a950aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a950ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a951540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a951a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a951fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a952530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a952a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a952fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a953520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a953a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a953fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a954510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a954a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a954fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a955500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a955a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a955fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a9564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a956a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a956f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a9574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a957a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a957f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a9584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a958a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a958f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a9594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a959a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a959f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a95a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a95aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a95af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a95b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a95b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a95bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a95c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a95c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a95cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a95d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a95d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a95df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a95e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a95e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a95ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a95f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a95f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a95fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a9600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a960580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a960a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a960ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a961360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a961800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a961ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a962140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a9625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a962a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a962f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a963470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a963b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a9642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a9649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a9650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a9653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a965ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a965e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a966470 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.735.411 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11b304bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11b305040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11b3054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11b305920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11b305d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11b306200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11b306670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11b306ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11b306f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11b3073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11b307830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11b307f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11b308a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11b3091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11b309a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11b30a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11b30a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11b30af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11b30b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11b30bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11b30c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11b30cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11b30d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11b30da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11b30e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11b30e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11b30e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11b30eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11b30efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11b30f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11b30f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11b30fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11b310230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11b3104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11b310960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11b310dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11b311240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11b3116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11b311b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11b311f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11b312400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11b312870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11b312ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11b313150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11b3135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11b313a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11b313ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11b314310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11b314780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11b314bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11b315060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11b3154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11b315940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11b315db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11b316220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11b316690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11b316c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11b317100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11b317570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11b3179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11b317e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11b3182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11b318730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11b318ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11b319010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11b319480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11b3198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11b319d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11b31a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11b31a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11b31aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11b31af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11b31b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a9466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a946120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a949960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a947df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a966120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a9477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a948400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a949f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a910e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a90b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a90b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a904c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a91a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a91be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a928350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a965670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a94a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a948a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a912580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a9668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a966b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a966e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a967110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a9673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a967690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a967950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a967c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a967ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a968190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a968450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a968710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a9689d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a968c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a968f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a969210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a9694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a969790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a969a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a969d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a969fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a96a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a96a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a96a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a96aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a96ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a96b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a96b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a96b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a96b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a96bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a96be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a96c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a96c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a96c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a96c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a96cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a96ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a96d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a96d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a96d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a96d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a96dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a96df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a96e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a96e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a96e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a96ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a96ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a96ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a96f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a96f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a96f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a96fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a96fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a970010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a9702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a970590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a970850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a970b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a970dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a971090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a971350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a971610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a9718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a971b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a971e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a972110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a9723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a972690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a972950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a972c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a972ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a973190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a973450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a973710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a9739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a973c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a973f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a974210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a9744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a974790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a974a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a974d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a974fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a975290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a975550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a975810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a975ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a975d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a976050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a976310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a9765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a976890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a976b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a976e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a9770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a977390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a977650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a977910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a977bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a977e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a978150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a978410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a9786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a978990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a978c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a978f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a9791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a979490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a979750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a979a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a979cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a979f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a97a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a97a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a97a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a97aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a97b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a97b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a97b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a97b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a97bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a97be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a97c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a97c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a97c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a97c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a97cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a97cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a97d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a97d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a97d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a97d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a97dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a97df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a97e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a97e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a97e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a97ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a97ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a97efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a97f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a97f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a97f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a97faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a97fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a980020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a9802e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a9805a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a980860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a980b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a980de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a9810a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a981360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a981620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a9818e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a981ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a981e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a982120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a9823e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a9826a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a982960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a982c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a982ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a9831a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a983460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a983720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a9839e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a983ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a983f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a984220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a9844e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a9847a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a984a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a984d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a984fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a9852a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a985560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a985820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a985ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a985da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a986060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a986320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a9865e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a9868a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a986b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a986e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a9870e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a9873a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a987660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a987920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a987be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a987ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a988160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a988420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a9886e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a9889a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a988c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a9891d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a989490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a9898d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a989b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a989e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a98a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a98a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a98a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a98a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a98ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a98aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a98b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a98b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a98bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a98c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a98c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a98c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a98cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a98ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a98d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a98d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a98d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a98d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a98dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a98dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a98e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a98e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a98e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a98e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a98eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a98ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a98f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a98f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a98f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a98fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a98fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a98ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a9902a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a990560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a990820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a990ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a990da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a991060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a991320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a9915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a9918a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a991b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a991e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a9920e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a9923a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a992660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a992920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a992be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a992ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a993160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a993420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a9936e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a9939a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a993c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a993f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a9941e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a9944a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a994760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a994a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a994ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a994fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a995260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a995520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a9957e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a995aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a995d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a996020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a9962e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a9965a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a996860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a996b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a996de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a9970a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a997360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a997620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a9978e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a997ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a997e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a998120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a9983e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a9986a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a998960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a998c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a998ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a9991a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a999460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a999720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a9999e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a999ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a999f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a99a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a99a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a99a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a99aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a99ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a99afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a99b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a99b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a99b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a99bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a99bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a99c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a99c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a99c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a99c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a99cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a99ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a99d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a99d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a99d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a99d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a99dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a99dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a99e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a99e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a99e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a99e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a99ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a99ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a99f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a99f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a99f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a99fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a99fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a99ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a9a0260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a9a0520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a9a07e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a9a0aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a9a0d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a9a1020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a9a12e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a9a15a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a9a1860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a9a1b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a9a1de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a9a20a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a9a2360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a9a2620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a9a28e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a9a2ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a9a2e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a9a3120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a9a33e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a9a36a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a9a3960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a9a3c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a9a3ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a9a41a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a9a4460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a9a4720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a9a49e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a9a4ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a9a4f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a9a5220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a9a54e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a9a57a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a9a5a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a9a5d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a9a5fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a9a62a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a9a6560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a9a6820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a9a6ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a9a6da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a9a7060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a9a7320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a9a75e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a9a78a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a9a7b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a9a7e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a9a80e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a9a83a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a9a8660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a9a8920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a9a8be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a9a8ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a9a9160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a9a9420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a9a96e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a9a99a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a9a9c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a9a9f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a9aa1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a9aa4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a9aa760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a9aaa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a9aace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a9aafa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a9ab260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a9ab520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a9ab7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a9abaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a9abd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a9ac020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a9ac2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a9ac5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a9ac860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a9acb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a9acde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a9ad0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a9ad360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a9ad620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a9ad8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a9adba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a9ae170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a9ae430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a9ae6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a9ae9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a9aec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a9aef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a9af1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a9af4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a9af770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a9afa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a9afcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a9affb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a9b0270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a9b0530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a9b07f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a9b0ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a9b0d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a9b1030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a9b12f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a9b15b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a9b1870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a9b1b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a9b1df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a9b20b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a9b2370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a9b2630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a9b28f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a9b2bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a9b2e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a9b3130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a9b33f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a9b36b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a9b3970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a9b3c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a9b3ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a9b41b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a9b4470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a9b4730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a9b4c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a9b51d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a9b5720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a9b5c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a9b61c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a9b6710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a9b6c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a9b71b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a9b7700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a9b7c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a9b81a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a9b86f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a9b8c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a9b9190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a9b96e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a9b9c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a9ba180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a9ba6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a9bac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a9bb0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a9bb560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a9bba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a9bbea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a9bc340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a9bc7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a9bcc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a9bd120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a9bd5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a9bda60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a9bdf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a9be3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a9be840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a9bece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a9bf180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a9bf6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a9bfdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a9c0510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a9c0c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a9c1350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a9c1610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a9c1e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a9c20c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a9c26d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.798s
user	0m0.280s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4706 (c7f460ab)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bf105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bf10d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bf112b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bf11860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bf11e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bf123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bf12970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bf12f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bf134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bf139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bf13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bf143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bf14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bf156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bf15eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bf165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bf16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bf17410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bf17b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bf18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bf18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bf19140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bf19860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bf1a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bf1a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bf1aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bf1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bf1bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bf1c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bf1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bf1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bf1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bf1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bf1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bf1dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bf1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bf1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bf1eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bf1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bf1f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bf1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bf1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bf20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bf206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bf209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bf20fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bf215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bf21ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bf22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bf22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bf23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bf23730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bf23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bf24350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bf24b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bf24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bf25480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bf25740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bf25d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bf26540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bf26800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bf26ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bf27140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bf275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bf27a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bf27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bf283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bf28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bf28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bf291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bf29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bf29ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bf29f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bf2a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bf2aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bf2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bf2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bf2ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bf2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bf2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bf2ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bf2cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bf2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bf2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bf2df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bf2e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bf2e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bf2ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bf2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bf2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bf2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bf30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bf309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bf30f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bf31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bf319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bf31f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bf21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bf32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bf32b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bf33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bf335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bf33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bf34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bf345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bf34b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bf35050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bf355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bf35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bf36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bf36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bf36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bf37030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bf374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bf37970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bf37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bf382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bf38750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bf38bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bf39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bf39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bf399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bf39e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bf3a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bf3a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bf3ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bf3b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bf3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bf3ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bf3bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bf3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bf3c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bf3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bf3d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bf3d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bf3da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bf3df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bf3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bf3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bf3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bf3f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bf3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bf3faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bf3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bf40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bf408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bf40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bf41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bf416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bf41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bf41ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bf42490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bf42930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bf42dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bf43270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bf43710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bf43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bf44050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bf444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bf44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bf44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bf452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bf45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bf45c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bf460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bf46550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bf469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bf46e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bf47330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bf477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bf47c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bf48110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bf485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bf48a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bf48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bf49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bf49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bf49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bf4a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bf4a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bf4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bf4af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bf4b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bf4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bf4bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bf4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bf4c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bf4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bf4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bf4d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bf4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bf4dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bf4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bf4e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bf4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bf4f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bf4f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bf4fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bf50040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bf50650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bf50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bf51450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bf518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bf51bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bf521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bf527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bf52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bf53460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bf53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bf53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bf54550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bf54aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bf54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bf55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bf55a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bf55fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bf56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf56fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf57520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf57a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf57fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf58510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf58a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf58fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf59500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf59a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf59fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf5a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf5aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf5af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf5b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf5ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf5bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf5c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf5ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf5cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf5d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf5da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf5e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf5ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf5ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf5f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf5f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf5ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf60490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf60f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf61480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf61f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf62470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf629c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf62f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf63460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf63f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf64450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf649a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf64ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf65440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf65ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf66430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf66980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf66ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf67370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf67810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf67cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf68150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf685f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf68a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bf68f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bf693d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bf69870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bf69d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf6a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf6a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bf6aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf6af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf6b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bf6b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bf6c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bf6c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bf6cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bf6d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bf6d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bf6e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bf6e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf6e980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.784 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14be05310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14be05780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14be05bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14be06060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14be064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14be06940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14be06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14be07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14be07690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14be07b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14be07f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14be08600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14be09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14be098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14be0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14be0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14be0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14be0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14be0bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14be0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14be0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14be0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14be0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14be0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14be0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14be0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14be0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14be0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14be0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14be0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14be10010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14be10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14be109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14be10c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14be110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14be11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14be119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14be11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14be122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14be12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14be12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14be12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14be13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14be138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14be13d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14be141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14be14620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14be14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14be14f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14be15370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14be157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14be15c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14be160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14be16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14be169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14be16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14be17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14be17880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14be17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14be18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14be185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14be18a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14be18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14be19320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14be19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14be19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14be1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14be1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14be1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14be1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14be1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14be1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14be1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14be1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14be1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14be1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14be1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14be1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14be1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14be1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14be1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14be1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14be1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14be1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14be1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14be1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14be1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14be1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14be20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14be20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14be20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14be20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14be213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14be21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14be21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14be22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14be22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14be22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14be22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14be232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14be23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14be23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14be24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14be244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14be24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14be24d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14be251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14be25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14be25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14be25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14be263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14be26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14be26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14be27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14be27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14be279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14be27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14be282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14be28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14be28ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14be29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14be29480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14be298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14be29d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14be2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14be2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14be2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14be2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14be2b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14be2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14be2bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14be2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14be2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14be2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14be2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14be2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14be2d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14be2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14be2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14be2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14be2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14be2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14be2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14be2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14be2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14be2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14be30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14be307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14be30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14be310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14be31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14be319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14be31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14be32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14be326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14be32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14be32fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14be33440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14be338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14be33d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14be34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14be34600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14be34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14be34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14be35350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14be357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14be363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14be366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14be36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14be36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14be37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14be376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14be37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14be37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14be38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14be38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14be38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14be39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14be395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14be39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14be39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14be3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14be3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14be3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14be3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14be3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14be3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14be3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14be3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14be3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14be3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14be3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14be3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14be3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14be3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14be3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14be3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14be3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14be3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14be3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14be3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14be3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14be40140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14be40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14be40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14be40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14be413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14be41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14be41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14be42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14be42db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14be43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14be43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14be43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14be441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14be44770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14be44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14be452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14be458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14be45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14be46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14be469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14be46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14be47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14be47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14be480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14be486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14be48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14be49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14be497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14be49db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14be4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14be4a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14be4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14be4b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14be4ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14be4c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14be4c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14be4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14be4d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14be4d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14be4dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14be4e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14be4e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14be4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14be4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14be4f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14be4ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14be50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14be50af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14be510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14be51670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14be51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14be521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14be527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14be52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14be53330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14be538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14be53eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14be54470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14be54a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14be54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14be555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14be55b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14be56130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14be566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14be56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14be57270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14be57770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14be57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14be58170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14be58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14be58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14be59070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14be59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14be59a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14be59f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14be5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14be5a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14be5ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14be5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14be5b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14be5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14be5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14be5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14be5d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14be5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14be5dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14be5e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14be5ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14be5f060 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ca044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ca04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ca04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ca05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ca056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ca05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ca05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ca063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ca06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ca06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ca07240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ca078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ca083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ca08b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ca093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ca09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ca0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ca0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ca0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ca0b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ca0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ca0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ca0cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ca0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ca0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ca0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ca0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ca0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ca0e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ca0ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ca0f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ca0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ca0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ca0ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ca103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ca10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ca10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ca110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ca11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ca119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ca11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ca122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ca12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ca12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ca13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ca13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ca138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ca13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ca141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ca14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ca14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ca14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ca15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ca157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ca15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ca160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ca16640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ca16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ca16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ca17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ca17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ca17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ca18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ca185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ca18a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ca18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ca19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ca197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ca19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ca1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ca1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ca1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ca1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ca1b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ca1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ca1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ca1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ca1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ca1c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ca1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ca1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ca1d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ca1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ca1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ca1e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ca1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ca1ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ca1f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ca1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ca1f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ca1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ca20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ca20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ca20b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ca20f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ca213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ca21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ca21cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ca22130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ca225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ca22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ca22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ca232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ca23b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ca23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ca242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ca24720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ca24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ca25000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ca25470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ca258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ca25d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ca261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ca26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ca26aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ca26f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ca27380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ca277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ca27c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ca280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ca28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ca289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ca28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ca29290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ca29700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ca29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ca29fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ca2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ca2a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ca2ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ca2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ca2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ca2ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ca2bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ca2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ca2c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ca2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ca2d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ca2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ca2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ca2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ca2e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ca2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ca2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ca2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ca2f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ca2f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ca2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ca30180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ca305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ca30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ca30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ca31340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ca317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ca31c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ca32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ca32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ca32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ca32de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ca33250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ca336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ca33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ca33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ca34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ca34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ca34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ca35160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ca355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ca35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ca35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ca36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ca36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ca36c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ca37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ca374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ca37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ca37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ca38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ca386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ca38b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ca38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ca393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ca39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ca39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ca3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ca3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ca3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ca3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ca3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ca3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ca3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ca3c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ca3c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ca3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ca3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ca3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ca3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ca3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ca3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ca3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ca3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ca3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ca3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ca3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ca3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ca3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ca402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ca40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ca40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ca41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ca41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ca41e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ca42130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ca425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ca42a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ca42e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ca432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ca43760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ca43bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ca44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ca444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ca44920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ca44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ca45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ca45670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ca45ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ca45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ca463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ca46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ca46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ca47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ca47580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ca479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ca47e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ca482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ca48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ca48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ca49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ca49490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ca49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ca49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ca4a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ca4a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ca4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ca4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ca4b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ca4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ca4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ca4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ca4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ca4c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ca4ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ca4d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ca4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ca4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ca4e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ca4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ca4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ca4ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ca4f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ca4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ca4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ca4ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ca50380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ca507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ca50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ca510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ca51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ca519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ca51e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ca52290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ca52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ca52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ca52fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ca53450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ca538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ca53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ca541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ca54610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ca54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ca54ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ca55360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ca557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ca56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ca56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ca57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ca577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ca57a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ca57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ca584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ca58ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.936s
user	0m0.230s
sys	0m0.170s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
