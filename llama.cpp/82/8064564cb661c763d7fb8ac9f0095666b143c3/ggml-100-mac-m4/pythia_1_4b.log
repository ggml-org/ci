Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.536s
user	0m0.897s
sys	0m1.220s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Built target build_info
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 32%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-quantize-stats
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-chat
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Built target test-chat-template
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 62%] Built target test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-autorelease
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target test-rope
[ 71%] Built target llama-batched
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-embedding
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-imatrix
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-cli
[ 80%] Built target llama-parallel
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-passkey
[ 81%] Built target llama-perplexity
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tts
[ 89%] Linking CXX executable ../../bin/llama-gen-docs
[ 89%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-gen-docs
[ 92%] Built target llama-tts
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-run
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.083s
user	0m6.862s
sys	0m11.302s

main: quantize time =  5873.59 ms
main:    total time =  5873.59 ms

main: quantize time =  4515.62 ms
main:    total time =  4515.62 ms

main: quantize time =  3365.20 ms
main:    total time =  3365.20 ms

main: quantize time =  3401.81 ms
main:    total time =  3401.81 ms

main: quantize time =  2356.64 ms
main:    total time =  2356.64 ms

main: quantize time =  5354.80 ms
main:    total time =  5354.80 ms

main: quantize time =  5961.75 ms
main:    total time =  5961.75 ms

main: quantize time =  6857.01 ms
main:    total time =  6857.01 ms

main: quantize time =  6283.99 ms
main:    total time =  6283.99 ms

main: quantize time =  4321.97 ms
main:    total time =  4321.97 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.209 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.358 I main: llama backend init
0.00.000.363 I main: load the model and apply lora adapter, if any
0.00.051.123 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.066.338 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.364 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.367 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.369 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.370 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.371 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.374 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.377 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.084.695 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.084.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.084.698 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.084.699 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.084.699 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.084.700 I llama_model_loader: - type  f32:  194 tensors
0.00.084.700 I llama_model_loader: - type  f16:   98 tensors
0.00.084.701 I print_info: file format = GGUF V3 (latest)
0.00.084.702 I print_info: file type   = all F32 (guessed)
0.00.084.703 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.098.026 I load: special tokens cache size = 25
0.00.106.594 I load: token to piece cache size = 0.2984 MB
0.00.106.617 I print_info: arch             = gptneox
0.00.106.618 I print_info: vocab_only       = 0
0.00.106.619 I print_info: n_ctx_train      = 2048
0.00.106.619 I print_info: n_embd           = 2048
0.00.106.619 I print_info: n_layer          = 24
0.00.106.622 I print_info: n_head           = 16
0.00.106.623 I print_info: n_head_kv        = 16
0.00.106.623 I print_info: n_rot            = 32
0.00.106.624 I print_info: n_swa            = 0
0.00.106.625 I print_info: n_embd_head_k    = 128
0.00.106.625 I print_info: n_embd_head_v    = 128
0.00.106.627 I print_info: n_gqa            = 1
0.00.106.628 I print_info: n_embd_k_gqa     = 2048
0.00.106.629 I print_info: n_embd_v_gqa     = 2048
0.00.106.630 I print_info: f_norm_eps       = 1.0e-05
0.00.106.630 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.106.630 I print_info: f_clamp_kqv      = 0.0e+00
0.00.106.632 I print_info: f_max_alibi_bias = 0.0e+00
0.00.106.632 I print_info: f_logit_scale    = 0.0e+00
0.00.106.633 I print_info: n_ff             = 8192
0.00.106.633 I print_info: n_expert         = 0
0.00.106.633 I print_info: n_expert_used    = 0
0.00.106.633 I print_info: causal attn      = 1
0.00.106.634 I print_info: pooling type     = 0
0.00.106.634 I print_info: rope type        = 2
0.00.106.634 I print_info: rope scaling     = linear
0.00.106.634 I print_info: freq_base_train  = 10000.0
0.00.106.635 I print_info: freq_scale_train = 1
0.00.106.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.106.641 I print_info: rope_finetuned   = unknown
0.00.106.641 I print_info: ssm_d_conv       = 0
0.00.106.641 I print_info: ssm_d_inner      = 0
0.00.106.641 I print_info: ssm_d_state      = 0
0.00.106.642 I print_info: ssm_dt_rank      = 0
0.00.106.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.106.642 I print_info: model type       = 1.4B
0.00.106.642 I print_info: model params     = 1.41 B
0.00.106.643 I print_info: general.name     = 1.4B
0.00.106.643 I print_info: vocab type       = BPE
0.00.106.647 I print_info: n_vocab          = 50304
0.00.106.649 I print_info: n_merges         = 50009
0.00.106.650 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.106.650 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.106.650 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.106.650 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.106.650 I print_info: LF token         = 187 'Ċ'
0.00.106.651 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.106.651 I print_info: max token length = 1024
0.00.106.651 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.138.541 I load_tensors: offloading 24 repeating layers to GPU
0.00.138.545 I load_tensors: offloading output layer to GPU
0.00.138.546 I load_tensors: offloaded 25/25 layers to GPU
0.00.138.567 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.138.569 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.138.969 I llama_context: n_seq_max     = 1
0.00.138.969 I llama_context: n_ctx         = 2048
0.00.138.969 I llama_context: n_ctx_per_seq = 2048
0.00.138.970 I llama_context: n_batch       = 2048
0.00.138.970 I llama_context: n_ubatch      = 512
0.00.138.970 I llama_context: flash_attn    = 0
0.00.138.970 I llama_context: freq_base     = 10000.0
0.00.138.971 I llama_context: freq_scale    = 1
0.00.138.971 I ggml_metal_init: allocating
0.00.138.997 I ggml_metal_init: found device: Apple M4
0.00.139.003 I ggml_metal_init: picking default device: Apple M4
0.00.139.581 I ggml_metal_init: using embedded metal library
0.00.151.717 I ggml_metal_init: GPU name:   Apple M4
0.00.151.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.151.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.151.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.151.720 I ggml_metal_init: simdgroup reduction   = true
0.00.151.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.151.720 I ggml_metal_init: has residency sets    = true
0.00.151.721 I ggml_metal_init: has bfloat            = true
0.00.151.721 I ggml_metal_init: use bfloat            = true
0.00.151.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.151.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.191.841 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.191.844 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.220.631 I init:      Metal KV buffer size =   384.00 MiB
0.00.220.637 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.225.194 I init:      Metal compute buffer size =   102.25 MiB
0.00.225.196 I init:        CPU compute buffer size =     8.01 MiB
0.00.225.196 I init: graph nodes  = 967
0.00.225.197 I init: graph splits = 2
0.00.225.200 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.225.329 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.225.330 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.279.051 I main: llama threadpool init, n_threads = 4
0.00.279.086 I 
0.00.279.113 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.279.116 I 
0.00.279.157 I sampler seed: 1234
0.00.279.161 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.279.184 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.279.186 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.279.186 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.085.460 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.02.085.460 I llama_perf_context_print:        load time =     227.10 ms
0.02.085.462 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.61 tokens per second)
0.02.085.463 I llama_perf_context_print:        eval time =    1759.91 ms /    63 runs   (   27.94 ms per token,    35.80 tokens per second)
0.02.085.463 I llama_perf_context_print:       total time =    1807.23 ms /    70 tokens
0.02.088.632 I ggml_metal_free: deallocating

real	0m2.403s
user	0m0.133s
sys	0m0.121s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.398 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.223 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.255 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.184 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.184 I llama_model_loader: - type  f32:  194 tensors
0.00.034.185 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.185 I print_info: file format = GGUF V3 (latest)
0.00.034.186 I print_info: file type   = Q8_0
0.00.034.187 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.096 I load: special tokens cache size = 25
0.00.048.868 I load: token to piece cache size = 0.2984 MB
0.00.048.884 I print_info: arch             = gptneox
0.00.048.885 I print_info: vocab_only       = 0
0.00.048.886 I print_info: n_ctx_train      = 2048
0.00.048.886 I print_info: n_embd           = 2048
0.00.048.886 I print_info: n_layer          = 24
0.00.048.891 I print_info: n_head           = 16
0.00.048.892 I print_info: n_head_kv        = 16
0.00.048.895 I print_info: n_rot            = 32
0.00.048.895 I print_info: n_swa            = 0
0.00.048.896 I print_info: n_embd_head_k    = 128
0.00.048.896 I print_info: n_embd_head_v    = 128
0.00.048.896 I print_info: n_gqa            = 1
0.00.048.897 I print_info: n_embd_k_gqa     = 2048
0.00.048.898 I print_info: n_embd_v_gqa     = 2048
0.00.048.898 I print_info: f_norm_eps       = 1.0e-05
0.00.048.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.900 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.900 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.900 I print_info: f_logit_scale    = 0.0e+00
0.00.048.901 I print_info: n_ff             = 8192
0.00.048.901 I print_info: n_expert         = 0
0.00.048.901 I print_info: n_expert_used    = 0
0.00.048.901 I print_info: causal attn      = 1
0.00.048.901 I print_info: pooling type     = 0
0.00.048.901 I print_info: rope type        = 2
0.00.048.901 I print_info: rope scaling     = linear
0.00.048.902 I print_info: freq_base_train  = 10000.0
0.00.048.902 I print_info: freq_scale_train = 1
0.00.048.902 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.902 I print_info: rope_finetuned   = unknown
0.00.048.902 I print_info: ssm_d_conv       = 0
0.00.048.903 I print_info: ssm_d_inner      = 0
0.00.048.904 I print_info: ssm_d_state      = 0
0.00.048.904 I print_info: ssm_dt_rank      = 0
0.00.048.904 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.904 I print_info: model type       = 1.4B
0.00.048.905 I print_info: model params     = 1.41 B
0.00.048.905 I print_info: general.name     = 1.4B
0.00.048.905 I print_info: vocab type       = BPE
0.00.048.905 I print_info: n_vocab          = 50304
0.00.048.906 I print_info: n_merges         = 50009
0.00.048.906 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.906 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.906 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.906 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.906 I print_info: LF token         = 187 'Ċ'
0.00.048.907 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.907 I print_info: max token length = 1024
0.00.048.908 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.211.735 I load_tensors: offloading 24 repeating layers to GPU
0.01.211.740 I load_tensors: offloading output layer to GPU
0.01.211.741 I load_tensors: offloaded 25/25 layers to GPU
0.01.211.765 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.211.767 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.212.580 I llama_context: n_seq_max     = 1
0.01.212.582 I llama_context: n_ctx         = 2048
0.01.212.582 I llama_context: n_ctx_per_seq = 2048
0.01.212.583 I llama_context: n_batch       = 2048
0.01.212.583 I llama_context: n_ubatch      = 512
0.01.212.583 I llama_context: flash_attn    = 0
0.01.212.584 I llama_context: freq_base     = 10000.0
0.01.212.584 I llama_context: freq_scale    = 1
0.01.212.585 I ggml_metal_init: allocating
0.01.212.598 I ggml_metal_init: found device: Apple M4
0.01.212.605 I ggml_metal_init: picking default device: Apple M4
0.01.213.741 I ggml_metal_init: using embedded metal library
0.01.218.867 I ggml_metal_init: GPU name:   Apple M4
0.01.218.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.218.871 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.218.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.218.873 I ggml_metal_init: simdgroup reduction   = true
0.01.218.873 I ggml_metal_init: simdgroup matrix mul. = true
0.01.218.873 I ggml_metal_init: has residency sets    = true
0.01.218.873 I ggml_metal_init: has bfloat            = true
0.01.218.873 I ggml_metal_init: use bfloat            = true
0.01.218.874 I ggml_metal_init: hasUnifiedMemory      = true
0.01.218.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.233.460 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.233.464 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.290.571 I init:      Metal KV buffer size =   384.00 MiB
0.01.290.581 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.296.173 I init:      Metal compute buffer size =   102.25 MiB
0.01.296.175 I init:        CPU compute buffer size =     8.01 MiB
0.01.296.176 I init: graph nodes  = 967
0.01.296.176 I init: graph splits = 2
0.01.296.181 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.296.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.296.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.346.310 I main: llama threadpool init, n_threads = 4
0.01.346.353 I 
0.01.346.378 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.346.378 I 
0.01.346.491 I sampler seed: 1234
0.01.346.496 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.346.506 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.346.507 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.346.507 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.432.938 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.02.432.939 I llama_perf_context_print:        load time =    1336.19 ms
0.02.432.941 I llama_perf_context_print: prompt eval time =      48.82 ms /     7 tokens (    6.97 ms per token,   143.38 tokens per second)
0.02.432.942 I llama_perf_context_print:        eval time =    1034.82 ms /    63 runs   (   16.43 ms per token,    60.88 tokens per second)
0.02.432.942 I llama_perf_context_print:       total time =    1087.35 ms /    70 tokens
0.02.436.296 I ggml_metal_free: deallocating

real	0m2.454s
user	0m0.105s
sys	0m0.342s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.475 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.380 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.381 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.381 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.382 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.382 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.383 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.383 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.384 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.384 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.385 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.385 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.387 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.387 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.388 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.216 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.076 I llama_model_loader: - type  f32:  194 tensors
0.00.027.077 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.077 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.078 I print_info: file format = GGUF V3 (latest)
0.00.027.079 I print_info: file type   = Q4_0
0.00.027.079 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.552 I load: special tokens cache size = 25
0.00.041.339 I load: token to piece cache size = 0.2984 MB
0.00.041.354 I print_info: arch             = gptneox
0.00.041.355 I print_info: vocab_only       = 0
0.00.041.355 I print_info: n_ctx_train      = 2048
0.00.041.355 I print_info: n_embd           = 2048
0.00.041.356 I print_info: n_layer          = 24
0.00.041.359 I print_info: n_head           = 16
0.00.041.360 I print_info: n_head_kv        = 16
0.00.041.360 I print_info: n_rot            = 32
0.00.041.360 I print_info: n_swa            = 0
0.00.041.361 I print_info: n_embd_head_k    = 128
0.00.041.361 I print_info: n_embd_head_v    = 128
0.00.041.361 I print_info: n_gqa            = 1
0.00.041.362 I print_info: n_embd_k_gqa     = 2048
0.00.041.363 I print_info: n_embd_v_gqa     = 2048
0.00.041.363 I print_info: f_norm_eps       = 1.0e-05
0.00.041.364 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.364 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.364 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.364 I print_info: f_logit_scale    = 0.0e+00
0.00.041.365 I print_info: n_ff             = 8192
0.00.041.365 I print_info: n_expert         = 0
0.00.041.365 I print_info: n_expert_used    = 0
0.00.041.365 I print_info: causal attn      = 1
0.00.041.365 I print_info: pooling type     = 0
0.00.041.365 I print_info: rope type        = 2
0.00.041.366 I print_info: rope scaling     = linear
0.00.041.366 I print_info: freq_base_train  = 10000.0
0.00.041.366 I print_info: freq_scale_train = 1
0.00.041.366 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.367 I print_info: rope_finetuned   = unknown
0.00.041.367 I print_info: ssm_d_conv       = 0
0.00.041.367 I print_info: ssm_d_inner      = 0
0.00.041.369 I print_info: ssm_d_state      = 0
0.00.041.369 I print_info: ssm_dt_rank      = 0
0.00.041.369 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.370 I print_info: model type       = 1.4B
0.00.041.370 I print_info: model params     = 1.41 B
0.00.041.370 I print_info: general.name     = 1.4B
0.00.041.371 I print_info: vocab type       = BPE
0.00.041.371 I print_info: n_vocab          = 50304
0.00.041.371 I print_info: n_merges         = 50009
0.00.041.371 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.371 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.372 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.372 I print_info: LF token         = 187 'Ċ'
0.00.041.372 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.372 I print_info: max token length = 1024
0.00.041.373 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.646.000 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.015 I load_tensors: offloading output layer to GPU
0.00.646.016 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.048 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.646.052 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.647.373 I llama_context: n_seq_max     = 1
0.00.647.375 I llama_context: n_ctx         = 2048
0.00.647.376 I llama_context: n_ctx_per_seq = 2048
0.00.647.377 I llama_context: n_batch       = 2048
0.00.647.378 I llama_context: n_ubatch      = 512
0.00.647.378 I llama_context: flash_attn    = 0
0.00.647.380 I llama_context: freq_base     = 10000.0
0.00.647.380 I llama_context: freq_scale    = 1
0.00.647.382 I ggml_metal_init: allocating
0.00.647.441 I ggml_metal_init: found device: Apple M4
0.00.647.455 I ggml_metal_init: picking default device: Apple M4
0.00.649.233 I ggml_metal_init: using embedded metal library
0.00.655.257 I ggml_metal_init: GPU name:   Apple M4
0.00.655.261 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.262 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.263 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.264 I ggml_metal_init: simdgroup reduction   = true
0.00.655.264 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.264 I ggml_metal_init: has residency sets    = true
0.00.655.265 I ggml_metal_init: has bfloat            = true
0.00.655.265 I ggml_metal_init: use bfloat            = true
0.00.655.266 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.268 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.174 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.674.180 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.728.836 I init:      Metal KV buffer size =   384.00 MiB
0.00.728.843 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.734.052 I init:      Metal compute buffer size =   102.25 MiB
0.00.734.054 I init:        CPU compute buffer size =     8.01 MiB
0.00.734.054 I init: graph nodes  = 967
0.00.734.055 I init: graph splits = 2
0.00.734.060 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.191 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.192 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.901 I main: llama threadpool init, n_threads = 4
0.00.780.948 I 
0.00.780.971 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.975 I 
0.00.781.108 I sampler seed: 1234
0.00.781.112 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.123 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.123 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.123 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.466.957 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.01.466.958 I llama_perf_context_print:        load time =     769.71 ms
0.01.466.959 I llama_perf_context_print: prompt eval time =      49.39 ms /     7 tokens (    7.06 ms per token,   141.74 tokens per second)
0.01.466.960 I llama_perf_context_print:        eval time =     633.53 ms /    63 runs   (   10.06 ms per token,    99.44 tokens per second)
0.01.466.960 I llama_perf_context_print:       total time =     686.77 ms /    70 tokens
0.01.470.074 I ggml_metal_free: deallocating

real	0m1.485s
user	0m0.109s
sys	0m0.240s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.407 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.873 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.879 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.880 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.880 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.883 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.883 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.883 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.884 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.884 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.884 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.885 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.141 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.939 I llama_model_loader: - type  f32:  194 tensors
0.00.028.940 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.940 I print_info: file format = GGUF V3 (latest)
0.00.028.941 I print_info: file type   = Q4_1
0.00.028.942 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.769 I load: special tokens cache size = 25
0.00.042.530 I load: token to piece cache size = 0.2984 MB
0.00.042.544 I print_info: arch             = gptneox
0.00.042.545 I print_info: vocab_only       = 0
0.00.042.545 I print_info: n_ctx_train      = 2048
0.00.042.545 I print_info: n_embd           = 2048
0.00.042.546 I print_info: n_layer          = 24
0.00.042.548 I print_info: n_head           = 16
0.00.042.549 I print_info: n_head_kv        = 16
0.00.042.549 I print_info: n_rot            = 32
0.00.042.549 I print_info: n_swa            = 0
0.00.042.550 I print_info: n_embd_head_k    = 128
0.00.042.550 I print_info: n_embd_head_v    = 128
0.00.042.551 I print_info: n_gqa            = 1
0.00.042.551 I print_info: n_embd_k_gqa     = 2048
0.00.042.553 I print_info: n_embd_v_gqa     = 2048
0.00.042.554 I print_info: f_norm_eps       = 1.0e-05
0.00.042.554 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.554 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.555 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.555 I print_info: f_logit_scale    = 0.0e+00
0.00.042.555 I print_info: n_ff             = 8192
0.00.042.556 I print_info: n_expert         = 0
0.00.042.556 I print_info: n_expert_used    = 0
0.00.042.556 I print_info: causal attn      = 1
0.00.042.556 I print_info: pooling type     = 0
0.00.042.556 I print_info: rope type        = 2
0.00.042.558 I print_info: rope scaling     = linear
0.00.042.559 I print_info: freq_base_train  = 10000.0
0.00.042.559 I print_info: freq_scale_train = 1
0.00.042.559 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.559 I print_info: rope_finetuned   = unknown
0.00.042.559 I print_info: ssm_d_conv       = 0
0.00.042.559 I print_info: ssm_d_inner      = 0
0.00.042.561 I print_info: ssm_d_state      = 0
0.00.042.561 I print_info: ssm_dt_rank      = 0
0.00.042.561 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.561 I print_info: model type       = 1.4B
0.00.042.562 I print_info: model params     = 1.41 B
0.00.042.562 I print_info: general.name     = 1.4B
0.00.042.562 I print_info: vocab type       = BPE
0.00.042.563 I print_info: n_vocab          = 50304
0.00.042.563 I print_info: n_merges         = 50009
0.00.042.563 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.563 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.563 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.563 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.563 I print_info: LF token         = 187 'Ċ'
0.00.042.564 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.565 I print_info: max token length = 1024
0.00.042.565 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.653 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.657 I load_tensors: offloading output layer to GPU
0.00.626.659 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.680 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.626.683 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.627.659 I llama_context: n_seq_max     = 1
0.00.627.660 I llama_context: n_ctx         = 2048
0.00.627.661 I llama_context: n_ctx_per_seq = 2048
0.00.627.662 I llama_context: n_batch       = 2048
0.00.627.662 I llama_context: n_ubatch      = 512
0.00.627.662 I llama_context: flash_attn    = 0
0.00.627.663 I llama_context: freq_base     = 10000.0
0.00.627.664 I llama_context: freq_scale    = 1
0.00.627.665 I ggml_metal_init: allocating
0.00.627.678 I ggml_metal_init: found device: Apple M4
0.00.627.685 I ggml_metal_init: picking default device: Apple M4
0.00.629.001 I ggml_metal_init: using embedded metal library
0.00.634.393 I ggml_metal_init: GPU name:   Apple M4
0.00.634.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.634.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.634.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.634.399 I ggml_metal_init: simdgroup reduction   = true
0.00.634.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.634.399 I ggml_metal_init: has residency sets    = true
0.00.634.399 I ggml_metal_init: has bfloat            = true
0.00.634.400 I ggml_metal_init: use bfloat            = true
0.00.634.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.634.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.202 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.650.206 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.290 I init:      Metal KV buffer size =   384.00 MiB
0.00.703.299 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.708.012 I init:      Metal compute buffer size =   102.25 MiB
0.00.708.014 I init:        CPU compute buffer size =     8.01 MiB
0.00.708.014 I init: graph nodes  = 967
0.00.708.015 I init: graph splits = 2
0.00.708.020 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.708.162 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.708.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.893 I main: llama threadpool init, n_threads = 4
0.00.756.937 I 
0.00.756.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.960 I 
0.00.757.079 I sampler seed: 1234
0.00.757.084 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.757.093 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.757.094 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.757.094 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.488.588 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.01.488.589 I llama_perf_context_print:        load time =     745.78 ms
0.01.488.590 I llama_perf_context_print: prompt eval time =      49.69 ms /     7 tokens (    7.10 ms per token,   140.89 tokens per second)
0.01.488.592 I llama_perf_context_print:        eval time =     678.97 ms /    63 runs   (   10.78 ms per token,    92.79 tokens per second)
0.01.488.593 I llama_perf_context_print:       total time =     732.40 ms /    70 tokens
0.01.491.957 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.107s
sys	0m0.254s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.642 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.357 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.363 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.363 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.365 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.367 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.372 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.261 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.981 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.982 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.983 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.983 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.984 I llama_model_loader: - type  f32:  194 tensors
0.00.026.984 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.985 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.985 I print_info: file format = GGUF V3 (latest)
0.00.026.986 I print_info: file type   = Q5_0
0.00.026.987 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.812 I load: special tokens cache size = 25
0.00.041.031 I load: token to piece cache size = 0.2984 MB
0.00.041.045 I print_info: arch             = gptneox
0.00.041.047 I print_info: vocab_only       = 0
0.00.041.047 I print_info: n_ctx_train      = 2048
0.00.041.047 I print_info: n_embd           = 2048
0.00.041.047 I print_info: n_layer          = 24
0.00.041.050 I print_info: n_head           = 16
0.00.041.051 I print_info: n_head_kv        = 16
0.00.041.051 I print_info: n_rot            = 32
0.00.041.051 I print_info: n_swa            = 0
0.00.041.051 I print_info: n_embd_head_k    = 128
0.00.041.051 I print_info: n_embd_head_v    = 128
0.00.041.052 I print_info: n_gqa            = 1
0.00.041.053 I print_info: n_embd_k_gqa     = 2048
0.00.041.054 I print_info: n_embd_v_gqa     = 2048
0.00.041.054 I print_info: f_norm_eps       = 1.0e-05
0.00.041.054 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.056 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.056 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.056 I print_info: f_logit_scale    = 0.0e+00
0.00.041.057 I print_info: n_ff             = 8192
0.00.041.057 I print_info: n_expert         = 0
0.00.041.057 I print_info: n_expert_used    = 0
0.00.041.057 I print_info: causal attn      = 1
0.00.041.057 I print_info: pooling type     = 0
0.00.041.058 I print_info: rope type        = 2
0.00.041.059 I print_info: rope scaling     = linear
0.00.041.059 I print_info: freq_base_train  = 10000.0
0.00.041.060 I print_info: freq_scale_train = 1
0.00.041.060 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.060 I print_info: rope_finetuned   = unknown
0.00.041.060 I print_info: ssm_d_conv       = 0
0.00.041.060 I print_info: ssm_d_inner      = 0
0.00.041.060 I print_info: ssm_d_state      = 0
0.00.041.060 I print_info: ssm_dt_rank      = 0
0.00.041.060 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.061 I print_info: model type       = 1.4B
0.00.041.061 I print_info: model params     = 1.41 B
0.00.041.061 I print_info: general.name     = 1.4B
0.00.041.062 I print_info: vocab type       = BPE
0.00.041.062 I print_info: n_vocab          = 50304
0.00.041.062 I print_info: n_merges         = 50009
0.00.041.066 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.068 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.068 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.068 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.068 I print_info: LF token         = 187 'Ċ'
0.00.041.069 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.070 I print_info: max token length = 1024
0.00.041.070 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.777.416 I load_tensors: offloading 24 repeating layers to GPU
0.00.777.421 I load_tensors: offloading output layer to GPU
0.00.777.423 I load_tensors: offloaded 25/25 layers to GPU
0.00.777.447 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.777.449 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.778.544 I llama_context: n_seq_max     = 1
0.00.778.545 I llama_context: n_ctx         = 2048
0.00.778.546 I llama_context: n_ctx_per_seq = 2048
0.00.778.546 I llama_context: n_batch       = 2048
0.00.778.547 I llama_context: n_ubatch      = 512
0.00.778.548 I llama_context: flash_attn    = 0
0.00.778.549 I llama_context: freq_base     = 10000.0
0.00.778.549 I llama_context: freq_scale    = 1
0.00.778.550 I ggml_metal_init: allocating
0.00.778.562 I ggml_metal_init: found device: Apple M4
0.00.778.569 I ggml_metal_init: picking default device: Apple M4
0.00.779.896 I ggml_metal_init: using embedded metal library
0.00.785.425 I ggml_metal_init: GPU name:   Apple M4
0.00.785.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.785.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.785.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.785.431 I ggml_metal_init: simdgroup reduction   = true
0.00.785.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.785.431 I ggml_metal_init: has residency sets    = true
0.00.785.431 I ggml_metal_init: has bfloat            = true
0.00.785.432 I ggml_metal_init: use bfloat            = true
0.00.785.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.785.433 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.801.281 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.801.284 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.853.483 I init:      Metal KV buffer size =   384.00 MiB
0.00.853.489 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.858.184 I init:      Metal compute buffer size =   102.25 MiB
0.00.858.186 I init:        CPU compute buffer size =     8.01 MiB
0.00.858.187 I init: graph nodes  = 967
0.00.858.187 I init: graph splits = 2
0.00.858.192 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.858.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.858.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.906.482 I main: llama threadpool init, n_threads = 4
0.00.906.531 I 
0.00.906.555 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.906.560 I 
0.00.906.684 I sampler seed: 1234
0.00.906.689 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.906.699 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.906.699 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.906.703 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.687.942 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.687.943 I llama_perf_context_print:        load time =     895.12 ms
0.01.687.944 I llama_perf_context_print: prompt eval time =      42.76 ms /     7 tokens (    6.11 ms per token,   163.71 tokens per second)
0.01.687.945 I llama_perf_context_print:        eval time =     735.48 ms /    63 runs   (   11.67 ms per token,    85.66 tokens per second)
0.01.687.945 I llama_perf_context_print:       total time =     782.18 ms /    70 tokens
0.01.691.234 I ggml_metal_free: deallocating

real	0m1.707s
user	0m0.106s
sys	0m0.271s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.557 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.350 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.358 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.359 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.374 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.374 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.118 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.820 I llama_model_loader: - type  f32:  194 tensors
0.00.025.820 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.820 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.821 I print_info: file format = GGUF V3 (latest)
0.00.025.821 I print_info: file type   = Q5_1
0.00.025.822 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.564 I load: special tokens cache size = 25
0.00.039.676 I load: token to piece cache size = 0.2984 MB
0.00.039.690 I print_info: arch             = gptneox
0.00.039.691 I print_info: vocab_only       = 0
0.00.039.691 I print_info: n_ctx_train      = 2048
0.00.039.692 I print_info: n_embd           = 2048
0.00.039.692 I print_info: n_layer          = 24
0.00.039.694 I print_info: n_head           = 16
0.00.039.695 I print_info: n_head_kv        = 16
0.00.039.695 I print_info: n_rot            = 32
0.00.039.695 I print_info: n_swa            = 0
0.00.039.696 I print_info: n_embd_head_k    = 128
0.00.039.696 I print_info: n_embd_head_v    = 128
0.00.039.697 I print_info: n_gqa            = 1
0.00.039.697 I print_info: n_embd_k_gqa     = 2048
0.00.039.698 I print_info: n_embd_v_gqa     = 2048
0.00.039.699 I print_info: f_norm_eps       = 1.0e-05
0.00.039.699 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.699 I print_info: f_logit_scale    = 0.0e+00
0.00.039.700 I print_info: n_ff             = 8192
0.00.039.700 I print_info: n_expert         = 0
0.00.039.701 I print_info: n_expert_used    = 0
0.00.039.701 I print_info: causal attn      = 1
0.00.039.701 I print_info: pooling type     = 0
0.00.039.701 I print_info: rope type        = 2
0.00.039.701 I print_info: rope scaling     = linear
0.00.039.702 I print_info: freq_base_train  = 10000.0
0.00.039.702 I print_info: freq_scale_train = 1
0.00.039.702 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.702 I print_info: rope_finetuned   = unknown
0.00.039.702 I print_info: ssm_d_conv       = 0
0.00.039.703 I print_info: ssm_d_inner      = 0
0.00.039.703 I print_info: ssm_d_state      = 0
0.00.039.704 I print_info: ssm_dt_rank      = 0
0.00.039.704 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.704 I print_info: model type       = 1.4B
0.00.039.705 I print_info: model params     = 1.41 B
0.00.039.705 I print_info: general.name     = 1.4B
0.00.039.705 I print_info: vocab type       = BPE
0.00.039.705 I print_info: n_vocab          = 50304
0.00.039.706 I print_info: n_merges         = 50009
0.00.039.707 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.707 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.707 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.707 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.707 I print_info: LF token         = 187 'Ċ'
0.00.039.709 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.709 I print_info: max token length = 1024
0.00.039.709 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.743.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.743.956 I load_tensors: offloading output layer to GPU
0.00.743.957 I load_tensors: offloaded 25/25 layers to GPU
0.00.743.986 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.743.988 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.745.015 I llama_context: n_seq_max     = 1
0.00.745.017 I llama_context: n_ctx         = 2048
0.00.745.017 I llama_context: n_ctx_per_seq = 2048
0.00.745.017 I llama_context: n_batch       = 2048
0.00.745.018 I llama_context: n_ubatch      = 512
0.00.745.018 I llama_context: flash_attn    = 0
0.00.745.019 I llama_context: freq_base     = 10000.0
0.00.745.019 I llama_context: freq_scale    = 1
0.00.745.020 I ggml_metal_init: allocating
0.00.745.028 I ggml_metal_init: found device: Apple M4
0.00.745.037 I ggml_metal_init: picking default device: Apple M4
0.00.746.361 I ggml_metal_init: using embedded metal library
0.00.751.852 I ggml_metal_init: GPU name:   Apple M4
0.00.751.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.751.856 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.751.857 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.751.857 I ggml_metal_init: simdgroup reduction   = true
0.00.751.858 I ggml_metal_init: simdgroup matrix mul. = true
0.00.751.858 I ggml_metal_init: has residency sets    = true
0.00.751.858 I ggml_metal_init: has bfloat            = true
0.00.751.858 I ggml_metal_init: use bfloat            = true
0.00.751.859 I ggml_metal_init: hasUnifiedMemory      = true
0.00.751.860 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.767.455 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.767.459 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.818.193 I init:      Metal KV buffer size =   384.00 MiB
0.00.818.199 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.823.351 I init:      Metal compute buffer size =   102.25 MiB
0.00.823.354 I init:        CPU compute buffer size =     8.01 MiB
0.00.823.354 I init: graph nodes  = 967
0.00.823.355 I init: graph splits = 2
0.00.823.359 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.823.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.823.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.875.053 I main: llama threadpool init, n_threads = 4
0.00.875.100 I 
0.00.875.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.875.127 I 
0.00.875.239 I sampler seed: 1234
0.00.875.243 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.875.253 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.875.254 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.875.254 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.710.246 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.710.246 I llama_perf_context_print:        load time =     864.79 ms
0.01.710.247 I llama_perf_context_print: prompt eval time =      44.79 ms /     7 tokens (    6.40 ms per token,   156.28 tokens per second)
0.01.710.248 I llama_perf_context_print:        eval time =     787.30 ms /    63 runs   (   12.50 ms per token,    80.02 tokens per second)
0.01.710.248 I llama_perf_context_print:       total time =     835.90 ms /    70 tokens
0.01.713.394 I ggml_metal_free: deallocating

real	0m1.730s
user	0m0.104s
sys	0m0.260s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.320 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.326 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.331 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.332 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.333 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.333 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.335 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.336 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.337 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.337 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.341 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.343 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.058 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.807 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.807 I llama_model_loader: - type  f32:  194 tensors
0.00.023.808 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.808 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.808 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.809 I print_info: file format = GGUF V3 (latest)
0.00.023.809 I print_info: file type   = Q2_K - Medium
0.00.023.810 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.573 I load: special tokens cache size = 25
0.00.037.680 I load: token to piece cache size = 0.2984 MB
0.00.037.694 I print_info: arch             = gptneox
0.00.037.695 I print_info: vocab_only       = 0
0.00.037.696 I print_info: n_ctx_train      = 2048
0.00.037.696 I print_info: n_embd           = 2048
0.00.037.696 I print_info: n_layer          = 24
0.00.037.698 I print_info: n_head           = 16
0.00.037.699 I print_info: n_head_kv        = 16
0.00.037.699 I print_info: n_rot            = 32
0.00.037.699 I print_info: n_swa            = 0
0.00.037.700 I print_info: n_embd_head_k    = 128
0.00.037.700 I print_info: n_embd_head_v    = 128
0.00.037.700 I print_info: n_gqa            = 1
0.00.037.701 I print_info: n_embd_k_gqa     = 2048
0.00.037.703 I print_info: n_embd_v_gqa     = 2048
0.00.037.703 I print_info: f_norm_eps       = 1.0e-05
0.00.037.703 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.704 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.704 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.704 I print_info: f_logit_scale    = 0.0e+00
0.00.037.705 I print_info: n_ff             = 8192
0.00.037.705 I print_info: n_expert         = 0
0.00.037.705 I print_info: n_expert_used    = 0
0.00.037.705 I print_info: causal attn      = 1
0.00.037.706 I print_info: pooling type     = 0
0.00.037.708 I print_info: rope type        = 2
0.00.037.708 I print_info: rope scaling     = linear
0.00.037.708 I print_info: freq_base_train  = 10000.0
0.00.037.708 I print_info: freq_scale_train = 1
0.00.037.709 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.709 I print_info: rope_finetuned   = unknown
0.00.037.710 I print_info: ssm_d_conv       = 0
0.00.037.710 I print_info: ssm_d_inner      = 0
0.00.037.710 I print_info: ssm_d_state      = 0
0.00.037.710 I print_info: ssm_dt_rank      = 0
0.00.037.710 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.711 I print_info: model type       = 1.4B
0.00.037.711 I print_info: model params     = 1.41 B
0.00.037.711 I print_info: general.name     = 1.4B
0.00.037.711 I print_info: vocab type       = BPE
0.00.037.712 I print_info: n_vocab          = 50304
0.00.037.712 I print_info: n_merges         = 50009
0.00.037.712 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.713 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.713 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.713 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.713 I print_info: LF token         = 187 'Ċ'
0.00.037.714 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.714 I print_info: max token length = 1024
0.00.037.714 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.388.118 I load_tensors: offloading 24 repeating layers to GPU
0.00.388.131 I load_tensors: offloading output layer to GPU
0.00.388.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.388.164 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.388.166 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.389.795 I llama_context: n_seq_max     = 1
0.00.389.799 I llama_context: n_ctx         = 2048
0.00.389.799 I llama_context: n_ctx_per_seq = 2048
0.00.389.800 I llama_context: n_batch       = 2048
0.00.389.801 I llama_context: n_ubatch      = 512
0.00.389.801 I llama_context: flash_attn    = 0
0.00.389.803 I llama_context: freq_base     = 10000.0
0.00.389.804 I llama_context: freq_scale    = 1
0.00.389.805 I ggml_metal_init: allocating
0.00.389.874 I ggml_metal_init: found device: Apple M4
0.00.389.890 I ggml_metal_init: picking default device: Apple M4
0.00.391.997 I ggml_metal_init: using embedded metal library
0.00.398.310 I ggml_metal_init: GPU name:   Apple M4
0.00.398.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.398.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.398.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.398.322 I ggml_metal_init: simdgroup reduction   = true
0.00.398.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.398.323 I ggml_metal_init: has residency sets    = true
0.00.398.323 I ggml_metal_init: has bfloat            = true
0.00.398.323 I ggml_metal_init: use bfloat            = true
0.00.398.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.398.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.419.615 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.419.620 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.481.959 I init:      Metal KV buffer size =   384.00 MiB
0.00.481.966 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.487.069 I init:      Metal compute buffer size =   102.25 MiB
0.00.487.071 I init:        CPU compute buffer size =     8.01 MiB
0.00.487.071 I init: graph nodes  = 967
0.00.487.071 I init: graph splits = 2
0.00.487.077 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.487.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.487.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.248 I main: llama threadpool init, n_threads = 4
0.00.535.295 I 
0.00.535.317 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.535.319 I 
0.00.535.454 I sampler seed: 1234
0.00.535.459 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.535.499 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.535.502 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.535.503 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.214.944 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.214.944 I llama_perf_context_print:        load time =     525.82 ms
0.01.214.945 I llama_perf_context_print: prompt eval time =      35.87 ms /     7 tokens (    5.12 ms per token,   195.13 tokens per second)
0.01.214.946 I llama_perf_context_print:        eval time =     640.66 ms /    63 runs   (   10.17 ms per token,    98.34 tokens per second)
0.01.214.947 I llama_perf_context_print:       total time =     680.39 ms /    70 tokens
0.01.218.378 I ggml_metal_free: deallocating

real	0m1.234s
user	0m0.113s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.324 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.196 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.201 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.203 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.203 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.204 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.204 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.204 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.205 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.205 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.206 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.206 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.207 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.207 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.207 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.211 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.788 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.791 I llama_model_loader: - type  f32:  194 tensors
0.00.024.791 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.791 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.792 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.792 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.793 I print_info: file format = GGUF V3 (latest)
0.00.024.793 I print_info: file type   = Q3_K - Medium
0.00.024.794 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.629 I load: special tokens cache size = 25
0.00.038.752 I load: token to piece cache size = 0.2984 MB
0.00.038.766 I print_info: arch             = gptneox
0.00.038.767 I print_info: vocab_only       = 0
0.00.038.768 I print_info: n_ctx_train      = 2048
0.00.038.768 I print_info: n_embd           = 2048
0.00.038.768 I print_info: n_layer          = 24
0.00.038.771 I print_info: n_head           = 16
0.00.038.771 I print_info: n_head_kv        = 16
0.00.038.772 I print_info: n_rot            = 32
0.00.038.772 I print_info: n_swa            = 0
0.00.038.772 I print_info: n_embd_head_k    = 128
0.00.038.772 I print_info: n_embd_head_v    = 128
0.00.038.773 I print_info: n_gqa            = 1
0.00.038.774 I print_info: n_embd_k_gqa     = 2048
0.00.038.774 I print_info: n_embd_v_gqa     = 2048
0.00.038.775 I print_info: f_norm_eps       = 1.0e-05
0.00.038.775 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.776 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.776 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.776 I print_info: f_logit_scale    = 0.0e+00
0.00.038.777 I print_info: n_ff             = 8192
0.00.038.777 I print_info: n_expert         = 0
0.00.038.777 I print_info: n_expert_used    = 0
0.00.038.778 I print_info: causal attn      = 1
0.00.038.779 I print_info: pooling type     = 0
0.00.038.779 I print_info: rope type        = 2
0.00.038.780 I print_info: rope scaling     = linear
0.00.038.783 I print_info: freq_base_train  = 10000.0
0.00.038.783 I print_info: freq_scale_train = 1
0.00.038.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.784 I print_info: rope_finetuned   = unknown
0.00.038.784 I print_info: ssm_d_conv       = 0
0.00.038.784 I print_info: ssm_d_inner      = 0
0.00.038.784 I print_info: ssm_d_state      = 0
0.00.038.784 I print_info: ssm_dt_rank      = 0
0.00.038.786 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.786 I print_info: model type       = 1.4B
0.00.038.786 I print_info: model params     = 1.41 B
0.00.038.786 I print_info: general.name     = 1.4B
0.00.038.787 I print_info: vocab type       = BPE
0.00.038.787 I print_info: n_vocab          = 50304
0.00.038.787 I print_info: n_merges         = 50009
0.00.038.787 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.787 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.787 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.787 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.788 I print_info: LF token         = 187 'Ċ'
0.00.038.788 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.788 I print_info: max token length = 1024
0.00.038.791 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.497.870 I load_tensors: offloading 24 repeating layers to GPU
0.00.497.880 I load_tensors: offloading output layer to GPU
0.00.497.881 I load_tensors: offloaded 25/25 layers to GPU
0.00.497.913 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.497.915 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.499.232 I llama_context: n_seq_max     = 1
0.00.499.235 I llama_context: n_ctx         = 2048
0.00.499.236 I llama_context: n_ctx_per_seq = 2048
0.00.499.236 I llama_context: n_batch       = 2048
0.00.499.237 I llama_context: n_ubatch      = 512
0.00.499.237 I llama_context: flash_attn    = 0
0.00.499.239 I llama_context: freq_base     = 10000.0
0.00.499.240 I llama_context: freq_scale    = 1
0.00.499.248 I ggml_metal_init: allocating
0.00.499.313 I ggml_metal_init: found device: Apple M4
0.00.499.326 I ggml_metal_init: picking default device: Apple M4
0.00.501.238 I ggml_metal_init: using embedded metal library
0.00.507.524 I ggml_metal_init: GPU name:   Apple M4
0.00.507.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.507.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.507.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.507.532 I ggml_metal_init: simdgroup reduction   = true
0.00.507.532 I ggml_metal_init: simdgroup matrix mul. = true
0.00.507.532 I ggml_metal_init: has residency sets    = true
0.00.507.533 I ggml_metal_init: has bfloat            = true
0.00.507.533 I ggml_metal_init: use bfloat            = true
0.00.507.534 I ggml_metal_init: hasUnifiedMemory      = true
0.00.507.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.525.930 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.525.934 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.580.494 I init:      Metal KV buffer size =   384.00 MiB
0.00.580.503 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.586.065 I init:      Metal compute buffer size =   102.25 MiB
0.00.586.069 I init:        CPU compute buffer size =     8.01 MiB
0.00.586.069 I init: graph nodes  = 967
0.00.586.069 I init: graph splits = 2
0.00.586.074 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.586.199 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.586.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.656 I main: llama threadpool init, n_threads = 4
0.00.637.701 I 
0.00.637.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.726 I 
0.00.637.842 I sampler seed: 1234
0.00.637.846 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.637.856 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.637.856 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.637.860 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.387.781 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.387.782 I llama_perf_context_print:        load time =     628.63 ms
0.01.387.784 I llama_perf_context_print: prompt eval time =      49.40 ms /     7 tokens (    7.06 ms per token,   141.70 tokens per second)
0.01.387.785 I llama_perf_context_print:        eval time =     697.46 ms /    63 runs   (   11.07 ms per token,    90.33 tokens per second)
0.01.387.785 I llama_perf_context_print:       total time =     750.83 ms /    70 tokens
0.01.391.164 I ggml_metal_free: deallocating

real	0m1.407s
user	0m0.109s
sys	0m0.223s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.330 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.279 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.290 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.179 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.258 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.074 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.076 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.076 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.077 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.077 I llama_model_loader: - type  f32:  194 tensors
0.00.027.078 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.078 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.078 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.078 I print_info: file format = GGUF V3 (latest)
0.00.027.079 I print_info: file type   = Q4_K - Medium
0.00.027.080 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.269 I load: special tokens cache size = 25
0.00.041.218 I load: token to piece cache size = 0.2984 MB
0.00.041.232 I print_info: arch             = gptneox
0.00.041.233 I print_info: vocab_only       = 0
0.00.041.233 I print_info: n_ctx_train      = 2048
0.00.041.233 I print_info: n_embd           = 2048
0.00.041.234 I print_info: n_layer          = 24
0.00.041.236 I print_info: n_head           = 16
0.00.041.237 I print_info: n_head_kv        = 16
0.00.041.237 I print_info: n_rot            = 32
0.00.041.237 I print_info: n_swa            = 0
0.00.041.237 I print_info: n_embd_head_k    = 128
0.00.041.238 I print_info: n_embd_head_v    = 128
0.00.041.238 I print_info: n_gqa            = 1
0.00.041.239 I print_info: n_embd_k_gqa     = 2048
0.00.041.240 I print_info: n_embd_v_gqa     = 2048
0.00.041.240 I print_info: f_norm_eps       = 1.0e-05
0.00.041.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.241 I print_info: f_logit_scale    = 0.0e+00
0.00.041.242 I print_info: n_ff             = 8192
0.00.041.242 I print_info: n_expert         = 0
0.00.041.243 I print_info: n_expert_used    = 0
0.00.041.243 I print_info: causal attn      = 1
0.00.041.244 I print_info: pooling type     = 0
0.00.041.245 I print_info: rope type        = 2
0.00.041.245 I print_info: rope scaling     = linear
0.00.041.245 I print_info: freq_base_train  = 10000.0
0.00.041.246 I print_info: freq_scale_train = 1
0.00.041.246 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.246 I print_info: rope_finetuned   = unknown
0.00.041.246 I print_info: ssm_d_conv       = 0
0.00.041.246 I print_info: ssm_d_inner      = 0
0.00.041.248 I print_info: ssm_d_state      = 0
0.00.041.248 I print_info: ssm_dt_rank      = 0
0.00.041.248 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.248 I print_info: model type       = 1.4B
0.00.041.248 I print_info: model params     = 1.41 B
0.00.041.249 I print_info: general.name     = 1.4B
0.00.041.250 I print_info: vocab type       = BPE
0.00.041.250 I print_info: n_vocab          = 50304
0.00.041.250 I print_info: n_merges         = 50009
0.00.041.250 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.250 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.250 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.251 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.251 I print_info: LF token         = 187 'Ċ'
0.00.041.251 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.251 I print_info: max token length = 1024
0.00.041.252 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.537 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.542 I load_tensors: offloading output layer to GPU
0.00.588.544 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.568 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.588.569 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.589.686 I llama_context: n_seq_max     = 1
0.00.589.688 I llama_context: n_ctx         = 2048
0.00.589.688 I llama_context: n_ctx_per_seq = 2048
0.00.589.688 I llama_context: n_batch       = 2048
0.00.589.689 I llama_context: n_ubatch      = 512
0.00.589.689 I llama_context: flash_attn    = 0
0.00.589.690 I llama_context: freq_base     = 10000.0
0.00.589.690 I llama_context: freq_scale    = 1
0.00.589.691 I ggml_metal_init: allocating
0.00.589.702 I ggml_metal_init: found device: Apple M4
0.00.589.709 I ggml_metal_init: picking default device: Apple M4
0.00.591.046 I ggml_metal_init: using embedded metal library
0.00.596.588 I ggml_metal_init: GPU name:   Apple M4
0.00.596.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.593 I ggml_metal_init: simdgroup reduction   = true
0.00.596.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.593 I ggml_metal_init: has residency sets    = true
0.00.596.594 I ggml_metal_init: has bfloat            = true
0.00.596.594 I ggml_metal_init: use bfloat            = true
0.00.596.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.700 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.612.703 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.677 I init:      Metal KV buffer size =   384.00 MiB
0.00.665.687 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.671.196 I init:      Metal compute buffer size =   102.25 MiB
0.00.671.199 I init:        CPU compute buffer size =     8.01 MiB
0.00.671.200 I init: graph nodes  = 967
0.00.671.200 I init: graph splits = 2
0.00.671.205 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.671.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.671.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.417 I main: llama threadpool init, n_threads = 4
0.00.719.471 I 
0.00.719.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.494 I 
0.00.719.622 I sampler seed: 1234
0.00.719.626 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.663 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.666 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.666 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.485.520 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.01.485.520 I llama_perf_context_print:        load time =     708.38 ms
0.01.485.521 I llama_perf_context_print: prompt eval time =      58.18 ms /     7 tokens (    8.31 ms per token,   120.31 tokens per second)
0.01.485.522 I llama_perf_context_print:        eval time =     704.64 ms /    63 runs   (   11.18 ms per token,    89.41 tokens per second)
0.01.485.522 I llama_perf_context_print:       total time =     766.81 ms /    70 tokens
0.01.488.946 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.107s
sys	0m0.240s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.529 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.329 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.336 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.336 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.337 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.337 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.337 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.338 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.339 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.341 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.117 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.118 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.119 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.119 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.120 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.120 I llama_model_loader: - type  f32:  194 tensors
0.00.027.120 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.121 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.121 I print_info: file format = GGUF V3 (latest)
0.00.027.122 I print_info: file type   = Q5_K - Medium
0.00.027.122 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.903 I load: special tokens cache size = 25
0.00.040.925 I load: token to piece cache size = 0.2984 MB
0.00.040.940 I print_info: arch             = gptneox
0.00.040.941 I print_info: vocab_only       = 0
0.00.040.941 I print_info: n_ctx_train      = 2048
0.00.040.941 I print_info: n_embd           = 2048
0.00.040.941 I print_info: n_layer          = 24
0.00.040.944 I print_info: n_head           = 16
0.00.040.945 I print_info: n_head_kv        = 16
0.00.040.946 I print_info: n_rot            = 32
0.00.040.946 I print_info: n_swa            = 0
0.00.040.946 I print_info: n_embd_head_k    = 128
0.00.040.946 I print_info: n_embd_head_v    = 128
0.00.040.947 I print_info: n_gqa            = 1
0.00.040.948 I print_info: n_embd_k_gqa     = 2048
0.00.040.948 I print_info: n_embd_v_gqa     = 2048
0.00.040.949 I print_info: f_norm_eps       = 1.0e-05
0.00.040.949 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.950 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.950 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.950 I print_info: f_logit_scale    = 0.0e+00
0.00.040.951 I print_info: n_ff             = 8192
0.00.040.951 I print_info: n_expert         = 0
0.00.040.951 I print_info: n_expert_used    = 0
0.00.040.951 I print_info: causal attn      = 1
0.00.040.951 I print_info: pooling type     = 0
0.00.040.951 I print_info: rope type        = 2
0.00.040.952 I print_info: rope scaling     = linear
0.00.040.952 I print_info: freq_base_train  = 10000.0
0.00.040.952 I print_info: freq_scale_train = 1
0.00.040.952 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.953 I print_info: rope_finetuned   = unknown
0.00.040.953 I print_info: ssm_d_conv       = 0
0.00.040.953 I print_info: ssm_d_inner      = 0
0.00.040.954 I print_info: ssm_d_state      = 0
0.00.040.954 I print_info: ssm_dt_rank      = 0
0.00.040.954 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.954 I print_info: model type       = 1.4B
0.00.040.955 I print_info: model params     = 1.41 B
0.00.040.955 I print_info: general.name     = 1.4B
0.00.040.955 I print_info: vocab type       = BPE
0.00.040.955 I print_info: n_vocab          = 50304
0.00.040.956 I print_info: n_merges         = 50009
0.00.040.956 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.956 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.956 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.956 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.956 I print_info: LF token         = 187 'Ċ'
0.00.040.960 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.960 I print_info: max token length = 1024
0.00.040.960 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.666.598 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.603 I load_tensors: offloading output layer to GPU
0.00.666.604 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.629 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.666.631 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.667.490 I llama_context: n_seq_max     = 1
0.00.667.492 I llama_context: n_ctx         = 2048
0.00.667.492 I llama_context: n_ctx_per_seq = 2048
0.00.667.493 I llama_context: n_batch       = 2048
0.00.667.493 I llama_context: n_ubatch      = 512
0.00.667.493 I llama_context: flash_attn    = 0
0.00.667.494 I llama_context: freq_base     = 10000.0
0.00.667.494 I llama_context: freq_scale    = 1
0.00.667.495 I ggml_metal_init: allocating
0.00.667.515 I ggml_metal_init: found device: Apple M4
0.00.667.523 I ggml_metal_init: picking default device: Apple M4
0.00.668.696 I ggml_metal_init: using embedded metal library
0.00.673.907 I ggml_metal_init: GPU name:   Apple M4
0.00.673.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.912 I ggml_metal_init: simdgroup reduction   = true
0.00.673.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.912 I ggml_metal_init: has residency sets    = true
0.00.673.912 I ggml_metal_init: has bfloat            = true
0.00.673.913 I ggml_metal_init: use bfloat            = true
0.00.673.913 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.914 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.180 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.689.184 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.741.554 I init:      Metal KV buffer size =   384.00 MiB
0.00.741.561 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.717 I init:      Metal compute buffer size =   102.25 MiB
0.00.746.720 I init:        CPU compute buffer size =     8.01 MiB
0.00.746.720 I init: graph nodes  = 967
0.00.746.720 I init: graph splits = 2
0.00.746.724 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.746.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.211 I main: llama threadpool init, n_threads = 4
0.00.800.261 I 
0.00.800.287 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.287 I 
0.00.800.392 I sampler seed: 1234
0.00.800.397 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.407 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.407 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.407 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.642.693 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.642.693 I llama_perf_context_print:        load time =     788.98 ms
0.01.642.695 I llama_perf_context_print: prompt eval time =      51.19 ms /     7 tokens (    7.31 ms per token,   136.75 tokens per second)
0.01.642.696 I llama_perf_context_print:        eval time =     788.07 ms /    63 runs   (   12.51 ms per token,    79.94 tokens per second)
0.01.642.696 I llama_perf_context_print:       total time =     843.19 ms /    70 tokens
0.01.646.024 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.105s
sys	0m0.254s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.507 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.317 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.318 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.319 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.319 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.319 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.320 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.321 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.321 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.321 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.322 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.322 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.323 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.323 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.325 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.325 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.268 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.125 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.126 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.126 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.127 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.127 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.127 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.128 I llama_model_loader: - type  f32:  194 tensors
0.00.027.128 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.129 I print_info: file format = GGUF V3 (latest)
0.00.027.129 I print_info: file type   = Q6_K
0.00.027.130 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.258 I load: special tokens cache size = 25
0.00.041.156 I load: token to piece cache size = 0.2984 MB
0.00.041.170 I print_info: arch             = gptneox
0.00.041.171 I print_info: vocab_only       = 0
0.00.041.172 I print_info: n_ctx_train      = 2048
0.00.041.172 I print_info: n_embd           = 2048
0.00.041.172 I print_info: n_layer          = 24
0.00.041.175 I print_info: n_head           = 16
0.00.041.176 I print_info: n_head_kv        = 16
0.00.041.176 I print_info: n_rot            = 32
0.00.041.176 I print_info: n_swa            = 0
0.00.041.176 I print_info: n_embd_head_k    = 128
0.00.041.176 I print_info: n_embd_head_v    = 128
0.00.041.177 I print_info: n_gqa            = 1
0.00.041.178 I print_info: n_embd_k_gqa     = 2048
0.00.041.178 I print_info: n_embd_v_gqa     = 2048
0.00.041.179 I print_info: f_norm_eps       = 1.0e-05
0.00.041.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.180 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.180 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.180 I print_info: f_logit_scale    = 0.0e+00
0.00.041.180 I print_info: n_ff             = 8192
0.00.041.182 I print_info: n_expert         = 0
0.00.041.182 I print_info: n_expert_used    = 0
0.00.041.182 I print_info: causal attn      = 1
0.00.041.182 I print_info: pooling type     = 0
0.00.041.182 I print_info: rope type        = 2
0.00.041.183 I print_info: rope scaling     = linear
0.00.041.183 I print_info: freq_base_train  = 10000.0
0.00.041.183 I print_info: freq_scale_train = 1
0.00.041.184 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.184 I print_info: rope_finetuned   = unknown
0.00.041.184 I print_info: ssm_d_conv       = 0
0.00.041.184 I print_info: ssm_d_inner      = 0
0.00.041.184 I print_info: ssm_d_state      = 0
0.00.041.185 I print_info: ssm_dt_rank      = 0
0.00.041.185 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.185 I print_info: model type       = 1.4B
0.00.041.192 I print_info: model params     = 1.41 B
0.00.041.193 I print_info: general.name     = 1.4B
0.00.041.194 I print_info: vocab type       = BPE
0.00.041.194 I print_info: n_vocab          = 50304
0.00.041.194 I print_info: n_merges         = 50009
0.00.041.195 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.196 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.196 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.196 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.196 I print_info: LF token         = 187 'Ċ'
0.00.041.196 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.197 I print_info: max token length = 1024
0.00.041.197 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.728.955 I load_tensors: offloading 24 repeating layers to GPU
0.00.728.958 I load_tensors: offloading output layer to GPU
0.00.728.958 I load_tensors: offloaded 25/25 layers to GPU
0.00.728.978 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.728.980 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.730.072 I llama_context: n_seq_max     = 1
0.00.730.073 I llama_context: n_ctx         = 2048
0.00.730.074 I llama_context: n_ctx_per_seq = 2048
0.00.730.075 I llama_context: n_batch       = 2048
0.00.730.075 I llama_context: n_ubatch      = 512
0.00.730.075 I llama_context: flash_attn    = 0
0.00.730.076 I llama_context: freq_base     = 10000.0
0.00.730.076 I llama_context: freq_scale    = 1
0.00.730.077 I ggml_metal_init: allocating
0.00.730.101 I ggml_metal_init: found device: Apple M4
0.00.730.108 I ggml_metal_init: picking default device: Apple M4
0.00.731.379 I ggml_metal_init: using embedded metal library
0.00.736.635 I ggml_metal_init: GPU name:   Apple M4
0.00.736.638 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.736.638 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.736.639 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.736.640 I ggml_metal_init: simdgroup reduction   = true
0.00.736.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.736.640 I ggml_metal_init: has residency sets    = true
0.00.736.640 I ggml_metal_init: has bfloat            = true
0.00.736.641 I ggml_metal_init: use bfloat            = true
0.00.736.641 I ggml_metal_init: hasUnifiedMemory      = true
0.00.736.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.751.668 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.751.672 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.785.741 I init:      Metal KV buffer size =   384.00 MiB
0.00.785.746 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.791.000 I init:      Metal compute buffer size =   102.25 MiB
0.00.791.004 I init:        CPU compute buffer size =     8.01 MiB
0.00.791.004 I init: graph nodes  = 967
0.00.791.004 I init: graph splits = 2
0.00.791.009 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.791.142 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.791.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.851.102 I main: llama threadpool init, n_threads = 4
0.00.851.148 I 
0.00.851.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.851.173 I 
0.00.851.289 I sampler seed: 1234
0.00.851.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.851.329 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.851.331 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.851.331 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.718.525 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.718.526 I llama_perf_context_print:        load time =     839.89 ms
0.01.718.528 I llama_perf_context_print: prompt eval time =      54.51 ms /     7 tokens (    7.79 ms per token,   128.41 tokens per second)
0.01.718.529 I llama_perf_context_print:        eval time =     809.67 ms /    63 runs   (   12.85 ms per token,    77.81 tokens per second)
0.01.718.529 I llama_perf_context_print:       total time =     868.13 ms /    70 tokens
0.01.721.904 I ggml_metal_free: deallocating

real	0m1.741s
user	0m0.105s
sys	0m0.249s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.701 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.603 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.750 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.758 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.759 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.762 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.763 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.764 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.765 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.767 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.767 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.541 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.492 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.499 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.499 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.501 I llama_model_loader: - type  f32:  194 tensors
0.00.055.501 I llama_model_loader: - type  f16:   98 tensors
0.00.055.502 I print_info: file format = GGUF V3 (latest)
0.00.055.503 I print_info: file type   = all F32 (guessed)
0.00.055.504 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.290 I load: special tokens cache size = 25
0.00.076.371 I load: token to piece cache size = 0.2984 MB
0.00.076.386 I print_info: arch             = gptneox
0.00.076.387 I print_info: vocab_only       = 0
0.00.076.387 I print_info: n_ctx_train      = 2048
0.00.076.388 I print_info: n_embd           = 2048
0.00.076.388 I print_info: n_layer          = 24
0.00.076.391 I print_info: n_head           = 16
0.00.076.392 I print_info: n_head_kv        = 16
0.00.076.392 I print_info: n_rot            = 32
0.00.076.392 I print_info: n_swa            = 0
0.00.076.394 I print_info: n_embd_head_k    = 128
0.00.076.394 I print_info: n_embd_head_v    = 128
0.00.076.395 I print_info: n_gqa            = 1
0.00.076.396 I print_info: n_embd_k_gqa     = 2048
0.00.076.397 I print_info: n_embd_v_gqa     = 2048
0.00.076.397 I print_info: f_norm_eps       = 1.0e-05
0.00.076.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.405 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.405 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.405 I print_info: f_logit_scale    = 0.0e+00
0.00.076.408 I print_info: n_ff             = 8192
0.00.076.408 I print_info: n_expert         = 0
0.00.076.408 I print_info: n_expert_used    = 0
0.00.076.408 I print_info: causal attn      = 1
0.00.076.408 I print_info: pooling type     = 0
0.00.076.409 I print_info: rope type        = 2
0.00.076.409 I print_info: rope scaling     = linear
0.00.076.409 I print_info: freq_base_train  = 10000.0
0.00.076.410 I print_info: freq_scale_train = 1
0.00.076.410 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.410 I print_info: rope_finetuned   = unknown
0.00.076.412 I print_info: ssm_d_conv       = 0
0.00.076.412 I print_info: ssm_d_inner      = 0
0.00.076.412 I print_info: ssm_d_state      = 0
0.00.076.412 I print_info: ssm_dt_rank      = 0
0.00.076.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.413 I print_info: model type       = 1.4B
0.00.076.413 I print_info: model params     = 1.41 B
0.00.076.413 I print_info: general.name     = 1.4B
0.00.076.414 I print_info: vocab type       = BPE
0.00.076.414 I print_info: n_vocab          = 50304
0.00.076.414 I print_info: n_merges         = 50009
0.00.076.415 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.416 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.416 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.416 I print_info: LF token         = 187 'Ċ'
0.00.076.417 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.418 I print_info: max token length = 1024
0.00.076.418 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.950.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.950.304 I load_tensors: offloading output layer to GPU
0.00.950.305 I load_tensors: offloaded 25/25 layers to GPU
0.00.950.334 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.950.336 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.950.833 I llama_context: n_seq_max     = 1
0.00.950.835 I llama_context: n_ctx         = 128
0.00.950.835 I llama_context: n_ctx_per_seq = 128
0.00.950.835 I llama_context: n_batch       = 128
0.00.950.835 I llama_context: n_ubatch      = 128
0.00.950.835 I llama_context: flash_attn    = 0
0.00.950.836 I llama_context: freq_base     = 10000.0
0.00.950.836 I llama_context: freq_scale    = 1
0.00.950.837 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.950.837 I ggml_metal_init: allocating
0.00.950.876 I ggml_metal_init: found device: Apple M4
0.00.950.882 I ggml_metal_init: picking default device: Apple M4
0.00.951.928 I ggml_metal_init: using embedded metal library
0.00.955.400 I ggml_metal_init: GPU name:   Apple M4
0.00.955.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.955.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.955.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.955.404 I ggml_metal_init: simdgroup reduction   = true
0.00.955.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.955.404 I ggml_metal_init: has residency sets    = true
0.00.955.404 I ggml_metal_init: has bfloat            = true
0.00.955.405 I ggml_metal_init: use bfloat            = true
0.00.955.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.955.407 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.965.164 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.965.166 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.966.917 I init:      Metal KV buffer size =    24.00 MiB
0.00.966.919 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.968.386 I init:      Metal compute buffer size =    25.56 MiB
0.00.968.387 I init:        CPU compute buffer size =     1.06 MiB
0.00.968.388 I init: graph nodes  = 967
0.00.968.388 I init: graph splits = 2
0.00.968.389 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.968.389 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.002.106 I 
0.01.002.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.002.166 I perplexity: tokenizing the input ..
0.01.006.350 I perplexity: tokenization took 4.18 ms
0.01.006.370 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.124.019 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.125.296 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.125.305 I llama_perf_context_print:        load time =     978.49 ms
0.01.125.307 I llama_perf_context_print: prompt eval time =     117.39 ms /   128 tokens (    0.92 ms per token,  1090.36 tokens per second)
0.01.125.307 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.125.309 I llama_perf_context_print:       total time =     123.20 ms /   129 tokens
0.01.125.857 I ggml_metal_free: deallocating

real	0m1.324s
user	0m0.100s
sys	0m0.264s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.727 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.952 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.959 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.972 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.975 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.975 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.860 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.740 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.742 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.742 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.742 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.743 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.743 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.744 I llama_model_loader: - type  f32:  194 tensors
0.00.025.744 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.745 I print_info: file format = GGUF V3 (latest)
0.00.025.745 I print_info: file type   = Q8_0
0.00.025.746 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.222 I load: special tokens cache size = 25
0.00.040.390 I load: token to piece cache size = 0.2984 MB
0.00.040.408 I print_info: arch             = gptneox
0.00.040.409 I print_info: vocab_only       = 0
0.00.040.409 I print_info: n_ctx_train      = 2048
0.00.040.409 I print_info: n_embd           = 2048
0.00.040.409 I print_info: n_layer          = 24
0.00.040.413 I print_info: n_head           = 16
0.00.040.413 I print_info: n_head_kv        = 16
0.00.040.413 I print_info: n_rot            = 32
0.00.040.414 I print_info: n_swa            = 0
0.00.040.414 I print_info: n_embd_head_k    = 128
0.00.040.414 I print_info: n_embd_head_v    = 128
0.00.040.414 I print_info: n_gqa            = 1
0.00.040.415 I print_info: n_embd_k_gqa     = 2048
0.00.040.415 I print_info: n_embd_v_gqa     = 2048
0.00.040.418 I print_info: f_norm_eps       = 1.0e-05
0.00.040.418 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.419 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.419 I print_info: f_logit_scale    = 0.0e+00
0.00.040.420 I print_info: n_ff             = 8192
0.00.040.420 I print_info: n_expert         = 0
0.00.040.420 I print_info: n_expert_used    = 0
0.00.040.420 I print_info: causal attn      = 1
0.00.040.420 I print_info: pooling type     = 0
0.00.040.423 I print_info: rope type        = 2
0.00.040.423 I print_info: rope scaling     = linear
0.00.040.424 I print_info: freq_base_train  = 10000.0
0.00.040.424 I print_info: freq_scale_train = 1
0.00.040.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.424 I print_info: rope_finetuned   = unknown
0.00.040.424 I print_info: ssm_d_conv       = 0
0.00.040.424 I print_info: ssm_d_inner      = 0
0.00.040.425 I print_info: ssm_d_state      = 0
0.00.040.425 I print_info: ssm_dt_rank      = 0
0.00.040.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.425 I print_info: model type       = 1.4B
0.00.040.425 I print_info: model params     = 1.41 B
0.00.040.425 I print_info: general.name     = 1.4B
0.00.040.426 I print_info: vocab type       = BPE
0.00.040.426 I print_info: n_vocab          = 50304
0.00.040.428 I print_info: n_merges         = 50009
0.00.040.428 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.428 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.429 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.429 I print_info: LF token         = 187 'Ċ'
0.00.040.429 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.434 I print_info: max token length = 1024
0.00.040.435 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.892.517 I load_tensors: offloading 24 repeating layers to GPU
0.00.892.523 I load_tensors: offloading output layer to GPU
0.00.892.524 I load_tensors: offloaded 25/25 layers to GPU
0.00.892.547 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.892.549 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.893.438 I llama_context: n_seq_max     = 1
0.00.893.439 I llama_context: n_ctx         = 128
0.00.893.439 I llama_context: n_ctx_per_seq = 128
0.00.893.439 I llama_context: n_batch       = 128
0.00.893.440 I llama_context: n_ubatch      = 128
0.00.893.440 I llama_context: flash_attn    = 0
0.00.893.441 I llama_context: freq_base     = 10000.0
0.00.893.441 I llama_context: freq_scale    = 1
0.00.893.441 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.893.442 I ggml_metal_init: allocating
0.00.893.493 I ggml_metal_init: found device: Apple M4
0.00.893.501 I ggml_metal_init: picking default device: Apple M4
0.00.894.660 I ggml_metal_init: using embedded metal library
0.00.898.950 I ggml_metal_init: GPU name:   Apple M4
0.00.898.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.898.953 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.898.954 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.898.954 I ggml_metal_init: simdgroup reduction   = true
0.00.898.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.898.954 I ggml_metal_init: has residency sets    = true
0.00.898.955 I ggml_metal_init: has bfloat            = true
0.00.898.955 I ggml_metal_init: use bfloat            = true
0.00.898.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.898.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.911.517 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.911.519 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.913.337 I init:      Metal KV buffer size =    24.00 MiB
0.00.913.342 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.915.044 I init:      Metal compute buffer size =    25.56 MiB
0.00.915.046 I init:        CPU compute buffer size =     1.06 MiB
0.00.915.046 I init: graph nodes  = 967
0.00.915.047 I init: graph splits = 2
0.00.915.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.915.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.938.699 I 
0.00.938.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.938.731 I perplexity: tokenizing the input ..
0.00.943.608 I perplexity: tokenization took 4.875 ms
0.00.943.622 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.080.968 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.082.216 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.082.230 I llama_perf_context_print:        load time =     928.97 ms
0.01.082.231 I llama_perf_context_print: prompt eval time =     137.12 ms /   128 tokens (    1.07 ms per token,   933.45 tokens per second)
0.01.082.232 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.082.232 I llama_perf_context_print:       total time =     143.53 ms /   129 tokens
0.01.082.878 I ggml_metal_free: deallocating

real	0m1.097s
user	0m0.071s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.465 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.588 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.594 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.600 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.449 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.475 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.319 I llama_model_loader: - type  f32:  194 tensors
0.00.026.319 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.319 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.320 I print_info: file format = GGUF V3 (latest)
0.00.026.321 I print_info: file type   = Q4_0
0.00.026.322 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.709 I load: special tokens cache size = 25
0.00.040.965 I load: token to piece cache size = 0.2984 MB
0.00.040.982 I print_info: arch             = gptneox
0.00.040.983 I print_info: vocab_only       = 0
0.00.040.983 I print_info: n_ctx_train      = 2048
0.00.040.983 I print_info: n_embd           = 2048
0.00.040.983 I print_info: n_layer          = 24
0.00.040.987 I print_info: n_head           = 16
0.00.040.988 I print_info: n_head_kv        = 16
0.00.040.988 I print_info: n_rot            = 32
0.00.040.988 I print_info: n_swa            = 0
0.00.040.988 I print_info: n_embd_head_k    = 128
0.00.040.988 I print_info: n_embd_head_v    = 128
0.00.040.989 I print_info: n_gqa            = 1
0.00.040.990 I print_info: n_embd_k_gqa     = 2048
0.00.040.990 I print_info: n_embd_v_gqa     = 2048
0.00.040.991 I print_info: f_norm_eps       = 1.0e-05
0.00.040.991 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.991 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.991 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.991 I print_info: f_logit_scale    = 0.0e+00
0.00.040.992 I print_info: n_ff             = 8192
0.00.040.994 I print_info: n_expert         = 0
0.00.040.994 I print_info: n_expert_used    = 0
0.00.040.994 I print_info: causal attn      = 1
0.00.040.999 I print_info: pooling type     = 0
0.00.040.999 I print_info: rope type        = 2
0.00.041.000 I print_info: rope scaling     = linear
0.00.041.000 I print_info: freq_base_train  = 10000.0
0.00.041.000 I print_info: freq_scale_train = 1
0.00.041.000 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.000 I print_info: rope_finetuned   = unknown
0.00.041.001 I print_info: ssm_d_conv       = 0
0.00.041.001 I print_info: ssm_d_inner      = 0
0.00.041.001 I print_info: ssm_d_state      = 0
0.00.041.001 I print_info: ssm_dt_rank      = 0
0.00.041.001 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.001 I print_info: model type       = 1.4B
0.00.041.002 I print_info: model params     = 1.41 B
0.00.041.002 I print_info: general.name     = 1.4B
0.00.041.002 I print_info: vocab type       = BPE
0.00.041.002 I print_info: n_vocab          = 50304
0.00.041.003 I print_info: n_merges         = 50009
0.00.041.003 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.003 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.003 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.003 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.003 I print_info: LF token         = 187 'Ċ'
0.00.041.004 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.004 I print_info: max token length = 1024
0.00.041.004 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.344 I load_tensors: offloading output layer to GPU
0.00.642.345 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.369 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.642.372 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.643.636 I llama_context: n_seq_max     = 1
0.00.643.638 I llama_context: n_ctx         = 128
0.00.643.638 I llama_context: n_ctx_per_seq = 128
0.00.643.639 I llama_context: n_batch       = 128
0.00.643.639 I llama_context: n_ubatch      = 128
0.00.643.640 I llama_context: flash_attn    = 0
0.00.643.641 I llama_context: freq_base     = 10000.0
0.00.643.642 I llama_context: freq_scale    = 1
0.00.643.642 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.643.643 I ggml_metal_init: allocating
0.00.643.702 I ggml_metal_init: found device: Apple M4
0.00.643.714 I ggml_metal_init: picking default device: Apple M4
0.00.645.085 I ggml_metal_init: using embedded metal library
0.00.650.873 I ggml_metal_init: GPU name:   Apple M4
0.00.650.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.879 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.880 I ggml_metal_init: simdgroup reduction   = true
0.00.650.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.880 I ggml_metal_init: has residency sets    = true
0.00.650.880 I ggml_metal_init: has bfloat            = true
0.00.650.881 I ggml_metal_init: use bfloat            = true
0.00.650.882 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.883 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.961 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.666.965 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.670.316 I init:      Metal KV buffer size =    24.00 MiB
0.00.670.324 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.190 I init:      Metal compute buffer size =    25.56 MiB
0.00.673.191 I init:        CPU compute buffer size =     1.06 MiB
0.00.673.192 I init: graph nodes  = 967
0.00.673.192 I init: graph splits = 2
0.00.673.196 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.673.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.182 I 
0.00.701.250 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.269 I perplexity: tokenizing the input ..
0.00.707.896 I perplexity: tokenization took 6.622 ms
0.00.707.914 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.089 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.840.389 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.840.404 I llama_perf_context_print:        load time =     690.71 ms
0.00.840.405 I llama_perf_context_print: prompt eval time =     130.33 ms /   128 tokens (    1.02 ms per token,   982.09 tokens per second)
0.00.840.406 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.406 I llama_perf_context_print:       total time =     139.22 ms /   129 tokens
0.00.841.073 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.078s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.322 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.735 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.736 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.736 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.737 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.737 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.738 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.738 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.739 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.739 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.739 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.740 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.740 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.742 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.743 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.743 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.657 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.720 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.654 I llama_model_loader: - type  f32:  194 tensors
0.00.025.655 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.658 I print_info: file format = GGUF V3 (latest)
0.00.025.658 I print_info: file type   = Q4_1
0.00.025.659 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.093 I load: special tokens cache size = 25
0.00.040.500 I load: token to piece cache size = 0.2984 MB
0.00.040.517 I print_info: arch             = gptneox
0.00.040.518 I print_info: vocab_only       = 0
0.00.040.519 I print_info: n_ctx_train      = 2048
0.00.040.519 I print_info: n_embd           = 2048
0.00.040.519 I print_info: n_layer          = 24
0.00.040.523 I print_info: n_head           = 16
0.00.040.524 I print_info: n_head_kv        = 16
0.00.040.524 I print_info: n_rot            = 32
0.00.040.524 I print_info: n_swa            = 0
0.00.040.525 I print_info: n_embd_head_k    = 128
0.00.040.525 I print_info: n_embd_head_v    = 128
0.00.040.525 I print_info: n_gqa            = 1
0.00.040.526 I print_info: n_embd_k_gqa     = 2048
0.00.040.526 I print_info: n_embd_v_gqa     = 2048
0.00.040.527 I print_info: f_norm_eps       = 1.0e-05
0.00.040.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.528 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.528 I print_info: f_logit_scale    = 0.0e+00
0.00.040.528 I print_info: n_ff             = 8192
0.00.040.529 I print_info: n_expert         = 0
0.00.040.530 I print_info: n_expert_used    = 0
0.00.040.530 I print_info: causal attn      = 1
0.00.040.530 I print_info: pooling type     = 0
0.00.040.533 I print_info: rope type        = 2
0.00.040.533 I print_info: rope scaling     = linear
0.00.040.533 I print_info: freq_base_train  = 10000.0
0.00.040.533 I print_info: freq_scale_train = 1
0.00.040.534 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.534 I print_info: rope_finetuned   = unknown
0.00.040.534 I print_info: ssm_d_conv       = 0
0.00.040.534 I print_info: ssm_d_inner      = 0
0.00.040.534 I print_info: ssm_d_state      = 0
0.00.040.534 I print_info: ssm_dt_rank      = 0
0.00.040.534 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.535 I print_info: model type       = 1.4B
0.00.040.535 I print_info: model params     = 1.41 B
0.00.040.535 I print_info: general.name     = 1.4B
0.00.040.536 I print_info: vocab type       = BPE
0.00.040.536 I print_info: n_vocab          = 50304
0.00.040.536 I print_info: n_merges         = 50009
0.00.040.536 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.536 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.537 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.537 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.537 I print_info: LF token         = 187 'Ċ'
0.00.040.537 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.538 I print_info: max token length = 1024
0.00.040.539 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.618.988 I load_tensors: offloading 24 repeating layers to GPU
0.00.618.993 I load_tensors: offloading output layer to GPU
0.00.618.994 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.022 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.619.024 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.620.307 I llama_context: n_seq_max     = 1
0.00.620.309 I llama_context: n_ctx         = 128
0.00.620.310 I llama_context: n_ctx_per_seq = 128
0.00.620.310 I llama_context: n_batch       = 128
0.00.620.311 I llama_context: n_ubatch      = 128
0.00.620.311 I llama_context: flash_attn    = 0
0.00.620.312 I llama_context: freq_base     = 10000.0
0.00.620.313 I llama_context: freq_scale    = 1
0.00.620.314 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.315 I ggml_metal_init: allocating
0.00.620.367 I ggml_metal_init: found device: Apple M4
0.00.620.380 I ggml_metal_init: picking default device: Apple M4
0.00.621.790 I ggml_metal_init: using embedded metal library
0.00.627.696 I ggml_metal_init: GPU name:   Apple M4
0.00.627.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.701 I ggml_metal_init: simdgroup reduction   = true
0.00.627.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.701 I ggml_metal_init: has residency sets    = true
0.00.627.702 I ggml_metal_init: has bfloat            = true
0.00.627.702 I ggml_metal_init: use bfloat            = true
0.00.627.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.931 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.643.936 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.261 I init:      Metal KV buffer size =    24.00 MiB
0.00.647.267 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.650.205 I init:      Metal compute buffer size =    25.56 MiB
0.00.650.206 I init:        CPU compute buffer size =     1.06 MiB
0.00.650.207 I init: graph nodes  = 967
0.00.650.207 I init: graph splits = 2
0.00.650.210 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.210 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.546 I 
0.00.678.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.633 I perplexity: tokenizing the input ..
0.00.685.641 I perplexity: tokenization took 7.004 ms
0.00.685.661 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.487 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.819.745 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.819.758 I llama_perf_context_print:        load time =     669.22 ms
0.00.819.759 I llama_perf_context_print: prompt eval time =     131.91 ms /   128 tokens (    1.03 ms per token,   970.36 tokens per second)
0.00.819.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.760 I llama_perf_context_print:       total time =     141.22 ms /   129 tokens
0.00.820.427 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.079s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.602 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.882 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.646 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.646 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.499 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.501 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.501 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.501 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.502 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.503 I llama_model_loader: - type  f32:  194 tensors
0.00.026.503 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.503 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.504 I print_info: file format = GGUF V3 (latest)
0.00.026.504 I print_info: file type   = Q5_0
0.00.026.506 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.032 I load: special tokens cache size = 25
0.00.041.028 I load: token to piece cache size = 0.2984 MB
0.00.041.045 I print_info: arch             = gptneox
0.00.041.046 I print_info: vocab_only       = 0
0.00.041.046 I print_info: n_ctx_train      = 2048
0.00.041.046 I print_info: n_embd           = 2048
0.00.041.046 I print_info: n_layer          = 24
0.00.041.050 I print_info: n_head           = 16
0.00.041.051 I print_info: n_head_kv        = 16
0.00.041.053 I print_info: n_rot            = 32
0.00.041.053 I print_info: n_swa            = 0
0.00.041.053 I print_info: n_embd_head_k    = 128
0.00.041.053 I print_info: n_embd_head_v    = 128
0.00.041.054 I print_info: n_gqa            = 1
0.00.041.055 I print_info: n_embd_k_gqa     = 2048
0.00.041.055 I print_info: n_embd_v_gqa     = 2048
0.00.041.056 I print_info: f_norm_eps       = 1.0e-05
0.00.041.056 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.057 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.057 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.057 I print_info: f_logit_scale    = 0.0e+00
0.00.041.057 I print_info: n_ff             = 8192
0.00.041.058 I print_info: n_expert         = 0
0.00.041.058 I print_info: n_expert_used    = 0
0.00.041.062 I print_info: causal attn      = 1
0.00.041.062 I print_info: pooling type     = 0
0.00.041.062 I print_info: rope type        = 2
0.00.041.062 I print_info: rope scaling     = linear
0.00.041.062 I print_info: freq_base_train  = 10000.0
0.00.041.063 I print_info: freq_scale_train = 1
0.00.041.063 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.063 I print_info: rope_finetuned   = unknown
0.00.041.063 I print_info: ssm_d_conv       = 0
0.00.041.063 I print_info: ssm_d_inner      = 0
0.00.041.063 I print_info: ssm_d_state      = 0
0.00.041.064 I print_info: ssm_dt_rank      = 0
0.00.041.064 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.064 I print_info: model type       = 1.4B
0.00.041.064 I print_info: model params     = 1.41 B
0.00.041.064 I print_info: general.name     = 1.4B
0.00.041.065 I print_info: vocab type       = BPE
0.00.041.065 I print_info: n_vocab          = 50304
0.00.041.065 I print_info: n_merges         = 50009
0.00.041.065 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.066 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.068 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.068 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.068 I print_info: LF token         = 187 'Ċ'
0.00.041.069 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.069 I print_info: max token length = 1024
0.00.041.069 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.766.691 I load_tensors: offloading 24 repeating layers to GPU
0.00.766.696 I load_tensors: offloading output layer to GPU
0.00.766.697 I load_tensors: offloaded 25/25 layers to GPU
0.00.766.719 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.766.720 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.767.743 I llama_context: n_seq_max     = 1
0.00.767.745 I llama_context: n_ctx         = 128
0.00.767.745 I llama_context: n_ctx_per_seq = 128
0.00.767.746 I llama_context: n_batch       = 128
0.00.767.746 I llama_context: n_ubatch      = 128
0.00.767.746 I llama_context: flash_attn    = 0
0.00.767.747 I llama_context: freq_base     = 10000.0
0.00.767.748 I llama_context: freq_scale    = 1
0.00.767.748 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.767.750 I ggml_metal_init: allocating
0.00.767.784 I ggml_metal_init: found device: Apple M4
0.00.767.791 I ggml_metal_init: picking default device: Apple M4
0.00.769.153 I ggml_metal_init: using embedded metal library
0.00.775.160 I ggml_metal_init: GPU name:   Apple M4
0.00.775.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.775.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.775.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.775.166 I ggml_metal_init: simdgroup reduction   = true
0.00.775.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.775.167 I ggml_metal_init: has residency sets    = true
0.00.775.167 I ggml_metal_init: has bfloat            = true
0.00.775.167 I ggml_metal_init: use bfloat            = true
0.00.775.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.775.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.791.716 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.791.721 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.795.039 I init:      Metal KV buffer size =    24.00 MiB
0.00.795.043 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.797.892 I init:      Metal compute buffer size =    25.56 MiB
0.00.797.893 I init:        CPU compute buffer size =     1.06 MiB
0.00.797.894 I init: graph nodes  = 967
0.00.797.894 I init: graph splits = 2
0.00.797.898 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.797.898 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.410 I 
0.00.828.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.504 I perplexity: tokenizing the input ..
0.00.835.845 I perplexity: tokenization took 7.338 ms
0.00.835.867 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.984.485 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.985.740 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.985.751 I llama_perf_context_print:        load time =     817.80 ms
0.00.985.752 I llama_perf_context_print: prompt eval time =     147.72 ms /   128 tokens (    1.15 ms per token,   866.49 tokens per second)
0.00.985.753 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.985.754 I llama_perf_context_print:       total time =     157.35 ms /   129 tokens
0.00.986.393 I ggml_metal_free: deallocating

real	0m1.004s
user	0m0.079s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.777 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.040 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.046 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.047 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.056 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.057 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.914 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.792 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.792 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.793 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.793 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.793 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.794 I llama_model_loader: - type  f32:  194 tensors
0.00.025.794 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.795 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.795 I print_info: file format = GGUF V3 (latest)
0.00.025.796 I print_info: file type   = Q5_1
0.00.025.797 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.076 I load: special tokens cache size = 25
0.00.040.148 I load: token to piece cache size = 0.2984 MB
0.00.040.163 I print_info: arch             = gptneox
0.00.040.165 I print_info: vocab_only       = 0
0.00.040.165 I print_info: n_ctx_train      = 2048
0.00.040.165 I print_info: n_embd           = 2048
0.00.040.166 I print_info: n_layer          = 24
0.00.040.169 I print_info: n_head           = 16
0.00.040.170 I print_info: n_head_kv        = 16
0.00.040.170 I print_info: n_rot            = 32
0.00.040.171 I print_info: n_swa            = 0
0.00.040.171 I print_info: n_embd_head_k    = 128
0.00.040.171 I print_info: n_embd_head_v    = 128
0.00.040.174 I print_info: n_gqa            = 1
0.00.040.174 I print_info: n_embd_k_gqa     = 2048
0.00.040.175 I print_info: n_embd_v_gqa     = 2048
0.00.040.175 I print_info: f_norm_eps       = 1.0e-05
0.00.040.176 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.176 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.176 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.176 I print_info: f_logit_scale    = 0.0e+00
0.00.040.177 I print_info: n_ff             = 8192
0.00.040.177 I print_info: n_expert         = 0
0.00.040.177 I print_info: n_expert_used    = 0
0.00.040.178 I print_info: causal attn      = 1
0.00.040.178 I print_info: pooling type     = 0
0.00.040.178 I print_info: rope type        = 2
0.00.040.178 I print_info: rope scaling     = linear
0.00.040.179 I print_info: freq_base_train  = 10000.0
0.00.040.179 I print_info: freq_scale_train = 1
0.00.040.179 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.179 I print_info: rope_finetuned   = unknown
0.00.040.179 I print_info: ssm_d_conv       = 0
0.00.040.180 I print_info: ssm_d_inner      = 0
0.00.040.180 I print_info: ssm_d_state      = 0
0.00.040.180 I print_info: ssm_dt_rank      = 0
0.00.040.180 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.180 I print_info: model type       = 1.4B
0.00.040.181 I print_info: model params     = 1.41 B
0.00.040.181 I print_info: general.name     = 1.4B
0.00.040.181 I print_info: vocab type       = BPE
0.00.040.182 I print_info: n_vocab          = 50304
0.00.040.182 I print_info: n_merges         = 50009
0.00.040.182 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.182 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.182 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.182 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.183 I print_info: LF token         = 187 'Ċ'
0.00.040.183 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.183 I print_info: max token length = 1024
0.00.040.184 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.746.746 I load_tensors: offloading 24 repeating layers to GPU
0.00.746.751 I load_tensors: offloading output layer to GPU
0.00.746.753 I load_tensors: offloaded 25/25 layers to GPU
0.00.746.778 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.746.780 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.747.915 I llama_context: n_seq_max     = 1
0.00.747.919 I llama_context: n_ctx         = 128
0.00.747.920 I llama_context: n_ctx_per_seq = 128
0.00.747.920 I llama_context: n_batch       = 128
0.00.747.921 I llama_context: n_ubatch      = 128
0.00.747.921 I llama_context: flash_attn    = 0
0.00.747.922 I llama_context: freq_base     = 10000.0
0.00.747.923 I llama_context: freq_scale    = 1
0.00.747.923 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.747.924 I ggml_metal_init: allocating
0.00.747.941 I ggml_metal_init: found device: Apple M4
0.00.747.949 I ggml_metal_init: picking default device: Apple M4
0.00.749.226 I ggml_metal_init: using embedded metal library
0.00.755.138 I ggml_metal_init: GPU name:   Apple M4
0.00.755.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.755.142 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.755.143 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.755.144 I ggml_metal_init: simdgroup reduction   = true
0.00.755.144 I ggml_metal_init: simdgroup matrix mul. = true
0.00.755.144 I ggml_metal_init: has residency sets    = true
0.00.755.144 I ggml_metal_init: has bfloat            = true
0.00.755.145 I ggml_metal_init: use bfloat            = true
0.00.755.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.755.150 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.771.744 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.771.747 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.775.076 I init:      Metal KV buffer size =    24.00 MiB
0.00.775.079 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.778.052 I init:      Metal compute buffer size =    25.56 MiB
0.00.778.054 I init:        CPU compute buffer size =     1.06 MiB
0.00.778.054 I init: graph nodes  = 967
0.00.778.055 I init: graph splits = 2
0.00.778.058 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.778.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.211 I 
0.00.809.286 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.306 I perplexity: tokenizing the input ..
0.00.816.240 I perplexity: tokenization took 6.93 ms
0.00.816.259 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.965.510 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.966.785 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.966.803 I llama_perf_context_print:        load time =     799.42 ms
0.00.966.804 I llama_perf_context_print: prompt eval time =     148.40 ms /   128 tokens (    1.16 ms per token,   862.56 tokens per second)
0.00.966.805 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.966.806 I llama_perf_context_print:       total time =     157.60 ms /   129 tokens
0.00.967.435 I ggml_metal_free: deallocating

real	0m0.981s
user	0m0.079s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.608 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.525 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.532 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.534 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.538 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.540 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.541 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.361 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.445 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.226 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.231 I llama_model_loader: - type  f32:  194 tensors
0.00.026.232 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.232 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.232 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.233 I print_info: file format = GGUF V3 (latest)
0.00.026.233 I print_info: file type   = Q2_K - Medium
0.00.026.235 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.379 I load: special tokens cache size = 25
0.00.040.213 I load: token to piece cache size = 0.2984 MB
0.00.040.230 I print_info: arch             = gptneox
0.00.040.231 I print_info: vocab_only       = 0
0.00.040.231 I print_info: n_ctx_train      = 2048
0.00.040.231 I print_info: n_embd           = 2048
0.00.040.232 I print_info: n_layer          = 24
0.00.040.235 I print_info: n_head           = 16
0.00.040.235 I print_info: n_head_kv        = 16
0.00.040.236 I print_info: n_rot            = 32
0.00.040.236 I print_info: n_swa            = 0
0.00.040.238 I print_info: n_embd_head_k    = 128
0.00.040.238 I print_info: n_embd_head_v    = 128
0.00.040.239 I print_info: n_gqa            = 1
0.00.040.240 I print_info: n_embd_k_gqa     = 2048
0.00.040.240 I print_info: n_embd_v_gqa     = 2048
0.00.040.241 I print_info: f_norm_eps       = 1.0e-05
0.00.040.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.242 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.242 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.242 I print_info: f_logit_scale    = 0.0e+00
0.00.040.243 I print_info: n_ff             = 8192
0.00.040.244 I print_info: n_expert         = 0
0.00.040.244 I print_info: n_expert_used    = 0
0.00.040.244 I print_info: causal attn      = 1
0.00.040.244 I print_info: pooling type     = 0
0.00.040.244 I print_info: rope type        = 2
0.00.040.246 I print_info: rope scaling     = linear
0.00.040.247 I print_info: freq_base_train  = 10000.0
0.00.040.247 I print_info: freq_scale_train = 1
0.00.040.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.247 I print_info: rope_finetuned   = unknown
0.00.040.247 I print_info: ssm_d_conv       = 0
0.00.040.247 I print_info: ssm_d_inner      = 0
0.00.040.248 I print_info: ssm_d_state      = 0
0.00.040.248 I print_info: ssm_dt_rank      = 0
0.00.040.248 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.248 I print_info: model type       = 1.4B
0.00.040.249 I print_info: model params     = 1.41 B
0.00.040.249 I print_info: general.name     = 1.4B
0.00.040.249 I print_info: vocab type       = BPE
0.00.040.250 I print_info: n_vocab          = 50304
0.00.040.250 I print_info: n_merges         = 50009
0.00.040.250 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.251 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.255 I print_info: LF token         = 187 'Ċ'
0.00.040.256 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.256 I print_info: max token length = 1024
0.00.040.256 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.393.177 I load_tensors: offloading 24 repeating layers to GPU
0.00.393.189 I load_tensors: offloading output layer to GPU
0.00.393.190 I load_tensors: offloaded 25/25 layers to GPU
0.00.393.220 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.393.222 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.394.900 I llama_context: n_seq_max     = 1
0.00.394.903 I llama_context: n_ctx         = 128
0.00.394.904 I llama_context: n_ctx_per_seq = 128
0.00.394.905 I llama_context: n_batch       = 128
0.00.394.906 I llama_context: n_ubatch      = 128
0.00.394.906 I llama_context: flash_attn    = 0
0.00.394.908 I llama_context: freq_base     = 10000.0
0.00.394.909 I llama_context: freq_scale    = 1
0.00.394.909 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.394.911 I ggml_metal_init: allocating
0.00.394.993 I ggml_metal_init: found device: Apple M4
0.00.395.010 I ggml_metal_init: picking default device: Apple M4
0.00.397.084 I ggml_metal_init: using embedded metal library
0.00.403.294 I ggml_metal_init: GPU name:   Apple M4
0.00.403.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.403.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.403.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.403.303 I ggml_metal_init: simdgroup reduction   = true
0.00.403.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.403.303 I ggml_metal_init: has residency sets    = true
0.00.403.304 I ggml_metal_init: has bfloat            = true
0.00.403.304 I ggml_metal_init: use bfloat            = true
0.00.403.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.403.309 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.424.271 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.424.277 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.427.964 I init:      Metal KV buffer size =    24.00 MiB
0.00.427.970 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.431.204 I init:      Metal compute buffer size =    25.56 MiB
0.00.431.206 I init:        CPU compute buffer size =     1.06 MiB
0.00.431.206 I init: graph nodes  = 967
0.00.431.207 I init: graph splits = 2
0.00.431.210 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.431.210 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.462.283 I 
0.00.462.358 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.462.380 I perplexity: tokenizing the input ..
0.00.469.098 I perplexity: tokenization took 6.715 ms
0.00.469.118 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.614.740 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.616.026 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.616.037 I llama_perf_context_print:        load time =     451.67 ms
0.00.616.038 I llama_perf_context_print: prompt eval time =     144.75 ms /   128 tokens (    1.13 ms per token,   884.31 tokens per second)
0.00.616.038 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.039 I llama_perf_context_print:       total time =     153.76 ms /   129 tokens
0.00.616.647 I ggml_metal_free: deallocating

real	0m0.632s
user	0m0.082s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.038 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.172 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.178 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.181 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.181 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.182 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.182 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.184 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.185 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.185 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.187 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.836 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.838 I llama_model_loader: - type  f32:  194 tensors
0.00.025.839 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.839 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.839 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.839 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.845 I print_info: file format = GGUF V3 (latest)
0.00.025.846 I print_info: file type   = Q3_K - Medium
0.00.025.847 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.643 I load: special tokens cache size = 25
0.00.039.355 I load: token to piece cache size = 0.2984 MB
0.00.039.370 I print_info: arch             = gptneox
0.00.039.371 I print_info: vocab_only       = 0
0.00.039.371 I print_info: n_ctx_train      = 2048
0.00.039.372 I print_info: n_embd           = 2048
0.00.039.372 I print_info: n_layer          = 24
0.00.039.376 I print_info: n_head           = 16
0.00.039.377 I print_info: n_head_kv        = 16
0.00.039.377 I print_info: n_rot            = 32
0.00.039.377 I print_info: n_swa            = 0
0.00.039.377 I print_info: n_embd_head_k    = 128
0.00.039.377 I print_info: n_embd_head_v    = 128
0.00.039.378 I print_info: n_gqa            = 1
0.00.039.379 I print_info: n_embd_k_gqa     = 2048
0.00.039.380 I print_info: n_embd_v_gqa     = 2048
0.00.039.380 I print_info: f_norm_eps       = 1.0e-05
0.00.039.381 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.381 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.381 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.381 I print_info: f_logit_scale    = 0.0e+00
0.00.039.382 I print_info: n_ff             = 8192
0.00.039.382 I print_info: n_expert         = 0
0.00.039.382 I print_info: n_expert_used    = 0
0.00.039.382 I print_info: causal attn      = 1
0.00.039.382 I print_info: pooling type     = 0
0.00.039.382 I print_info: rope type        = 2
0.00.039.383 I print_info: rope scaling     = linear
0.00.039.383 I print_info: freq_base_train  = 10000.0
0.00.039.383 I print_info: freq_scale_train = 1
0.00.039.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.384 I print_info: rope_finetuned   = unknown
0.00.039.384 I print_info: ssm_d_conv       = 0
0.00.039.384 I print_info: ssm_d_inner      = 0
0.00.039.384 I print_info: ssm_d_state      = 0
0.00.039.384 I print_info: ssm_dt_rank      = 0
0.00.039.384 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.385 I print_info: model type       = 1.4B
0.00.039.385 I print_info: model params     = 1.41 B
0.00.039.385 I print_info: general.name     = 1.4B
0.00.039.386 I print_info: vocab type       = BPE
0.00.039.386 I print_info: n_vocab          = 50304
0.00.039.386 I print_info: n_merges         = 50009
0.00.039.386 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.386 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.386 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.387 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.387 I print_info: LF token         = 187 'Ċ'
0.00.039.387 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.388 I print_info: max token length = 1024
0.00.039.389 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.499.700 I load_tensors: offloading 24 repeating layers to GPU
0.00.499.713 I load_tensors: offloading output layer to GPU
0.00.499.714 I load_tensors: offloaded 25/25 layers to GPU
0.00.499.748 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.499.749 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.501.076 I llama_context: n_seq_max     = 1
0.00.501.078 I llama_context: n_ctx         = 128
0.00.501.078 I llama_context: n_ctx_per_seq = 128
0.00.501.078 I llama_context: n_batch       = 128
0.00.501.079 I llama_context: n_ubatch      = 128
0.00.501.080 I llama_context: flash_attn    = 0
0.00.501.081 I llama_context: freq_base     = 10000.0
0.00.501.081 I llama_context: freq_scale    = 1
0.00.501.082 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.501.085 I ggml_metal_init: allocating
0.00.501.120 I ggml_metal_init: found device: Apple M4
0.00.501.130 I ggml_metal_init: picking default device: Apple M4
0.00.502.691 I ggml_metal_init: using embedded metal library
0.00.508.957 I ggml_metal_init: GPU name:   Apple M4
0.00.508.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.508.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.508.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.508.963 I ggml_metal_init: simdgroup reduction   = true
0.00.508.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.508.964 I ggml_metal_init: has residency sets    = true
0.00.508.964 I ggml_metal_init: has bfloat            = true
0.00.508.964 I ggml_metal_init: use bfloat            = true
0.00.508.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.508.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.526.371 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.526.375 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.529.722 I init:      Metal KV buffer size =    24.00 MiB
0.00.529.725 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.532.752 I init:      Metal compute buffer size =    25.56 MiB
0.00.532.754 I init:        CPU compute buffer size =     1.06 MiB
0.00.532.755 I init: graph nodes  = 967
0.00.532.755 I init: graph splits = 2
0.00.532.758 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.532.759 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.560.922 I 
0.00.560.952 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.560.961 I perplexity: tokenizing the input ..
0.00.567.212 I perplexity: tokenization took 6.249 ms
0.00.567.226 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.712.327 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.713.576 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.713.590 I llama_perf_context_print:        load time =     550.88 ms
0.00.713.591 I llama_perf_context_print: prompt eval time =     144.72 ms /   128 tokens (    1.13 ms per token,   884.47 tokens per second)
0.00.713.592 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.713.592 I llama_perf_context_print:       total time =     152.67 ms /   129 tokens
0.00.714.215 I ggml_metal_free: deallocating

real	0m0.728s
user	0m0.078s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.843 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.999 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.009 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.011 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.012 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.014 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.934 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.991 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.898 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.902 I llama_model_loader: - type  f32:  194 tensors
0.00.026.902 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.903 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.903 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.903 I print_info: file format = GGUF V3 (latest)
0.00.026.904 I print_info: file type   = Q4_K - Medium
0.00.026.905 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.305 I load: special tokens cache size = 25
0.00.041.534 I load: token to piece cache size = 0.2984 MB
0.00.041.552 I print_info: arch             = gptneox
0.00.041.553 I print_info: vocab_only       = 0
0.00.041.554 I print_info: n_ctx_train      = 2048
0.00.041.554 I print_info: n_embd           = 2048
0.00.041.554 I print_info: n_layer          = 24
0.00.041.558 I print_info: n_head           = 16
0.00.041.559 I print_info: n_head_kv        = 16
0.00.041.559 I print_info: n_rot            = 32
0.00.041.559 I print_info: n_swa            = 0
0.00.041.559 I print_info: n_embd_head_k    = 128
0.00.041.559 I print_info: n_embd_head_v    = 128
0.00.041.560 I print_info: n_gqa            = 1
0.00.041.563 I print_info: n_embd_k_gqa     = 2048
0.00.041.563 I print_info: n_embd_v_gqa     = 2048
0.00.041.565 I print_info: f_norm_eps       = 1.0e-05
0.00.041.565 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.566 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.567 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.567 I print_info: f_logit_scale    = 0.0e+00
0.00.041.567 I print_info: n_ff             = 8192
0.00.041.568 I print_info: n_expert         = 0
0.00.041.568 I print_info: n_expert_used    = 0
0.00.041.568 I print_info: causal attn      = 1
0.00.041.568 I print_info: pooling type     = 0
0.00.041.568 I print_info: rope type        = 2
0.00.041.568 I print_info: rope scaling     = linear
0.00.041.569 I print_info: freq_base_train  = 10000.0
0.00.041.569 I print_info: freq_scale_train = 1
0.00.041.569 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.569 I print_info: rope_finetuned   = unknown
0.00.041.569 I print_info: ssm_d_conv       = 0
0.00.041.570 I print_info: ssm_d_inner      = 0
0.00.041.570 I print_info: ssm_d_state      = 0
0.00.041.570 I print_info: ssm_dt_rank      = 0
0.00.041.570 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.570 I print_info: model type       = 1.4B
0.00.041.571 I print_info: model params     = 1.41 B
0.00.041.571 I print_info: general.name     = 1.4B
0.00.041.571 I print_info: vocab type       = BPE
0.00.041.572 I print_info: n_vocab          = 50304
0.00.041.572 I print_info: n_merges         = 50009
0.00.041.572 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.573 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.573 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.576 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.576 I print_info: LF token         = 187 'Ċ'
0.00.041.577 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.577 I print_info: max token length = 1024
0.00.041.578 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.889 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.895 I load_tensors: offloading output layer to GPU
0.00.586.896 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.919 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.586.922 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.588.012 I llama_context: n_seq_max     = 1
0.00.588.014 I llama_context: n_ctx         = 128
0.00.588.014 I llama_context: n_ctx_per_seq = 128
0.00.588.015 I llama_context: n_batch       = 128
0.00.588.015 I llama_context: n_ubatch      = 128
0.00.588.015 I llama_context: flash_attn    = 0
0.00.588.016 I llama_context: freq_base     = 10000.0
0.00.588.017 I llama_context: freq_scale    = 1
0.00.588.018 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.019 I ggml_metal_init: allocating
0.00.588.036 I ggml_metal_init: found device: Apple M4
0.00.588.044 I ggml_metal_init: picking default device: Apple M4
0.00.589.296 I ggml_metal_init: using embedded metal library
0.00.595.285 I ggml_metal_init: GPU name:   Apple M4
0.00.595.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.291 I ggml_metal_init: simdgroup reduction   = true
0.00.595.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.292 I ggml_metal_init: has residency sets    = true
0.00.595.292 I ggml_metal_init: has bfloat            = true
0.00.595.292 I ggml_metal_init: use bfloat            = true
0.00.595.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.958 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.611.963 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.333 I init:      Metal KV buffer size =    24.00 MiB
0.00.615.337 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.199 I init:      Metal compute buffer size =    25.56 MiB
0.00.618.201 I init:        CPU compute buffer size =     1.06 MiB
0.00.618.202 I init: graph nodes  = 967
0.00.618.202 I init: graph splits = 2
0.00.618.205 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.618.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.265 I 
0.00.647.312 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.326 I perplexity: tokenizing the input ..
0.00.654.165 I perplexity: tokenization took 6.836 ms
0.00.654.182 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.394 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.798.688 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.798.706 I llama_perf_context_print:        load time =     636.42 ms
0.00.798.707 I llama_perf_context_print: prompt eval time =     142.35 ms /   128 tokens (    1.11 ms per token,   899.19 tokens per second)
0.00.798.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.708 I llama_perf_context_print:       total time =     151.44 ms /   129 tokens
0.00.799.353 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.079s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.878 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.042 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.048 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.055 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.056 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.057 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.059 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.874 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.875 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.876 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.876 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.877 I llama_model_loader: - type  f32:  194 tensors
0.00.025.877 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.878 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.878 I print_info: file format = GGUF V3 (latest)
0.00.025.879 I print_info: file type   = Q5_K - Medium
0.00.025.880 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.219 I load: special tokens cache size = 25
0.00.040.505 I load: token to piece cache size = 0.2984 MB
0.00.040.522 I print_info: arch             = gptneox
0.00.040.523 I print_info: vocab_only       = 0
0.00.040.523 I print_info: n_ctx_train      = 2048
0.00.040.524 I print_info: n_embd           = 2048
0.00.040.524 I print_info: n_layer          = 24
0.00.040.527 I print_info: n_head           = 16
0.00.040.528 I print_info: n_head_kv        = 16
0.00.040.528 I print_info: n_rot            = 32
0.00.040.528 I print_info: n_swa            = 0
0.00.040.532 I print_info: n_embd_head_k    = 128
0.00.040.532 I print_info: n_embd_head_v    = 128
0.00.040.533 I print_info: n_gqa            = 1
0.00.040.534 I print_info: n_embd_k_gqa     = 2048
0.00.040.534 I print_info: n_embd_v_gqa     = 2048
0.00.040.535 I print_info: f_norm_eps       = 1.0e-05
0.00.040.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.535 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.536 I print_info: f_logit_scale    = 0.0e+00
0.00.040.536 I print_info: n_ff             = 8192
0.00.040.536 I print_info: n_expert         = 0
0.00.040.537 I print_info: n_expert_used    = 0
0.00.040.537 I print_info: causal attn      = 1
0.00.040.537 I print_info: pooling type     = 0
0.00.040.537 I print_info: rope type        = 2
0.00.040.537 I print_info: rope scaling     = linear
0.00.040.538 I print_info: freq_base_train  = 10000.0
0.00.040.538 I print_info: freq_scale_train = 1
0.00.040.538 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.538 I print_info: rope_finetuned   = unknown
0.00.040.538 I print_info: ssm_d_conv       = 0
0.00.040.539 I print_info: ssm_d_inner      = 0
0.00.040.539 I print_info: ssm_d_state      = 0
0.00.040.539 I print_info: ssm_dt_rank      = 0
0.00.040.539 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.539 I print_info: model type       = 1.4B
0.00.040.539 I print_info: model params     = 1.41 B
0.00.040.540 I print_info: general.name     = 1.4B
0.00.040.540 I print_info: vocab type       = BPE
0.00.040.540 I print_info: n_vocab          = 50304
0.00.040.540 I print_info: n_merges         = 50009
0.00.040.541 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.541 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.541 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.541 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.541 I print_info: LF token         = 187 'Ċ'
0.00.040.542 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.542 I print_info: max token length = 1024
0.00.040.542 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.359 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.372 I load_tensors: offloading output layer to GPU
0.00.653.373 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.403 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.653.405 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.654.658 I llama_context: n_seq_max     = 1
0.00.654.659 I llama_context: n_ctx         = 128
0.00.654.660 I llama_context: n_ctx_per_seq = 128
0.00.654.660 I llama_context: n_batch       = 128
0.00.654.660 I llama_context: n_ubatch      = 128
0.00.654.660 I llama_context: flash_attn    = 0
0.00.654.662 I llama_context: freq_base     = 10000.0
0.00.654.663 I llama_context: freq_scale    = 1
0.00.654.663 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.654.665 I ggml_metal_init: allocating
0.00.654.722 I ggml_metal_init: found device: Apple M4
0.00.654.730 I ggml_metal_init: picking default device: Apple M4
0.00.655.812 I ggml_metal_init: using embedded metal library
0.00.659.642 I ggml_metal_init: GPU name:   Apple M4
0.00.659.644 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.645 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.645 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.645 I ggml_metal_init: simdgroup reduction   = true
0.00.659.646 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.646 I ggml_metal_init: has residency sets    = true
0.00.659.646 I ggml_metal_init: has bfloat            = true
0.00.659.646 I ggml_metal_init: use bfloat            = true
0.00.659.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.146 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.670.148 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.003 I init:      Metal KV buffer size =    24.00 MiB
0.00.672.005 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.625 I init:      Metal compute buffer size =    25.56 MiB
0.00.673.626 I init:        CPU compute buffer size =     1.06 MiB
0.00.673.626 I init: graph nodes  = 967
0.00.673.627 I init: graph splits = 2
0.00.673.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.673.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.901 I 
0.00.704.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.950 I perplexity: tokenizing the input ..
0.00.708.808 I perplexity: tokenization took 3.856 ms
0.00.708.821 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.172 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.855.271 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.855.290 I llama_perf_context_print:        load time =     695.02 ms
0.00.855.291 I llama_perf_context_print: prompt eval time =     145.12 ms /   128 tokens (    1.13 ms per token,   882.02 tokens per second)
0.00.855.293 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.294 I llama_perf_context_print:       total time =     150.39 ms /   129 tokens
0.00.855.909 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.066s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.152 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.047 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.058 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.909 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.958 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.790 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.792 I llama_model_loader: - type  f32:  194 tensors
0.00.026.792 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.792 I print_info: file format = GGUF V3 (latest)
0.00.026.793 I print_info: file type   = Q6_K
0.00.026.794 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.862 I load: special tokens cache size = 25
0.00.040.559 I load: token to piece cache size = 0.2984 MB
0.00.040.574 I print_info: arch             = gptneox
0.00.040.575 I print_info: vocab_only       = 0
0.00.040.575 I print_info: n_ctx_train      = 2048
0.00.040.575 I print_info: n_embd           = 2048
0.00.040.575 I print_info: n_layer          = 24
0.00.040.578 I print_info: n_head           = 16
0.00.040.579 I print_info: n_head_kv        = 16
0.00.040.579 I print_info: n_rot            = 32
0.00.040.579 I print_info: n_swa            = 0
0.00.040.579 I print_info: n_embd_head_k    = 128
0.00.040.579 I print_info: n_embd_head_v    = 128
0.00.040.580 I print_info: n_gqa            = 1
0.00.040.581 I print_info: n_embd_k_gqa     = 2048
0.00.040.582 I print_info: n_embd_v_gqa     = 2048
0.00.040.582 I print_info: f_norm_eps       = 1.0e-05
0.00.040.583 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.583 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.583 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.584 I print_info: f_logit_scale    = 0.0e+00
0.00.040.585 I print_info: n_ff             = 8192
0.00.040.585 I print_info: n_expert         = 0
0.00.040.585 I print_info: n_expert_used    = 0
0.00.040.585 I print_info: causal attn      = 1
0.00.040.585 I print_info: pooling type     = 0
0.00.040.586 I print_info: rope type        = 2
0.00.040.590 I print_info: rope scaling     = linear
0.00.040.590 I print_info: freq_base_train  = 10000.0
0.00.040.590 I print_info: freq_scale_train = 1
0.00.040.591 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.591 I print_info: rope_finetuned   = unknown
0.00.040.591 I print_info: ssm_d_conv       = 0
0.00.040.591 I print_info: ssm_d_inner      = 0
0.00.040.594 I print_info: ssm_d_state      = 0
0.00.040.594 I print_info: ssm_dt_rank      = 0
0.00.040.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.594 I print_info: model type       = 1.4B
0.00.040.594 I print_info: model params     = 1.41 B
0.00.040.595 I print_info: general.name     = 1.4B
0.00.040.595 I print_info: vocab type       = BPE
0.00.040.595 I print_info: n_vocab          = 50304
0.00.040.595 I print_info: n_merges         = 50009
0.00.040.596 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.597 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.597 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.597 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.597 I print_info: LF token         = 187 'Ċ'
0.00.040.598 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.599 I print_info: max token length = 1024
0.00.040.599 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.072.038 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.040 I load_tensors: offloading output layer to GPU
0.00.072.040 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.053 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.072.054 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.072.416 I llama_context: n_seq_max     = 1
0.00.072.417 I llama_context: n_ctx         = 128
0.00.072.418 I llama_context: n_ctx_per_seq = 128
0.00.072.418 I llama_context: n_batch       = 128
0.00.072.418 I llama_context: n_ubatch      = 128
0.00.072.418 I llama_context: flash_attn    = 0
0.00.072.419 I llama_context: freq_base     = 10000.0
0.00.072.419 I llama_context: freq_scale    = 1
0.00.072.419 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.420 I ggml_metal_init: allocating
0.00.072.433 I ggml_metal_init: found device: Apple M4
0.00.072.437 I ggml_metal_init: picking default device: Apple M4
0.00.072.990 I ggml_metal_init: using embedded metal library
0.00.075.640 I ggml_metal_init: GPU name:   Apple M4
0.00.075.642 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.643 I ggml_metal_init: simdgroup reduction   = true
0.00.075.644 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.644 I ggml_metal_init: has residency sets    = true
0.00.075.644 I ggml_metal_init: has bfloat            = true
0.00.075.644 I ggml_metal_init: use bfloat            = true
0.00.075.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.402 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.086.405 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.315 I init:      Metal KV buffer size =    24.00 MiB
0.00.088.318 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.089.944 I init:      Metal compute buffer size =    25.56 MiB
0.00.089.946 I init:        CPU compute buffer size =     1.06 MiB
0.00.089.946 I init: graph nodes  = 967
0.00.089.947 I init: graph splits = 2
0.00.089.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.089.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.120.741 I 
0.00.120.765 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.120.772 I perplexity: tokenizing the input ..
0.00.124.490 I perplexity: tokenization took 3.717 ms
0.00.124.500 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.262.662 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.263.910 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.263.919 I llama_perf_context_print:        load time =     109.59 ms
0.00.263.921 I llama_perf_context_print: prompt eval time =     137.93 ms /   128 tokens (    1.08 ms per token,   928.01 tokens per second)
0.00.263.922 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.263.923 I llama_perf_context_print:       total time =     143.18 ms /   129 tokens
0.00.264.560 I ggml_metal_free: deallocating

real	0m0.279s
user	0m0.065s
sys	0m0.036s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4734 (82806456) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.783 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.796 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.803 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.228 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.638 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.640 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.641 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.642 I llama_model_loader: - type  f32:  194 tensors
0.00.053.643 I llama_model_loader: - type  f16:   98 tensors
0.00.053.644 I print_info: file format = GGUF V3 (latest)
0.00.053.645 I print_info: file type   = all F32 (guessed)
0.00.053.646 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.252 I load: special tokens cache size = 25
0.00.073.762 I load: token to piece cache size = 0.2984 MB
0.00.073.776 I print_info: arch             = gptneox
0.00.073.778 I print_info: vocab_only       = 0
0.00.073.778 I print_info: n_ctx_train      = 2048
0.00.073.778 I print_info: n_embd           = 2048
0.00.073.778 I print_info: n_layer          = 24
0.00.073.781 I print_info: n_head           = 16
0.00.073.782 I print_info: n_head_kv        = 16
0.00.073.783 I print_info: n_rot            = 32
0.00.073.783 I print_info: n_swa            = 0
0.00.073.784 I print_info: n_embd_head_k    = 128
0.00.073.784 I print_info: n_embd_head_v    = 128
0.00.073.784 I print_info: n_gqa            = 1
0.00.073.785 I print_info: n_embd_k_gqa     = 2048
0.00.073.786 I print_info: n_embd_v_gqa     = 2048
0.00.073.787 I print_info: f_norm_eps       = 1.0e-05
0.00.073.787 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.787 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.789 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.789 I print_info: f_logit_scale    = 0.0e+00
0.00.073.789 I print_info: n_ff             = 8192
0.00.073.789 I print_info: n_expert         = 0
0.00.073.790 I print_info: n_expert_used    = 0
0.00.073.790 I print_info: causal attn      = 1
0.00.073.790 I print_info: pooling type     = 0
0.00.073.790 I print_info: rope type        = 2
0.00.073.790 I print_info: rope scaling     = linear
0.00.073.791 I print_info: freq_base_train  = 10000.0
0.00.073.791 I print_info: freq_scale_train = 1
0.00.073.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.796 I print_info: rope_finetuned   = unknown
0.00.073.796 I print_info: ssm_d_conv       = 0
0.00.073.796 I print_info: ssm_d_inner      = 0
0.00.073.796 I print_info: ssm_d_state      = 0
0.00.073.796 I print_info: ssm_dt_rank      = 0
0.00.073.799 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.800 I print_info: model type       = 1.4B
0.00.073.800 I print_info: model params     = 1.41 B
0.00.073.800 I print_info: general.name     = 1.4B
0.00.073.801 I print_info: vocab type       = BPE
0.00.073.801 I print_info: n_vocab          = 50304
0.00.073.801 I print_info: n_merges         = 50009
0.00.073.801 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.802 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.804 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.804 I print_info: LF token         = 187 'Ċ'
0.00.073.804 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.804 I print_info: max token length = 1024
0.00.073.805 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.418.527 I load_tensors: offloading 24 repeating layers to GPU
0.01.418.535 I load_tensors: offloading output layer to GPU
0.01.418.537 I load_tensors: offloaded 25/25 layers to GPU
0.01.418.562 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.418.562 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.419.264 I llama_context: n_seq_max     = 1
0.01.419.265 I llama_context: n_ctx         = 128
0.01.419.265 I llama_context: n_ctx_per_seq = 128
0.01.419.265 I llama_context: n_batch       = 128
0.01.419.265 I llama_context: n_ubatch      = 128
0.01.419.266 I llama_context: flash_attn    = 0
0.01.419.266 I llama_context: freq_base     = 10000.0
0.01.419.266 I llama_context: freq_scale    = 1
0.01.419.267 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.419.267 I ggml_metal_init: allocating
0.01.419.299 I ggml_metal_init: found device: Apple M4
0.01.419.305 I ggml_metal_init: picking default device: Apple M4
0.01.420.219 I ggml_metal_init: using embedded metal library
0.01.423.640 I ggml_metal_init: GPU name:   Apple M4
0.01.423.642 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.423.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.423.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.423.643 I ggml_metal_init: simdgroup reduction   = true
0.01.423.643 I ggml_metal_init: simdgroup matrix mul. = true
0.01.423.643 I ggml_metal_init: has residency sets    = true
0.01.423.644 I ggml_metal_init: has bfloat            = true
0.01.423.644 I ggml_metal_init: use bfloat            = true
0.01.423.644 I ggml_metal_init: hasUnifiedMemory      = true
0.01.423.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.433.437 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.433.440 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.435.077 I init:      Metal KV buffer size =    24.00 MiB
0.01.435.079 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.436.539 I init:      Metal compute buffer size =    25.56 MiB
0.01.436.540 I init:        CPU compute buffer size =     1.06 MiB
0.01.436.540 I init: graph nodes  = 967
0.01.436.540 I init: graph splits = 2
0.01.436.542 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.436.542 I 
0.01.436.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.436.580 I compute_imatrix: tokenizing the input ..
0.01.440.162 I compute_imatrix: tokenization took 3.582 ms
0.01.440.164 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.700.321 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.703.057 I llama_perf_context_print:        load time =    1678.46 ms
0.01.703.058 I llama_perf_context_print: prompt eval time =     259.04 ms /   128 tokens (    2.02 ms per token,   494.14 tokens per second)
0.01.703.059 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.703.059 I llama_perf_context_print:       total time =    1681.19 ms /   129 tokens
0.01.703.818 I ggml_metal_free: deallocating

real	0m1.952s
user	0m0.127s
sys	0m0.347s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4734 (82806456)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133109f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13310a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13310ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13310b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13310b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13310bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13310c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13310c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13310ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13310d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13310d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13310dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13310e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13310f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13310f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13310ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133110650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133110d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133111490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133111c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133112380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133112aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1331131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133113a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133114180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133114440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133114a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1331156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133115c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133115ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133116360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133116620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133116eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1331173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1331176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133117b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133117ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133118490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133118930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133118dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133119270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133119710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133119bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13311a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13311a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13311a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13311af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13311b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13311be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13311c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13311ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13311d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13311d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13311dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13311e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13311e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13311ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13311f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13311f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13311fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133120160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133120600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133120aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133120f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1331213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133121880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133121d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1331221c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133122660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133122b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133122fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133123440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1331238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133123e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133124380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1331248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133124e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133125370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1331258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133125e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133126360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1331268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133126e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133127350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1331278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133127df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133128340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133128890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133128de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133129330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133129880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133129dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13312a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13312a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13312adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13312b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13312b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13311b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13312bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13312c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13312c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13312cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13312d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13312d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13312df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13312e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13312e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13312ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13312f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13312f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13312fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133130440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133130990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133130e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1331312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133131770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133131c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1331320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133132550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1331329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133132e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133133330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1331337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133133c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133134110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1331345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133134a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133134ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133135390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133135830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133135cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133136170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133136610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133136ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133136f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1331373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133137890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133137d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1331381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133138670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133138b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133138fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133139450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1331398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133139d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13313a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13313a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13313ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13313b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13313b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13313b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13313bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13313c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13313c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13313cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13313d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13313d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13313d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13313de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13313e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13313e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13313ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13313f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13313f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13313fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13313feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133140350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1331407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133140c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133141130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1331415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133141a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133141f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1331423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133142850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133142cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133143190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133143630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133143ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133143f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133144410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1331448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133144d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1331451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133145690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133145b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133145fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133146470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133146910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133146db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133147250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1331476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133147b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1331480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133148630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133148b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1331490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133149390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1331499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133149fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13314a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13314adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13314b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13314b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13314bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13314c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13314c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13314cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13314d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13314d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13314deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13314e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13314e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13314eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13314f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13314f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13314fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1331503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133150930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133150e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1331513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133151920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133151e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1331523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133152910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133152e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1331533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133153900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133153e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1331543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1331548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133154e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133155390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1331558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133155e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133156380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1331568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133156e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133157370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1331578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133157e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133158360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1331588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133158e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133159350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1331598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133159df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13315a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13315a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13315ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13315b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13315b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13315bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13315c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13315c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13315cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13315d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13315d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13315ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13315e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13315e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13315eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13315f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13315f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13315fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1331602e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133160830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133160cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133161170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133161610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133161ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133161f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1331623f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133162890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133162d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1331631d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133163670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133163b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133163fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133164450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1331648f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133164d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1331652e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133165a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133166120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133166840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133166f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133167220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133167a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133167cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1331682e0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
0.00.744.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.744.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133207190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133207600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133207a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13320a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13320a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13320a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13320b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13320b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13320bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13320bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13320c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13320c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13320d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13320d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13320e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13320e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13320ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13320f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13320fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133210730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133210e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133211570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133211c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1332123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133212ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133212d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1332133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1332139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133213fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1332147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133214c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133214f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1332157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133215ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133215fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133216440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1332168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133216d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133217220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1332176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133217b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133218000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1332184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133218940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133218c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133219210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133219820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133219e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13321a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13321aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13321b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13321b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13321bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13321c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13321ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13321cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13321d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13321d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13321dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13321e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13321e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13321edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13321f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13321f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13321fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133220040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1332204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133220980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133220e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1332212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133221760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133221c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1332220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1332225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133222b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133223090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1332235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133223b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133224080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1332245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133224b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133225070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1332255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133225b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133226060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1332265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133226b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133227050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1332275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133227af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133228040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133228590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133228ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133229030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133229580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133229ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13322a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13322a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13322aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13322b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13322b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13322bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13322c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13322c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13322caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13322cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13322d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13322da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13322dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13322e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13322ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13322efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13322f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13322f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13322fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133230300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1332307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133230c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1332310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133231580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133231a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133231ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133232360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133232800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133232ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133233140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1332335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133233a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133233f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1332343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133234860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133234d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1332351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133235640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133235ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133235f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133236420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1332368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133236d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133237200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1332376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133237b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133237fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133238480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133238920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133238dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133239260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133239700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133239ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13323a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13323a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13323a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13323ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13323b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13323b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13323bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13323c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13323c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13323c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13323ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13323d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13323d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13323dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13323e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13323e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13323ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13323eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13323f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13323f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13323fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133240160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133240600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133240aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133240f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1332413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133241880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133241d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1332421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133242660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133242b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133242fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133243440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1332438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133243d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133244220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1332446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133244b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133245000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1332454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133245940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133245de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133246280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133246720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133246c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1332471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133247710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133247c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133247f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133248530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133248b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133249150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133249940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133249de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13324a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13324a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13324acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13324b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13324b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13324bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13324c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13324ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13324cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13324d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13324da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13324df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13324e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13324ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13324ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13324f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13324fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13324ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1332504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133250a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133250f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1332514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1332519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133251f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133252490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1332529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133252f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133253480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1332539d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133253f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133254470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1332549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133254f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133255460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1332559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133255f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133256450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1332569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133256ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133257440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133257990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133257ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133258430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133258980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133258ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133259420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133259970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133259ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13325a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13325a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13325aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13325b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13325b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13325bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13325c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13325c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13325ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13325d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13325d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13325de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13325e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13325e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13325ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13325f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13325f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13325fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1332601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133260640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133260ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133260f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133261420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1332618c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133261d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133262200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1332626a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133262b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133262fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133263480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133263920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133263e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133264590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133264cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1332653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133265af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133265db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1332665a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133266860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133266e70 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133167f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133149c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133149650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13314a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13311d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13311cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13311f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13314bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133114700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13311b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13311bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13311c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13311a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13311c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133113700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13311f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13312bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1331674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1331168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133116ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13314c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13314a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133114d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133114fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133115290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133168740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133168a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133168cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133168f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133169240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133169500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1331697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133169a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133169d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13316a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13316a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13316a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13316a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13316ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13316adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13316b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13316b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13316b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13316b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13316bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13316be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13316c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13316c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13316c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13316c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13316cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13316cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13316d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13316d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13316d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13316d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13316dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13316df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13316e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13316e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13316e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13316ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13316ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13316efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13316f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13316f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13316f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13316fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13316fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133170040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133170300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1331705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133170880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133170b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133170e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1331710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133171380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133171640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133171900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133171bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133171e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133172140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133172400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1331726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133172980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133172c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133172f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1331731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133173480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133173740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133173a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133173cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133173f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133174240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133174500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1331747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133174a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133174d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133175000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1331752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133175580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133175840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133175b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133175dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133176080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133176340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133176600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1331768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133176b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133176e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133177100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1331773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133177680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133177940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133177c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133177ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133178180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133178440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133178700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1331789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133178c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133178f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133179200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1331794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133179780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133179a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133179d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133179fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13317a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13317a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13317a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13317aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13317ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13317b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13317b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13317b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13317b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13317bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13317be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13317c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13317c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13317c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13317c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13317cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13317ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13317d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13317d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13317d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13317d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13317dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13317df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13317e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13317e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13317e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13317ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13317ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13317ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13317f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13317f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13317f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13317fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13317fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133180000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1331802c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133180580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133180840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133180b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133180dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133181080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133181340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133181600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1331818c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133181b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133181e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133182100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1331823c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133182680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133182940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133182c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133182ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133183180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133183440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133183700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1331839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133183c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133183f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133184200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1331844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133184780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133184a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133184d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133184fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133185280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133185540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133185800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133185ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133185d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133186040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133186300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1331865c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133186880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133186b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133186e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1331870c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133187380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133187640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133187900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133187bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133187fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133188460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133188c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133188ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133189190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133189600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133189a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133189ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13318a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13318a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13318ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13318b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13318b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13318b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13318bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13318c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13318c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13318cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13318cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13318d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13318d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13318dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13318e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13318e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13318ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13318eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13318f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13318f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13318fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133190080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1331904f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133190960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133190dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133191240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1331916b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133191b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133191f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133192400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133192870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133192ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133193150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1331935c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133193a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133193ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133194310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133194780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133194bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133195060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1331954d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133195940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133195db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133196220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133196690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133196b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133196f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1331973e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133197850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133197cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133198130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1331985a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133198a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133198e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1331992f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133199760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133199bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13319a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13319a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13319a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13319ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13319b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13319b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13319bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13319bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13319c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13319c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13319d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13319d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13319e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13319e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13319eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13319f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13319f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13319fb80 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.773s
user	0m0.269s
sys	0m0.324s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4734 (82806456)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a60b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a60bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a60c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a60c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a60cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a60d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a60d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a60dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a60e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a60e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a60ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a60f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a60fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a610510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a610d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a611440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a611b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a612280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a6129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a613170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a613fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a6146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a614f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a615690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a615950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a615f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a616bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a617110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a6173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a617870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a617b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a6183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a618900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a618bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a619500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a6199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a619e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a61a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a61a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a61ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a61b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a61b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a61b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a61be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a61c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a61cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a61d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a61d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a61df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a61e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a61ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a61f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a61f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a61fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a6202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a6205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a620bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a6213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a621670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a621b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a622450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a6228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a622d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a623230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a6236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a624010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a6244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a624950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a624df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a625de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a626330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a626880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a626dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a627320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a627870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a627dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a628310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a628860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a628db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a629850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a629da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a62a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a62a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a62ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a62b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a62b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a62bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a62c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a62c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a62cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a61ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a62d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a62d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a62dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a62e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a62e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a62eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a62f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a62f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a62fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a630410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a630960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a630eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a631400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a631ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a632340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a6327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a632c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a633120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a6335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a633a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a633f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a6343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a634840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a634ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a635180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a635620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a635ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a635f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a636400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a6368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a636d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a6371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a637b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a637fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a638460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a638900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a638da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a639240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a6396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a639b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a63a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a63a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a63a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a63ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a63b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a63b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a63bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a63c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a63c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a63c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a63ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a63d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a63d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a63dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a63e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a63e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a63ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a63eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a63f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a63f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a63fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a640140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a6405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a640a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a6413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a641d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a6421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a642640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a642ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a642f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a643420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a6438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a643d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a644200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a6446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a644b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a644fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a645480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a645920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a645dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a646260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a646ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a647040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a6474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a647980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a647e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a6482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a648760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a648c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a6490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a6495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a649b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a64a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a64a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a64a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a64aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a64b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a64bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a64c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a64c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a64ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a64d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a64d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a64de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a64e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a64e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a64ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a64f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a64f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a64fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a6503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a650900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a650e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a6513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a6518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a651e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a6528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a652e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a653380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a6538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a653e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a6548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a654e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a655360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a6558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a655e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a656350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a6568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a656df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a657340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a657890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a657de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a658330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a658880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a658dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a659320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a659870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a659dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a65a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a65a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a65adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a65b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a65b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a65bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a65c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a65c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a65cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a65d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a65d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a65dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a65e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a65e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a65ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a65f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a65f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a65fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a6602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a660800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a660d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a6612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a6617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a661d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a6621e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a662680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a662b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a662fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a663460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a663900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a663da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a664240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a6646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a664b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a665020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a6654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a665960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a665e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a6662a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a6667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a666f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a667630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a667d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a668470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a668730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a668f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a6691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a6697f0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
0.00.098.394 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b804d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b8051f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b805660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b805ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b805f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b8063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b806820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b806c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b807100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b807570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b8079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b8080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b808bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b8093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b809bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b80a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b80a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b80b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b80b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b80bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b80c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b80cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b80d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b80dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b80e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b80e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b80e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b80ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b80f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b80f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b80fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b80ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b8103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b8106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b810b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b810f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b8113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b811860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b811cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b812140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b8125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b812a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b812e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b813300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b813770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b813be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b814050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b8144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b814930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b814da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b815210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b815680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b815af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b815f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b8163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b816840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b816db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b8172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b817720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b817b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b818000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b818470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b8188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b818d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b8191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b819630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b819aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b819f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b81a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b81a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b81ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b81b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b81b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b81b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b81be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b81c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b81c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b81cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b81cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b81d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b81d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b81dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b81e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b81e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b81ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b81eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b81f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b81f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b81fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b8200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b820520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b820990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b820e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b821270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b8216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b821b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b821fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b822430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b8228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b822d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b823180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b8235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b823a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b823ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b824340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b8247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b824c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b825090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b825500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b825970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b825de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b826250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b8266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b826fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b827410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b827880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b827cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b828160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b8285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b828a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b828eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b829320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b829790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b829c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b82a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b82a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b82a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b82adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b82b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b82b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b82bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b82bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b82c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b82c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b82ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b82d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b82d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b82da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b82de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b82e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b82e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b82ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b82f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b82f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b82f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b82fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b830210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b830680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b830af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b830f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b8313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b831840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b831cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b832120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b832590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b832a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b832e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b8332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b833750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b833bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b834030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b8344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b834910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b834d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b8351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b835e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b8360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b8363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b836810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b836c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b8370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b837560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b8379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b837e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b8382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b838720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b838b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b839000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b839470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b8398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b839d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b83a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b83a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b83aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b83af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b83b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b83b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b83bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b83c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b83c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b83c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b83ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b83d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b83d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b83db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b83dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b83e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b83e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b83ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b83f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b83f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b83fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b840080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b8404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b840960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b840dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b841240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b841760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b841c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b8427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b842aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b843060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b843620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b843be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b8441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b844760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b844d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b8452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b8458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b845e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b846420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b8469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b846fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b847560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b847b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b8480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b8486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b848c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b849220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b8497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b849da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b84a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b84a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b84aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b84b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b84ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b84c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b84c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b84cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b84d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b84d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b84dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b84e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b84e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b84ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b84f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b84f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b84ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b850520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b850ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b8510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b851660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b851c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b8521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b8527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b852d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b853320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b8538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b853ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b854460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b854a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b854fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b8555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b856120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b8566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b856ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b8571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b8576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b857ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b8580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b8585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b858aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b858fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b8594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b8599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b859ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b85a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b85a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b85ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b85b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b85b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b85c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b85c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b85cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b85d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b85d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b85e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b85e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b85ea90 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a70a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a70ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a70b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a70b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a70bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a70c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a70c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a70cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a70d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a70d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a70db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a70ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a70e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a70ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a70f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a70fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a7105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a710d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a711420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a711dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a7124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a712c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a713330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a713a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a714170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a714430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a714a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a715050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a715660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a715e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a7162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a7165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a716e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a717380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a717640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a717ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a718420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a7188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a718d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a719200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a7196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a719b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a719fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a71a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a71a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a71aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a71b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a71bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a71c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a71c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a71cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a71d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a71d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a71e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a71e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a71ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a71ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a71f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a71fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a71ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a720460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a720900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a720da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a721240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a7216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a721b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a722020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a7224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a722960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a722e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a7232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a723c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a7241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a724730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a724c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a7251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a725720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a725c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a7261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a726710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a726c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a7271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a727700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a727c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a7281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a7286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a728c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a729190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a7296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a729c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a72a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a72a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a72ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a72b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a72b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a72bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a72c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a72c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a72cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a72d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a72d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a72dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a72e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a72e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a72ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a72f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a72f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a72fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a730120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a730670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a730bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a731060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a731500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a7319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a731e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a7322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a732780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a732c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a7330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a733560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a733a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a733ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a734340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a7347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a734c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a735120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a7355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a735a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a735f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a7363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a736840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a736ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a737180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a737620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a737ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a737f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a738400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a7388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a738d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a7391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a739680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a739b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a739fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a73a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a73a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a73ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a73b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a73b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a73bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a73c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a73c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a73c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a73ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a73d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a73d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a73dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a73e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a73e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a73e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a73ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a73f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a73f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a73fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a7400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a740580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a740a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a740ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a741360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a741800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a741ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a7425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a742a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a742f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a7433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a743860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a743d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a7441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a744640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a744ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a744f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a745420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a7458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a745d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a746200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a7466a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a746b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a746fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a747480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a747920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a747dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a748310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a748860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a748db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a749300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a7495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a749bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a74a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a74a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a74afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a74b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a74b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a74bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a74c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a74cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a74cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a74d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a74d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a74e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a74e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a74eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a74f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a74f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a74fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a7500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a750610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a750b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a7510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a751600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a751b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a7520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a7525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a752b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a753090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a7535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a753b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a754080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a7545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a754b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a755070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a7555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a755b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a756060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a7565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a756b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a757050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a7575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a757af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a758040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a758590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a758ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a759030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a759580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a759ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a75a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a75a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a75aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a75b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a75b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a75bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a75c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a75c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a75caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a75cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a75d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a75da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a75dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a75e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a75ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a75efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a75f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a75fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a75ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a760510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a760a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a760f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a7613a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a761840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a761ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a762180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a762620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a762ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a762f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a763400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a7638a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a763d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a7641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a764680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a764b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a764fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a765510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a765c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a766350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a766a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a767190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a767450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a767c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a767f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a768510 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.910s
user	0m0.230s
sys	0m0.143s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
