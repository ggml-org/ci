Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.610s
user	0m0.874s
sys	0m1.295s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  6%] Built target build_info
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha256
[  6%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 18%] Linking CXX executable ../../bin/llama-gguf
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking C executable ../bin/test-c
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Built target llava_shared
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-chat
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Built target test-log
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 62%] Built target test-barrier
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-autorelease
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Built target test-model-load-cancel
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Built target test-quantize-perf
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target llama-batched-bench
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-embedding
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Built target llama-infill
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookahead
[ 78%] Built target llama-lookup
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookup-create
[ 79%] Generating loading.html.hpp
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Built target llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-cli
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-parallel
[ 86%] Built target llama-passkey
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Built target llama-retrieval
[ 86%] Built target llama-quantize
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Built target llama-speculative
[ 92%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.159s
user	0m6.705s
sys	0m9.905s

main: quantize time =  2503.87 ms
main:    total time =  2503.87 ms

main: quantize time =  1937.78 ms
main:    total time =  1937.78 ms

main: quantize time =  2239.70 ms
main:    total time =  2239.70 ms

main: quantize time =  2432.26 ms
main:    total time =  2432.26 ms

main: quantize time =  1817.63 ms
main:    total time =  1817.63 ms

main: quantize time =  5633.25 ms
main:    total time =  5633.25 ms

main: quantize time =  5856.66 ms
main:    total time =  5856.66 ms

main: quantize time =  7546.91 ms
main:    total time =  7546.91 ms

main: quantize time =  6424.94 ms
main:    total time =  6424.94 ms

main: quantize time =  4692.07 ms
main:    total time =  4692.07 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.151 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.335 I main: llama backend init
0.00.000.339 I main: load the model and apply lora adapter, if any
0.00.030.750 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.219 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.246 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.247 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.247 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.251 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.253 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.254 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.255 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.743 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.952 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.886 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.889 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.889 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.891 I llama_model_loader: - type  f32:  194 tensors
0.00.061.891 I llama_model_loader: - type  f16:   98 tensors
0.00.061.892 I print_info: file format = GGUF V3 (latest)
0.00.061.894 I print_info: file type   = all F32 (guessed)
0.00.061.895 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.075.261 I load: special tokens cache size = 25
0.00.083.004 I load: token to piece cache size = 0.2984 MB
0.00.083.027 I print_info: arch             = gptneox
0.00.083.027 I print_info: vocab_only       = 0
0.00.083.028 I print_info: n_ctx_train      = 2048
0.00.083.028 I print_info: n_embd           = 2048
0.00.083.028 I print_info: n_layer          = 24
0.00.083.031 I print_info: n_head           = 16
0.00.083.032 I print_info: n_head_kv        = 16
0.00.083.032 I print_info: n_rot            = 32
0.00.083.032 I print_info: n_swa            = 0
0.00.083.032 I print_info: n_embd_head_k    = 128
0.00.083.032 I print_info: n_embd_head_v    = 128
0.00.083.033 I print_info: n_gqa            = 1
0.00.083.034 I print_info: n_embd_k_gqa     = 2048
0.00.083.035 I print_info: n_embd_v_gqa     = 2048
0.00.083.035 I print_info: f_norm_eps       = 1.0e-05
0.00.083.036 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.083.036 I print_info: f_clamp_kqv      = 0.0e+00
0.00.083.036 I print_info: f_max_alibi_bias = 0.0e+00
0.00.083.036 I print_info: f_logit_scale    = 0.0e+00
0.00.083.037 I print_info: n_ff             = 8192
0.00.083.037 I print_info: n_expert         = 0
0.00.083.037 I print_info: n_expert_used    = 0
0.00.083.037 I print_info: causal attn      = 1
0.00.083.037 I print_info: pooling type     = 0
0.00.083.037 I print_info: rope type        = 2
0.00.083.038 I print_info: rope scaling     = linear
0.00.083.038 I print_info: freq_base_train  = 10000.0
0.00.083.039 I print_info: freq_scale_train = 1
0.00.083.039 I print_info: n_ctx_orig_yarn  = 2048
0.00.083.041 I print_info: rope_finetuned   = unknown
0.00.083.041 I print_info: ssm_d_conv       = 0
0.00.083.041 I print_info: ssm_d_inner      = 0
0.00.083.041 I print_info: ssm_d_state      = 0
0.00.083.042 I print_info: ssm_dt_rank      = 0
0.00.083.042 I print_info: ssm_dt_b_c_rms   = 0
0.00.083.042 I print_info: model type       = 1.4B
0.00.083.042 I print_info: model params     = 1.41 B
0.00.083.042 I print_info: general.name     = 1.4B
0.00.083.043 I print_info: vocab type       = BPE
0.00.083.043 I print_info: n_vocab          = 50304
0.00.083.043 I print_info: n_merges         = 50009
0.00.083.044 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.083.044 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.083.044 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.083.044 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.083.044 I print_info: LF token         = 187 'Ċ'
0.00.083.045 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.083.045 I print_info: max token length = 1024
0.00.083.045 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.130.497 I load_tensors: offloading 24 repeating layers to GPU
0.00.130.501 I load_tensors: offloading output layer to GPU
0.00.130.501 I load_tensors: offloaded 25/25 layers to GPU
0.00.130.524 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.130.526 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.130.887 I llama_context: constructing llama_context
0.00.130.888 I llama_context: n_seq_max     = 1
0.00.130.888 I llama_context: n_ctx         = 2048
0.00.130.888 I llama_context: n_ctx_per_seq = 2048
0.00.130.888 I llama_context: n_batch       = 2048
0.00.130.888 I llama_context: n_ubatch      = 512
0.00.130.888 I llama_context: flash_attn    = 0
0.00.130.889 I llama_context: freq_base     = 10000.0
0.00.130.889 I llama_context: freq_scale    = 1
0.00.130.890 I ggml_metal_init: allocating
0.00.130.909 I ggml_metal_init: found device: Apple M4
0.00.130.916 I ggml_metal_init: picking default device: Apple M4
0.00.131.596 I ggml_metal_init: using embedded metal library
0.00.141.004 I ggml_metal_init: GPU name:   Apple M4
0.00.141.005 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.141.006 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.141.006 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.141.007 I ggml_metal_init: simdgroup reduction   = true
0.00.141.007 I ggml_metal_init: simdgroup matrix mul. = true
0.00.141.007 I ggml_metal_init: has residency sets    = true
0.00.141.007 I ggml_metal_init: has bfloat            = true
0.00.141.007 I ggml_metal_init: use bfloat            = true
0.00.141.007 I ggml_metal_init: hasUnifiedMemory      = true
0.00.141.008 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.165.682 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.165.683 I llama_context_kv_self: constructing llama_context_kv_self
0.00.165.685 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.194.405 I init:      Metal KV buffer size =   384.00 MiB
0.00.194.411 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.198.729 I init:      Metal compute buffer size =   102.25 MiB
0.00.198.731 I init:        CPU compute buffer size =     8.01 MiB
0.00.198.732 I init: graph nodes  = 967
0.00.198.732 I init: graph splits = 2
0.00.198.736 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.198.865 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.198.866 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.265.771 I main: llama threadpool init, n_threads = 4
0.00.265.824 I 
0.00.265.840 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.265.840 I 
0.00.265.884 I sampler seed: 1234
0.00.265.888 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.265.913 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.265.914 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.265.914 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.094.575 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.02.094.576 I llama_perf_context_print:        load time =     234.16 ms
0.02.094.577 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.54 tokens per second)
0.02.094.577 I llama_perf_context_print:        eval time =    1782.28 ms /    63 runs   (   28.29 ms per token,    35.35 tokens per second)
0.02.094.578 I llama_perf_context_print:       total time =    1829.65 ms /    70 tokens
0.02.098.332 I ggml_metal_free: deallocating

real	0m2.410s
user	0m0.128s
sys	0m0.141s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.877 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.212 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.225 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.226 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.227 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.231 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.231 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.231 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.214 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.261 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.280 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.282 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.282 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.282 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.283 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.283 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.284 I llama_model_loader: - type  f32:  194 tensors
0.00.037.284 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.285 I print_info: file format = GGUF V3 (latest)
0.00.037.285 I print_info: file type   = Q8_0
0.00.037.288 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.180 I load: special tokens cache size = 25
0.00.053.012 I load: token to piece cache size = 0.2984 MB
0.00.053.027 I print_info: arch             = gptneox
0.00.053.028 I print_info: vocab_only       = 0
0.00.053.029 I print_info: n_ctx_train      = 2048
0.00.053.029 I print_info: n_embd           = 2048
0.00.053.029 I print_info: n_layer          = 24
0.00.053.035 I print_info: n_head           = 16
0.00.053.035 I print_info: n_head_kv        = 16
0.00.053.035 I print_info: n_rot            = 32
0.00.053.036 I print_info: n_swa            = 0
0.00.053.036 I print_info: n_embd_head_k    = 128
0.00.053.036 I print_info: n_embd_head_v    = 128
0.00.053.037 I print_info: n_gqa            = 1
0.00.053.037 I print_info: n_embd_k_gqa     = 2048
0.00.053.038 I print_info: n_embd_v_gqa     = 2048
0.00.053.039 I print_info: f_norm_eps       = 1.0e-05
0.00.053.039 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.039 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.040 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.040 I print_info: f_logit_scale    = 0.0e+00
0.00.053.040 I print_info: n_ff             = 8192
0.00.053.041 I print_info: n_expert         = 0
0.00.053.041 I print_info: n_expert_used    = 0
0.00.053.041 I print_info: causal attn      = 1
0.00.053.041 I print_info: pooling type     = 0
0.00.053.041 I print_info: rope type        = 2
0.00.053.041 I print_info: rope scaling     = linear
0.00.053.042 I print_info: freq_base_train  = 10000.0
0.00.053.042 I print_info: freq_scale_train = 1
0.00.053.042 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.042 I print_info: rope_finetuned   = unknown
0.00.053.042 I print_info: ssm_d_conv       = 0
0.00.053.043 I print_info: ssm_d_inner      = 0
0.00.053.043 I print_info: ssm_d_state      = 0
0.00.053.043 I print_info: ssm_dt_rank      = 0
0.00.053.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.043 I print_info: model type       = 1.4B
0.00.053.044 I print_info: model params     = 1.41 B
0.00.053.044 I print_info: general.name     = 1.4B
0.00.053.044 I print_info: vocab type       = BPE
0.00.053.045 I print_info: n_vocab          = 50304
0.00.053.045 I print_info: n_merges         = 50009
0.00.053.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.045 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.046 I print_info: LF token         = 187 'Ċ'
0.00.053.048 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.048 I print_info: max token length = 1024
0.00.053.049 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.268.891 I load_tensors: offloading 24 repeating layers to GPU
0.01.268.896 I load_tensors: offloading output layer to GPU
0.01.268.898 I load_tensors: offloaded 25/25 layers to GPU
0.01.268.922 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.268.923 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.269.985 I llama_context: constructing llama_context
0.01.269.987 I llama_context: n_seq_max     = 1
0.01.269.987 I llama_context: n_ctx         = 2048
0.01.269.988 I llama_context: n_ctx_per_seq = 2048
0.01.269.988 I llama_context: n_batch       = 2048
0.01.269.988 I llama_context: n_ubatch      = 512
0.01.269.989 I llama_context: flash_attn    = 0
0.01.269.989 I llama_context: freq_base     = 10000.0
0.01.269.990 I llama_context: freq_scale    = 1
0.01.269.991 I ggml_metal_init: allocating
0.01.270.005 I ggml_metal_init: found device: Apple M4
0.01.270.013 I ggml_metal_init: picking default device: Apple M4
0.01.271.492 I ggml_metal_init: using embedded metal library
0.01.276.909 I ggml_metal_init: GPU name:   Apple M4
0.01.276.913 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.276.913 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.276.914 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.276.914 I ggml_metal_init: simdgroup reduction   = true
0.01.276.914 I ggml_metal_init: simdgroup matrix mul. = true
0.01.276.915 I ggml_metal_init: has residency sets    = true
0.01.276.915 I ggml_metal_init: has bfloat            = true
0.01.276.915 I ggml_metal_init: use bfloat            = true
0.01.276.916 I ggml_metal_init: hasUnifiedMemory      = true
0.01.276.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.298.457 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.298.459 I llama_context_kv_self: constructing llama_context_kv_self
0.01.298.462 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.351.453 I init:      Metal KV buffer size =   384.00 MiB
0.01.351.460 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.355.770 I init:      Metal compute buffer size =   102.25 MiB
0.01.355.772 I init:        CPU compute buffer size =     8.01 MiB
0.01.355.772 I init: graph nodes  = 967
0.01.355.772 I init: graph splits = 2
0.01.355.778 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.355.899 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.355.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.408.999 I main: llama threadpool init, n_threads = 4
0.01.409.036 I 
0.01.409.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.409.050 I 
0.01.409.200 I sampler seed: 1234
0.01.409.205 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.409.242 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.409.245 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.409.245 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.487.573 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.02.487.574 I llama_perf_context_print:        load time =    1398.41 ms
0.02.487.574 I llama_perf_context_print: prompt eval time =      39.87 ms /     7 tokens (    5.70 ms per token,   175.58 tokens per second)
0.02.487.575 I llama_perf_context_print:        eval time =    1035.65 ms /    63 runs   (   16.44 ms per token,    60.83 tokens per second)
0.02.487.575 I llama_perf_context_print:       total time =    1079.28 ms /    70 tokens
0.02.491.351 I ggml_metal_free: deallocating

real	0m2.510s
user	0m0.110s
sys	0m0.275s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.759 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.533 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.534 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.537 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.343 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.341 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.135 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.136 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.137 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.137 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.138 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.138 I llama_model_loader: - type  f32:  194 tensors
0.00.027.138 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.139 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.139 I print_info: file format = GGUF V3 (latest)
0.00.027.140 I print_info: file type   = Q4_0
0.00.027.141 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.214 I load: special tokens cache size = 25
0.00.041.250 I load: token to piece cache size = 0.2984 MB
0.00.041.266 I print_info: arch             = gptneox
0.00.041.267 I print_info: vocab_only       = 0
0.00.041.267 I print_info: n_ctx_train      = 2048
0.00.041.267 I print_info: n_embd           = 2048
0.00.041.268 I print_info: n_layer          = 24
0.00.041.272 I print_info: n_head           = 16
0.00.041.273 I print_info: n_head_kv        = 16
0.00.041.273 I print_info: n_rot            = 32
0.00.041.275 I print_info: n_swa            = 0
0.00.041.275 I print_info: n_embd_head_k    = 128
0.00.041.275 I print_info: n_embd_head_v    = 128
0.00.041.276 I print_info: n_gqa            = 1
0.00.041.277 I print_info: n_embd_k_gqa     = 2048
0.00.041.277 I print_info: n_embd_v_gqa     = 2048
0.00.041.279 I print_info: f_norm_eps       = 1.0e-05
0.00.041.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.280 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.280 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.280 I print_info: f_logit_scale    = 0.0e+00
0.00.041.281 I print_info: n_ff             = 8192
0.00.041.281 I print_info: n_expert         = 0
0.00.041.281 I print_info: n_expert_used    = 0
0.00.041.281 I print_info: causal attn      = 1
0.00.041.281 I print_info: pooling type     = 0
0.00.041.281 I print_info: rope type        = 2
0.00.041.285 I print_info: rope scaling     = linear
0.00.041.286 I print_info: freq_base_train  = 10000.0
0.00.041.286 I print_info: freq_scale_train = 1
0.00.041.287 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.287 I print_info: rope_finetuned   = unknown
0.00.041.287 I print_info: ssm_d_conv       = 0
0.00.041.288 I print_info: ssm_d_inner      = 0
0.00.041.288 I print_info: ssm_d_state      = 0
0.00.041.289 I print_info: ssm_dt_rank      = 0
0.00.041.289 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.289 I print_info: model type       = 1.4B
0.00.041.289 I print_info: model params     = 1.41 B
0.00.041.289 I print_info: general.name     = 1.4B
0.00.041.290 I print_info: vocab type       = BPE
0.00.041.290 I print_info: n_vocab          = 50304
0.00.041.290 I print_info: n_merges         = 50009
0.00.041.291 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.291 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.291 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.291 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.292 I print_info: LF token         = 187 'Ċ'
0.00.041.293 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.293 I print_info: max token length = 1024
0.00.041.294 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.623.488 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.503 I load_tensors: offloading output layer to GPU
0.00.623.504 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.537 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.623.538 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.625.206 I llama_context: constructing llama_context
0.00.625.209 I llama_context: n_seq_max     = 1
0.00.625.210 I llama_context: n_ctx         = 2048
0.00.625.210 I llama_context: n_ctx_per_seq = 2048
0.00.625.211 I llama_context: n_batch       = 2048
0.00.625.211 I llama_context: n_ubatch      = 512
0.00.625.212 I llama_context: flash_attn    = 0
0.00.625.214 I llama_context: freq_base     = 10000.0
0.00.625.214 I llama_context: freq_scale    = 1
0.00.625.216 I ggml_metal_init: allocating
0.00.625.312 I ggml_metal_init: found device: Apple M4
0.00.625.326 I ggml_metal_init: picking default device: Apple M4
0.00.627.198 I ggml_metal_init: using embedded metal library
0.00.633.734 I ggml_metal_init: GPU name:   Apple M4
0.00.633.739 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.633.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.633.740 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.633.741 I ggml_metal_init: simdgroup reduction   = true
0.00.633.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.633.742 I ggml_metal_init: has residency sets    = true
0.00.633.742 I ggml_metal_init: has bfloat            = true
0.00.633.742 I ggml_metal_init: use bfloat            = true
0.00.633.744 I ggml_metal_init: hasUnifiedMemory      = true
0.00.633.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.278 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.652.281 I llama_context_kv_self: constructing llama_context_kv_self
0.00.652.283 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.752 I init:      Metal KV buffer size =   384.00 MiB
0.00.707.757 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.712.733 I init:      Metal compute buffer size =   102.25 MiB
0.00.712.735 I init:        CPU compute buffer size =     8.01 MiB
0.00.712.736 I init: graph nodes  = 967
0.00.712.736 I init: graph splits = 2
0.00.712.742 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.712.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.462 I main: llama threadpool init, n_threads = 4
0.00.770.504 I 
0.00.770.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.520 I 
0.00.770.704 I sampler seed: 1234
0.00.770.708 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.727 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.727 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.727 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.448.679 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49685.09 tokens per second)
0.01.448.680 I llama_perf_context_print:        load time =     759.01 ms
0.01.448.681 I llama_perf_context_print: prompt eval time =      49.24 ms /     7 tokens (    7.03 ms per token,   142.16 tokens per second)
0.01.448.682 I llama_perf_context_print:        eval time =     625.80 ms /    63 runs   (    9.93 ms per token,   100.67 tokens per second)
0.01.448.684 I llama_perf_context_print:       total time =     678.91 ms /    70 tokens
0.01.452.494 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.109s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.640 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.171 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.176 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.188 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.189 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.191 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.049 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.882 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.883 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.884 I llama_model_loader: - type  f32:  194 tensors
0.00.024.885 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.885 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.886 I print_info: file format = GGUF V3 (latest)
0.00.024.886 I print_info: file type   = Q4_1
0.00.024.891 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.743 I load: special tokens cache size = 25
0.00.038.791 I load: token to piece cache size = 0.2984 MB
0.00.038.805 I print_info: arch             = gptneox
0.00.038.806 I print_info: vocab_only       = 0
0.00.038.807 I print_info: n_ctx_train      = 2048
0.00.038.807 I print_info: n_embd           = 2048
0.00.038.807 I print_info: n_layer          = 24
0.00.038.810 I print_info: n_head           = 16
0.00.038.811 I print_info: n_head_kv        = 16
0.00.038.811 I print_info: n_rot            = 32
0.00.038.811 I print_info: n_swa            = 0
0.00.038.811 I print_info: n_embd_head_k    = 128
0.00.038.811 I print_info: n_embd_head_v    = 128
0.00.038.812 I print_info: n_gqa            = 1
0.00.038.813 I print_info: n_embd_k_gqa     = 2048
0.00.038.814 I print_info: n_embd_v_gqa     = 2048
0.00.038.814 I print_info: f_norm_eps       = 1.0e-05
0.00.038.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.815 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.815 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.815 I print_info: f_logit_scale    = 0.0e+00
0.00.038.816 I print_info: n_ff             = 8192
0.00.038.816 I print_info: n_expert         = 0
0.00.038.816 I print_info: n_expert_used    = 0
0.00.038.817 I print_info: causal attn      = 1
0.00.038.818 I print_info: pooling type     = 0
0.00.038.818 I print_info: rope type        = 2
0.00.038.818 I print_info: rope scaling     = linear
0.00.038.818 I print_info: freq_base_train  = 10000.0
0.00.038.818 I print_info: freq_scale_train = 1
0.00.038.819 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.819 I print_info: rope_finetuned   = unknown
0.00.038.819 I print_info: ssm_d_conv       = 0
0.00.038.819 I print_info: ssm_d_inner      = 0
0.00.038.819 I print_info: ssm_d_state      = 0
0.00.038.819 I print_info: ssm_dt_rank      = 0
0.00.038.819 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.819 I print_info: model type       = 1.4B
0.00.038.820 I print_info: model params     = 1.41 B
0.00.038.820 I print_info: general.name     = 1.4B
0.00.038.820 I print_info: vocab type       = BPE
0.00.038.821 I print_info: n_vocab          = 50304
0.00.038.821 I print_info: n_merges         = 50009
0.00.038.822 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: LF token         = 187 'Ċ'
0.00.038.823 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: max token length = 1024
0.00.038.823 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.663.516 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.530 I load_tensors: offloading output layer to GPU
0.00.663.531 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.566 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.663.567 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.665.086 I llama_context: constructing llama_context
0.00.665.089 I llama_context: n_seq_max     = 1
0.00.665.090 I llama_context: n_ctx         = 2048
0.00.665.091 I llama_context: n_ctx_per_seq = 2048
0.00.665.091 I llama_context: n_batch       = 2048
0.00.665.091 I llama_context: n_ubatch      = 512
0.00.665.092 I llama_context: flash_attn    = 0
0.00.665.094 I llama_context: freq_base     = 10000.0
0.00.665.094 I llama_context: freq_scale    = 1
0.00.665.096 I ggml_metal_init: allocating
0.00.665.171 I ggml_metal_init: found device: Apple M4
0.00.665.185 I ggml_metal_init: picking default device: Apple M4
0.00.667.054 I ggml_metal_init: using embedded metal library
0.00.673.719 I ggml_metal_init: GPU name:   Apple M4
0.00.673.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.725 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.726 I ggml_metal_init: simdgroup reduction   = true
0.00.673.726 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.727 I ggml_metal_init: has residency sets    = true
0.00.673.727 I ggml_metal_init: has bfloat            = true
0.00.673.727 I ggml_metal_init: use bfloat            = true
0.00.673.728 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.729 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.048 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.692.051 I llama_context_kv_self: constructing llama_context_kv_self
0.00.692.052 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.779 I init:      Metal KV buffer size =   384.00 MiB
0.00.747.785 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.751.801 I init:      Metal compute buffer size =   102.25 MiB
0.00.751.803 I init:        CPU compute buffer size =     8.01 MiB
0.00.751.804 I init: graph nodes  = 967
0.00.751.804 I init: graph splits = 2
0.00.751.810 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.751.938 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.751.939 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.753 I main: llama threadpool init, n_threads = 4
0.00.804.798 I 
0.00.804.812 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.812 I 
0.00.804.989 I sampler seed: 1234
0.00.804.994 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.005 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.005 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.005 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.540.147 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.540.147 I llama_perf_context_print:        load time =     795.40 ms
0.01.540.148 I llama_perf_context_print: prompt eval time =      48.83 ms /     7 tokens (    6.98 ms per token,   143.37 tokens per second)
0.01.540.149 I llama_perf_context_print:        eval time =     683.58 ms /    63 runs   (   10.85 ms per token,    92.16 tokens per second)
0.01.540.149 I llama_perf_context_print:       total time =     736.10 ms /    70 tokens
0.01.544.161 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.109s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.109 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.114 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.116 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.116 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.117 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.117 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.117 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.118 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.119 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.119 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.120 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.125 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.964 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.820 I llama_model_loader: - type  f32:  194 tensors
0.00.025.820 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.821 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.821 I print_info: file format = GGUF V3 (latest)
0.00.025.822 I print_info: file type   = Q5_0
0.00.025.823 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.699 I load: special tokens cache size = 25
0.00.039.824 I load: token to piece cache size = 0.2984 MB
0.00.039.838 I print_info: arch             = gptneox
0.00.039.839 I print_info: vocab_only       = 0
0.00.039.839 I print_info: n_ctx_train      = 2048
0.00.039.839 I print_info: n_embd           = 2048
0.00.039.839 I print_info: n_layer          = 24
0.00.039.842 I print_info: n_head           = 16
0.00.039.843 I print_info: n_head_kv        = 16
0.00.039.843 I print_info: n_rot            = 32
0.00.039.843 I print_info: n_swa            = 0
0.00.039.844 I print_info: n_embd_head_k    = 128
0.00.039.844 I print_info: n_embd_head_v    = 128
0.00.039.844 I print_info: n_gqa            = 1
0.00.039.845 I print_info: n_embd_k_gqa     = 2048
0.00.039.846 I print_info: n_embd_v_gqa     = 2048
0.00.039.846 I print_info: f_norm_eps       = 1.0e-05
0.00.039.847 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.847 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.847 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.847 I print_info: f_logit_scale    = 0.0e+00
0.00.039.848 I print_info: n_ff             = 8192
0.00.039.848 I print_info: n_expert         = 0
0.00.039.848 I print_info: n_expert_used    = 0
0.00.039.849 I print_info: causal attn      = 1
0.00.039.849 I print_info: pooling type     = 0
0.00.039.849 I print_info: rope type        = 2
0.00.039.851 I print_info: rope scaling     = linear
0.00.039.851 I print_info: freq_base_train  = 10000.0
0.00.039.852 I print_info: freq_scale_train = 1
0.00.039.852 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.852 I print_info: rope_finetuned   = unknown
0.00.039.852 I print_info: ssm_d_conv       = 0
0.00.039.852 I print_info: ssm_d_inner      = 0
0.00.039.853 I print_info: ssm_d_state      = 0
0.00.039.853 I print_info: ssm_dt_rank      = 0
0.00.039.853 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.853 I print_info: model type       = 1.4B
0.00.039.853 I print_info: model params     = 1.41 B
0.00.039.855 I print_info: general.name     = 1.4B
0.00.039.855 I print_info: vocab type       = BPE
0.00.039.855 I print_info: n_vocab          = 50304
0.00.039.855 I print_info: n_merges         = 50009
0.00.039.856 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: LF token         = 187 'Ċ'
0.00.039.857 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.862 I print_info: max token length = 1024
0.00.039.864 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.692.940 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.955 I load_tensors: offloading output layer to GPU
0.00.692.956 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.989 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.692.991 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.694.723 I llama_context: constructing llama_context
0.00.694.727 I llama_context: n_seq_max     = 1
0.00.694.727 I llama_context: n_ctx         = 2048
0.00.694.728 I llama_context: n_ctx_per_seq = 2048
0.00.694.728 I llama_context: n_batch       = 2048
0.00.694.729 I llama_context: n_ubatch      = 512
0.00.694.729 I llama_context: flash_attn    = 0
0.00.694.731 I llama_context: freq_base     = 10000.0
0.00.694.732 I llama_context: freq_scale    = 1
0.00.694.734 I ggml_metal_init: allocating
0.00.694.827 I ggml_metal_init: found device: Apple M4
0.00.694.842 I ggml_metal_init: picking default device: Apple M4
0.00.696.550 I ggml_metal_init: using embedded metal library
0.00.702.991 I ggml_metal_init: GPU name:   Apple M4
0.00.702.995 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.996 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.997 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.998 I ggml_metal_init: simdgroup reduction   = true
0.00.702.998 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.998 I ggml_metal_init: has residency sets    = true
0.00.702.999 I ggml_metal_init: has bfloat            = true
0.00.702.999 I ggml_metal_init: use bfloat            = true
0.00.703.000 I ggml_metal_init: hasUnifiedMemory      = true
0.00.703.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.719.952 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.719.954 I llama_context_kv_self: constructing llama_context_kv_self
0.00.719.956 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.779.620 I init:      Metal KV buffer size =   384.00 MiB
0.00.779.627 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.783.948 I init:      Metal compute buffer size =   102.25 MiB
0.00.783.950 I init:        CPU compute buffer size =     8.01 MiB
0.00.783.950 I init: graph nodes  = 967
0.00.783.950 I init: graph splits = 2
0.00.783.954 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.784.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.784.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.292 I main: llama threadpool init, n_threads = 4
0.00.843.341 I 
0.00.843.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.843.359 I 
0.00.843.511 I sampler seed: 1234
0.00.843.516 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.843.536 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.843.536 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.843.536 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.641.864 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.641.865 I llama_perf_context_print:        load time =     832.75 ms
0.01.641.865 I llama_perf_context_print: prompt eval time =      51.24 ms /     7 tokens (    7.32 ms per token,   136.61 tokens per second)
0.01.641.866 I llama_perf_context_print:        eval time =     744.13 ms /    63 runs   (   11.81 ms per token,    84.66 tokens per second)
0.01.641.866 I llama_perf_context_print:       total time =     799.29 ms /    70 tokens
0.01.645.913 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.108s
sys	0m0.224s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.596 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.153 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.164 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.165 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.166 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.166 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.167 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.168 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.168 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.169 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.171 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.950 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.715 I llama_model_loader: - type  f32:  194 tensors
0.00.024.716 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.716 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.717 I print_info: file format = GGUF V3 (latest)
0.00.024.717 I print_info: file type   = Q5_1
0.00.024.718 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.563 I load: special tokens cache size = 25
0.00.038.591 I load: token to piece cache size = 0.2984 MB
0.00.038.605 I print_info: arch             = gptneox
0.00.038.606 I print_info: vocab_only       = 0
0.00.038.607 I print_info: n_ctx_train      = 2048
0.00.038.607 I print_info: n_embd           = 2048
0.00.038.607 I print_info: n_layer          = 24
0.00.038.610 I print_info: n_head           = 16
0.00.038.611 I print_info: n_head_kv        = 16
0.00.038.611 I print_info: n_rot            = 32
0.00.038.611 I print_info: n_swa            = 0
0.00.038.611 I print_info: n_embd_head_k    = 128
0.00.038.611 I print_info: n_embd_head_v    = 128
0.00.038.612 I print_info: n_gqa            = 1
0.00.038.613 I print_info: n_embd_k_gqa     = 2048
0.00.038.614 I print_info: n_embd_v_gqa     = 2048
0.00.038.614 I print_info: f_norm_eps       = 1.0e-05
0.00.038.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.615 I print_info: f_logit_scale    = 0.0e+00
0.00.038.616 I print_info: n_ff             = 8192
0.00.038.616 I print_info: n_expert         = 0
0.00.038.616 I print_info: n_expert_used    = 0
0.00.038.617 I print_info: causal attn      = 1
0.00.038.618 I print_info: pooling type     = 0
0.00.038.618 I print_info: rope type        = 2
0.00.038.618 I print_info: rope scaling     = linear
0.00.038.618 I print_info: freq_base_train  = 10000.0
0.00.038.619 I print_info: freq_scale_train = 1
0.00.038.619 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.619 I print_info: rope_finetuned   = unknown
0.00.038.619 I print_info: ssm_d_conv       = 0
0.00.038.619 I print_info: ssm_d_inner      = 0
0.00.038.620 I print_info: ssm_d_state      = 0
0.00.038.621 I print_info: ssm_dt_rank      = 0
0.00.038.621 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.621 I print_info: model type       = 1.4B
0.00.038.622 I print_info: model params     = 1.41 B
0.00.038.622 I print_info: general.name     = 1.4B
0.00.038.622 I print_info: vocab type       = BPE
0.00.038.622 I print_info: n_vocab          = 50304
0.00.038.622 I print_info: n_merges         = 50009
0.00.038.623 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.623 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.623 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.623 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.623 I print_info: LF token         = 187 'Ċ'
0.00.038.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: max token length = 1024
0.00.038.624 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.106 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.118 I load_tensors: offloading output layer to GPU
0.00.608.119 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.153 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.608.155 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.609.636 I llama_context: constructing llama_context
0.00.609.639 I llama_context: n_seq_max     = 1
0.00.609.640 I llama_context: n_ctx         = 2048
0.00.609.641 I llama_context: n_ctx_per_seq = 2048
0.00.609.641 I llama_context: n_batch       = 2048
0.00.609.641 I llama_context: n_ubatch      = 512
0.00.609.642 I llama_context: flash_attn    = 0
0.00.609.643 I llama_context: freq_base     = 10000.0
0.00.609.644 I llama_context: freq_scale    = 1
0.00.609.646 I ggml_metal_init: allocating
0.00.609.718 I ggml_metal_init: found device: Apple M4
0.00.609.732 I ggml_metal_init: picking default device: Apple M4
0.00.611.630 I ggml_metal_init: using embedded metal library
0.00.618.092 I ggml_metal_init: GPU name:   Apple M4
0.00.618.095 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.096 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.097 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.097 I ggml_metal_init: simdgroup reduction   = true
0.00.618.097 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.098 I ggml_metal_init: has residency sets    = true
0.00.618.098 I ggml_metal_init: has bfloat            = true
0.00.618.098 I ggml_metal_init: use bfloat            = true
0.00.618.099 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.076 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.635.078 I llama_context_kv_self: constructing llama_context_kv_self
0.00.635.080 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.452 I init:      Metal KV buffer size =   384.00 MiB
0.00.689.459 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.794 I init:      Metal compute buffer size =   102.25 MiB
0.00.693.796 I init:        CPU compute buffer size =     8.01 MiB
0.00.693.797 I init: graph nodes  = 967
0.00.693.797 I init: graph splits = 2
0.00.693.803 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.693.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.693.937 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.021 I main: llama threadpool init, n_threads = 4
0.00.751.067 I 
0.00.751.083 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.084 I 
0.00.751.256 I sampler seed: 1234
0.00.751.261 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.302 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.306 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.306 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.592.727 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.592.727 I llama_perf_context_print:        load time =     741.72 ms
0.01.592.728 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.80 tokens per second)
0.01.592.729 I llama_perf_context_print:        eval time =     796.27 ms /    63 runs   (   12.64 ms per token,    79.12 tokens per second)
0.01.592.730 I llama_perf_context_print:       total time =     842.41 ms /    70 tokens
0.01.596.687 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.108s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.796 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.364 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.368 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.370 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.371 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.371 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.372 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.373 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.373 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.378 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.194 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.193 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.985 I llama_model_loader: - type  f32:  194 tensors
0.00.024.985 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.985 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.986 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.986 I print_info: file format = GGUF V3 (latest)
0.00.024.987 I print_info: file type   = Q2_K - Medium
0.00.024.987 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.856 I load: special tokens cache size = 25
0.00.038.819 I load: token to piece cache size = 0.2984 MB
0.00.038.833 I print_info: arch             = gptneox
0.00.038.834 I print_info: vocab_only       = 0
0.00.038.834 I print_info: n_ctx_train      = 2048
0.00.038.834 I print_info: n_embd           = 2048
0.00.038.835 I print_info: n_layer          = 24
0.00.038.837 I print_info: n_head           = 16
0.00.038.838 I print_info: n_head_kv        = 16
0.00.038.838 I print_info: n_rot            = 32
0.00.038.838 I print_info: n_swa            = 0
0.00.038.839 I print_info: n_embd_head_k    = 128
0.00.038.839 I print_info: n_embd_head_v    = 128
0.00.038.840 I print_info: n_gqa            = 1
0.00.038.840 I print_info: n_embd_k_gqa     = 2048
0.00.038.841 I print_info: n_embd_v_gqa     = 2048
0.00.038.842 I print_info: f_norm_eps       = 1.0e-05
0.00.038.842 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.842 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.842 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.842 I print_info: f_logit_scale    = 0.0e+00
0.00.038.843 I print_info: n_ff             = 8192
0.00.038.843 I print_info: n_expert         = 0
0.00.038.843 I print_info: n_expert_used    = 0
0.00.038.844 I print_info: causal attn      = 1
0.00.038.844 I print_info: pooling type     = 0
0.00.038.844 I print_info: rope type        = 2
0.00.038.846 I print_info: rope scaling     = linear
0.00.038.846 I print_info: freq_base_train  = 10000.0
0.00.038.846 I print_info: freq_scale_train = 1
0.00.038.847 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.847 I print_info: rope_finetuned   = unknown
0.00.038.847 I print_info: ssm_d_conv       = 0
0.00.038.847 I print_info: ssm_d_inner      = 0
0.00.038.847 I print_info: ssm_d_state      = 0
0.00.038.847 I print_info: ssm_dt_rank      = 0
0.00.038.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.848 I print_info: model type       = 1.4B
0.00.038.848 I print_info: model params     = 1.41 B
0.00.038.848 I print_info: general.name     = 1.4B
0.00.038.852 I print_info: vocab type       = BPE
0.00.038.852 I print_info: n_vocab          = 50304
0.00.038.852 I print_info: n_merges         = 50009
0.00.038.852 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.852 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.852 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.853 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.853 I print_info: LF token         = 187 'Ċ'
0.00.038.854 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.854 I print_info: max token length = 1024
0.00.038.855 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.349.612 I load_tensors: offloading 24 repeating layers to GPU
0.00.349.626 I load_tensors: offloading output layer to GPU
0.00.349.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.349.660 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.349.662 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.351.283 I llama_context: constructing llama_context
0.00.351.290 I llama_context: n_seq_max     = 1
0.00.351.290 I llama_context: n_ctx         = 2048
0.00.351.291 I llama_context: n_ctx_per_seq = 2048
0.00.351.291 I llama_context: n_batch       = 2048
0.00.351.291 I llama_context: n_ubatch      = 512
0.00.351.292 I llama_context: flash_attn    = 0
0.00.351.294 I llama_context: freq_base     = 10000.0
0.00.351.295 I llama_context: freq_scale    = 1
0.00.351.296 I ggml_metal_init: allocating
0.00.351.385 I ggml_metal_init: found device: Apple M4
0.00.351.398 I ggml_metal_init: picking default device: Apple M4
0.00.353.325 I ggml_metal_init: using embedded metal library
0.00.358.770 I ggml_metal_init: GPU name:   Apple M4
0.00.358.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.358.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.358.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.358.782 I ggml_metal_init: simdgroup reduction   = true
0.00.358.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.358.783 I ggml_metal_init: has residency sets    = true
0.00.358.783 I ggml_metal_init: has bfloat            = true
0.00.358.783 I ggml_metal_init: use bfloat            = true
0.00.358.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.358.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.380.096 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.380.099 I llama_context_kv_self: constructing llama_context_kv_self
0.00.380.101 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.440.715 I init:      Metal KV buffer size =   384.00 MiB
0.00.440.722 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.445.050 I init:      Metal compute buffer size =   102.25 MiB
0.00.445.052 I init:        CPU compute buffer size =     8.01 MiB
0.00.445.053 I init: graph nodes  = 967
0.00.445.053 I init: graph splits = 2
0.00.445.060 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.445.189 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.445.189 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.713 I main: llama threadpool init, n_threads = 4
0.00.503.757 I 
0.00.503.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.774 I 
0.00.503.948 I sampler seed: 1234
0.00.503.953 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.503.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.503.999 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.504.000 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.173.838 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.01.173.839 I llama_perf_context_print:        load time =     493.17 ms
0.01.173.840 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.48 tokens per second)
0.01.173.841 I llama_perf_context_print:        eval time =     631.27 ms /    63 runs   (   10.02 ms per token,    99.80 tokens per second)
0.01.173.841 I llama_perf_context_print:       total time =     670.87 ms /    70 tokens
0.01.177.084 I ggml_metal_free: deallocating

real	0m1.191s
user	0m0.111s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.458 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.321 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.323 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.326 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.327 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.329 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.331 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.237 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.065 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.066 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.067 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.067 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.067 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.068 I llama_model_loader: - type  f32:  194 tensors
0.00.025.068 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.068 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.069 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.069 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.070 I print_info: file format = GGUF V3 (latest)
0.00.025.070 I print_info: file type   = Q3_K - Medium
0.00.025.071 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.314 I load: special tokens cache size = 25
0.00.039.190 I load: token to piece cache size = 0.2984 MB
0.00.039.204 I print_info: arch             = gptneox
0.00.039.205 I print_info: vocab_only       = 0
0.00.039.206 I print_info: n_ctx_train      = 2048
0.00.039.206 I print_info: n_embd           = 2048
0.00.039.206 I print_info: n_layer          = 24
0.00.039.208 I print_info: n_head           = 16
0.00.039.209 I print_info: n_head_kv        = 16
0.00.039.210 I print_info: n_rot            = 32
0.00.039.210 I print_info: n_swa            = 0
0.00.039.210 I print_info: n_embd_head_k    = 128
0.00.039.210 I print_info: n_embd_head_v    = 128
0.00.039.211 I print_info: n_gqa            = 1
0.00.039.212 I print_info: n_embd_k_gqa     = 2048
0.00.039.212 I print_info: n_embd_v_gqa     = 2048
0.00.039.213 I print_info: f_norm_eps       = 1.0e-05
0.00.039.214 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.214 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.214 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.214 I print_info: f_logit_scale    = 0.0e+00
0.00.039.215 I print_info: n_ff             = 8192
0.00.039.223 I print_info: n_expert         = 0
0.00.039.224 I print_info: n_expert_used    = 0
0.00.039.224 I print_info: causal attn      = 1
0.00.039.224 I print_info: pooling type     = 0
0.00.039.225 I print_info: rope type        = 2
0.00.039.225 I print_info: rope scaling     = linear
0.00.039.226 I print_info: freq_base_train  = 10000.0
0.00.039.227 I print_info: freq_scale_train = 1
0.00.039.227 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.227 I print_info: rope_finetuned   = unknown
0.00.039.227 I print_info: ssm_d_conv       = 0
0.00.039.227 I print_info: ssm_d_inner      = 0
0.00.039.227 I print_info: ssm_d_state      = 0
0.00.039.228 I print_info: ssm_dt_rank      = 0
0.00.039.228 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.229 I print_info: model type       = 1.4B
0.00.039.229 I print_info: model params     = 1.41 B
0.00.039.229 I print_info: general.name     = 1.4B
0.00.039.230 I print_info: vocab type       = BPE
0.00.039.230 I print_info: n_vocab          = 50304
0.00.039.230 I print_info: n_merges         = 50009
0.00.039.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: LF token         = 187 'Ċ'
0.00.039.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: max token length = 1024
0.00.039.232 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.462.711 I load_tensors: offloading 24 repeating layers to GPU
0.00.462.727 I load_tensors: offloading output layer to GPU
0.00.462.728 I load_tensors: offloaded 25/25 layers to GPU
0.00.462.763 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.462.764 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.464.286 I llama_context: constructing llama_context
0.00.464.289 I llama_context: n_seq_max     = 1
0.00.464.290 I llama_context: n_ctx         = 2048
0.00.464.290 I llama_context: n_ctx_per_seq = 2048
0.00.464.290 I llama_context: n_batch       = 2048
0.00.464.291 I llama_context: n_ubatch      = 512
0.00.464.291 I llama_context: flash_attn    = 0
0.00.464.293 I llama_context: freq_base     = 10000.0
0.00.464.294 I llama_context: freq_scale    = 1
0.00.464.303 I ggml_metal_init: allocating
0.00.464.381 I ggml_metal_init: found device: Apple M4
0.00.464.396 I ggml_metal_init: picking default device: Apple M4
0.00.466.283 I ggml_metal_init: using embedded metal library
0.00.471.956 I ggml_metal_init: GPU name:   Apple M4
0.00.471.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.471.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.471.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.471.963 I ggml_metal_init: simdgroup reduction   = true
0.00.471.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.471.964 I ggml_metal_init: has residency sets    = true
0.00.471.964 I ggml_metal_init: has bfloat            = true
0.00.471.964 I ggml_metal_init: use bfloat            = true
0.00.471.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.471.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.491.146 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.491.148 I llama_context_kv_self: constructing llama_context_kv_self
0.00.491.150 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.816 I init:      Metal KV buffer size =   384.00 MiB
0.00.546.823 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.551.354 I init:      Metal compute buffer size =   102.25 MiB
0.00.551.356 I init:        CPU compute buffer size =     8.01 MiB
0.00.551.356 I init: graph nodes  = 967
0.00.551.356 I init: graph splits = 2
0.00.551.363 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.551.491 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.551.491 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.987 I main: llama threadpool init, n_threads = 4
0.00.609.031 I 
0.00.609.048 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.048 I 
0.00.609.199 I sampler seed: 1234
0.00.609.204 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.249 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.252 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.253 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.352.601 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51263.54 tokens per second)
0.01.352.602 I llama_perf_context_print:        load time =     599.84 ms
0.01.352.603 I llama_perf_context_print: prompt eval time =      50.12 ms /     7 tokens (    7.16 ms per token,   139.68 tokens per second)
0.01.352.604 I llama_perf_context_print:        eval time =     690.29 ms /    63 runs   (   10.96 ms per token,    91.27 tokens per second)
0.01.352.604 I llama_perf_context_print:       total time =     744.30 ms /    70 tokens
0.01.356.476 I ggml_metal_free: deallocating

real	0m1.372s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.207 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.982 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.986 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.986 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.986 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.886 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.938 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.774 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.775 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.775 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.775 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.776 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.776 I llama_model_loader: - type  f32:  194 tensors
0.00.025.776 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.776 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.777 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.777 I print_info: file format = GGUF V3 (latest)
0.00.025.777 I print_info: file type   = Q4_K - Medium
0.00.025.779 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.658 I load: special tokens cache size = 25
0.00.039.822 I load: token to piece cache size = 0.2984 MB
0.00.039.831 I print_info: arch             = gptneox
0.00.039.832 I print_info: vocab_only       = 0
0.00.039.832 I print_info: n_ctx_train      = 2048
0.00.039.832 I print_info: n_embd           = 2048
0.00.039.833 I print_info: n_layer          = 24
0.00.039.835 I print_info: n_head           = 16
0.00.039.836 I print_info: n_head_kv        = 16
0.00.039.836 I print_info: n_rot            = 32
0.00.039.838 I print_info: n_swa            = 0
0.00.039.838 I print_info: n_embd_head_k    = 128
0.00.039.838 I print_info: n_embd_head_v    = 128
0.00.039.839 I print_info: n_gqa            = 1
0.00.039.840 I print_info: n_embd_k_gqa     = 2048
0.00.039.840 I print_info: n_embd_v_gqa     = 2048
0.00.039.841 I print_info: f_norm_eps       = 1.0e-05
0.00.039.841 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.841 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.842 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.843 I print_info: f_logit_scale    = 0.0e+00
0.00.039.844 I print_info: n_ff             = 8192
0.00.039.844 I print_info: n_expert         = 0
0.00.039.844 I print_info: n_expert_used    = 0
0.00.039.844 I print_info: causal attn      = 1
0.00.039.846 I print_info: pooling type     = 0
0.00.039.847 I print_info: rope type        = 2
0.00.039.847 I print_info: rope scaling     = linear
0.00.039.847 I print_info: freq_base_train  = 10000.0
0.00.039.847 I print_info: freq_scale_train = 1
0.00.039.848 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.848 I print_info: rope_finetuned   = unknown
0.00.039.848 I print_info: ssm_d_conv       = 0
0.00.039.849 I print_info: ssm_d_inner      = 0
0.00.039.849 I print_info: ssm_d_state      = 0
0.00.039.849 I print_info: ssm_dt_rank      = 0
0.00.039.849 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.850 I print_info: model type       = 1.4B
0.00.039.850 I print_info: model params     = 1.41 B
0.00.039.850 I print_info: general.name     = 1.4B
0.00.039.851 I print_info: vocab type       = BPE
0.00.039.852 I print_info: n_vocab          = 50304
0.00.039.852 I print_info: n_merges         = 50009
0.00.039.852 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.853 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.854 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.854 I print_info: LF token         = 187 'Ċ'
0.00.039.854 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.854 I print_info: max token length = 1024
0.00.039.854 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.520.090 I load_tensors: offloading 24 repeating layers to GPU
0.00.520.102 I load_tensors: offloading output layer to GPU
0.00.520.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.520.132 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.520.134 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.521.492 I llama_context: constructing llama_context
0.00.521.497 I llama_context: n_seq_max     = 1
0.00.521.497 I llama_context: n_ctx         = 2048
0.00.521.498 I llama_context: n_ctx_per_seq = 2048
0.00.521.499 I llama_context: n_batch       = 2048
0.00.521.499 I llama_context: n_ubatch      = 512
0.00.521.499 I llama_context: flash_attn    = 0
0.00.521.501 I llama_context: freq_base     = 10000.0
0.00.521.502 I llama_context: freq_scale    = 1
0.00.521.504 I ggml_metal_init: allocating
0.00.521.566 I ggml_metal_init: found device: Apple M4
0.00.521.577 I ggml_metal_init: picking default device: Apple M4
0.00.523.570 I ggml_metal_init: using embedded metal library
0.00.530.441 I ggml_metal_init: GPU name:   Apple M4
0.00.530.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.530.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.530.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.530.448 I ggml_metal_init: simdgroup reduction   = true
0.00.530.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.530.449 I ggml_metal_init: has residency sets    = true
0.00.530.449 I ggml_metal_init: has bfloat            = true
0.00.530.449 I ggml_metal_init: use bfloat            = true
0.00.530.450 I ggml_metal_init: hasUnifiedMemory      = true
0.00.530.451 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.548.590 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.548.592 I llama_context_kv_self: constructing llama_context_kv_self
0.00.548.594 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.898 I init:      Metal KV buffer size =   384.00 MiB
0.00.609.907 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.840 I init:      Metal compute buffer size =   102.25 MiB
0.00.614.842 I init:        CPU compute buffer size =     8.01 MiB
0.00.614.843 I init: graph nodes  = 967
0.00.614.843 I init: graph splits = 2
0.00.614.850 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.614.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.614.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.295 I main: llama threadpool init, n_threads = 4
0.00.672.336 I 
0.00.672.350 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.350 I 
0.00.672.497 I sampler seed: 1234
0.00.672.502 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.672.512 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.672.514 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.672.514 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.434.004 I llama_perf_sampler_print:    sampling time =       1.59 ms /    71 runs   (    0.02 ms per token, 44682.19 tokens per second)
0.01.434.004 I llama_perf_context_print:        load time =     662.40 ms
0.01.434.005 I llama_perf_context_print: prompt eval time =      56.25 ms /     7 tokens (    8.04 ms per token,   124.44 tokens per second)
0.01.434.006 I llama_perf_context_print:        eval time =     702.71 ms /    63 runs   (   11.15 ms per token,    89.65 tokens per second)
0.01.434.006 I llama_perf_context_print:       total time =     762.40 ms /    70 tokens
0.01.436.743 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.360 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.118 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.136 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.136 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.137 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.137 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.140 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.115 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.026 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.026 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.027 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.027 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.028 I llama_model_loader: - type  f32:  194 tensors
0.00.026.028 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.028 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.029 I print_info: file format = GGUF V3 (latest)
0.00.026.030 I print_info: file type   = Q5_K - Medium
0.00.026.035 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.520 I load: special tokens cache size = 25
0.00.040.581 I load: token to piece cache size = 0.2984 MB
0.00.040.600 I print_info: arch             = gptneox
0.00.040.601 I print_info: vocab_only       = 0
0.00.040.601 I print_info: n_ctx_train      = 2048
0.00.040.601 I print_info: n_embd           = 2048
0.00.040.601 I print_info: n_layer          = 24
0.00.040.606 I print_info: n_head           = 16
0.00.040.607 I print_info: n_head_kv        = 16
0.00.040.608 I print_info: n_rot            = 32
0.00.040.609 I print_info: n_swa            = 0
0.00.040.609 I print_info: n_embd_head_k    = 128
0.00.040.609 I print_info: n_embd_head_v    = 128
0.00.040.610 I print_info: n_gqa            = 1
0.00.040.610 I print_info: n_embd_k_gqa     = 2048
0.00.040.611 I print_info: n_embd_v_gqa     = 2048
0.00.040.611 I print_info: f_norm_eps       = 1.0e-05
0.00.040.612 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.612 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.612 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.612 I print_info: f_logit_scale    = 0.0e+00
0.00.040.613 I print_info: n_ff             = 8192
0.00.040.613 I print_info: n_expert         = 0
0.00.040.613 I print_info: n_expert_used    = 0
0.00.040.613 I print_info: causal attn      = 1
0.00.040.614 I print_info: pooling type     = 0
0.00.040.614 I print_info: rope type        = 2
0.00.040.614 I print_info: rope scaling     = linear
0.00.040.614 I print_info: freq_base_train  = 10000.0
0.00.040.615 I print_info: freq_scale_train = 1
0.00.040.615 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.615 I print_info: rope_finetuned   = unknown
0.00.040.615 I print_info: ssm_d_conv       = 0
0.00.040.615 I print_info: ssm_d_inner      = 0
0.00.040.615 I print_info: ssm_d_state      = 0
0.00.040.615 I print_info: ssm_dt_rank      = 0
0.00.040.616 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.616 I print_info: model type       = 1.4B
0.00.040.616 I print_info: model params     = 1.41 B
0.00.040.616 I print_info: general.name     = 1.4B
0.00.040.617 I print_info: vocab type       = BPE
0.00.040.617 I print_info: n_vocab          = 50304
0.00.040.617 I print_info: n_merges         = 50009
0.00.040.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: LF token         = 187 'Ċ'
0.00.040.618 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.619 I print_info: max token length = 1024
0.00.040.619 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.583.403 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.410 I load_tensors: offloading output layer to GPU
0.00.583.410 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.422 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.583.423 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.584.163 I llama_context: constructing llama_context
0.00.584.168 I llama_context: n_seq_max     = 1
0.00.584.169 I llama_context: n_ctx         = 2048
0.00.584.169 I llama_context: n_ctx_per_seq = 2048
0.00.584.169 I llama_context: n_batch       = 2048
0.00.584.170 I llama_context: n_ubatch      = 512
0.00.584.170 I llama_context: flash_attn    = 0
0.00.584.171 I llama_context: freq_base     = 10000.0
0.00.584.172 I llama_context: freq_scale    = 1
0.00.584.173 I ggml_metal_init: allocating
0.00.584.219 I ggml_metal_init: found device: Apple M4
0.00.584.233 I ggml_metal_init: picking default device: Apple M4
0.00.585.402 I ggml_metal_init: using embedded metal library
0.00.589.767 I ggml_metal_init: GPU name:   Apple M4
0.00.589.775 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.589.775 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.589.776 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.589.776 I ggml_metal_init: simdgroup reduction   = true
0.00.589.777 I ggml_metal_init: simdgroup matrix mul. = true
0.00.589.777 I ggml_metal_init: has residency sets    = true
0.00.589.777 I ggml_metal_init: has bfloat            = true
0.00.589.777 I ggml_metal_init: use bfloat            = true
0.00.589.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.589.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.604.609 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.604.610 I llama_context_kv_self: constructing llama_context_kv_self
0.00.604.612 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.092 I init:      Metal KV buffer size =   384.00 MiB
0.00.635.099 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.640.339 I init:      Metal compute buffer size =   102.25 MiB
0.00.640.341 I init:        CPU compute buffer size =     8.01 MiB
0.00.640.342 I init: graph nodes  = 967
0.00.640.342 I init: graph splits = 2
0.00.640.347 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.640.479 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.640.479 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.113 I main: llama threadpool init, n_threads = 4
0.00.704.155 I 
0.00.704.173 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.173 I 
0.00.704.346 I sampler seed: 1234
0.00.704.350 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.704.388 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.704.391 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.704.392 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.548.775 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.01.548.778 I llama_perf_context_print:        load time =     694.07 ms
0.01.548.779 I llama_perf_context_print: prompt eval time =      52.92 ms /     7 tokens (    7.56 ms per token,   132.28 tokens per second)
0.01.548.780 I llama_perf_context_print:        eval time =     788.87 ms /    63 runs   (   12.52 ms per token,    79.86 tokens per second)
0.01.548.780 I llama_perf_context_print:       total time =     845.35 ms /    70 tokens
0.01.552.542 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.104s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.290 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.301 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.302 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.303 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.303 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.304 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.304 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.305 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.305 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.306 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.307 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.308 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.308 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.178 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.025 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.026 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.026 I llama_model_loader: - type  f32:  194 tensors
0.00.026.027 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.027 I print_info: file format = GGUF V3 (latest)
0.00.026.028 I print_info: file type   = Q6_K
0.00.026.029 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.967 I load: special tokens cache size = 25
0.00.039.966 I load: token to piece cache size = 0.2984 MB
0.00.039.981 I print_info: arch             = gptneox
0.00.039.982 I print_info: vocab_only       = 0
0.00.039.982 I print_info: n_ctx_train      = 2048
0.00.039.982 I print_info: n_embd           = 2048
0.00.039.982 I print_info: n_layer          = 24
0.00.039.985 I print_info: n_head           = 16
0.00.039.986 I print_info: n_head_kv        = 16
0.00.039.986 I print_info: n_rot            = 32
0.00.039.986 I print_info: n_swa            = 0
0.00.039.986 I print_info: n_embd_head_k    = 128
0.00.039.987 I print_info: n_embd_head_v    = 128
0.00.039.987 I print_info: n_gqa            = 1
0.00.039.988 I print_info: n_embd_k_gqa     = 2048
0.00.039.989 I print_info: n_embd_v_gqa     = 2048
0.00.039.989 I print_info: f_norm_eps       = 1.0e-05
0.00.039.990 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.990 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.990 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.994 I print_info: f_logit_scale    = 0.0e+00
0.00.039.994 I print_info: n_ff             = 8192
0.00.039.994 I print_info: n_expert         = 0
0.00.039.995 I print_info: n_expert_used    = 0
0.00.039.995 I print_info: causal attn      = 1
0.00.039.995 I print_info: pooling type     = 0
0.00.039.995 I print_info: rope type        = 2
0.00.039.995 I print_info: rope scaling     = linear
0.00.039.996 I print_info: freq_base_train  = 10000.0
0.00.039.996 I print_info: freq_scale_train = 1
0.00.039.996 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.996 I print_info: rope_finetuned   = unknown
0.00.039.996 I print_info: ssm_d_conv       = 0
0.00.039.996 I print_info: ssm_d_inner      = 0
0.00.039.997 I print_info: ssm_d_state      = 0
0.00.039.997 I print_info: ssm_dt_rank      = 0
0.00.039.997 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.998 I print_info: model type       = 1.4B
0.00.039.998 I print_info: model params     = 1.41 B
0.00.039.998 I print_info: general.name     = 1.4B
0.00.039.999 I print_info: vocab type       = BPE
0.00.039.999 I print_info: n_vocab          = 50304
0.00.039.999 I print_info: n_merges         = 50009
0.00.039.999 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.001 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.001 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.001 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.001 I print_info: LF token         = 187 'Ċ'
0.00.040.001 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.001 I print_info: max token length = 1024
0.00.040.002 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.297 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.303 I load_tensors: offloading output layer to GPU
0.00.650.304 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.329 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.650.331 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.651.549 I llama_context: constructing llama_context
0.00.651.552 I llama_context: n_seq_max     = 1
0.00.651.553 I llama_context: n_ctx         = 2048
0.00.651.553 I llama_context: n_ctx_per_seq = 2048
0.00.651.553 I llama_context: n_batch       = 2048
0.00.651.554 I llama_context: n_ubatch      = 512
0.00.651.555 I llama_context: flash_attn    = 0
0.00.651.556 I llama_context: freq_base     = 10000.0
0.00.651.556 I llama_context: freq_scale    = 1
0.00.651.557 I ggml_metal_init: allocating
0.00.651.577 I ggml_metal_init: found device: Apple M4
0.00.651.587 I ggml_metal_init: picking default device: Apple M4
0.00.653.041 I ggml_metal_init: using embedded metal library
0.00.658.751 I ggml_metal_init: GPU name:   Apple M4
0.00.658.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.756 I ggml_metal_init: simdgroup reduction   = true
0.00.658.756 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.757 I ggml_metal_init: has residency sets    = true
0.00.658.757 I ggml_metal_init: has bfloat            = true
0.00.658.757 I ggml_metal_init: use bfloat            = true
0.00.658.758 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.759 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.100 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.675.102 I llama_context_kv_self: constructing llama_context_kv_self
0.00.675.105 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.680 I init:      Metal KV buffer size =   384.00 MiB
0.00.725.686 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.957 I init:      Metal compute buffer size =   102.25 MiB
0.00.730.960 I init:        CPU compute buffer size =     8.01 MiB
0.00.730.960 I init: graph nodes  = 967
0.00.730.960 I init: graph splits = 2
0.00.730.965 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.731.096 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.097 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.870 I main: llama threadpool init, n_threads = 4
0.00.800.911 I 
0.00.800.926 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.926 I 
0.00.801.079 I sampler seed: 1234
0.00.801.084 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.121 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.124 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.124 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.683.517 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.683.518 I llama_perf_context_print:        load time =     790.39 ms
0.01.683.519 I llama_perf_context_print: prompt eval time =      57.92 ms /     7 tokens (    8.27 ms per token,   120.85 tokens per second)
0.01.683.520 I llama_perf_context_print:        eval time =     821.60 ms /    63 runs   (   13.04 ms per token,    76.68 tokens per second)
0.01.683.520 I llama_perf_context_print:       total time =     883.34 ms /    70 tokens
0.01.687.637 I ggml_metal_free: deallocating

real	0m1.705s
user	0m0.107s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.562 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.473 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.224 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.238 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.244 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.248 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.249 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.249 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.250 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.257 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.257 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.258 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.370 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.600 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.606 I llama_model_loader: - type  f32:  194 tensors
0.00.054.606 I llama_model_loader: - type  f16:   98 tensors
0.00.054.607 I print_info: file format = GGUF V3 (latest)
0.00.054.608 I print_info: file type   = all F32 (guessed)
0.00.054.610 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.390 I load: special tokens cache size = 25
0.00.077.193 I load: token to piece cache size = 0.2984 MB
0.00.077.210 I print_info: arch             = gptneox
0.00.077.211 I print_info: vocab_only       = 0
0.00.077.211 I print_info: n_ctx_train      = 2048
0.00.077.211 I print_info: n_embd           = 2048
0.00.077.212 I print_info: n_layer          = 24
0.00.077.216 I print_info: n_head           = 16
0.00.077.217 I print_info: n_head_kv        = 16
0.00.077.217 I print_info: n_rot            = 32
0.00.077.217 I print_info: n_swa            = 0
0.00.077.218 I print_info: n_embd_head_k    = 128
0.00.077.218 I print_info: n_embd_head_v    = 128
0.00.077.219 I print_info: n_gqa            = 1
0.00.077.220 I print_info: n_embd_k_gqa     = 2048
0.00.077.221 I print_info: n_embd_v_gqa     = 2048
0.00.077.221 I print_info: f_norm_eps       = 1.0e-05
0.00.077.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.228 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.228 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.229 I print_info: f_logit_scale    = 0.0e+00
0.00.077.229 I print_info: n_ff             = 8192
0.00.077.230 I print_info: n_expert         = 0
0.00.077.230 I print_info: n_expert_used    = 0
0.00.077.230 I print_info: causal attn      = 1
0.00.077.230 I print_info: pooling type     = 0
0.00.077.230 I print_info: rope type        = 2
0.00.077.231 I print_info: rope scaling     = linear
0.00.077.232 I print_info: freq_base_train  = 10000.0
0.00.077.234 I print_info: freq_scale_train = 1
0.00.077.234 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.234 I print_info: rope_finetuned   = unknown
0.00.077.235 I print_info: ssm_d_conv       = 0
0.00.077.235 I print_info: ssm_d_inner      = 0
0.00.077.235 I print_info: ssm_d_state      = 0
0.00.077.235 I print_info: ssm_dt_rank      = 0
0.00.077.235 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.236 I print_info: model type       = 1.4B
0.00.077.236 I print_info: model params     = 1.41 B
0.00.077.236 I print_info: general.name     = 1.4B
0.00.077.238 I print_info: vocab type       = BPE
0.00.077.238 I print_info: n_vocab          = 50304
0.00.077.238 I print_info: n_merges         = 50009
0.00.077.238 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.238 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.239 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.239 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.239 I print_info: LF token         = 187 'Ċ'
0.00.077.239 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.240 I print_info: max token length = 1024
0.00.077.240 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.194.261 I load_tensors: offloading 24 repeating layers to GPU
0.01.194.265 I load_tensors: offloading output layer to GPU
0.01.194.265 I load_tensors: offloaded 25/25 layers to GPU
0.01.194.293 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.194.294 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.195.099 I llama_context: constructing llama_context
0.01.195.100 I llama_context: n_seq_max     = 1
0.01.195.100 I llama_context: n_ctx         = 128
0.01.195.101 I llama_context: n_ctx_per_seq = 128
0.01.195.101 I llama_context: n_batch       = 128
0.01.195.101 I llama_context: n_ubatch      = 128
0.01.195.101 I llama_context: flash_attn    = 0
0.01.195.102 I llama_context: freq_base     = 10000.0
0.01.195.102 I llama_context: freq_scale    = 1
0.01.195.102 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.195.103 I ggml_metal_init: allocating
0.01.195.175 I ggml_metal_init: found device: Apple M4
0.01.195.180 I ggml_metal_init: picking default device: Apple M4
0.01.196.382 I ggml_metal_init: using embedded metal library
0.01.200.291 I ggml_metal_init: GPU name:   Apple M4
0.01.200.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.200.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.200.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.200.295 I ggml_metal_init: simdgroup reduction   = true
0.01.200.295 I ggml_metal_init: simdgroup matrix mul. = true
0.01.200.296 I ggml_metal_init: has residency sets    = true
0.01.200.296 I ggml_metal_init: has bfloat            = true
0.01.200.296 I ggml_metal_init: use bfloat            = true
0.01.200.296 I ggml_metal_init: hasUnifiedMemory      = true
0.01.200.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.211.675 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.211.677 I llama_context_kv_self: constructing llama_context_kv_self
0.01.211.678 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.213.471 I init:      Metal KV buffer size =    24.00 MiB
0.01.213.476 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.215.150 I init:      Metal compute buffer size =    25.56 MiB
0.01.215.152 I init:        CPU compute buffer size =     1.06 MiB
0.01.215.152 I init: graph nodes  = 967
0.01.215.153 I init: graph splits = 2
0.01.215.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.215.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.250.445 I 
0.01.250.476 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.250.480 I perplexity: tokenizing the input ..
0.01.256.115 I perplexity: tokenization took 5.633 ms
0.01.256.120 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.374.668 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.376.010 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.376.043 I llama_perf_context_print:        load time =    1226.96 ms
0.01.376.044 I llama_perf_context_print: prompt eval time =     118.28 ms /   128 tokens (    0.92 ms per token,  1082.18 tokens per second)
0.01.376.045 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.376.045 I llama_perf_context_print:       total time =     125.60 ms /   129 tokens
0.01.376.624 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.100s
sys	0m0.253s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.280 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.526 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.379 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.386 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.387 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.387 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.387 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.388 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.394 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.394 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.394 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.306 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.306 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.306 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.307 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.307 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.307 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.308 I llama_model_loader: - type  f32:  194 tensors
0.00.026.308 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.309 I print_info: file format = GGUF V3 (latest)
0.00.026.310 I print_info: file type   = Q8_0
0.00.026.311 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.480 I load: special tokens cache size = 25
0.00.040.522 I load: token to piece cache size = 0.2984 MB
0.00.040.539 I print_info: arch             = gptneox
0.00.040.540 I print_info: vocab_only       = 0
0.00.040.540 I print_info: n_ctx_train      = 2048
0.00.040.540 I print_info: n_embd           = 2048
0.00.040.540 I print_info: n_layer          = 24
0.00.040.550 I print_info: n_head           = 16
0.00.040.551 I print_info: n_head_kv        = 16
0.00.040.552 I print_info: n_rot            = 32
0.00.040.552 I print_info: n_swa            = 0
0.00.040.552 I print_info: n_embd_head_k    = 128
0.00.040.552 I print_info: n_embd_head_v    = 128
0.00.040.553 I print_info: n_gqa            = 1
0.00.040.553 I print_info: n_embd_k_gqa     = 2048
0.00.040.554 I print_info: n_embd_v_gqa     = 2048
0.00.040.557 I print_info: f_norm_eps       = 1.0e-05
0.00.040.558 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.558 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.558 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.558 I print_info: f_logit_scale    = 0.0e+00
0.00.040.559 I print_info: n_ff             = 8192
0.00.040.559 I print_info: n_expert         = 0
0.00.040.559 I print_info: n_expert_used    = 0
0.00.040.559 I print_info: causal attn      = 1
0.00.040.559 I print_info: pooling type     = 0
0.00.040.560 I print_info: rope type        = 2
0.00.040.560 I print_info: rope scaling     = linear
0.00.040.560 I print_info: freq_base_train  = 10000.0
0.00.040.560 I print_info: freq_scale_train = 1
0.00.040.561 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.561 I print_info: rope_finetuned   = unknown
0.00.040.562 I print_info: ssm_d_conv       = 0
0.00.040.563 I print_info: ssm_d_inner      = 0
0.00.040.563 I print_info: ssm_d_state      = 0
0.00.040.563 I print_info: ssm_dt_rank      = 0
0.00.040.564 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.564 I print_info: model type       = 1.4B
0.00.040.564 I print_info: model params     = 1.41 B
0.00.040.564 I print_info: general.name     = 1.4B
0.00.040.565 I print_info: vocab type       = BPE
0.00.040.565 I print_info: n_vocab          = 50304
0.00.040.565 I print_info: n_merges         = 50009
0.00.040.565 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.565 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.566 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.566 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.566 I print_info: LF token         = 187 'Ċ'
0.00.040.566 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.566 I print_info: max token length = 1024
0.00.040.567 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.857.189 I load_tensors: offloading 24 repeating layers to GPU
0.00.857.195 I load_tensors: offloading output layer to GPU
0.00.857.196 I load_tensors: offloaded 25/25 layers to GPU
0.00.857.222 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.857.224 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.858.557 I llama_context: constructing llama_context
0.00.858.559 I llama_context: n_seq_max     = 1
0.00.858.560 I llama_context: n_ctx         = 128
0.00.858.560 I llama_context: n_ctx_per_seq = 128
0.00.858.560 I llama_context: n_batch       = 128
0.00.858.561 I llama_context: n_ubatch      = 128
0.00.858.561 I llama_context: flash_attn    = 0
0.00.858.562 I llama_context: freq_base     = 10000.0
0.00.858.562 I llama_context: freq_scale    = 1
0.00.858.563 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.858.564 I ggml_metal_init: allocating
0.00.858.620 I ggml_metal_init: found device: Apple M4
0.00.858.630 I ggml_metal_init: picking default device: Apple M4
0.00.859.896 I ggml_metal_init: using embedded metal library
0.00.865.156 I ggml_metal_init: GPU name:   Apple M4
0.00.865.159 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.865.160 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.865.160 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.865.161 I ggml_metal_init: simdgroup reduction   = true
0.00.865.161 I ggml_metal_init: simdgroup matrix mul. = true
0.00.865.161 I ggml_metal_init: has residency sets    = true
0.00.865.161 I ggml_metal_init: has bfloat            = true
0.00.865.162 I ggml_metal_init: use bfloat            = true
0.00.865.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.865.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.880.318 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.880.320 I llama_context_kv_self: constructing llama_context_kv_self
0.00.880.322 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.883.666 I init:      Metal KV buffer size =    24.00 MiB
0.00.883.674 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.886.668 I init:      Metal compute buffer size =    25.56 MiB
0.00.886.670 I init:        CPU compute buffer size =     1.06 MiB
0.00.886.671 I init: graph nodes  = 967
0.00.886.671 I init: graph splits = 2
0.00.886.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.886.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.912.300 I 
0.00.912.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.912.385 I perplexity: tokenizing the input ..
0.00.919.605 I perplexity: tokenization took 7.217 ms
0.00.919.612 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.057.885 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.059.222 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.059.258 I llama_perf_context_print:        load time =     901.76 ms
0.01.059.259 I llama_perf_context_print: prompt eval time =     137.35 ms /   128 tokens (    1.07 ms per token,   931.91 tokens per second)
0.01.059.260 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.059.260 I llama_perf_context_print:       total time =     146.96 ms /   129 tokens
0.01.059.854 I ggml_metal_free: deallocating

real	0m1.076s
user	0m0.076s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.176 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.182 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.185 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.185 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.186 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.187 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.187 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.188 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.190 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.191 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.110 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.939 I llama_model_loader: - type  f32:  194 tensors
0.00.025.940 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.941 I print_info: file format = GGUF V3 (latest)
0.00.025.941 I print_info: file type   = Q4_0
0.00.025.943 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.254 I load: special tokens cache size = 25
0.00.040.220 I load: token to piece cache size = 0.2984 MB
0.00.040.237 I print_info: arch             = gptneox
0.00.040.238 I print_info: vocab_only       = 0
0.00.040.238 I print_info: n_ctx_train      = 2048
0.00.040.238 I print_info: n_embd           = 2048
0.00.040.239 I print_info: n_layer          = 24
0.00.040.244 I print_info: n_head           = 16
0.00.040.245 I print_info: n_head_kv        = 16
0.00.040.245 I print_info: n_rot            = 32
0.00.040.245 I print_info: n_swa            = 0
0.00.040.246 I print_info: n_embd_head_k    = 128
0.00.040.246 I print_info: n_embd_head_v    = 128
0.00.040.246 I print_info: n_gqa            = 1
0.00.040.247 I print_info: n_embd_k_gqa     = 2048
0.00.040.248 I print_info: n_embd_v_gqa     = 2048
0.00.040.249 I print_info: f_norm_eps       = 1.0e-05
0.00.040.249 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.249 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.249 I print_info: f_logit_scale    = 0.0e+00
0.00.040.250 I print_info: n_ff             = 8192
0.00.040.250 I print_info: n_expert         = 0
0.00.040.250 I print_info: n_expert_used    = 0
0.00.040.250 I print_info: causal attn      = 1
0.00.040.251 I print_info: pooling type     = 0
0.00.040.251 I print_info: rope type        = 2
0.00.040.251 I print_info: rope scaling     = linear
0.00.040.251 I print_info: freq_base_train  = 10000.0
0.00.040.252 I print_info: freq_scale_train = 1
0.00.040.252 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.252 I print_info: rope_finetuned   = unknown
0.00.040.252 I print_info: ssm_d_conv       = 0
0.00.040.252 I print_info: ssm_d_inner      = 0
0.00.040.252 I print_info: ssm_d_state      = 0
0.00.040.252 I print_info: ssm_dt_rank      = 0
0.00.040.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.253 I print_info: model type       = 1.4B
0.00.040.255 I print_info: model params     = 1.41 B
0.00.040.255 I print_info: general.name     = 1.4B
0.00.040.256 I print_info: vocab type       = BPE
0.00.040.256 I print_info: n_vocab          = 50304
0.00.040.256 I print_info: n_merges         = 50009
0.00.040.256 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.256 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.258 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.258 I print_info: LF token         = 187 'Ċ'
0.00.040.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.259 I print_info: max token length = 1024
0.00.040.259 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.933 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.949 I load_tensors: offloading output layer to GPU
0.00.596.950 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.983 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.596.984 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.598.673 I llama_context: constructing llama_context
0.00.598.676 I llama_context: n_seq_max     = 1
0.00.598.677 I llama_context: n_ctx         = 128
0.00.598.678 I llama_context: n_ctx_per_seq = 128
0.00.598.678 I llama_context: n_batch       = 128
0.00.598.678 I llama_context: n_ubatch      = 128
0.00.598.679 I llama_context: flash_attn    = 0
0.00.598.681 I llama_context: freq_base     = 10000.0
0.00.598.681 I llama_context: freq_scale    = 1
0.00.598.682 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.598.684 I ggml_metal_init: allocating
0.00.598.772 I ggml_metal_init: found device: Apple M4
0.00.598.787 I ggml_metal_init: picking default device: Apple M4
0.00.600.629 I ggml_metal_init: using embedded metal library
0.00.606.412 I ggml_metal_init: GPU name:   Apple M4
0.00.606.421 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.422 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.423 I ggml_metal_init: simdgroup reduction   = true
0.00.606.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.424 I ggml_metal_init: has residency sets    = true
0.00.606.425 I ggml_metal_init: has bfloat            = true
0.00.606.425 I ggml_metal_init: use bfloat            = true
0.00.606.426 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.430 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.259 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.625.264 I llama_context_kv_self: constructing llama_context_kv_self
0.00.625.267 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.874 I init:      Metal KV buffer size =    24.00 MiB
0.00.628.880 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.187 I init:      Metal compute buffer size =    25.56 MiB
0.00.632.189 I init:        CPU compute buffer size =     1.06 MiB
0.00.632.190 I init: graph nodes  = 967
0.00.632.190 I init: graph splits = 2
0.00.632.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.198 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.133 I 
0.00.661.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.203 I perplexity: tokenizing the input ..
0.00.668.744 I perplexity: tokenization took 7.538 ms
0.00.668.752 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.532 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.806.952 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.806.978 I llama_perf_context_print:        load time =     651.25 ms
0.00.806.979 I llama_perf_context_print: prompt eval time =     135.89 ms /   128 tokens (    1.06 ms per token,   941.92 tokens per second)
0.00.806.980 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.806.980 I llama_perf_context_print:       total time =     145.85 ms /   129 tokens
0.00.807.519 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.080s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.873 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.879 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.880 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.883 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.885 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.886 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.886 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.738 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.840 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.033 I llama_model_loader: - type  f32:  194 tensors
0.00.025.033 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.034 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.034 I print_info: file format = GGUF V3 (latest)
0.00.025.035 I print_info: file type   = Q4_1
0.00.025.037 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.307 I load: special tokens cache size = 25
0.00.039.299 I load: token to piece cache size = 0.2984 MB
0.00.039.315 I print_info: arch             = gptneox
0.00.039.316 I print_info: vocab_only       = 0
0.00.039.316 I print_info: n_ctx_train      = 2048
0.00.039.316 I print_info: n_embd           = 2048
0.00.039.316 I print_info: n_layer          = 24
0.00.039.320 I print_info: n_head           = 16
0.00.039.321 I print_info: n_head_kv        = 16
0.00.039.321 I print_info: n_rot            = 32
0.00.039.321 I print_info: n_swa            = 0
0.00.039.321 I print_info: n_embd_head_k    = 128
0.00.039.322 I print_info: n_embd_head_v    = 128
0.00.039.322 I print_info: n_gqa            = 1
0.00.039.323 I print_info: n_embd_k_gqa     = 2048
0.00.039.323 I print_info: n_embd_v_gqa     = 2048
0.00.039.324 I print_info: f_norm_eps       = 1.0e-05
0.00.039.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.325 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.325 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.325 I print_info: f_logit_scale    = 0.0e+00
0.00.039.328 I print_info: n_ff             = 8192
0.00.039.328 I print_info: n_expert         = 0
0.00.039.328 I print_info: n_expert_used    = 0
0.00.039.328 I print_info: causal attn      = 1
0.00.039.328 I print_info: pooling type     = 0
0.00.039.328 I print_info: rope type        = 2
0.00.039.329 I print_info: rope scaling     = linear
0.00.039.331 I print_info: freq_base_train  = 10000.0
0.00.039.332 I print_info: freq_scale_train = 1
0.00.039.332 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.332 I print_info: rope_finetuned   = unknown
0.00.039.332 I print_info: ssm_d_conv       = 0
0.00.039.333 I print_info: ssm_d_inner      = 0
0.00.039.333 I print_info: ssm_d_state      = 0
0.00.039.333 I print_info: ssm_dt_rank      = 0
0.00.039.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.333 I print_info: model type       = 1.4B
0.00.039.333 I print_info: model params     = 1.41 B
0.00.039.333 I print_info: general.name     = 1.4B
0.00.039.334 I print_info: vocab type       = BPE
0.00.039.334 I print_info: n_vocab          = 50304
0.00.039.337 I print_info: n_merges         = 50009
0.00.039.337 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.337 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.337 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.337 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.337 I print_info: LF token         = 187 'Ċ'
0.00.039.338 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.338 I print_info: max token length = 1024
0.00.039.338 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.671.476 I load_tensors: offloading 24 repeating layers to GPU
0.00.671.491 I load_tensors: offloading output layer to GPU
0.00.671.492 I load_tensors: offloaded 25/25 layers to GPU
0.00.671.549 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.671.552 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.672.705 I llama_context: constructing llama_context
0.00.672.708 I llama_context: n_seq_max     = 1
0.00.672.709 I llama_context: n_ctx         = 128
0.00.672.710 I llama_context: n_ctx_per_seq = 128
0.00.672.710 I llama_context: n_batch       = 128
0.00.672.710 I llama_context: n_ubatch      = 128
0.00.672.711 I llama_context: flash_attn    = 0
0.00.672.712 I llama_context: freq_base     = 10000.0
0.00.672.713 I llama_context: freq_scale    = 1
0.00.672.714 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.672.716 I ggml_metal_init: allocating
0.00.672.809 I ggml_metal_init: found device: Apple M4
0.00.672.825 I ggml_metal_init: picking default device: Apple M4
0.00.674.749 I ggml_metal_init: using embedded metal library
0.00.681.083 I ggml_metal_init: GPU name:   Apple M4
0.00.681.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.092 I ggml_metal_init: simdgroup reduction   = true
0.00.681.093 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.093 I ggml_metal_init: has residency sets    = true
0.00.681.093 I ggml_metal_init: has bfloat            = true
0.00.681.094 I ggml_metal_init: use bfloat            = true
0.00.681.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.699.426 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.699.428 I llama_context_kv_self: constructing llama_context_kv_self
0.00.699.431 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.979 I init:      Metal KV buffer size =    24.00 MiB
0.00.702.983 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.706.418 I init:      Metal compute buffer size =    25.56 MiB
0.00.706.420 I init:        CPU compute buffer size =     1.06 MiB
0.00.706.420 I init: graph nodes  = 967
0.00.706.421 I init: graph splits = 2
0.00.706.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.706.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.368 I 
0.00.731.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.429 I perplexity: tokenizing the input ..
0.00.738.351 I perplexity: tokenization took 6.923 ms
0.00.738.357 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.872.773 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.874.142 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.874.166 I llama_perf_context_print:        load time =     722.60 ms
0.00.874.167 I llama_perf_context_print: prompt eval time =     134.03 ms /   128 tokens (    1.05 ms per token,   954.99 tokens per second)
0.00.874.167 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.874.168 I llama_perf_context_print:       total time =     142.80 ms /   129 tokens
0.00.874.772 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.079s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.996 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.028 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.033 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.037 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.038 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.041 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.041 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.042 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.044 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.044 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.044 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.861 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.861 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.862 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.863 I llama_model_loader: - type  f32:  194 tensors
0.00.025.863 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.863 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.864 I print_info: file format = GGUF V3 (latest)
0.00.025.866 I print_info: file type   = Q5_0
0.00.025.868 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.475 I load: special tokens cache size = 25
0.00.040.696 I load: token to piece cache size = 0.2984 MB
0.00.040.714 I print_info: arch             = gptneox
0.00.040.715 I print_info: vocab_only       = 0
0.00.040.715 I print_info: n_ctx_train      = 2048
0.00.040.715 I print_info: n_embd           = 2048
0.00.040.715 I print_info: n_layer          = 24
0.00.040.720 I print_info: n_head           = 16
0.00.040.720 I print_info: n_head_kv        = 16
0.00.040.721 I print_info: n_rot            = 32
0.00.040.721 I print_info: n_swa            = 0
0.00.040.721 I print_info: n_embd_head_k    = 128
0.00.040.721 I print_info: n_embd_head_v    = 128
0.00.040.722 I print_info: n_gqa            = 1
0.00.040.722 I print_info: n_embd_k_gqa     = 2048
0.00.040.723 I print_info: n_embd_v_gqa     = 2048
0.00.040.723 I print_info: f_norm_eps       = 1.0e-05
0.00.040.724 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.724 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.724 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.724 I print_info: f_logit_scale    = 0.0e+00
0.00.040.725 I print_info: n_ff             = 8192
0.00.040.725 I print_info: n_expert         = 0
0.00.040.725 I print_info: n_expert_used    = 0
0.00.040.725 I print_info: causal attn      = 1
0.00.040.726 I print_info: pooling type     = 0
0.00.040.726 I print_info: rope type        = 2
0.00.040.726 I print_info: rope scaling     = linear
0.00.040.729 I print_info: freq_base_train  = 10000.0
0.00.040.729 I print_info: freq_scale_train = 1
0.00.040.729 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.729 I print_info: rope_finetuned   = unknown
0.00.040.730 I print_info: ssm_d_conv       = 0
0.00.040.730 I print_info: ssm_d_inner      = 0
0.00.040.730 I print_info: ssm_d_state      = 0
0.00.040.730 I print_info: ssm_dt_rank      = 0
0.00.040.730 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.730 I print_info: model type       = 1.4B
0.00.040.732 I print_info: model params     = 1.41 B
0.00.040.732 I print_info: general.name     = 1.4B
0.00.040.733 I print_info: vocab type       = BPE
0.00.040.733 I print_info: n_vocab          = 50304
0.00.040.733 I print_info: n_merges         = 50009
0.00.040.733 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.733 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.734 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.734 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.734 I print_info: LF token         = 187 'Ċ'
0.00.040.734 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.734 I print_info: max token length = 1024
0.00.040.735 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.689.236 I load_tensors: offloading 24 repeating layers to GPU
0.00.689.250 I load_tensors: offloading output layer to GPU
0.00.689.251 I load_tensors: offloaded 25/25 layers to GPU
0.00.689.284 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.689.285 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.691.011 I llama_context: constructing llama_context
0.00.691.015 I llama_context: n_seq_max     = 1
0.00.691.016 I llama_context: n_ctx         = 128
0.00.691.016 I llama_context: n_ctx_per_seq = 128
0.00.691.016 I llama_context: n_batch       = 128
0.00.691.017 I llama_context: n_ubatch      = 128
0.00.691.017 I llama_context: flash_attn    = 0
0.00.691.019 I llama_context: freq_base     = 10000.0
0.00.691.019 I llama_context: freq_scale    = 1
0.00.691.020 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.691.023 I ggml_metal_init: allocating
0.00.691.113 I ggml_metal_init: found device: Apple M4
0.00.691.126 I ggml_metal_init: picking default device: Apple M4
0.00.692.926 I ggml_metal_init: using embedded metal library
0.00.699.672 I ggml_metal_init: GPU name:   Apple M4
0.00.699.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.699.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.699.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.699.682 I ggml_metal_init: simdgroup reduction   = true
0.00.699.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.699.682 I ggml_metal_init: has residency sets    = true
0.00.699.683 I ggml_metal_init: has bfloat            = true
0.00.699.683 I ggml_metal_init: use bfloat            = true
0.00.699.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.699.689 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.717.493 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.717.496 I llama_context_kv_self: constructing llama_context_kv_self
0.00.717.507 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.231 I init:      Metal KV buffer size =    24.00 MiB
0.00.721.238 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.724.446 I init:      Metal compute buffer size =    25.56 MiB
0.00.724.448 I init:        CPU compute buffer size =     1.06 MiB
0.00.724.448 I init: graph nodes  = 967
0.00.724.449 I init: graph splits = 2
0.00.724.452 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.724.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.769 I 
0.00.754.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.838 I perplexity: tokenizing the input ..
0.00.762.011 I perplexity: tokenization took 7.169 ms
0.00.762.025 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.910.982 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.912.333 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.912.355 I llama_perf_context_print:        load time =     744.76 ms
0.00.912.355 I llama_perf_context_print: prompt eval time =     147.99 ms /   128 tokens (    1.16 ms per token,   864.93 tokens per second)
0.00.912.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.912.357 I llama_perf_context_print:       total time =     157.59 ms /   129 tokens
0.00.912.902 I ggml_metal_free: deallocating

real	0m0.928s
user	0m0.081s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.528 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.535 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.536 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.537 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.537 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.538 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.539 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.539 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.539 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.540 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.542 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.542 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.542 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.420 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.435 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.328 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.329 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.330 I llama_model_loader: - type  f32:  194 tensors
0.00.024.330 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.330 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.331 I print_info: file format = GGUF V3 (latest)
0.00.024.331 I print_info: file type   = Q5_1
0.00.024.333 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.714 I load: special tokens cache size = 25
0.00.038.601 I load: token to piece cache size = 0.2984 MB
0.00.038.616 I print_info: arch             = gptneox
0.00.038.618 I print_info: vocab_only       = 0
0.00.038.618 I print_info: n_ctx_train      = 2048
0.00.038.618 I print_info: n_embd           = 2048
0.00.038.618 I print_info: n_layer          = 24
0.00.038.623 I print_info: n_head           = 16
0.00.038.623 I print_info: n_head_kv        = 16
0.00.038.623 I print_info: n_rot            = 32
0.00.038.624 I print_info: n_swa            = 0
0.00.038.624 I print_info: n_embd_head_k    = 128
0.00.038.624 I print_info: n_embd_head_v    = 128
0.00.038.625 I print_info: n_gqa            = 1
0.00.038.627 I print_info: n_embd_k_gqa     = 2048
0.00.038.627 I print_info: n_embd_v_gqa     = 2048
0.00.038.628 I print_info: f_norm_eps       = 1.0e-05
0.00.038.628 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.629 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.629 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.629 I print_info: f_logit_scale    = 0.0e+00
0.00.038.630 I print_info: n_ff             = 8192
0.00.038.630 I print_info: n_expert         = 0
0.00.038.630 I print_info: n_expert_used    = 0
0.00.038.630 I print_info: causal attn      = 1
0.00.038.630 I print_info: pooling type     = 0
0.00.038.630 I print_info: rope type        = 2
0.00.038.630 I print_info: rope scaling     = linear
0.00.038.631 I print_info: freq_base_train  = 10000.0
0.00.038.631 I print_info: freq_scale_train = 1
0.00.038.631 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.631 I print_info: rope_finetuned   = unknown
0.00.038.632 I print_info: ssm_d_conv       = 0
0.00.038.632 I print_info: ssm_d_inner      = 0
0.00.038.632 I print_info: ssm_d_state      = 0
0.00.038.632 I print_info: ssm_dt_rank      = 0
0.00.038.632 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.632 I print_info: model type       = 1.4B
0.00.038.633 I print_info: model params     = 1.41 B
0.00.038.633 I print_info: general.name     = 1.4B
0.00.038.633 I print_info: vocab type       = BPE
0.00.038.634 I print_info: n_vocab          = 50304
0.00.038.634 I print_info: n_merges         = 50009
0.00.038.634 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.634 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.634 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.634 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.635 I print_info: LF token         = 187 'Ċ'
0.00.038.635 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.635 I print_info: max token length = 1024
0.00.038.637 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.665.723 I load_tensors: offloading 24 repeating layers to GPU
0.00.665.729 I load_tensors: offloading output layer to GPU
0.00.665.730 I load_tensors: offloaded 25/25 layers to GPU
0.00.665.754 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.665.756 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.667.222 I llama_context: constructing llama_context
0.00.667.225 I llama_context: n_seq_max     = 1
0.00.667.226 I llama_context: n_ctx         = 128
0.00.667.226 I llama_context: n_ctx_per_seq = 128
0.00.667.227 I llama_context: n_batch       = 128
0.00.667.227 I llama_context: n_ubatch      = 128
0.00.667.227 I llama_context: flash_attn    = 0
0.00.667.229 I llama_context: freq_base     = 10000.0
0.00.667.230 I llama_context: freq_scale    = 1
0.00.667.230 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.667.233 I ggml_metal_init: allocating
0.00.667.288 I ggml_metal_init: found device: Apple M4
0.00.667.300 I ggml_metal_init: picking default device: Apple M4
0.00.668.846 I ggml_metal_init: using embedded metal library
0.00.675.554 I ggml_metal_init: GPU name:   Apple M4
0.00.675.559 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.560 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.561 I ggml_metal_init: simdgroup reduction   = true
0.00.675.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.561 I ggml_metal_init: has residency sets    = true
0.00.675.562 I ggml_metal_init: has bfloat            = true
0.00.675.562 I ggml_metal_init: use bfloat            = true
0.00.675.563 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.694.283 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.694.285 I llama_context_kv_self: constructing llama_context_kv_self
0.00.694.288 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.941 I init:      Metal KV buffer size =    24.00 MiB
0.00.697.946 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.701.381 I init:      Metal compute buffer size =    25.56 MiB
0.00.701.383 I init:        CPU compute buffer size =     1.06 MiB
0.00.701.384 I init: graph nodes  = 967
0.00.701.385 I init: graph splits = 2
0.00.701.389 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.701.391 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.742 I 
0.00.730.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.807 I perplexity: tokenizing the input ..
0.00.737.867 I perplexity: tokenization took 7.059 ms
0.00.737.875 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.873.748 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.875.257 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.875.280 I llama_perf_context_print:        load time =     721.86 ms
0.00.875.281 I llama_perf_context_print: prompt eval time =     134.87 ms /   128 tokens (    1.05 ms per token,   949.06 tokens per second)
0.00.875.281 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.875.282 I llama_perf_context_print:       total time =     144.54 ms /   129 tokens
0.00.875.814 I ggml_metal_free: deallocating

real	0m0.890s
user	0m0.079s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.220 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.317 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.022.323 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.330 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.330 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.331 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.331 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.331 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.332 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.333 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.333 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.333 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.334 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.334 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.335 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.336 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.337 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.337 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.214 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.022 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.024 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.024 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.025 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.025 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.025 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.031.026 I llama_model_loader: - type  f32:  194 tensors
0.00.031.026 I llama_model_loader: - type q2_K:   49 tensors
0.00.031.027 I llama_model_loader: - type q3_K:   48 tensors
0.00.031.027 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.028 I print_info: file format = GGUF V3 (latest)
0.00.031.028 I print_info: file type   = Q2_K - Medium
0.00.031.029 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.039.058 I load: special tokens cache size = 25
0.00.045.024 I load: token to piece cache size = 0.2984 MB
0.00.045.036 I print_info: arch             = gptneox
0.00.045.036 I print_info: vocab_only       = 0
0.00.045.037 I print_info: n_ctx_train      = 2048
0.00.045.037 I print_info: n_embd           = 2048
0.00.045.037 I print_info: n_layer          = 24
0.00.045.041 I print_info: n_head           = 16
0.00.045.041 I print_info: n_head_kv        = 16
0.00.045.041 I print_info: n_rot            = 32
0.00.045.042 I print_info: n_swa            = 0
0.00.045.042 I print_info: n_embd_head_k    = 128
0.00.045.042 I print_info: n_embd_head_v    = 128
0.00.045.044 I print_info: n_gqa            = 1
0.00.045.045 I print_info: n_embd_k_gqa     = 2048
0.00.045.045 I print_info: n_embd_v_gqa     = 2048
0.00.045.046 I print_info: f_norm_eps       = 1.0e-05
0.00.045.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.047 I print_info: f_logit_scale    = 0.0e+00
0.00.045.047 I print_info: n_ff             = 8192
0.00.045.048 I print_info: n_expert         = 0
0.00.045.048 I print_info: n_expert_used    = 0
0.00.045.048 I print_info: causal attn      = 1
0.00.045.048 I print_info: pooling type     = 0
0.00.045.048 I print_info: rope type        = 2
0.00.045.048 I print_info: rope scaling     = linear
0.00.045.049 I print_info: freq_base_train  = 10000.0
0.00.045.049 I print_info: freq_scale_train = 1
0.00.045.049 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.049 I print_info: rope_finetuned   = unknown
0.00.045.049 I print_info: ssm_d_conv       = 0
0.00.045.051 I print_info: ssm_d_inner      = 0
0.00.045.051 I print_info: ssm_d_state      = 0
0.00.045.051 I print_info: ssm_dt_rank      = 0
0.00.045.051 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.051 I print_info: model type       = 1.4B
0.00.045.051 I print_info: model params     = 1.41 B
0.00.045.052 I print_info: general.name     = 1.4B
0.00.045.052 I print_info: vocab type       = BPE
0.00.045.052 I print_info: n_vocab          = 50304
0.00.045.053 I print_info: n_merges         = 50009
0.00.045.053 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.053 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.053 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.053 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.053 I print_info: LF token         = 187 'Ċ'
0.00.045.054 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.054 I print_info: max token length = 1024
0.00.045.054 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.375.762 I load_tensors: offloading 24 repeating layers to GPU
0.00.375.774 I load_tensors: offloading output layer to GPU
0.00.375.775 I load_tensors: offloaded 25/25 layers to GPU
0.00.375.809 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.375.811 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.377.542 I llama_context: constructing llama_context
0.00.377.550 I llama_context: n_seq_max     = 1
0.00.377.551 I llama_context: n_ctx         = 128
0.00.377.552 I llama_context: n_ctx_per_seq = 128
0.00.377.552 I llama_context: n_batch       = 128
0.00.377.552 I llama_context: n_ubatch      = 128
0.00.377.552 I llama_context: flash_attn    = 0
0.00.377.554 I llama_context: freq_base     = 10000.0
0.00.377.555 I llama_context: freq_scale    = 1
0.00.377.555 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.377.557 I ggml_metal_init: allocating
0.00.377.665 I ggml_metal_init: found device: Apple M4
0.00.377.679 I ggml_metal_init: picking default device: Apple M4
0.00.379.657 I ggml_metal_init: using embedded metal library
0.00.384.975 I ggml_metal_init: GPU name:   Apple M4
0.00.384.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.384.992 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.384.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.384.993 I ggml_metal_init: simdgroup reduction   = true
0.00.384.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.384.994 I ggml_metal_init: has residency sets    = true
0.00.384.994 I ggml_metal_init: has bfloat            = true
0.00.384.994 I ggml_metal_init: use bfloat            = true
0.00.384.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.385.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.406.600 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.406.602 I llama_context_kv_self: constructing llama_context_kv_self
0.00.406.605 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.410.119 I init:      Metal KV buffer size =    24.00 MiB
0.00.410.123 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.413.524 I init:      Metal compute buffer size =    25.56 MiB
0.00.413.526 I init:        CPU compute buffer size =     1.06 MiB
0.00.413.526 I init: graph nodes  = 967
0.00.413.527 I init: graph splits = 2
0.00.413.530 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.413.531 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.446.449 I 
0.00.446.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.446.523 I perplexity: tokenizing the input ..
0.00.453.602 I perplexity: tokenization took 7.075 ms
0.00.453.608 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.599.562 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.600.919 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.600.942 I llama_perf_context_print:        load time =     434.22 ms
0.00.600.943 I llama_perf_context_print: prompt eval time =     145.04 ms /   128 tokens (    1.13 ms per token,   882.53 tokens per second)
0.00.600.944 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.600.944 I llama_perf_context_print:       total time =     154.50 ms /   129 tokens
0.00.601.488 I ggml_metal_free: deallocating

real	0m0.620s
user	0m0.083s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.651 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.662 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.662 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.663 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.664 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.664 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.665 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.665 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.665 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.666 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.668 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.668 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.669 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.634 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.631 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.543 I llama_model_loader: - type  f32:  194 tensors
0.00.026.543 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.544 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.544 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.545 I print_info: file format = GGUF V3 (latest)
0.00.026.545 I print_info: file type   = Q3_K - Medium
0.00.026.546 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.899 I load: special tokens cache size = 25
0.00.040.772 I load: token to piece cache size = 0.2984 MB
0.00.040.787 I print_info: arch             = gptneox
0.00.040.789 I print_info: vocab_only       = 0
0.00.040.789 I print_info: n_ctx_train      = 2048
0.00.040.789 I print_info: n_embd           = 2048
0.00.040.789 I print_info: n_layer          = 24
0.00.040.794 I print_info: n_head           = 16
0.00.040.794 I print_info: n_head_kv        = 16
0.00.040.795 I print_info: n_rot            = 32
0.00.040.795 I print_info: n_swa            = 0
0.00.040.795 I print_info: n_embd_head_k    = 128
0.00.040.795 I print_info: n_embd_head_v    = 128
0.00.040.796 I print_info: n_gqa            = 1
0.00.040.797 I print_info: n_embd_k_gqa     = 2048
0.00.040.797 I print_info: n_embd_v_gqa     = 2048
0.00.040.798 I print_info: f_norm_eps       = 1.0e-05
0.00.040.798 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.799 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.799 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.801 I print_info: f_logit_scale    = 0.0e+00
0.00.040.802 I print_info: n_ff             = 8192
0.00.040.802 I print_info: n_expert         = 0
0.00.040.802 I print_info: n_expert_used    = 0
0.00.040.802 I print_info: causal attn      = 1
0.00.040.802 I print_info: pooling type     = 0
0.00.040.803 I print_info: rope type        = 2
0.00.040.803 I print_info: rope scaling     = linear
0.00.040.803 I print_info: freq_base_train  = 10000.0
0.00.040.803 I print_info: freq_scale_train = 1
0.00.040.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.804 I print_info: rope_finetuned   = unknown
0.00.040.804 I print_info: ssm_d_conv       = 0
0.00.040.804 I print_info: ssm_d_inner      = 0
0.00.040.804 I print_info: ssm_d_state      = 0
0.00.040.804 I print_info: ssm_dt_rank      = 0
0.00.040.804 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.804 I print_info: model type       = 1.4B
0.00.040.805 I print_info: model params     = 1.41 B
0.00.040.808 I print_info: general.name     = 1.4B
0.00.040.809 I print_info: vocab type       = BPE
0.00.040.809 I print_info: n_vocab          = 50304
0.00.040.809 I print_info: n_merges         = 50009
0.00.040.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: LF token         = 187 'Ċ'
0.00.040.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: max token length = 1024
0.00.040.811 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.454.920 I load_tensors: offloading 24 repeating layers to GPU
0.00.454.933 I load_tensors: offloading output layer to GPU
0.00.454.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.454.965 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.454.967 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.456.592 I llama_context: constructing llama_context
0.00.456.595 I llama_context: n_seq_max     = 1
0.00.456.596 I llama_context: n_ctx         = 128
0.00.456.596 I llama_context: n_ctx_per_seq = 128
0.00.456.596 I llama_context: n_batch       = 128
0.00.456.597 I llama_context: n_ubatch      = 128
0.00.456.597 I llama_context: flash_attn    = 0
0.00.456.599 I llama_context: freq_base     = 10000.0
0.00.456.600 I llama_context: freq_scale    = 1
0.00.456.600 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.456.602 I ggml_metal_init: allocating
0.00.456.669 I ggml_metal_init: found device: Apple M4
0.00.456.682 I ggml_metal_init: picking default device: Apple M4
0.00.458.513 I ggml_metal_init: using embedded metal library
0.00.464.989 I ggml_metal_init: GPU name:   Apple M4
0.00.464.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.464.995 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.464.996 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.464.997 I ggml_metal_init: simdgroup reduction   = true
0.00.464.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.464.997 I ggml_metal_init: has residency sets    = true
0.00.464.998 I ggml_metal_init: has bfloat            = true
0.00.464.998 I ggml_metal_init: use bfloat            = true
0.00.464.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.465.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.484.284 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.484.287 I llama_context_kv_self: constructing llama_context_kv_self
0.00.484.289 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.487.971 I init:      Metal KV buffer size =    24.00 MiB
0.00.487.979 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.491.188 I init:      Metal compute buffer size =    25.56 MiB
0.00.491.190 I init:        CPU compute buffer size =     1.06 MiB
0.00.491.190 I init: graph nodes  = 967
0.00.491.190 I init: graph splits = 2
0.00.491.194 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.491.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.246 I 
0.00.518.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.318 I perplexity: tokenizing the input ..
0.00.525.289 I perplexity: tokenization took 6.968 ms
0.00.525.297 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.669.299 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.670.634 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.670.658 I llama_perf_context_print:        load time =     509.59 ms
0.00.670.659 I llama_perf_context_print: prompt eval time =     143.13 ms /   128 tokens (    1.12 ms per token,   894.28 tokens per second)
0.00.670.660 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.670.660 I llama_perf_context_print:       total time =     152.42 ms /   129 tokens
0.00.671.201 I ggml_metal_free: deallocating

real	0m0.686s
user	0m0.080s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.736 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.578 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.027.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.597 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.598 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.600 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.600 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.422 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.507 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.967 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.968 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.969 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.969 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.969 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.970 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.036.970 I llama_model_loader: - type  f32:  194 tensors
0.00.036.970 I llama_model_loader: - type q4_K:   61 tensors
0.00.036.971 I llama_model_loader: - type q5_K:   24 tensors
0.00.036.971 I llama_model_loader: - type q6_K:   13 tensors
0.00.036.972 I print_info: file format = GGUF V3 (latest)
0.00.036.972 I print_info: file type   = Q4_K - Medium
0.00.036.974 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.588 I load: special tokens cache size = 25
0.00.052.357 I load: token to piece cache size = 0.2984 MB
0.00.052.371 I print_info: arch             = gptneox
0.00.052.372 I print_info: vocab_only       = 0
0.00.052.372 I print_info: n_ctx_train      = 2048
0.00.052.372 I print_info: n_embd           = 2048
0.00.052.373 I print_info: n_layer          = 24
0.00.052.376 I print_info: n_head           = 16
0.00.052.377 I print_info: n_head_kv        = 16
0.00.052.377 I print_info: n_rot            = 32
0.00.052.377 I print_info: n_swa            = 0
0.00.052.377 I print_info: n_embd_head_k    = 128
0.00.052.377 I print_info: n_embd_head_v    = 128
0.00.052.378 I print_info: n_gqa            = 1
0.00.052.379 I print_info: n_embd_k_gqa     = 2048
0.00.052.380 I print_info: n_embd_v_gqa     = 2048
0.00.052.381 I print_info: f_norm_eps       = 1.0e-05
0.00.052.381 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.381 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.381 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.382 I print_info: f_logit_scale    = 0.0e+00
0.00.052.382 I print_info: n_ff             = 8192
0.00.052.382 I print_info: n_expert         = 0
0.00.052.382 I print_info: n_expert_used    = 0
0.00.052.383 I print_info: causal attn      = 1
0.00.052.383 I print_info: pooling type     = 0
0.00.052.383 I print_info: rope type        = 2
0.00.052.383 I print_info: rope scaling     = linear
0.00.052.383 I print_info: freq_base_train  = 10000.0
0.00.052.383 I print_info: freq_scale_train = 1
0.00.052.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.384 I print_info: rope_finetuned   = unknown
0.00.052.384 I print_info: ssm_d_conv       = 0
0.00.052.384 I print_info: ssm_d_inner      = 0
0.00.052.384 I print_info: ssm_d_state      = 0
0.00.052.385 I print_info: ssm_dt_rank      = 0
0.00.052.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.386 I print_info: model type       = 1.4B
0.00.052.387 I print_info: model params     = 1.41 B
0.00.052.387 I print_info: general.name     = 1.4B
0.00.052.387 I print_info: vocab type       = BPE
0.00.052.387 I print_info: n_vocab          = 50304
0.00.052.388 I print_info: n_merges         = 50009
0.00.052.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.388 I print_info: LF token         = 187 'Ċ'
0.00.052.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.389 I print_info: max token length = 1024
0.00.052.389 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.547.234 I load_tensors: offloading 24 repeating layers to GPU
0.00.547.241 I load_tensors: offloading output layer to GPU
0.00.547.242 I load_tensors: offloaded 25/25 layers to GPU
0.00.547.271 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.547.272 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.548.732 I llama_context: constructing llama_context
0.00.548.736 I llama_context: n_seq_max     = 1
0.00.548.737 I llama_context: n_ctx         = 128
0.00.548.737 I llama_context: n_ctx_per_seq = 128
0.00.548.738 I llama_context: n_batch       = 128
0.00.548.738 I llama_context: n_ubatch      = 128
0.00.548.739 I llama_context: flash_attn    = 0
0.00.548.740 I llama_context: freq_base     = 10000.0
0.00.548.741 I llama_context: freq_scale    = 1
0.00.548.741 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.548.744 I ggml_metal_init: allocating
0.00.548.802 I ggml_metal_init: found device: Apple M4
0.00.548.814 I ggml_metal_init: picking default device: Apple M4
0.00.550.872 I ggml_metal_init: using embedded metal library
0.00.557.810 I ggml_metal_init: GPU name:   Apple M4
0.00.557.815 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.557.816 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.557.817 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.557.817 I ggml_metal_init: simdgroup reduction   = true
0.00.557.817 I ggml_metal_init: simdgroup matrix mul. = true
0.00.557.818 I ggml_metal_init: has residency sets    = true
0.00.557.818 I ggml_metal_init: has bfloat            = true
0.00.557.818 I ggml_metal_init: use bfloat            = true
0.00.557.819 I ggml_metal_init: hasUnifiedMemory      = true
0.00.557.821 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.575.530 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.575.533 I llama_context_kv_self: constructing llama_context_kv_self
0.00.575.535 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.579.012 I init:      Metal KV buffer size =    24.00 MiB
0.00.579.019 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.582.080 I init:      Metal compute buffer size =    25.56 MiB
0.00.582.082 I init:        CPU compute buffer size =     1.06 MiB
0.00.582.083 I init: graph nodes  = 967
0.00.582.083 I init: graph splits = 2
0.00.582.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.582.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.554 I 
0.00.609.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.630 I perplexity: tokenizing the input ..
0.00.617.370 I perplexity: tokenization took 7.736 ms
0.00.617.375 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.607 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.753.026 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.753.052 I llama_perf_context_print:        load time =     593.81 ms
0.00.753.054 I llama_perf_context_print: prompt eval time =     133.28 ms /   128 tokens (    1.04 ms per token,   960.36 tokens per second)
0.00.753.055 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.753.055 I llama_perf_context_print:       total time =     143.50 ms /   129 tokens
0.00.753.656 I ggml_metal_free: deallocating

real	0m0.773s
user	0m0.082s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.925 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.022.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.591 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.593 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.417 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.419 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.420 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.031.421 I llama_model_loader: - type  f32:  194 tensors
0.00.031.421 I llama_model_loader: - type q5_K:   61 tensors
0.00.031.422 I llama_model_loader: - type q6_K:   37 tensors
0.00.031.422 I print_info: file format = GGUF V3 (latest)
0.00.031.423 I print_info: file type   = Q5_K - Medium
0.00.031.424 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.039.466 I load: special tokens cache size = 25
0.00.045.655 I load: token to piece cache size = 0.2984 MB
0.00.045.673 I print_info: arch             = gptneox
0.00.045.673 I print_info: vocab_only       = 0
0.00.045.674 I print_info: n_ctx_train      = 2048
0.00.045.674 I print_info: n_embd           = 2048
0.00.045.674 I print_info: n_layer          = 24
0.00.045.677 I print_info: n_head           = 16
0.00.045.678 I print_info: n_head_kv        = 16
0.00.045.678 I print_info: n_rot            = 32
0.00.045.678 I print_info: n_swa            = 0
0.00.045.679 I print_info: n_embd_head_k    = 128
0.00.045.679 I print_info: n_embd_head_v    = 128
0.00.045.680 I print_info: n_gqa            = 1
0.00.045.680 I print_info: n_embd_k_gqa     = 2048
0.00.045.681 I print_info: n_embd_v_gqa     = 2048
0.00.045.681 I print_info: f_norm_eps       = 1.0e-05
0.00.045.681 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.682 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.682 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.682 I print_info: f_logit_scale    = 0.0e+00
0.00.045.682 I print_info: n_ff             = 8192
0.00.045.682 I print_info: n_expert         = 0
0.00.045.683 I print_info: n_expert_used    = 0
0.00.045.683 I print_info: causal attn      = 1
0.00.045.683 I print_info: pooling type     = 0
0.00.045.683 I print_info: rope type        = 2
0.00.045.683 I print_info: rope scaling     = linear
0.00.045.684 I print_info: freq_base_train  = 10000.0
0.00.045.684 I print_info: freq_scale_train = 1
0.00.045.684 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.684 I print_info: rope_finetuned   = unknown
0.00.045.684 I print_info: ssm_d_conv       = 0
0.00.045.684 I print_info: ssm_d_inner      = 0
0.00.045.685 I print_info: ssm_d_state      = 0
0.00.045.685 I print_info: ssm_dt_rank      = 0
0.00.045.685 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.685 I print_info: model type       = 1.4B
0.00.045.685 I print_info: model params     = 1.41 B
0.00.045.686 I print_info: general.name     = 1.4B
0.00.045.686 I print_info: vocab type       = BPE
0.00.045.686 I print_info: n_vocab          = 50304
0.00.045.686 I print_info: n_merges         = 50009
0.00.045.686 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.687 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.687 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.687 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.687 I print_info: LF token         = 187 'Ċ'
0.00.045.687 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.688 I print_info: max token length = 1024
0.00.045.688 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.756.048 I load_tensors: offloading 24 repeating layers to GPU
0.00.756.064 I load_tensors: offloading output layer to GPU
0.00.756.065 I load_tensors: offloaded 25/25 layers to GPU
0.00.756.099 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.756.101 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.757.710 I llama_context: constructing llama_context
0.00.757.716 I llama_context: n_seq_max     = 1
0.00.757.716 I llama_context: n_ctx         = 128
0.00.757.717 I llama_context: n_ctx_per_seq = 128
0.00.757.717 I llama_context: n_batch       = 128
0.00.757.717 I llama_context: n_ubatch      = 128
0.00.757.718 I llama_context: flash_attn    = 0
0.00.757.719 I llama_context: freq_base     = 10000.0
0.00.757.720 I llama_context: freq_scale    = 1
0.00.757.720 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.757.722 I ggml_metal_init: allocating
0.00.757.804 I ggml_metal_init: found device: Apple M4
0.00.757.819 I ggml_metal_init: picking default device: Apple M4
0.00.759.536 I ggml_metal_init: using embedded metal library
0.00.763.559 I ggml_metal_init: GPU name:   Apple M4
0.00.763.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.763.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.763.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.763.565 I ggml_metal_init: simdgroup reduction   = true
0.00.763.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.763.565 I ggml_metal_init: has residency sets    = true
0.00.763.565 I ggml_metal_init: has bfloat            = true
0.00.763.565 I ggml_metal_init: use bfloat            = true
0.00.763.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.763.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.775.415 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.775.416 I llama_context_kv_self: constructing llama_context_kv_self
0.00.775.418 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.777.295 I init:      Metal KV buffer size =    24.00 MiB
0.00.777.299 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.779.086 I init:      Metal compute buffer size =    25.56 MiB
0.00.779.088 I init:        CPU compute buffer size =     1.06 MiB
0.00.779.088 I init: graph nodes  = 967
0.00.779.089 I init: graph splits = 2
0.00.779.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.779.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.279 I 
0.00.809.308 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.311 I perplexity: tokenizing the input ..
0.00.813.311 I perplexity: tokenization took 3.998 ms
0.00.813.315 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.949.314 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.950.654 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.950.679 I llama_perf_context_print:        load time =     800.35 ms
0.00.950.680 I llama_perf_context_print: prompt eval time =     135.77 ms /   128 tokens (    1.06 ms per token,   942.80 tokens per second)
0.00.950.681 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.950.681 I llama_perf_context_print:       total time =     141.40 ms /   129 tokens
0.00.951.223 I ggml_metal_free: deallocating

real	0m0.966s
user	0m0.068s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.880 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.022.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.492 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.493 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.493 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.494 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.494 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.495 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.495 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.495 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.498 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.500 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.500 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.500 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.373 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.374 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.375 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.375 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.375 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.376 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.031.376 I llama_model_loader: - type  f32:  194 tensors
0.00.031.377 I llama_model_loader: - type q6_K:   98 tensors
0.00.031.377 I print_info: file format = GGUF V3 (latest)
0.00.031.378 I print_info: file type   = Q6_K
0.00.031.379 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.039.790 I load: special tokens cache size = 25
0.00.045.969 I load: token to piece cache size = 0.2984 MB
0.00.045.983 I print_info: arch             = gptneox
0.00.045.984 I print_info: vocab_only       = 0
0.00.045.984 I print_info: n_ctx_train      = 2048
0.00.045.985 I print_info: n_embd           = 2048
0.00.045.985 I print_info: n_layer          = 24
0.00.045.988 I print_info: n_head           = 16
0.00.045.988 I print_info: n_head_kv        = 16
0.00.045.988 I print_info: n_rot            = 32
0.00.045.989 I print_info: n_swa            = 0
0.00.045.989 I print_info: n_embd_head_k    = 128
0.00.045.989 I print_info: n_embd_head_v    = 128
0.00.045.990 I print_info: n_gqa            = 1
0.00.045.990 I print_info: n_embd_k_gqa     = 2048
0.00.045.991 I print_info: n_embd_v_gqa     = 2048
0.00.045.991 I print_info: f_norm_eps       = 1.0e-05
0.00.045.992 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.992 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.992 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.992 I print_info: f_logit_scale    = 0.0e+00
0.00.045.993 I print_info: n_ff             = 8192
0.00.045.993 I print_info: n_expert         = 0
0.00.045.993 I print_info: n_expert_used    = 0
0.00.045.993 I print_info: causal attn      = 1
0.00.045.993 I print_info: pooling type     = 0
0.00.045.993 I print_info: rope type        = 2
0.00.045.994 I print_info: rope scaling     = linear
0.00.045.994 I print_info: freq_base_train  = 10000.0
0.00.045.994 I print_info: freq_scale_train = 1
0.00.045.996 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.996 I print_info: rope_finetuned   = unknown
0.00.045.997 I print_info: ssm_d_conv       = 0
0.00.045.997 I print_info: ssm_d_inner      = 0
0.00.045.997 I print_info: ssm_d_state      = 0
0.00.045.997 I print_info: ssm_dt_rank      = 0
0.00.045.997 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.997 I print_info: model type       = 1.4B
0.00.045.997 I print_info: model params     = 1.41 B
0.00.045.998 I print_info: general.name     = 1.4B
0.00.045.998 I print_info: vocab type       = BPE
0.00.046.001 I print_info: n_vocab          = 50304
0.00.046.002 I print_info: n_merges         = 50009
0.00.046.002 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.002 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.002 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.002 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.003 I print_info: LF token         = 187 'Ċ'
0.00.046.003 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.003 I print_info: max token length = 1024
0.00.046.003 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.446.365 I load_tensors: offloading 24 repeating layers to GPU
0.00.446.369 I load_tensors: offloading output layer to GPU
0.00.446.370 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.392 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.446.395 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.447.824 I llama_context: constructing llama_context
0.00.447.827 I llama_context: n_seq_max     = 1
0.00.447.827 I llama_context: n_ctx         = 128
0.00.447.828 I llama_context: n_ctx_per_seq = 128
0.00.447.828 I llama_context: n_batch       = 128
0.00.447.828 I llama_context: n_ubatch      = 128
0.00.447.829 I llama_context: flash_attn    = 0
0.00.447.830 I llama_context: freq_base     = 10000.0
0.00.447.830 I llama_context: freq_scale    = 1
0.00.447.831 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.447.832 I ggml_metal_init: allocating
0.00.447.848 I ggml_metal_init: found device: Apple M4
0.00.447.856 I ggml_metal_init: picking default device: Apple M4
0.00.449.283 I ggml_metal_init: using embedded metal library
0.00.455.096 I ggml_metal_init: GPU name:   Apple M4
0.00.455.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.101 I ggml_metal_init: simdgroup reduction   = true
0.00.455.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.102 I ggml_metal_init: has residency sets    = true
0.00.455.102 I ggml_metal_init: has bfloat            = true
0.00.455.102 I ggml_metal_init: use bfloat            = true
0.00.455.104 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.105 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.494 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.471.497 I llama_context_kv_self: constructing llama_context_kv_self
0.00.471.499 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.474.894 I init:      Metal KV buffer size =    24.00 MiB
0.00.474.897 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.478.187 I init:      Metal compute buffer size =    25.56 MiB
0.00.478.189 I init:        CPU compute buffer size =     1.06 MiB
0.00.478.190 I init: graph nodes  = 967
0.00.478.191 I init: graph splits = 2
0.00.478.194 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.670 I 
0.00.510.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.510.742 I perplexity: tokenizing the input ..
0.00.518.022 I perplexity: tokenization took 7.277 ms
0.00.518.028 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.649.840 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.651.178 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.651.203 I llama_perf_context_print:        load time =     500.78 ms
0.00.651.204 I llama_perf_context_print: prompt eval time =     131.22 ms /   128 tokens (    1.03 ms per token,   975.47 tokens per second)
0.00.651.205 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.651.205 I llama_perf_context_print:       total time =     140.54 ms /   129 tokens
0.00.651.796 I ggml_metal_free: deallocating

real	0m0.667s
user	0m0.078s
sys	0m0.115s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.313 I build: 4801 (f95b04a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.724 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.072 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.079 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.080 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.154 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.040 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.367 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.371 I llama_model_loader: - type  f32:  194 tensors
0.00.057.371 I llama_model_loader: - type  f16:   98 tensors
0.00.057.372 I print_info: file format = GGUF V3 (latest)
0.00.057.373 I print_info: file type   = all F32 (guessed)
0.00.057.374 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.680 I load: special tokens cache size = 25
0.00.076.253 I load: token to piece cache size = 0.2984 MB
0.00.076.264 I print_info: arch             = gptneox
0.00.076.265 I print_info: vocab_only       = 0
0.00.076.265 I print_info: n_ctx_train      = 2048
0.00.076.265 I print_info: n_embd           = 2048
0.00.076.266 I print_info: n_layer          = 24
0.00.076.268 I print_info: n_head           = 16
0.00.076.269 I print_info: n_head_kv        = 16
0.00.076.269 I print_info: n_rot            = 32
0.00.076.269 I print_info: n_swa            = 0
0.00.076.269 I print_info: n_embd_head_k    = 128
0.00.076.269 I print_info: n_embd_head_v    = 128
0.00.076.270 I print_info: n_gqa            = 1
0.00.076.271 I print_info: n_embd_k_gqa     = 2048
0.00.076.271 I print_info: n_embd_v_gqa     = 2048
0.00.076.272 I print_info: f_norm_eps       = 1.0e-05
0.00.076.272 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.275 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.275 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.275 I print_info: f_logit_scale    = 0.0e+00
0.00.076.276 I print_info: n_ff             = 8192
0.00.076.276 I print_info: n_expert         = 0
0.00.076.276 I print_info: n_expert_used    = 0
0.00.076.276 I print_info: causal attn      = 1
0.00.076.277 I print_info: pooling type     = 0
0.00.076.277 I print_info: rope type        = 2
0.00.076.277 I print_info: rope scaling     = linear
0.00.076.277 I print_info: freq_base_train  = 10000.0
0.00.076.278 I print_info: freq_scale_train = 1
0.00.076.278 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.278 I print_info: rope_finetuned   = unknown
0.00.076.278 I print_info: ssm_d_conv       = 0
0.00.076.278 I print_info: ssm_d_inner      = 0
0.00.076.278 I print_info: ssm_d_state      = 0
0.00.076.279 I print_info: ssm_dt_rank      = 0
0.00.076.279 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.279 I print_info: model type       = 1.4B
0.00.076.280 I print_info: model params     = 1.41 B
0.00.076.280 I print_info: general.name     = 1.4B
0.00.076.280 I print_info: vocab type       = BPE
0.00.076.280 I print_info: n_vocab          = 50304
0.00.076.281 I print_info: n_merges         = 50009
0.00.076.281 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.281 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.282 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.283 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.283 I print_info: LF token         = 187 'Ċ'
0.00.076.283 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.283 I print_info: max token length = 1024
0.00.076.284 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.309.459 I load_tensors: offloading 24 repeating layers to GPU
0.01.309.462 I load_tensors: offloading output layer to GPU
0.01.309.462 I load_tensors: offloaded 25/25 layers to GPU
0.01.309.479 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.309.480 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.310.006 I llama_context: constructing llama_context
0.01.310.007 I llama_context: n_seq_max     = 1
0.01.310.007 I llama_context: n_ctx         = 128
0.01.310.008 I llama_context: n_ctx_per_seq = 128
0.01.310.008 I llama_context: n_batch       = 128
0.01.310.008 I llama_context: n_ubatch      = 128
0.01.310.008 I llama_context: flash_attn    = 0
0.01.310.009 I llama_context: freq_base     = 10000.0
0.01.310.009 I llama_context: freq_scale    = 1
0.01.310.009 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.310.010 I ggml_metal_init: allocating
0.01.310.037 I ggml_metal_init: found device: Apple M4
0.01.310.043 I ggml_metal_init: picking default device: Apple M4
0.01.310.694 I ggml_metal_init: using embedded metal library
0.01.314.367 I ggml_metal_init: GPU name:   Apple M4
0.01.314.368 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.314.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.314.369 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.314.370 I ggml_metal_init: simdgroup reduction   = true
0.01.314.370 I ggml_metal_init: simdgroup matrix mul. = true
0.01.314.370 I ggml_metal_init: has residency sets    = true
0.01.314.370 I ggml_metal_init: has bfloat            = true
0.01.314.370 I ggml_metal_init: use bfloat            = true
0.01.314.371 I ggml_metal_init: hasUnifiedMemory      = true
0.01.314.373 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.324.393 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.324.394 I llama_context_kv_self: constructing llama_context_kv_self
0.01.324.395 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.326.010 I init:      Metal KV buffer size =    24.00 MiB
0.01.326.012 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.327.519 I init:      Metal compute buffer size =    25.56 MiB
0.01.327.520 I init:        CPU compute buffer size =     1.06 MiB
0.01.327.520 I init: graph nodes  = 967
0.01.327.521 I init: graph splits = 2
0.01.327.523 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.327.523 I 
0.01.327.552 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.327.554 I compute_imatrix: tokenizing the input ..
0.01.331.675 I compute_imatrix: tokenization took 4.12 ms
0.01.331.677 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.618.095 I compute_imatrix: 0.29 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.621.052 I llama_perf_context_print:        load time =    1596.37 ms
0.01.621.053 I llama_perf_context_print: prompt eval time =     284.51 ms /   128 tokens (    2.22 ms per token,   449.89 tokens per second)
0.01.621.053 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.621.054 I llama_perf_context_print:       total time =    1599.32 ms /   129 tokens
0.01.621.885 I ggml_metal_free: deallocating

real	0m1.807s
user	0m0.148s
sys	0m0.229s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4801 (f95b04a2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154206230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1542068a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154206d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154207180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1542075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154207a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154208020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1542085d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154208b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154209080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154209580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154209a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15420a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15420ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15420b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15420bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15420c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15420cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15420d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15420d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15420e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15420e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15420ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15420f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15420fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154210190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1542107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154211410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154211950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154211c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1542120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154212370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154212c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154213140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154213400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1542138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154213d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1542141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154214680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154214b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154214fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154215460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154215900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154215da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154216060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154216670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154216c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1542175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154217bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1542181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1542187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154218de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1542193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x154219a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15421a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15421a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15421ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15421adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15421b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15421bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15421beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15421c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15421c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15421cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15421d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15421d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15421da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15421df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15421e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15421e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15421ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15421f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15421f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15421fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1542200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154220620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154220b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1542210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154221610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154221b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1542220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154222600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154222b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1542230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1542235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154223b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154224090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1542245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154224b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154225080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1542255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154225b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154226070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1542265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154226b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154227060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1542275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154217290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154227a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1542281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154228720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154228c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1542291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154229710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154229c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15422a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15422a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15422ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15422b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15422b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15422bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15422c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15422c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15422cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15422d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15422d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15422d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15422de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15422e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15422e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15422ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15422f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15422f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15422f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15422fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154230300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1542307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154230c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1542310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154231580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154231a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154231ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154232360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154232800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154232ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154233140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1542335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154233a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154233f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1542343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154234860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154234d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1542351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154235640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154235ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154235f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154236420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1542368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154236d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154237200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1542376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154237b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x154237fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154238480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154238920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154238dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x154239260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154239700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154239ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15423a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15423a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15423a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15423ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15423b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15423b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15423bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15423c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15423c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15423c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15423ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15423d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15423d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15423dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15423e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15423e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15423ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15423eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15423f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15423f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15423fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154240160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154240600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154240aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154240f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1542413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154241880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154241d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1542421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154242660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154242b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154242fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154243440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1542438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154243e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154244380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1542448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154244e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1542450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1542456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154245d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154246310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x154246b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154246fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154247260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154247870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154247e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154248670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x154248b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154248fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154249450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154249c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15424a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15424a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15424abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15424b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15424b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15424bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15424c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15424c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15424cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15424d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15424d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15424dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15424e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15424e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15424ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15424f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15424f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15424fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1542500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154250640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154250b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1542510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154251630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154251b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1542520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154252620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154252b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1542530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154253610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154253b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1542540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154254600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154254b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1542550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1542555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154255b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154256090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1542565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154256b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154257080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1542575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154257b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154258070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1542585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x154258b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154259060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1542595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154259b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15425a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15425a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15425aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15425b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15425b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15425bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15425c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15425c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15425ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15425cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15425d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15425d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15425dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15425e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15425e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15425ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15425ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15425f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15425f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15425fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1542601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154260640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154260ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154261030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154261750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154261e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154262590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154262cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154262f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154263760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154263a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154264030 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
0.00.752.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.752.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136e04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136e051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136e05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136e05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136e05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136e06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136e067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136e06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136e070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136e07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136e079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136e080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136e08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136e09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136e09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136e0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136e0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136e0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136e0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136e0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136e0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136e0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136e0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136e0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136e0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136e0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136e0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136e0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136e0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136e0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136e0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136e0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136e103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136e10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136e10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136e10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136e113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136e11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136e11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136e12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136e12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136e129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136e12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136e132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136e13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136e13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136e14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136e14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136e14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136e14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136e151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136e15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136e15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136e15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136e163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136e16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136e16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136e17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136e176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136e17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e5ea60 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154263ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154247520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1542453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154245fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1542190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154218a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154210450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154216f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154217860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154217e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154216930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154216320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154218480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154209d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154219cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154227ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154263230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x154212630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1542128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1542465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154210a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154210d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154210fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154264490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154264750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154264a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154264cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154264f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154265250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154265510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1542657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154265a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154265d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154266010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1542662d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154266590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154266850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154266b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154266dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154267090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154267350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154267610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1542678d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154267b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154267e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154268110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1542683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154268690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154268950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154268c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x154268ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154269190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x154269450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x154269710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1542699d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154269c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x154269f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15426a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15426a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15426a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15426aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15426ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15426afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15426b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15426b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15426b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15426bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15426bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15426c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15426c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15426c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15426c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15426cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15426ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15426d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15426d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15426d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15426d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15426dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15426de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15426e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15426e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15426e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15426e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15426ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15426ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15426f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15426f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15426f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15426fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15426fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15426ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154270250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154270510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1542707d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154270a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154270d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154271010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1542712d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154271590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154271850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154271b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154271dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154272090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154272350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x154272610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1542728d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154272b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x154272e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154273110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1542733d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154273690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154273950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154273c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154273ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154274190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154274450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154274710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1542749d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154274c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154274f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154275210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1542754d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154275790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154275a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154275d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154275fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154276290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154276550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154276810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154276ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154276d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154277050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154277310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1542775d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154277890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154277b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154277e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1542780d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154278390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154278650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154278910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154278bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154278e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154279150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154279410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1542796d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154279990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154279c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154279f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15427a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15427a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15427a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15427aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15427acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15427af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15427b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15427b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15427b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15427ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15427bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15427c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15427c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15427c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15427c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15427cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15427cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15427d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15427d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15427d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15427d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15427db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15427de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15427e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15427e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15427e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15427e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15427ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15427eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15427f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15427f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15427f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15427f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15427fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15427ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154280210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1542804d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154280790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154280a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154280d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154280fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154281290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154281550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154281810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154281ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154281d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154282050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154282310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1542825d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154282890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154282b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x154282e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1542830d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154283390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154283650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154283910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154283bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x154283e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154284290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154284730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154284ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1542851a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154285460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1542858d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154285d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1542861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154286620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154286a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154286f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154287370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1542877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154287c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1542880c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154288530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1542889a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154288e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154289280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1542896f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154289b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154289fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15428a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15428a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15428ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15428b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15428b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15428ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15428bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15428c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15428c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15428cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15428d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15428d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15428d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15428ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15428e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15428e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15428eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15428efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15428f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15428f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15428fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154290170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1542905e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154290a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154290ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x154291330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1542917a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154291c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154292080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1542924f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154292960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154292dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154293240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1542936b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154293b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154293f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154294400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154294870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154294ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154295150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1542955c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154295a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154295ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154296310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154296780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154296bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154297060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1542974d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154297940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154297db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154298220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154298690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154298b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154299570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154299c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15429a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15429aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15429ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15429b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15429b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15429be50 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.800s
user	0m0.278s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4801 (f95b04a2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f70d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f70dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f70e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f70e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f70ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f70f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f70f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f70ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f710540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f710a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f710f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f711440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f711f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f712710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f712f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f713640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f713d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f714480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f714ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f715370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f715a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f7161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f7168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f717890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f717b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f718160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f718dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f7195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f719a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f719d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f71a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f71ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f71adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f71b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f71b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f71bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f71c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f71c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f71c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f71ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f71d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f71d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f71da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f71e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f71e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f71ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f71f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f71fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f720190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f7207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f720db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f7213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f721bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f722050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f7224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f7227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f7235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f723870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f723d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f7241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f724650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f724af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f724f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f725430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f7258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f725d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f726210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f7266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f726b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f726ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f727540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f727a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f727fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f728530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f728a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f728fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f729520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f729a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f72a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f72aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f72afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f72b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f72ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f72bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f72c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f72ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f72cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f72d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f72da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f72df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f72e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f72ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f72ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f71ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f72f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f72fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f7300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f730630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f730b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f7310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f731620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f731b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f7320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f732610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f732b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f7330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f733600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f733b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f7340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f734540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f7349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f734e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f735320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f7357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f735c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f736100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f7365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f736a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f736ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f737380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f737820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f737cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f738160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f738600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f738aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f738f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f7393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f739880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f739d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f73a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f73a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f73ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f73afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f73b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f73b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f73bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f73c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f73c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f73cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f73d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f73d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f73d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f73dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f73e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f73e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f73ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f73f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f73f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f73f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f73fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f7402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f740780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f740c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f7410c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f741560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f741a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f741ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f742340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f7427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f742c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f743120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f7435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f743a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f743f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f7443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f744840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f744ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f745180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f745620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f745f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f746400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f7468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f746d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f7471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f747680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f747b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f747fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f748460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f748900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f748da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f749240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f7496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f749b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f74a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f74a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f74a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f74ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f74b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f74b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f74bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f74c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f74c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f74caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f74d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f74d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f74dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f74e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f74e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f74ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f74f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f74f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f750030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f7504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f750970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f750e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f7515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f751b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f752060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f7525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f752b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f753050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f7535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f753af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f754040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f754590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f754ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f755030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f755580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f755ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f756020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f756570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f756ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f757010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f757560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f757ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f758000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f758550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f758aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f758ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f759540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f759a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f759fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f75a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f75aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f75afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f75b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f75ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f75bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f75c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f75ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f75cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f75d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f75da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f75dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f75e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f75ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f75ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f75f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f75fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f75ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f7604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f760a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f760f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f7614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f761a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f761f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f7624b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f762a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f762f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f7634a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f7639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f763f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f7643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f764880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f764d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f7651c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f765660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f765b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f765fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f766440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f7668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f766d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f7676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f767b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f768000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f7684a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f7689f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f769110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f769830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f769f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f76a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f76a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f76b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f76b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f76b9f0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
0.00.100.911 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.916 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120008570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1200089e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120008e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1200092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120009730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120009ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12000a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12000a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12000a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12000ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12000b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12000b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12000c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12000cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12000d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12000db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12000e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12000e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12000f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12000f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12000ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1200106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120010df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120011510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120011c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120011ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1200121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120012620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120012a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120012f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120013400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120013910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120013d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120014040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1200144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120014920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120014e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120015380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120015880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120015d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120016280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120016780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120016c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120017180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120017680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120017af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120017f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1200183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120018840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120018cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120019120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120019590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120019a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120019e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12001a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12001aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12001af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12001b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12001b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12001c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12001c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12001c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12001cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12001d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12001d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12001dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12001e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12001e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12001e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12001ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12001f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12001f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12001fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120020180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1200206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120020c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120021170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1200216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120022160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1200226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120022c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120023150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1200236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120023bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120024140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120024690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120024be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120025130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120025680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120026120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120026670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120026bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120027110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120027660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120028100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120028650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120028ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1200290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120029640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120029b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12002a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12002a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12002ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12002b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12002b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12002bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12002c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12002c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12002cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12002d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12002d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12002d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12002de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12002e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12002e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12002ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12002f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12002f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12002fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12002fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120030390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120030830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120030cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120031170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120031610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120031ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120031f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1200323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120032890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1200331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120033670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120033b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120033fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120034450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1200348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120034d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120035230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1200356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120035b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120036010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1200364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120036950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120036df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120037290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120037730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120037bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120038070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1200389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120038e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1200392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120039790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120039c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12003a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12003a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12003aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12003aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12003b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12003b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12003bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12003c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12003c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12003ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12003cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12003d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12003d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12003dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12003e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12003e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12003ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12003ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12003f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12003f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12003fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1200401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120040690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120040b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120040fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120041470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120041910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120041db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120042250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1200426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120042b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120043030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1200434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120043970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120043e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1200442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120044800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120044d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1200452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1200457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120045ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1200460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1200466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120046ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1200474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120047970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120047c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120048240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120048850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120049040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1200494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120049980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120049e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12004a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12004ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12004b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12004b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12004bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12004c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12004c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12004cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12004d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12004d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12004daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12004e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12004e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12004eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12004f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12004f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12004fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120050020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120050570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120050ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120051010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120051560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120051ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120052000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120052550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120052aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120052ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120053540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120053a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120053fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120054530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120054a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120054fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120055520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120055a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120055fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120056510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120056a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120056fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120057500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120057a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120057fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1200584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120058a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120058f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1200594e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120059a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120059f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12005a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12005aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12005af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12005b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12005ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12005bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12005c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12005ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12005cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12005d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12005d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12005dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12005e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12005e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12005eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12005efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12005f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12005f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12005fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120060230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1200606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120060b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120061010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1200614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120061a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120062120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120062840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120062f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120063680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120063940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120064130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1200643f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120064a00 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f7046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f704b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f704fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f705430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f7058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f705d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f706180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f7065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f706a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f706ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f707340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f707a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f708580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f708d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f709540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f709c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f70a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f70aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f70b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f70b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f70c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f70c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f70ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f70d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f70dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f70df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f70e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f70e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f70eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f70ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f70f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f70f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f70fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f710030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f7104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f710d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f7111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f711660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f711ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f711f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f7123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f712820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f712c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f713100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f713570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f7139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f713e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f7142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f714ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f715010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f7158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f715d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f7161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f716c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f7170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f717520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f717990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f717e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f718270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f7186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f718b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f718fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f719430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f7198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f719d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f71a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f71a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f71aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f71aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f71b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f71b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f71bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f71c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f71c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f71c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f71cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f71d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f71d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f71db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f71dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f71e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f71e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f71ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f71f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f71f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f71fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f71feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f720790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f721070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f7214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f721950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f721dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f722230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f7226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f722b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f722f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f7233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f723860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f7241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f724490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f724900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f724d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f7251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f725650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f725ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f725f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f7263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f726810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f726c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f7270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f727560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f7279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f727e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f7282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f728b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f729000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f729470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f7298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f729d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f72a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f72a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f72aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f72af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f72b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f72b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f72bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f72c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f72c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f72c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f72ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f72d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f72d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f72db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f72dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f72e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f72e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f72ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f72f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f72f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f72fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f72fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f730360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f7307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f730c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f7310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f731520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f731990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f731e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f732270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f7326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f732b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f732fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f733430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f7338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f733d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f734180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f7345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f734ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f735340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f7357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f735c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f736500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f736970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f736de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f7376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f737b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f737fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f738410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f738880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f738cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f739160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f7395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f739a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f739eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f73a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f73a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f73ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f73b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f73b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f73b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f73bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f73c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f73c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f73cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f73cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f73d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f73d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f73dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f73e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f73e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f73ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f73ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f73f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f73f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f73fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f740050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f7404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f740a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f740ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f741330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f741e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f742400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f742870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f742ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f743150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f7435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f743a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f743ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f744310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f744780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f744bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f745060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f7454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f745940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f745db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f746690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f746b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f746f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f7473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f747850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f747cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f7485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f748a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f7492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f749760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f749bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f74a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f74a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f74a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f74ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f74b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f74b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f74bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f74bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f74c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f74c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f74cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f74d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f74d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f74d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f74de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f74e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f74e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f74ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f74f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f74f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f74f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f74fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f7501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f750650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f750ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f750f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f7513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f751810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f751c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f7520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f752560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f7529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f752e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f7532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f753720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f753b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f754000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f754470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f7548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f754d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f7551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f755630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f755aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f756510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f756c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f757350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f757a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f757d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f7581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f7587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f758db0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.232s
sys	0m0.164s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
