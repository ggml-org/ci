### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.62 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.64 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.26 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.30 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.30 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.12 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.63 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.94 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.16 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.77 sec*proc (28 tests)

Total Test time (real) = 221.78 sec

real	3m41.810s
user	7m38.189s
sys	0m6.606s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.29 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.10 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.89 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.06 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.20 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.54 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   27.86 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.27 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.15 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  49.99 sec*proc (28 tests)

Total Test time (real) =  50.01 sec

real	0m50.017s
user	1m12.459s
sys	0m4.872s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.094 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.683 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.662 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.677 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.679 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.680 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.680 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.681 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.683 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.684 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.685 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.686 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.686 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.691 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.692 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.693 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.694 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.694 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.695 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.695 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.327 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.645 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.647 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.648 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.648 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.649 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.649 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.032.649 I llama_model_loader: - type  f32:  124 tensors
0.00.032.650 I llama_model_loader: - type  f16:   73 tensors
0.00.032.650 I print_info: file format = GGUF V3 (latest)
0.00.032.652 I print_info: file type   = F16
0.00.032.653 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.035.614 I load: special tokens cache size = 5
0.00.036.968 I load: token to piece cache size = 0.2032 MB
0.00.036.973 I print_info: arch             = bert
0.00.036.974 I print_info: vocab_only       = 0
0.00.036.974 I print_info: n_ctx_train      = 512
0.00.036.974 I print_info: n_embd           = 384
0.00.036.977 I print_info: n_layer          = 12
0.00.036.980 I print_info: n_head           = 12
0.00.036.981 I print_info: n_head_kv        = 12
0.00.036.981 I print_info: n_rot            = 32
0.00.036.981 I print_info: n_swa            = 0
0.00.036.981 I print_info: n_embd_head_k    = 32
0.00.036.983 I print_info: n_embd_head_v    = 32
0.00.036.984 I print_info: n_gqa            = 1
0.00.036.985 I print_info: n_embd_k_gqa     = 384
0.00.036.986 I print_info: n_embd_v_gqa     = 384
0.00.036.987 I print_info: f_norm_eps       = 1.0e-12
0.00.036.988 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.036.988 I print_info: f_clamp_kqv      = 0.0e+00
0.00.036.988 I print_info: f_max_alibi_bias = 0.0e+00
0.00.036.988 I print_info: f_logit_scale    = 0.0e+00
0.00.036.989 I print_info: n_ff             = 1536
0.00.036.989 I print_info: n_expert         = 0
0.00.036.989 I print_info: n_expert_used    = 0
0.00.036.990 I print_info: causal attn      = 0
0.00.036.990 I print_info: pooling type     = 2
0.00.036.991 I print_info: rope type        = 2
0.00.036.992 I print_info: rope scaling     = linear
0.00.036.992 I print_info: freq_base_train  = 10000.0
0.00.036.992 I print_info: freq_scale_train = 1
0.00.036.993 I print_info: n_ctx_orig_yarn  = 512
0.00.036.993 I print_info: rope_finetuned   = unknown
0.00.036.993 I print_info: ssm_d_conv       = 0
0.00.036.993 I print_info: ssm_d_inner      = 0
0.00.036.993 I print_info: ssm_d_state      = 0
0.00.036.994 I print_info: ssm_dt_rank      = 0
0.00.036.994 I print_info: ssm_dt_b_c_rms   = 0
0.00.036.994 I print_info: model type       = 33M
0.00.036.994 I print_info: model params     = 33.21 M
0.00.036.994 I print_info: general.name     = Bge Small
0.00.036.995 I print_info: vocab type       = WPM
0.00.036.995 I print_info: n_vocab          = 30522
0.00.036.996 I print_info: n_merges         = 0
0.00.036.996 I print_info: BOS token        = 101 '[CLS]'
0.00.036.996 I print_info: UNK token        = 100 '[UNK]'
0.00.036.996 I print_info: SEP token        = 102 '[SEP]'
0.00.036.996 I print_info: PAD token        = 0 '[PAD]'
0.00.036.997 I print_info: MASK token       = 103 '[MASK]'
0.00.036.997 I print_info: LF token         = 0 '[PAD]'
0.00.036.997 I print_info: max token length = 21
0.00.038.564 I load_tensors: offloading 12 repeating layers to GPU
0.00.038.565 I load_tensors: offloading output layer to GPU
0.00.038.565 I load_tensors: offloaded 13/13 layers to GPU
0.00.038.586 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.587 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.038.787 I llama_init_from_model: n_seq_max     = 1
0.00.038.788 I llama_init_from_model: n_ctx         = 512
0.00.038.788 I llama_init_from_model: n_ctx_per_seq = 512
0.00.038.788 I llama_init_from_model: n_batch       = 2048
0.00.038.788 I llama_init_from_model: n_ubatch      = 2048
0.00.038.789 I llama_init_from_model: flash_attn    = 0
0.00.038.789 I llama_init_from_model: freq_base     = 10000.0
0.00.038.789 I llama_init_from_model: freq_scale    = 1
0.00.038.790 I ggml_metal_init: allocating
0.00.038.793 I ggml_metal_init: found device: Apple M4
0.00.038.795 I ggml_metal_init: picking default device: Apple M4
0.00.039.544 I ggml_metal_init: using embedded metal library
0.00.042.650 I ggml_metal_init: GPU name:   Apple M4
0.00.042.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.654 I ggml_metal_init: simdgroup reduction   = true
0.00.042.654 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.654 I ggml_metal_init: has bfloat            = true
0.00.042.654 I ggml_metal_init: use bfloat            = true
0.00.042.655 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.655 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.233 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.763 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.766 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.767 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.354 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.355 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.355 I llama_init_from_model: graph nodes  = 429
0.00.053.355 I llama_init_from_model: graph splits = 2
0.00.053.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.361 I 
0.00.058.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.959 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.063.098 I llama_perf_context_print:        load time =      37.67 ms
0.00.063.099 I llama_perf_context_print: prompt eval time =       4.00 ms /     9 tokens (    0.44 ms per token,  2250.00 tokens per second)
0.00.063.107 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.063.107 I llama_perf_context_print:       total time =       4.73 ms /    10 tokens
0.00.063.313 I ggml_metal_free: deallocating

real	0m0.242s
user	0m0.044s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.716 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.112 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.116 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.118 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.120 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.121 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.121 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.122 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.122 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.123 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.123 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.124 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.126 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.126 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.127 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.127 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.127 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.128 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.222 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.824 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.825 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.826 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.826 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.826 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.827 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.013.827 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.013.828 I llama_model_loader: - type  f32:  124 tensors
0.00.013.828 I llama_model_loader: - type q8_0:   73 tensors
0.00.013.828 I print_info: file format = GGUF V3 (latest)
0.00.013.829 I print_info: file type   = Q8_0
0.00.013.830 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.051 I load: special tokens cache size = 5
0.00.017.221 I load: token to piece cache size = 0.2032 MB
0.00.017.224 I print_info: arch             = bert
0.00.017.224 I print_info: vocab_only       = 0
0.00.017.224 I print_info: n_ctx_train      = 512
0.00.017.224 I print_info: n_embd           = 384
0.00.017.225 I print_info: n_layer          = 12
0.00.017.227 I print_info: n_head           = 12
0.00.017.228 I print_info: n_head_kv        = 12
0.00.017.228 I print_info: n_rot            = 32
0.00.017.228 I print_info: n_swa            = 0
0.00.017.229 I print_info: n_embd_head_k    = 32
0.00.017.229 I print_info: n_embd_head_v    = 32
0.00.017.229 I print_info: n_gqa            = 1
0.00.017.230 I print_info: n_embd_k_gqa     = 384
0.00.017.231 I print_info: n_embd_v_gqa     = 384
0.00.017.231 I print_info: f_norm_eps       = 1.0e-12
0.00.017.232 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.232 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.232 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.232 I print_info: f_logit_scale    = 0.0e+00
0.00.017.233 I print_info: n_ff             = 1536
0.00.017.233 I print_info: n_expert         = 0
0.00.017.233 I print_info: n_expert_used    = 0
0.00.017.233 I print_info: causal attn      = 0
0.00.017.234 I print_info: pooling type     = 2
0.00.017.234 I print_info: rope type        = 2
0.00.017.234 I print_info: rope scaling     = linear
0.00.017.234 I print_info: freq_base_train  = 10000.0
0.00.017.235 I print_info: freq_scale_train = 1
0.00.017.235 I print_info: n_ctx_orig_yarn  = 512
0.00.017.235 I print_info: rope_finetuned   = unknown
0.00.017.235 I print_info: ssm_d_conv       = 0
0.00.017.235 I print_info: ssm_d_inner      = 0
0.00.017.236 I print_info: ssm_d_state      = 0
0.00.017.236 I print_info: ssm_dt_rank      = 0
0.00.017.236 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.236 I print_info: model type       = 33M
0.00.017.239 I print_info: model params     = 33.21 M
0.00.017.240 I print_info: general.name     = Bge Small
0.00.017.240 I print_info: vocab type       = WPM
0.00.017.241 I print_info: n_vocab          = 30522
0.00.017.241 I print_info: n_merges         = 0
0.00.017.241 I print_info: BOS token        = 101 '[CLS]'
0.00.017.241 I print_info: UNK token        = 100 '[UNK]'
0.00.017.241 I print_info: SEP token        = 102 '[SEP]'
0.00.017.242 I print_info: PAD token        = 0 '[PAD]'
0.00.017.242 I print_info: MASK token       = 103 '[MASK]'
0.00.017.242 I print_info: LF token         = 0 '[PAD]'
0.00.017.242 I print_info: max token length = 21
0.00.018.401 I load_tensors: offloading 12 repeating layers to GPU
0.00.018.401 I load_tensors: offloading output layer to GPU
0.00.018.401 I load_tensors: offloaded 13/13 layers to GPU
0.00.018.411 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.413 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.018.551 I llama_init_from_model: n_seq_max     = 1
0.00.018.551 I llama_init_from_model: n_ctx         = 512
0.00.018.552 I llama_init_from_model: n_ctx_per_seq = 512
0.00.018.552 I llama_init_from_model: n_batch       = 2048
0.00.018.552 I llama_init_from_model: n_ubatch      = 2048
0.00.018.552 I llama_init_from_model: flash_attn    = 0
0.00.018.553 I llama_init_from_model: freq_base     = 10000.0
0.00.018.553 I llama_init_from_model: freq_scale    = 1
0.00.018.553 I ggml_metal_init: allocating
0.00.018.556 I ggml_metal_init: found device: Apple M4
0.00.018.558 I ggml_metal_init: picking default device: Apple M4
0.00.019.180 I ggml_metal_init: using embedded metal library
0.00.021.507 I ggml_metal_init: GPU name:   Apple M4
0.00.021.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.510 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.510 I ggml_metal_init: simdgroup reduction   = true
0.00.021.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.510 I ggml_metal_init: has bfloat            = true
0.00.021.510 I ggml_metal_init: use bfloat            = true
0.00.021.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.659 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.032.145 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.147 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.149 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.032.834 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.032.835 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.032.835 I llama_init_from_model: graph nodes  = 429
0.00.032.835 I llama_init_from_model: graph splits = 2
0.00.032.837 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.032.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.910 I 
0.00.037.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.504 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.989 I llama_perf_context_print:        load time =      29.19 ms
0.00.042.990 I llama_perf_context_print: prompt eval time =       4.36 ms /     9 tokens (    0.48 ms per token,  2065.64 tokens per second)
0.00.042.990 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.991 I llama_perf_context_print:       total time =       5.08 ms /    10 tokens
0.00.043.165 I ggml_metal_free: deallocating

real	0m0.060s
user	0m0.028s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.211 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.021 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.294 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.300 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.302 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.312 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.313 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.314 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.315 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.316 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.317 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.317 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.318 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.322 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.322 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.323 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.327 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.327 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.952 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.610 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.612 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.612 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.612 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.613 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.613 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.613 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.614 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.052.614 I llama_model_loader: - type  f32:   40 tensors
0.00.052.619 I llama_model_loader: - type  f16:   30 tensors
0.00.052.620 I print_info: file format = GGUF V3 (latest)
0.00.052.621 I print_info: file type   = F16
0.00.052.623 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.069.247 W load: empty token at index 5
0.00.073.899 W load: model vocab missing newline token, using special_pad_id instead
0.00.075.246 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.278 I load: special tokens cache size = 5
0.00.338.061 I load: token to piece cache size = 1.5060 MB
0.00.338.066 I print_info: arch             = jina-bert-v2
0.00.338.066 I print_info: vocab_only       = 0
0.00.338.066 I print_info: n_ctx_train      = 8192
0.00.338.067 I print_info: n_embd           = 384
0.00.338.067 I print_info: n_layer          = 4
0.00.338.071 I print_info: n_head           = 12
0.00.338.072 I print_info: n_head_kv        = 12
0.00.338.072 I print_info: n_rot            = 32
0.00.338.072 I print_info: n_swa            = 0
0.00.338.072 I print_info: n_embd_head_k    = 32
0.00.338.072 I print_info: n_embd_head_v    = 32
0.00.338.073 I print_info: n_gqa            = 1
0.00.338.073 I print_info: n_embd_k_gqa     = 384
0.00.338.074 I print_info: n_embd_v_gqa     = 384
0.00.338.074 I print_info: f_norm_eps       = 1.0e-12
0.00.338.075 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.338.075 I print_info: f_clamp_kqv      = 0.0e+00
0.00.338.075 I print_info: f_max_alibi_bias = 8.0e+00
0.00.338.076 I print_info: f_logit_scale    = 0.0e+00
0.00.338.076 I print_info: n_ff             = 1536
0.00.338.076 I print_info: n_expert         = 0
0.00.338.076 I print_info: n_expert_used    = 0
0.00.338.076 I print_info: causal attn      = 0
0.00.338.077 I print_info: pooling type     = -1
0.00.338.077 I print_info: rope type        = -1
0.00.338.077 I print_info: rope scaling     = linear
0.00.338.077 I print_info: freq_base_train  = 10000.0
0.00.338.077 I print_info: freq_scale_train = 1
0.00.338.079 I print_info: n_ctx_orig_yarn  = 8192
0.00.338.080 I print_info: rope_finetuned   = unknown
0.00.338.080 I print_info: ssm_d_conv       = 0
0.00.338.080 I print_info: ssm_d_inner      = 0
0.00.338.081 I print_info: ssm_d_state      = 0
0.00.338.081 I print_info: ssm_dt_rank      = 0
0.00.338.081 I print_info: ssm_dt_b_c_rms   = 0
0.00.338.081 I print_info: model type       = 33M
0.00.338.082 I print_info: model params     = 32.90 M
0.00.338.082 I print_info: general.name     = Jina Bert Implementation
0.00.338.083 I print_info: vocab type       = BPE
0.00.338.083 I print_info: n_vocab          = 61056
0.00.338.083 I print_info: n_merges         = 39382
0.00.338.083 I print_info: BOS token        = 0 '<s>'
0.00.338.084 I print_info: EOS token        = 2 '</s>'
0.00.338.084 I print_info: UNK token        = 3 '<unk>'
0.00.338.084 I print_info: SEP token        = 2 '</s>'
0.00.338.084 I print_info: PAD token        = 1 '<pad>'
0.00.338.084 I print_info: MASK token       = 4 '<mask>'
0.00.338.085 I print_info: EOG token        = 2 '</s>'
0.00.338.085 I print_info: max token length = 45
0.00.339.135 I load_tensors: offloading 4 repeating layers to GPU
0.00.339.135 I load_tensors: offloading output layer to GPU
0.00.339.135 I load_tensors: offloaded 5/5 layers to GPU
0.00.339.158 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.339.160 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.339.505 I llama_init_from_model: n_seq_max     = 1
0.00.339.506 I llama_init_from_model: n_ctx         = 8192
0.00.339.506 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.339.506 I llama_init_from_model: n_batch       = 2048
0.00.339.506 I llama_init_from_model: n_ubatch      = 2048
0.00.339.506 I llama_init_from_model: flash_attn    = 0
0.00.339.507 I llama_init_from_model: freq_base     = 10000.0
0.00.339.507 I llama_init_from_model: freq_scale    = 1
0.00.339.508 I ggml_metal_init: allocating
0.00.339.510 I ggml_metal_init: found device: Apple M4
0.00.339.512 I ggml_metal_init: picking default device: Apple M4
0.00.340.302 I ggml_metal_init: using embedded metal library
0.00.343.174 I ggml_metal_init: GPU name:   Apple M4
0.00.343.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.177 I ggml_metal_init: simdgroup reduction   = true
0.00.343.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.177 I ggml_metal_init: has bfloat            = true
0.00.343.177 I ggml_metal_init: use bfloat            = true
0.00.343.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.178 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.352.607 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.355.156 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.355.159 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.355.162 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.355.808 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.355.809 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.355.809 I llama_init_from_model: graph nodes  = 154
0.00.355.810 I llama_init_from_model: graph splits = 2
0.00.355.811 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.355.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.106 I 
0.00.368.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.368.372 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.368.373 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.368.382 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.368.383 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.368.387 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.368.387 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.368.886 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.372.680 I llama_perf_context_print:        load time =     344.08 ms
0.00.372.681 I llama_perf_context_print: prompt eval time =       3.79 ms /    62 tokens (    0.06 ms per token, 16376.12 tokens per second)
0.00.372.682 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.372.683 I llama_perf_context_print:       total time =       4.57 ms /    63 tokens
0.00.372.904 I ggml_metal_free: deallocating

real	0m1.090s
user	0m0.343s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.201 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.309 I main: llama backend init
0.00.000.315 I main: load the model and apply lora adapter, if any
0.00.036.092 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.049.001 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.049.018 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.049.021 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.049.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.049.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.049.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.049.023 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.049.027 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.049.028 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.049.028 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.049.029 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.049.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.049.030 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.049.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.049.036 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.049.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.049.037 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.059.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.067.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.601 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.602 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.603 I llama_model_loader: - type  f32:  194 tensors
0.00.067.603 I llama_model_loader: - type  f16:   98 tensors
0.00.067.604 I print_info: file format = GGUF V3 (latest)
0.00.067.606 I print_info: file type   = all F32 (guessed)
0.00.067.608 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.097.755 I load: special tokens cache size = 25
0.00.104.786 I load: token to piece cache size = 0.2984 MB
0.00.104.789 I print_info: arch             = gptneox
0.00.104.789 I print_info: vocab_only       = 0
0.00.104.789 I print_info: n_ctx_train      = 2048
0.00.104.790 I print_info: n_embd           = 2048
0.00.104.790 I print_info: n_layer          = 24
0.00.104.795 I print_info: n_head           = 16
0.00.104.795 I print_info: n_head_kv        = 16
0.00.104.795 I print_info: n_rot            = 32
0.00.104.796 I print_info: n_swa            = 0
0.00.104.796 I print_info: n_embd_head_k    = 128
0.00.104.796 I print_info: n_embd_head_v    = 128
0.00.104.797 I print_info: n_gqa            = 1
0.00.104.797 I print_info: n_embd_k_gqa     = 2048
0.00.104.798 I print_info: n_embd_v_gqa     = 2048
0.00.104.799 I print_info: f_norm_eps       = 1.0e-05
0.00.104.799 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.104.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.104.801 I print_info: f_max_alibi_bias = 0.0e+00
0.00.104.801 I print_info: f_logit_scale    = 0.0e+00
0.00.104.802 I print_info: n_ff             = 8192
0.00.104.802 I print_info: n_expert         = 0
0.00.104.803 I print_info: n_expert_used    = 0
0.00.104.804 I print_info: causal attn      = 1
0.00.104.804 I print_info: pooling type     = 0
0.00.104.804 I print_info: rope type        = 2
0.00.104.804 I print_info: rope scaling     = linear
0.00.104.804 I print_info: freq_base_train  = 10000.0
0.00.104.805 I print_info: freq_scale_train = 1
0.00.104.805 I print_info: n_ctx_orig_yarn  = 2048
0.00.104.805 I print_info: rope_finetuned   = unknown
0.00.104.805 I print_info: ssm_d_conv       = 0
0.00.104.805 I print_info: ssm_d_inner      = 0
0.00.104.805 I print_info: ssm_d_state      = 0
0.00.104.806 I print_info: ssm_dt_rank      = 0
0.00.104.806 I print_info: ssm_dt_b_c_rms   = 0
0.00.104.806 I print_info: model type       = 1.4B
0.00.104.806 I print_info: model params     = 1.41 B
0.00.104.806 I print_info: general.name     = 1.4B
0.00.104.807 I print_info: vocab type       = BPE
0.00.104.807 I print_info: n_vocab          = 50304
0.00.104.807 I print_info: n_merges         = 50009
0.00.104.807 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.104.811 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.104.811 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.104.811 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.812 I print_info: LF token         = 128 'Ä'
0.00.104.812 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.812 I print_info: max token length = 1024
0.00.106.735 I load_tensors: offloading 24 repeating layers to GPU
0.00.106.735 I load_tensors: offloading output layer to GPU
0.00.106.736 I load_tensors: offloaded 25/25 layers to GPU
0.00.106.754 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.106.755 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.107.075 I llama_init_from_model: n_seq_max     = 1
0.00.107.076 I llama_init_from_model: n_ctx         = 2048
0.00.107.076 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.107.076 I llama_init_from_model: n_batch       = 2048
0.00.107.076 I llama_init_from_model: n_ubatch      = 512
0.00.107.077 I llama_init_from_model: flash_attn    = 0
0.00.107.077 I llama_init_from_model: freq_base     = 10000.0
0.00.107.077 I llama_init_from_model: freq_scale    = 1
0.00.107.078 I ggml_metal_init: allocating
0.00.107.081 I ggml_metal_init: found device: Apple M4
0.00.107.083 I ggml_metal_init: picking default device: Apple M4
0.00.107.764 I ggml_metal_init: using embedded metal library
0.00.117.741 I ggml_metal_init: GPU name:   Apple M4
0.00.117.742 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.117.743 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.117.743 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.117.744 I ggml_metal_init: simdgroup reduction   = true
0.00.117.744 I ggml_metal_init: simdgroup matrix mul. = true
0.00.117.744 I ggml_metal_init: has bfloat            = true
0.00.117.744 I ggml_metal_init: use bfloat            = true
0.00.117.744 I ggml_metal_init: hasUnifiedMemory      = true
0.00.117.745 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.141.715 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.163.036 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.163.043 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.163.067 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.164.007 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.164.008 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.164.008 I llama_init_from_model: graph nodes  = 967
0.00.164.009 I llama_init_from_model: graph splits = 2
0.00.164.012 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.164.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.164.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.155 I main: llama threadpool init, n_threads = 4
0.00.243.195 I 
0.00.243.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.243.215 I 
0.00.243.283 I sampler seed: 1234
0.00.243.288 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.243.313 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.243.314 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.243.314 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.078.993 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.02.078.994 I llama_perf_context_print:        load time =     207.05 ms
0.02.078.995 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.25 tokens per second)
0.02.078.996 I llama_perf_context_print:        eval time =    1789.09 ms /    63 runs   (   28.40 ms per token,    35.21 tokens per second)
0.02.078.996 I llama_perf_context_print:       total time =    1835.84 ms /    70 tokens
0.02.079.215 I ggml_metal_free: deallocating

real	0m2.391s
user	0m0.144s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.567 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.898 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.612 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.625 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.628 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.635 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.635 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.639 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.639 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.736 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.171 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.175 I llama_model_loader: - type  f32:  194 tensors
0.00.058.176 I llama_model_loader: - type  f16:   98 tensors
0.00.058.177 I print_info: file format = GGUF V3 (latest)
0.00.058.178 I print_info: file type   = all F32 (guessed)
0.00.058.180 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.641 I load: special tokens cache size = 25
0.00.092.179 I load: token to piece cache size = 0.2984 MB
0.00.092.181 I print_info: arch             = gptneox
0.00.092.182 I print_info: vocab_only       = 0
0.00.092.182 I print_info: n_ctx_train      = 2048
0.00.092.182 I print_info: n_embd           = 2048
0.00.092.182 I print_info: n_layer          = 24
0.00.092.185 I print_info: n_head           = 16
0.00.092.186 I print_info: n_head_kv        = 16
0.00.092.186 I print_info: n_rot            = 32
0.00.092.186 I print_info: n_swa            = 0
0.00.092.186 I print_info: n_embd_head_k    = 128
0.00.092.187 I print_info: n_embd_head_v    = 128
0.00.092.187 I print_info: n_gqa            = 1
0.00.092.188 I print_info: n_embd_k_gqa     = 2048
0.00.092.189 I print_info: n_embd_v_gqa     = 2048
0.00.092.189 I print_info: f_norm_eps       = 1.0e-05
0.00.092.189 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.190 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.190 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.190 I print_info: f_logit_scale    = 0.0e+00
0.00.092.190 I print_info: n_ff             = 8192
0.00.092.191 I print_info: n_expert         = 0
0.00.092.191 I print_info: n_expert_used    = 0
0.00.092.191 I print_info: causal attn      = 1
0.00.092.193 I print_info: pooling type     = 0
0.00.092.193 I print_info: rope type        = 2
0.00.092.193 I print_info: rope scaling     = linear
0.00.092.194 I print_info: freq_base_train  = 10000.0
0.00.092.194 I print_info: freq_scale_train = 1
0.00.092.194 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.194 I print_info: rope_finetuned   = unknown
0.00.092.194 I print_info: ssm_d_conv       = 0
0.00.092.195 I print_info: ssm_d_inner      = 0
0.00.092.195 I print_info: ssm_d_state      = 0
0.00.092.195 I print_info: ssm_dt_rank      = 0
0.00.092.195 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.195 I print_info: model type       = 1.4B
0.00.092.196 I print_info: model params     = 1.41 B
0.00.092.197 I print_info: general.name     = 1.4B
0.00.092.197 I print_info: vocab type       = BPE
0.00.092.197 I print_info: n_vocab          = 50304
0.00.092.197 I print_info: n_merges         = 50009
0.00.092.197 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.198 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.198 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.198 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.198 I print_info: LF token         = 128 'Ä'
0.00.092.198 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.202 I print_info: max token length = 1024
0.00.094.175 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.175 I load_tensors: offloading output layer to GPU
0.00.094.175 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.181 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.182 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.462 I llama_init_from_model: n_seq_max     = 1
0.00.094.463 I llama_init_from_model: n_ctx         = 128
0.00.094.464 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.464 I llama_init_from_model: n_batch       = 128
0.00.094.464 I llama_init_from_model: n_ubatch      = 128
0.00.094.464 I llama_init_from_model: flash_attn    = 0
0.00.094.465 I llama_init_from_model: freq_base     = 10000.0
0.00.094.465 I llama_init_from_model: freq_scale    = 1
0.00.094.465 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.466 I ggml_metal_init: allocating
0.00.094.469 I ggml_metal_init: found device: Apple M4
0.00.094.471 I ggml_metal_init: picking default device: Apple M4
0.00.095.074 I ggml_metal_init: using embedded metal library
0.00.097.630 I ggml_metal_init: GPU name:   Apple M4
0.00.097.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.633 I ggml_metal_init: simdgroup reduction   = true
0.00.097.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.633 I ggml_metal_init: has bfloat            = true
0.00.097.633 I ggml_metal_init: use bfloat            = true
0.00.097.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.634 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.271 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.594 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.596 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.609 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.495 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.109.496 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.109.496 I llama_init_from_model: graph nodes  = 967
0.00.109.496 I llama_init_from_model: graph splits = 2
0.00.109.498 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.498 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.937.973 I 
0.00.938.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.938.016 I perplexity: tokenizing the input ..
0.00.949.976 I perplexity: tokenization took 11.957 ms
0.00.949.981 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.072.299 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.075.250 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.075.334 I llama_perf_context_print:        load time =     911.06 ms
0.01.075.336 I llama_perf_context_print: prompt eval time =     121.92 ms /   128 tokens (    0.95 ms per token,  1049.88 tokens per second)
0.01.075.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.075.341 I llama_perf_context_print:       total time =     137.36 ms /   129 tokens
0.01.076.648 I ggml_metal_free: deallocating

real	0m1.270s
user	0m0.125s
sys	0m0.198s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.767 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.209 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.210 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.212 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.832 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.832 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.833 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.833 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.834 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.834 I llama_model_loader: - type  f32:  194 tensors
0.00.027.835 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.835 I print_info: file format = GGUF V3 (latest)
0.00.027.836 I print_info: file type   = Q8_0
0.00.027.837 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.261 I load: special tokens cache size = 25
0.00.054.333 I load: token to piece cache size = 0.2984 MB
0.00.054.338 I print_info: arch             = gptneox
0.00.054.338 I print_info: vocab_only       = 0
0.00.054.338 I print_info: n_ctx_train      = 2048
0.00.054.338 I print_info: n_embd           = 2048
0.00.054.339 I print_info: n_layer          = 24
0.00.054.344 I print_info: n_head           = 16
0.00.054.345 I print_info: n_head_kv        = 16
0.00.054.345 I print_info: n_rot            = 32
0.00.054.347 I print_info: n_swa            = 0
0.00.054.348 I print_info: n_embd_head_k    = 128
0.00.054.348 I print_info: n_embd_head_v    = 128
0.00.054.348 I print_info: n_gqa            = 1
0.00.054.350 I print_info: n_embd_k_gqa     = 2048
0.00.054.350 I print_info: n_embd_v_gqa     = 2048
0.00.054.351 I print_info: f_norm_eps       = 1.0e-05
0.00.054.351 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.352 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.352 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.352 I print_info: f_logit_scale    = 0.0e+00
0.00.054.353 I print_info: n_ff             = 8192
0.00.054.353 I print_info: n_expert         = 0
0.00.054.353 I print_info: n_expert_used    = 0
0.00.054.356 I print_info: causal attn      = 1
0.00.054.356 I print_info: pooling type     = 0
0.00.054.356 I print_info: rope type        = 2
0.00.054.356 I print_info: rope scaling     = linear
0.00.054.357 I print_info: freq_base_train  = 10000.0
0.00.054.357 I print_info: freq_scale_train = 1
0.00.054.357 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.358 I print_info: rope_finetuned   = unknown
0.00.054.358 I print_info: ssm_d_conv       = 0
0.00.054.358 I print_info: ssm_d_inner      = 0
0.00.054.358 I print_info: ssm_d_state      = 0
0.00.054.358 I print_info: ssm_dt_rank      = 0
0.00.054.358 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.359 I print_info: model type       = 1.4B
0.00.054.359 I print_info: model params     = 1.41 B
0.00.054.359 I print_info: general.name     = 1.4B
0.00.054.360 I print_info: vocab type       = BPE
0.00.054.360 I print_info: n_vocab          = 50304
0.00.054.360 I print_info: n_merges         = 50009
0.00.054.360 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.361 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.365 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.365 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.366 I print_info: LF token         = 128 'Ä'
0.00.054.366 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.366 I print_info: max token length = 1024
0.00.056.844 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.845 I load_tensors: offloading output layer to GPU
0.00.056.845 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.856 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.056.858 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.057.203 I llama_init_from_model: n_seq_max     = 1
0.00.057.203 I llama_init_from_model: n_ctx         = 2048
0.00.057.204 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.057.204 I llama_init_from_model: n_batch       = 2048
0.00.057.204 I llama_init_from_model: n_ubatch      = 512
0.00.057.204 I llama_init_from_model: flash_attn    = 0
0.00.057.204 I llama_init_from_model: freq_base     = 10000.0
0.00.057.205 I llama_init_from_model: freq_scale    = 1
0.00.057.205 I ggml_metal_init: allocating
0.00.057.209 I ggml_metal_init: found device: Apple M4
0.00.057.211 I ggml_metal_init: picking default device: Apple M4
0.00.057.950 I ggml_metal_init: using embedded metal library
0.00.060.535 I ggml_metal_init: GPU name:   Apple M4
0.00.060.537 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.537 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.537 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.538 I ggml_metal_init: simdgroup reduction   = true
0.00.060.538 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.538 I ggml_metal_init: has bfloat            = true
0.00.060.538 I ggml_metal_init: use bfloat            = true
0.00.060.539 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.997 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.016 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.031 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.055 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.375 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.378 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.378 I llama_init_from_model: graph nodes  = 967
0.00.097.379 I llama_init_from_model: graph splits = 2
0.00.097.383 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.511 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.184.677 I main: llama threadpool init, n_threads = 4
0.01.184.710 I 
0.01.184.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.184.731 I 
0.01.185.006 I sampler seed: 1234
0.01.185.011 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.185.022 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.185.022 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.185.023 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.272.293 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.02.272.294 I llama_perf_context_print:        load time =    1174.90 ms
0.02.272.295 I llama_perf_context_print: prompt eval time =      43.06 ms /     7 tokens (    6.15 ms per token,   162.54 tokens per second)
0.02.272.296 I llama_perf_context_print:        eval time =    1041.12 ms /    63 runs   (   16.53 ms per token,    60.51 tokens per second)
0.02.272.296 I llama_perf_context_print:       total time =    1087.62 ms /    70 tokens
0.02.272.523 I ggml_metal_free: deallocating

real	0m2.291s
user	0m0.112s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.129 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.220 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.640 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.651 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.652 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.652 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.653 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.654 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.655 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.655 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.656 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.656 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.657 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.251 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.616 I llama_model_loader: - type  f32:  194 tensors
0.00.035.617 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.617 I print_info: file format = GGUF V3 (latest)
0.00.035.618 I print_info: file type   = Q8_0
0.00.035.619 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.909 I load: special tokens cache size = 25
0.00.065.002 I load: token to piece cache size = 0.2984 MB
0.00.065.005 I print_info: arch             = gptneox
0.00.065.006 I print_info: vocab_only       = 0
0.00.065.006 I print_info: n_ctx_train      = 2048
0.00.065.006 I print_info: n_embd           = 2048
0.00.065.006 I print_info: n_layer          = 24
0.00.065.011 I print_info: n_head           = 16
0.00.065.011 I print_info: n_head_kv        = 16
0.00.065.012 I print_info: n_rot            = 32
0.00.065.012 I print_info: n_swa            = 0
0.00.065.013 I print_info: n_embd_head_k    = 128
0.00.065.013 I print_info: n_embd_head_v    = 128
0.00.065.014 I print_info: n_gqa            = 1
0.00.065.015 I print_info: n_embd_k_gqa     = 2048
0.00.065.017 I print_info: n_embd_v_gqa     = 2048
0.00.065.018 I print_info: f_norm_eps       = 1.0e-05
0.00.065.018 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.019 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.019 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.019 I print_info: f_logit_scale    = 0.0e+00
0.00.065.020 I print_info: n_ff             = 8192
0.00.065.020 I print_info: n_expert         = 0
0.00.065.020 I print_info: n_expert_used    = 0
0.00.065.020 I print_info: causal attn      = 1
0.00.065.021 I print_info: pooling type     = 0
0.00.065.021 I print_info: rope type        = 2
0.00.065.021 I print_info: rope scaling     = linear
0.00.065.021 I print_info: freq_base_train  = 10000.0
0.00.065.022 I print_info: freq_scale_train = 1
0.00.065.023 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.024 I print_info: rope_finetuned   = unknown
0.00.065.024 I print_info: ssm_d_conv       = 0
0.00.065.024 I print_info: ssm_d_inner      = 0
0.00.065.024 I print_info: ssm_d_state      = 0
0.00.065.024 I print_info: ssm_dt_rank      = 0
0.00.065.025 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.025 I print_info: model type       = 1.4B
0.00.065.025 I print_info: model params     = 1.41 B
0.00.065.026 I print_info: general.name     = 1.4B
0.00.065.027 I print_info: vocab type       = BPE
0.00.065.027 I print_info: n_vocab          = 50304
0.00.065.027 I print_info: n_merges         = 50009
0.00.065.027 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.028 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.029 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.029 I print_info: LF token         = 128 'Ä'
0.00.065.029 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.031 I print_info: max token length = 1024
0.00.067.399 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.399 I load_tensors: offloading output layer to GPU
0.00.067.399 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.410 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.412 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.716 I llama_init_from_model: n_seq_max     = 1
0.00.067.716 I llama_init_from_model: n_ctx         = 128
0.00.067.716 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.717 I llama_init_from_model: n_batch       = 128
0.00.067.717 I llama_init_from_model: n_ubatch      = 128
0.00.067.717 I llama_init_from_model: flash_attn    = 0
0.00.067.717 I llama_init_from_model: freq_base     = 10000.0
0.00.067.718 I llama_init_from_model: freq_scale    = 1
0.00.067.718 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.718 I ggml_metal_init: allocating
0.00.067.722 I ggml_metal_init: found device: Apple M4
0.00.067.724 I ggml_metal_init: picking default device: Apple M4
0.00.068.381 I ggml_metal_init: using embedded metal library
0.00.070.973 I ggml_metal_init: GPU name:   Apple M4
0.00.070.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.975 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.975 I ggml_metal_init: simdgroup reduction   = true
0.00.070.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.976 I ggml_metal_init: has bfloat            = true
0.00.070.976 I ggml_metal_init: use bfloat            = true
0.00.070.976 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.324 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.684 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.686 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.704 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.715 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.083.716 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.083.716 I llama_init_from_model: graph nodes  = 967
0.00.083.716 I llama_init_from_model: graph splits = 2
0.00.083.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.891.036 I 
0.00.891.061 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.891.064 I perplexity: tokenizing the input ..
0.00.898.777 I perplexity: tokenization took 7.711 ms
0.00.898.781 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.022.949 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.024.115 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.024.143 I llama_perf_context_print:        load time =     878.81 ms
0.01.024.144 I llama_perf_context_print: prompt eval time =     123.94 ms /   128 tokens (    0.97 ms per token,  1032.74 tokens per second)
0.01.024.145 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.024.145 I llama_perf_context_print:       total time =     133.11 ms /   129 tokens
0.01.024.614 I ggml_metal_free: deallocating

real	0m1.045s
user	0m0.093s
sys	0m0.164s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.015.085 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.020 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.020 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.021 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.022 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.022 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.023 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.023 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.025 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.026 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.026 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.029 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.222 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.224 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.225 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.225 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.226 I llama_model_loader: - type  f32:  194 tensors
0.00.041.226 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.226 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.227 I print_info: file format = GGUF V3 (latest)
0.00.041.228 I print_info: file type   = Q4_0
0.00.041.229 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.067.809 I load: special tokens cache size = 25
0.00.075.749 I load: token to piece cache size = 0.2984 MB
0.00.075.752 I print_info: arch             = gptneox
0.00.075.753 I print_info: vocab_only       = 0
0.00.075.753 I print_info: n_ctx_train      = 2048
0.00.075.753 I print_info: n_embd           = 2048
0.00.075.753 I print_info: n_layer          = 24
0.00.075.758 I print_info: n_head           = 16
0.00.075.759 I print_info: n_head_kv        = 16
0.00.075.759 I print_info: n_rot            = 32
0.00.075.759 I print_info: n_swa            = 0
0.00.075.759 I print_info: n_embd_head_k    = 128
0.00.075.759 I print_info: n_embd_head_v    = 128
0.00.075.760 I print_info: n_gqa            = 1
0.00.075.761 I print_info: n_embd_k_gqa     = 2048
0.00.075.761 I print_info: n_embd_v_gqa     = 2048
0.00.075.762 I print_info: f_norm_eps       = 1.0e-05
0.00.075.763 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.763 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.765 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.765 I print_info: f_logit_scale    = 0.0e+00
0.00.075.766 I print_info: n_ff             = 8192
0.00.075.767 I print_info: n_expert         = 0
0.00.075.767 I print_info: n_expert_used    = 0
0.00.075.767 I print_info: causal attn      = 1
0.00.075.767 I print_info: pooling type     = 0
0.00.075.767 I print_info: rope type        = 2
0.00.075.767 I print_info: rope scaling     = linear
0.00.075.769 I print_info: freq_base_train  = 10000.0
0.00.075.769 I print_info: freq_scale_train = 1
0.00.075.770 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.770 I print_info: rope_finetuned   = unknown
0.00.075.770 I print_info: ssm_d_conv       = 0
0.00.075.770 I print_info: ssm_d_inner      = 0
0.00.075.770 I print_info: ssm_d_state      = 0
0.00.075.770 I print_info: ssm_dt_rank      = 0
0.00.075.771 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.771 I print_info: model type       = 1.4B
0.00.075.771 I print_info: model params     = 1.41 B
0.00.075.771 I print_info: general.name     = 1.4B
0.00.075.772 I print_info: vocab type       = BPE
0.00.075.777 I print_info: n_vocab          = 50304
0.00.075.777 I print_info: n_merges         = 50009
0.00.075.778 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.778 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.779 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.782 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.782 I print_info: LF token         = 128 'Ä'
0.00.075.782 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.783 I print_info: max token length = 1024
0.00.078.537 I load_tensors: offloading 24 repeating layers to GPU
0.00.078.537 I load_tensors: offloading output layer to GPU
0.00.078.537 I load_tensors: offloaded 25/25 layers to GPU
0.00.078.550 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.078.552 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.079.074 I llama_init_from_model: n_seq_max     = 1
0.00.079.075 I llama_init_from_model: n_ctx         = 2048
0.00.079.075 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.079.075 I llama_init_from_model: n_batch       = 2048
0.00.079.075 I llama_init_from_model: n_ubatch      = 512
0.00.079.076 I llama_init_from_model: flash_attn    = 0
0.00.079.076 I llama_init_from_model: freq_base     = 10000.0
0.00.079.077 I llama_init_from_model: freq_scale    = 1
0.00.079.077 I ggml_metal_init: allocating
0.00.079.081 I ggml_metal_init: found device: Apple M4
0.00.079.084 I ggml_metal_init: picking default device: Apple M4
0.00.080.054 I ggml_metal_init: using embedded metal library
0.00.083.693 I ggml_metal_init: GPU name:   Apple M4
0.00.083.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.696 I ggml_metal_init: simdgroup reduction   = true
0.00.083.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.697 I ggml_metal_init: has bfloat            = true
0.00.083.697 I ggml_metal_init: use bfloat            = true
0.00.083.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.710 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.120.869 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.885 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.913 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.122.017 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.122.019 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.122.020 I llama_init_from_model: graph nodes  = 967
0.00.122.020 I llama_init_from_model: graph splits = 2
0.00.122.024 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.122.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.122.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.509 I main: llama threadpool init, n_threads = 4
0.00.751.549 I 
0.00.751.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.576 I 
0.00.751.795 I sampler seed: 1234
0.00.751.800 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.855 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.856 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.857 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.441.682 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.01.441.683 I llama_perf_context_print:        load time =     736.42 ms
0.01.441.684 I llama_perf_context_print: prompt eval time =      44.17 ms /     7 tokens (    6.31 ms per token,   158.48 tokens per second)
0.01.441.684 I llama_perf_context_print:        eval time =     642.63 ms /    63 runs   (   10.20 ms per token,    98.04 tokens per second)
0.01.441.685 I llama_perf_context_print:       total time =     690.18 ms /    70 tokens
0.01.441.950 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.127s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.001 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.152 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.154 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.155 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.156 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.157 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.158 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.158 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.158 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.159 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.160 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.161 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.162 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.162 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.908 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.740 I llama_model_loader: - type  f32:  194 tensors
0.00.025.740 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.740 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.741 I print_info: file format = GGUF V3 (latest)
0.00.025.741 I print_info: file type   = Q4_0
0.00.025.742 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.162 I load: special tokens cache size = 25
0.00.050.952 I load: token to piece cache size = 0.2984 MB
0.00.050.956 I print_info: arch             = gptneox
0.00.050.956 I print_info: vocab_only       = 0
0.00.050.956 I print_info: n_ctx_train      = 2048
0.00.050.957 I print_info: n_embd           = 2048
0.00.050.957 I print_info: n_layer          = 24
0.00.050.961 I print_info: n_head           = 16
0.00.050.961 I print_info: n_head_kv        = 16
0.00.050.962 I print_info: n_rot            = 32
0.00.050.962 I print_info: n_swa            = 0
0.00.050.962 I print_info: n_embd_head_k    = 128
0.00.050.962 I print_info: n_embd_head_v    = 128
0.00.050.963 I print_info: n_gqa            = 1
0.00.050.964 I print_info: n_embd_k_gqa     = 2048
0.00.050.964 I print_info: n_embd_v_gqa     = 2048
0.00.050.965 I print_info: f_norm_eps       = 1.0e-05
0.00.050.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.966 I print_info: f_logit_scale    = 0.0e+00
0.00.050.967 I print_info: n_ff             = 8192
0.00.050.967 I print_info: n_expert         = 0
0.00.050.967 I print_info: n_expert_used    = 0
0.00.050.967 I print_info: causal attn      = 1
0.00.050.968 I print_info: pooling type     = 0
0.00.050.968 I print_info: rope type        = 2
0.00.050.968 I print_info: rope scaling     = linear
0.00.050.968 I print_info: freq_base_train  = 10000.0
0.00.050.969 I print_info: freq_scale_train = 1
0.00.050.969 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.969 I print_info: rope_finetuned   = unknown
0.00.050.969 I print_info: ssm_d_conv       = 0
0.00.050.971 I print_info: ssm_d_inner      = 0
0.00.050.971 I print_info: ssm_d_state      = 0
0.00.050.972 I print_info: ssm_dt_rank      = 0
0.00.050.972 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.972 I print_info: model type       = 1.4B
0.00.050.972 I print_info: model params     = 1.41 B
0.00.050.973 I print_info: general.name     = 1.4B
0.00.050.973 I print_info: vocab type       = BPE
0.00.050.973 I print_info: n_vocab          = 50304
0.00.050.973 I print_info: n_merges         = 50009
0.00.050.974 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.974 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.974 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.978 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.978 I print_info: LF token         = 128 'Ä'
0.00.050.978 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.979 I print_info: max token length = 1024
0.00.052.937 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.938 I load_tensors: offloading output layer to GPU
0.00.052.938 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.948 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.950 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.235 I llama_init_from_model: n_seq_max     = 1
0.00.053.236 I llama_init_from_model: n_ctx         = 128
0.00.053.236 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.236 I llama_init_from_model: n_batch       = 128
0.00.053.237 I llama_init_from_model: n_ubatch      = 128
0.00.053.237 I llama_init_from_model: flash_attn    = 0
0.00.053.237 I llama_init_from_model: freq_base     = 10000.0
0.00.053.237 I llama_init_from_model: freq_scale    = 1
0.00.053.238 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.238 I ggml_metal_init: allocating
0.00.053.241 I ggml_metal_init: found device: Apple M4
0.00.053.243 I ggml_metal_init: picking default device: Apple M4
0.00.053.829 I ggml_metal_init: using embedded metal library
0.00.056.207 I ggml_metal_init: GPU name:   Apple M4
0.00.056.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.209 I ggml_metal_init: simdgroup reduction   = true
0.00.056.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.209 I ggml_metal_init: has bfloat            = true
0.00.056.210 I ggml_metal_init: use bfloat            = true
0.00.056.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.315 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.552 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.554 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.568 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.419 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.420 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.421 I llama_init_from_model: graph nodes  = 967
0.00.068.421 I llama_init_from_model: graph splits = 2
0.00.068.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.236 I 
0.00.563.270 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.282 I perplexity: tokenizing the input ..
0.00.570.899 I perplexity: tokenization took 7.615 ms
0.00.570.903 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.545 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.694.697 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.694.732 I llama_perf_context_print:        load time =     553.23 ms
0.00.694.733 I llama_perf_context_print: prompt eval time =     122.42 ms /   128 tokens (    0.96 ms per token,  1045.61 tokens per second)
0.00.694.734 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.694.734 I llama_perf_context_print:       total time =     131.50 ms /   129 tokens
0.00.695.243 I ggml_metal_free: deallocating

real	0m0.711s
user	0m0.078s
sys	0m0.090s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.016.574 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.043 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.038.048 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.050 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.051 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.052 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.054 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.057 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.954 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.317 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.159 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.160 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.161 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.049.161 I llama_model_loader: - type  f32:  194 tensors
0.00.049.162 I llama_model_loader: - type q4_1:   97 tensors
0.00.049.162 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.162 I print_info: file format = GGUF V3 (latest)
0.00.049.163 I print_info: file type   = Q4_1
0.00.049.164 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.077.438 I load: special tokens cache size = 25
0.00.089.244 I load: token to piece cache size = 0.2984 MB
0.00.089.250 I print_info: arch             = gptneox
0.00.089.250 I print_info: vocab_only       = 0
0.00.089.250 I print_info: n_ctx_train      = 2048
0.00.089.251 I print_info: n_embd           = 2048
0.00.089.251 I print_info: n_layer          = 24
0.00.089.256 I print_info: n_head           = 16
0.00.089.257 I print_info: n_head_kv        = 16
0.00.089.257 I print_info: n_rot            = 32
0.00.089.257 I print_info: n_swa            = 0
0.00.089.258 I print_info: n_embd_head_k    = 128
0.00.089.258 I print_info: n_embd_head_v    = 128
0.00.089.259 I print_info: n_gqa            = 1
0.00.089.260 I print_info: n_embd_k_gqa     = 2048
0.00.089.261 I print_info: n_embd_v_gqa     = 2048
0.00.089.261 I print_info: f_norm_eps       = 1.0e-05
0.00.089.262 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.262 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.263 I print_info: f_logit_scale    = 0.0e+00
0.00.089.264 I print_info: n_ff             = 8192
0.00.089.264 I print_info: n_expert         = 0
0.00.089.264 I print_info: n_expert_used    = 0
0.00.089.264 I print_info: causal attn      = 1
0.00.089.265 I print_info: pooling type     = 0
0.00.089.265 I print_info: rope type        = 2
0.00.089.265 I print_info: rope scaling     = linear
0.00.089.266 I print_info: freq_base_train  = 10000.0
0.00.089.266 I print_info: freq_scale_train = 1
0.00.089.266 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.267 I print_info: rope_finetuned   = unknown
0.00.089.267 I print_info: ssm_d_conv       = 0
0.00.089.267 I print_info: ssm_d_inner      = 0
0.00.089.267 I print_info: ssm_d_state      = 0
0.00.089.268 I print_info: ssm_dt_rank      = 0
0.00.089.271 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.272 I print_info: model type       = 1.4B
0.00.089.272 I print_info: model params     = 1.41 B
0.00.089.273 I print_info: general.name     = 1.4B
0.00.089.275 I print_info: vocab type       = BPE
0.00.089.276 I print_info: n_vocab          = 50304
0.00.089.276 I print_info: n_merges         = 50009
0.00.089.276 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.276 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.277 I print_info: LF token         = 128 'Ä'
0.00.089.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.278 I print_info: max token length = 1024
0.00.092.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.027 I load_tensors: offloading output layer to GPU
0.00.092.028 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.039 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.092.041 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.092.515 I llama_init_from_model: n_seq_max     = 1
0.00.092.517 I llama_init_from_model: n_ctx         = 2048
0.00.092.517 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.092.517 I llama_init_from_model: n_batch       = 2048
0.00.092.518 I llama_init_from_model: n_ubatch      = 512
0.00.092.518 I llama_init_from_model: flash_attn    = 0
0.00.092.518 I llama_init_from_model: freq_base     = 10000.0
0.00.092.519 I llama_init_from_model: freq_scale    = 1
0.00.092.520 I ggml_metal_init: allocating
0.00.092.525 I ggml_metal_init: found device: Apple M4
0.00.092.528 I ggml_metal_init: picking default device: Apple M4
0.00.093.446 I ggml_metal_init: using embedded metal library
0.00.097.369 I ggml_metal_init: GPU name:   Apple M4
0.00.097.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.372 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.373 I ggml_metal_init: simdgroup reduction   = true
0.00.097.373 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.373 I ggml_metal_init: has bfloat            = true
0.00.097.373 I ggml_metal_init: use bfloat            = true
0.00.097.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.375 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.856 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.134.667 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.134.678 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.134.700 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.135.684 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.135.686 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.135.686 I llama_init_from_model: graph nodes  = 967
0.00.135.686 I llama_init_from_model: graph splits = 2
0.00.135.689 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.135.804 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.135.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.842.223 I main: llama threadpool init, n_threads = 4
0.00.842.271 I 
0.00.842.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.842.299 I 
0.00.842.541 I sampler seed: 1234
0.00.842.546 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.842.593 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.842.595 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.842.596 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.574.030 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52321.30 tokens per second)
0.01.574.031 I llama_perf_context_print:        load time =     825.64 ms
0.01.574.032 I llama_perf_context_print: prompt eval time =      49.88 ms /     7 tokens (    7.13 ms per token,   140.33 tokens per second)
0.01.574.036 I llama_perf_context_print:        eval time =     679.16 ms /    63 runs   (   10.78 ms per token,    92.76 tokens per second)
0.01.574.037 I llama_perf_context_print:       total time =     731.81 ms /    70 tokens
0.01.574.270 I ggml_metal_free: deallocating

real	0m1.597s
user	0m0.134s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.514 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.685 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.686 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.690 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.690 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.691 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.693 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.694 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.409 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.111 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.113 I llama_model_loader: - type  f32:  194 tensors
0.00.024.113 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.114 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.114 I print_info: file format = GGUF V3 (latest)
0.00.024.115 I print_info: file type   = Q4_1
0.00.024.116 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.783 I load: special tokens cache size = 25
0.00.048.927 I load: token to piece cache size = 0.2984 MB
0.00.048.930 I print_info: arch             = gptneox
0.00.048.931 I print_info: vocab_only       = 0
0.00.048.931 I print_info: n_ctx_train      = 2048
0.00.048.931 I print_info: n_embd           = 2048
0.00.048.931 I print_info: n_layer          = 24
0.00.048.934 I print_info: n_head           = 16
0.00.048.934 I print_info: n_head_kv        = 16
0.00.048.935 I print_info: n_rot            = 32
0.00.048.936 I print_info: n_swa            = 0
0.00.048.937 I print_info: n_embd_head_k    = 128
0.00.048.937 I print_info: n_embd_head_v    = 128
0.00.048.938 I print_info: n_gqa            = 1
0.00.048.938 I print_info: n_embd_k_gqa     = 2048
0.00.048.939 I print_info: n_embd_v_gqa     = 2048
0.00.048.940 I print_info: f_norm_eps       = 1.0e-05
0.00.048.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.940 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.940 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.941 I print_info: f_logit_scale    = 0.0e+00
0.00.048.941 I print_info: n_ff             = 8192
0.00.048.941 I print_info: n_expert         = 0
0.00.048.942 I print_info: n_expert_used    = 0
0.00.048.942 I print_info: causal attn      = 1
0.00.048.942 I print_info: pooling type     = 0
0.00.048.942 I print_info: rope type        = 2
0.00.048.942 I print_info: rope scaling     = linear
0.00.048.945 I print_info: freq_base_train  = 10000.0
0.00.048.946 I print_info: freq_scale_train = 1
0.00.048.946 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.947 I print_info: rope_finetuned   = unknown
0.00.048.947 I print_info: ssm_d_conv       = 0
0.00.048.947 I print_info: ssm_d_inner      = 0
0.00.048.947 I print_info: ssm_d_state      = 0
0.00.048.947 I print_info: ssm_dt_rank      = 0
0.00.048.947 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.952 I print_info: model type       = 1.4B
0.00.048.953 I print_info: model params     = 1.41 B
0.00.048.953 I print_info: general.name     = 1.4B
0.00.048.953 I print_info: vocab type       = BPE
0.00.048.954 I print_info: n_vocab          = 50304
0.00.048.954 I print_info: n_merges         = 50009
0.00.048.954 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.954 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.954 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.955 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.956 I print_info: LF token         = 128 'Ä'
0.00.048.956 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.956 I print_info: max token length = 1024
0.00.050.973 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.974 I load_tensors: offloading output layer to GPU
0.00.050.974 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.985 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.986 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.307 I llama_init_from_model: n_seq_max     = 1
0.00.051.308 I llama_init_from_model: n_ctx         = 128
0.00.051.309 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.309 I llama_init_from_model: n_batch       = 128
0.00.051.309 I llama_init_from_model: n_ubatch      = 128
0.00.051.309 I llama_init_from_model: flash_attn    = 0
0.00.051.309 I llama_init_from_model: freq_base     = 10000.0
0.00.051.310 I llama_init_from_model: freq_scale    = 1
0.00.051.310 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.310 I ggml_metal_init: allocating
0.00.051.313 I ggml_metal_init: found device: Apple M4
0.00.051.315 I ggml_metal_init: picking default device: Apple M4
0.00.051.888 I ggml_metal_init: using embedded metal library
0.00.054.280 I ggml_metal_init: GPU name:   Apple M4
0.00.054.282 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.283 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.283 I ggml_metal_init: simdgroup reduction   = true
0.00.054.283 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.283 I ggml_metal_init: has bfloat            = true
0.00.054.283 I ggml_metal_init: use bfloat            = true
0.00.054.284 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.284 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.984 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.215 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.217 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.230 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.169 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.170 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.170 I llama_init_from_model: graph nodes  = 967
0.00.066.171 I llama_init_from_model: graph splits = 2
0.00.066.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.562 I 
0.00.632.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.605 I perplexity: tokenizing the input ..
0.00.640.230 I perplexity: tokenization took 7.624 ms
0.00.640.234 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.762.962 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.764.143 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.764.171 I llama_perf_context_print:        load time =     624.04 ms
0.00.764.172 I llama_perf_context_print: prompt eval time =     122.50 ms /   128 tokens (    0.96 ms per token,  1044.92 tokens per second)
0.00.764.173 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.174 I llama_perf_context_print:       total time =     131.61 ms /   129 tokens
0.00.764.696 I ggml_metal_free: deallocating

real	0m0.779s
user	0m0.077s
sys	0m0.107s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.262 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.826 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.827 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.827 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.827 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.833 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.833 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.834 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.834 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.834 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.835 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.839 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.839 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.174 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.030.175 I llama_model_loader: - type  f32:  194 tensors
0.00.030.175 I llama_model_loader: - type q5_0:   97 tensors
0.00.030.176 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.176 I print_info: file format = GGUF V3 (latest)
0.00.030.177 I print_info: file type   = Q5_0
0.00.030.178 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.052.972 I load: special tokens cache size = 25
0.00.059.208 I load: token to piece cache size = 0.2984 MB
0.00.059.214 I print_info: arch             = gptneox
0.00.059.214 I print_info: vocab_only       = 0
0.00.059.214 I print_info: n_ctx_train      = 2048
0.00.059.214 I print_info: n_embd           = 2048
0.00.059.214 I print_info: n_layer          = 24
0.00.059.219 I print_info: n_head           = 16
0.00.059.220 I print_info: n_head_kv        = 16
0.00.059.220 I print_info: n_rot            = 32
0.00.059.221 I print_info: n_swa            = 0
0.00.059.221 I print_info: n_embd_head_k    = 128
0.00.059.221 I print_info: n_embd_head_v    = 128
0.00.059.222 I print_info: n_gqa            = 1
0.00.059.222 I print_info: n_embd_k_gqa     = 2048
0.00.059.223 I print_info: n_embd_v_gqa     = 2048
0.00.059.224 I print_info: f_norm_eps       = 1.0e-05
0.00.059.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.224 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.224 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.224 I print_info: f_logit_scale    = 0.0e+00
0.00.059.225 I print_info: n_ff             = 8192
0.00.059.225 I print_info: n_expert         = 0
0.00.059.225 I print_info: n_expert_used    = 0
0.00.059.229 I print_info: causal attn      = 1
0.00.059.230 I print_info: pooling type     = 0
0.00.059.230 I print_info: rope type        = 2
0.00.059.230 I print_info: rope scaling     = linear
0.00.059.231 I print_info: freq_base_train  = 10000.0
0.00.059.232 I print_info: freq_scale_train = 1
0.00.059.232 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.232 I print_info: rope_finetuned   = unknown
0.00.059.232 I print_info: ssm_d_conv       = 0
0.00.059.232 I print_info: ssm_d_inner      = 0
0.00.059.233 I print_info: ssm_d_state      = 0
0.00.059.233 I print_info: ssm_dt_rank      = 0
0.00.059.233 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.234 I print_info: model type       = 1.4B
0.00.059.234 I print_info: model params     = 1.41 B
0.00.059.235 I print_info: general.name     = 1.4B
0.00.059.235 I print_info: vocab type       = BPE
0.00.059.235 I print_info: n_vocab          = 50304
0.00.059.235 I print_info: n_merges         = 50009
0.00.059.236 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.236 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.236 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.236 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.236 I print_info: LF token         = 128 'Ä'
0.00.059.237 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.237 I print_info: max token length = 1024
0.00.061.239 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.240 I load_tensors: offloading output layer to GPU
0.00.061.240 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.251 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.061.253 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.061.572 I llama_init_from_model: n_seq_max     = 1
0.00.061.573 I llama_init_from_model: n_ctx         = 2048
0.00.061.573 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.061.573 I llama_init_from_model: n_batch       = 2048
0.00.061.574 I llama_init_from_model: n_ubatch      = 512
0.00.061.574 I llama_init_from_model: flash_attn    = 0
0.00.061.574 I llama_init_from_model: freq_base     = 10000.0
0.00.061.574 I llama_init_from_model: freq_scale    = 1
0.00.061.575 I ggml_metal_init: allocating
0.00.061.580 I ggml_metal_init: found device: Apple M4
0.00.061.582 I ggml_metal_init: picking default device: Apple M4
0.00.062.260 I ggml_metal_init: using embedded metal library
0.00.064.692 I ggml_metal_init: GPU name:   Apple M4
0.00.064.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.695 I ggml_metal_init: simdgroup reduction   = true
0.00.064.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.696 I ggml_metal_init: has bfloat            = true
0.00.064.696 I ggml_metal_init: use bfloat            = true
0.00.064.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.502 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.738 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.744 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.096.721 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.096.723 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.096.723 I llama_init_from_model: graph nodes  = 967
0.00.096.723 I llama_init_from_model: graph splits = 2
0.00.096.727 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.855 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.856 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.930 I main: llama threadpool init, n_threads = 4
0.00.769.968 I 
0.00.769.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.988 I 
0.00.770.150 I sampler seed: 1234
0.00.770.154 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.176 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.176 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.177 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.551.885 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.551.886 I llama_perf_context_print:        load time =     759.66 ms
0.01.551.887 I llama_perf_context_print: prompt eval time =      43.14 ms /     7 tokens (    6.16 ms per token,   162.27 tokens per second)
0.01.551.887 I llama_perf_context_print:        eval time =     735.64 ms /    63 runs   (   11.68 ms per token,    85.64 tokens per second)
0.01.551.888 I llama_perf_context_print:       total time =     781.96 ms /    70 tokens
0.01.552.117 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.114s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.784 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.926 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.934 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.938 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.939 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.939 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.939 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.943 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.944 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.946 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.726 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.549 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.552 I llama_model_loader: - type  f32:  194 tensors
0.00.025.552 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.552 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.553 I print_info: file format = GGUF V3 (latest)
0.00.025.553 I print_info: file type   = Q5_0
0.00.025.554 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.924 I load: special tokens cache size = 25
0.00.050.683 I load: token to piece cache size = 0.2984 MB
0.00.050.686 I print_info: arch             = gptneox
0.00.050.687 I print_info: vocab_only       = 0
0.00.050.687 I print_info: n_ctx_train      = 2048
0.00.050.687 I print_info: n_embd           = 2048
0.00.050.687 I print_info: n_layer          = 24
0.00.050.691 I print_info: n_head           = 16
0.00.050.692 I print_info: n_head_kv        = 16
0.00.050.692 I print_info: n_rot            = 32
0.00.050.694 I print_info: n_swa            = 0
0.00.050.694 I print_info: n_embd_head_k    = 128
0.00.050.695 I print_info: n_embd_head_v    = 128
0.00.050.695 I print_info: n_gqa            = 1
0.00.050.697 I print_info: n_embd_k_gqa     = 2048
0.00.050.698 I print_info: n_embd_v_gqa     = 2048
0.00.050.698 I print_info: f_norm_eps       = 1.0e-05
0.00.050.699 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.700 I print_info: f_logit_scale    = 0.0e+00
0.00.050.701 I print_info: n_ff             = 8192
0.00.050.701 I print_info: n_expert         = 0
0.00.050.701 I print_info: n_expert_used    = 0
0.00.050.701 I print_info: causal attn      = 1
0.00.050.701 I print_info: pooling type     = 0
0.00.050.701 I print_info: rope type        = 2
0.00.050.702 I print_info: rope scaling     = linear
0.00.050.702 I print_info: freq_base_train  = 10000.0
0.00.050.702 I print_info: freq_scale_train = 1
0.00.050.703 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.703 I print_info: rope_finetuned   = unknown
0.00.050.703 I print_info: ssm_d_conv       = 0
0.00.050.703 I print_info: ssm_d_inner      = 0
0.00.050.703 I print_info: ssm_d_state      = 0
0.00.050.703 I print_info: ssm_dt_rank      = 0
0.00.050.705 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.705 I print_info: model type       = 1.4B
0.00.050.705 I print_info: model params     = 1.41 B
0.00.050.705 I print_info: general.name     = 1.4B
0.00.050.706 I print_info: vocab type       = BPE
0.00.050.706 I print_info: n_vocab          = 50304
0.00.050.706 I print_info: n_merges         = 50009
0.00.050.706 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.707 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.707 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.707 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.707 I print_info: LF token         = 128 'Ä'
0.00.050.708 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.708 I print_info: max token length = 1024
0.00.052.715 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.716 I load_tensors: offloading output layer to GPU
0.00.052.716 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.726 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.728 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.022 I llama_init_from_model: n_seq_max     = 1
0.00.053.023 I llama_init_from_model: n_ctx         = 128
0.00.053.023 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.023 I llama_init_from_model: n_batch       = 128
0.00.053.023 I llama_init_from_model: n_ubatch      = 128
0.00.053.024 I llama_init_from_model: flash_attn    = 0
0.00.053.024 I llama_init_from_model: freq_base     = 10000.0
0.00.053.024 I llama_init_from_model: freq_scale    = 1
0.00.053.025 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.025 I ggml_metal_init: allocating
0.00.053.028 I ggml_metal_init: found device: Apple M4
0.00.053.030 I ggml_metal_init: picking default device: Apple M4
0.00.053.618 I ggml_metal_init: using embedded metal library
0.00.055.970 I ggml_metal_init: GPU name:   Apple M4
0.00.055.971 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.972 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.972 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.972 I ggml_metal_init: simdgroup reduction   = true
0.00.055.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.972 I ggml_metal_init: has bfloat            = true
0.00.055.973 I ggml_metal_init: use bfloat            = true
0.00.055.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.796 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.147 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.149 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.165 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.121 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.122 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.122 I llama_init_from_model: graph nodes  = 967
0.00.068.122 I llama_init_from_model: graph splits = 2
0.00.068.123 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.715 I 
0.00.685.750 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.764 I perplexity: tokenizing the input ..
0.00.693.380 I perplexity: tokenization took 7.614 ms
0.00.693.384 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.384 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.829.627 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.829.658 I llama_perf_context_print:        load time =     675.92 ms
0.00.829.659 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.75 tokens per second)
0.00.829.660 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.660 I llama_perf_context_print:       total time =     143.95 ms /   129 tokens
0.00.830.207 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.077s
sys	0m0.105s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.793 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.011 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.016 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.023 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.024 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.025 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.025 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.026 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.810 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.823 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.555 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.555 I llama_model_loader: - type  f32:  194 tensors
0.00.026.555 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.556 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.556 I print_info: file format = GGUF V3 (latest)
0.00.026.557 I print_info: file type   = Q5_1
0.00.026.558 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.119 I load: special tokens cache size = 25
0.00.050.878 I load: token to piece cache size = 0.2984 MB
0.00.050.881 I print_info: arch             = gptneox
0.00.050.881 I print_info: vocab_only       = 0
0.00.050.881 I print_info: n_ctx_train      = 2048
0.00.050.881 I print_info: n_embd           = 2048
0.00.050.882 I print_info: n_layer          = 24
0.00.050.885 I print_info: n_head           = 16
0.00.050.885 I print_info: n_head_kv        = 16
0.00.050.886 I print_info: n_rot            = 32
0.00.050.886 I print_info: n_swa            = 0
0.00.050.886 I print_info: n_embd_head_k    = 128
0.00.050.886 I print_info: n_embd_head_v    = 128
0.00.050.887 I print_info: n_gqa            = 1
0.00.050.887 I print_info: n_embd_k_gqa     = 2048
0.00.050.890 I print_info: n_embd_v_gqa     = 2048
0.00.050.891 I print_info: f_norm_eps       = 1.0e-05
0.00.050.891 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.892 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.892 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.893 I print_info: f_logit_scale    = 0.0e+00
0.00.050.893 I print_info: n_ff             = 8192
0.00.050.894 I print_info: n_expert         = 0
0.00.050.894 I print_info: n_expert_used    = 0
0.00.050.894 I print_info: causal attn      = 1
0.00.050.894 I print_info: pooling type     = 0
0.00.050.894 I print_info: rope type        = 2
0.00.050.894 I print_info: rope scaling     = linear
0.00.050.895 I print_info: freq_base_train  = 10000.0
0.00.050.895 I print_info: freq_scale_train = 1
0.00.050.896 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.896 I print_info: rope_finetuned   = unknown
0.00.050.896 I print_info: ssm_d_conv       = 0
0.00.050.896 I print_info: ssm_d_inner      = 0
0.00.050.897 I print_info: ssm_d_state      = 0
0.00.050.897 I print_info: ssm_dt_rank      = 0
0.00.050.898 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.898 I print_info: model type       = 1.4B
0.00.050.898 I print_info: model params     = 1.41 B
0.00.050.898 I print_info: general.name     = 1.4B
0.00.050.899 I print_info: vocab type       = BPE
0.00.050.899 I print_info: n_vocab          = 50304
0.00.050.899 I print_info: n_merges         = 50009
0.00.050.900 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.900 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.900 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.900 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.900 I print_info: LF token         = 128 'Ä'
0.00.050.904 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.904 I print_info: max token length = 1024
0.00.052.640 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.640 I load_tensors: offloading output layer to GPU
0.00.052.641 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.646 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.646 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.031 I llama_init_from_model: n_seq_max     = 1
0.00.053.032 I llama_init_from_model: n_ctx         = 2048
0.00.053.032 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.032 I llama_init_from_model: n_batch       = 2048
0.00.053.032 I llama_init_from_model: n_ubatch      = 512
0.00.053.032 I llama_init_from_model: flash_attn    = 0
0.00.053.033 I llama_init_from_model: freq_base     = 10000.0
0.00.053.033 I llama_init_from_model: freq_scale    = 1
0.00.053.033 I ggml_metal_init: allocating
0.00.053.036 I ggml_metal_init: found device: Apple M4
0.00.053.038 I ggml_metal_init: picking default device: Apple M4
0.00.053.592 I ggml_metal_init: using embedded metal library
0.00.055.911 I ggml_metal_init: GPU name:   Apple M4
0.00.055.912 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.913 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.913 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.913 I ggml_metal_init: simdgroup reduction   = true
0.00.055.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.914 I ggml_metal_init: has bfloat            = true
0.00.055.914 I ggml_metal_init: use bfloat            = true
0.00.055.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.916 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.620 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.782 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.788 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.808 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.907 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.908 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.909 I llama_init_from_model: graph nodes  = 967
0.00.086.909 I llama_init_from_model: graph splits = 2
0.00.086.916 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.056 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.057 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.310 I main: llama threadpool init, n_threads = 4
0.00.800.351 I 
0.00.800.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.383 I 
0.00.800.604 I sampler seed: 1234
0.00.800.614 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.636 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.637 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.637 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.637.284 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.637.285 I llama_perf_context_print:        load time =     790.51 ms
0.01.637.285 I llama_perf_context_print: prompt eval time =      42.29 ms /     7 tokens (    6.04 ms per token,   165.52 tokens per second)
0.01.637.286 I llama_perf_context_print:        eval time =     791.37 ms /    63 runs   (   12.56 ms per token,    79.61 tokens per second)
0.01.637.286 I llama_perf_context_print:       total time =     836.98 ms /    70 tokens
0.01.637.494 I ggml_metal_free: deallocating

real	0m1.656s
user	0m0.109s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.801 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.979 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.984 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.986 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.986 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.987 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.994 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.994 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.999 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.999 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.000 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.004 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.005 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.864 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.627 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.628 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.631 I llama_model_loader: - type  f32:  194 tensors
0.00.024.632 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.632 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.633 I print_info: file format = GGUF V3 (latest)
0.00.024.633 I print_info: file type   = Q5_1
0.00.024.634 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.369 I load: special tokens cache size = 25
0.00.049.475 I load: token to piece cache size = 0.2984 MB
0.00.049.478 I print_info: arch             = gptneox
0.00.049.479 I print_info: vocab_only       = 0
0.00.049.479 I print_info: n_ctx_train      = 2048
0.00.049.479 I print_info: n_embd           = 2048
0.00.049.479 I print_info: n_layer          = 24
0.00.049.482 I print_info: n_head           = 16
0.00.049.483 I print_info: n_head_kv        = 16
0.00.049.483 I print_info: n_rot            = 32
0.00.049.483 I print_info: n_swa            = 0
0.00.049.485 I print_info: n_embd_head_k    = 128
0.00.049.485 I print_info: n_embd_head_v    = 128
0.00.049.486 I print_info: n_gqa            = 1
0.00.049.487 I print_info: n_embd_k_gqa     = 2048
0.00.049.487 I print_info: n_embd_v_gqa     = 2048
0.00.049.492 I print_info: f_norm_eps       = 1.0e-05
0.00.049.492 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.493 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.493 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.493 I print_info: f_logit_scale    = 0.0e+00
0.00.049.494 I print_info: n_ff             = 8192
0.00.049.494 I print_info: n_expert         = 0
0.00.049.494 I print_info: n_expert_used    = 0
0.00.049.494 I print_info: causal attn      = 1
0.00.049.495 I print_info: pooling type     = 0
0.00.049.495 I print_info: rope type        = 2
0.00.049.497 I print_info: rope scaling     = linear
0.00.049.498 I print_info: freq_base_train  = 10000.0
0.00.049.500 I print_info: freq_scale_train = 1
0.00.049.500 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.500 I print_info: rope_finetuned   = unknown
0.00.049.500 I print_info: ssm_d_conv       = 0
0.00.049.500 I print_info: ssm_d_inner      = 0
0.00.049.501 I print_info: ssm_d_state      = 0
0.00.049.501 I print_info: ssm_dt_rank      = 0
0.00.049.501 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.501 I print_info: model type       = 1.4B
0.00.049.501 I print_info: model params     = 1.41 B
0.00.049.501 I print_info: general.name     = 1.4B
0.00.049.502 I print_info: vocab type       = BPE
0.00.049.502 I print_info: n_vocab          = 50304
0.00.049.502 I print_info: n_merges         = 50009
0.00.049.503 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.503 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.503 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.504 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.504 I print_info: LF token         = 128 'Ä'
0.00.049.505 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.505 I print_info: max token length = 1024
0.00.051.546 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.547 I load_tensors: offloading output layer to GPU
0.00.051.547 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.557 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.559 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.948 I llama_init_from_model: n_seq_max     = 1
0.00.051.949 I llama_init_from_model: n_ctx         = 128
0.00.051.949 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.950 I llama_init_from_model: n_batch       = 128
0.00.051.950 I llama_init_from_model: n_ubatch      = 128
0.00.051.950 I llama_init_from_model: flash_attn    = 0
0.00.051.950 I llama_init_from_model: freq_base     = 10000.0
0.00.051.950 I llama_init_from_model: freq_scale    = 1
0.00.051.951 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.951 I ggml_metal_init: allocating
0.00.051.954 I ggml_metal_init: found device: Apple M4
0.00.051.956 I ggml_metal_init: picking default device: Apple M4
0.00.052.543 I ggml_metal_init: using embedded metal library
0.00.054.890 I ggml_metal_init: GPU name:   Apple M4
0.00.054.892 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.892 I ggml_metal_init: simdgroup reduction   = true
0.00.054.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.893 I ggml_metal_init: has bfloat            = true
0.00.054.893 I ggml_metal_init: use bfloat            = true
0.00.054.893 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.894 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.446 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.769 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.774 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.790 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.688 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.689 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.689 I llama_init_from_model: graph nodes  = 967
0.00.066.689 I llama_init_from_model: graph splits = 2
0.00.066.690 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.690 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.585 I 
0.00.750.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.613 I perplexity: tokenizing the input ..
0.00.757.997 I perplexity: tokenization took 7.382 ms
0.00.758.001 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.892.829 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.894.011 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.894.036 I llama_perf_context_print:        load time =     741.78 ms
0.00.894.037 I llama_perf_context_print: prompt eval time =     134.60 ms /   128 tokens (    1.05 ms per token,   950.97 tokens per second)
0.00.894.038 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.894.038 I llama_perf_context_print:       total time =     143.45 ms /   129 tokens
0.00.894.361 I ggml_metal_free: deallocating

real	0m0.908s
user	0m0.077s
sys	0m0.124s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.366 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.054 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.059 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.061 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.061 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.062 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.062 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.063 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.064 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.064 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.064 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.879 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.886 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.655 I llama_model_loader: - type  f32:  194 tensors
0.00.024.655 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.655 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.656 I print_info: file format = GGUF V3 (latest)
0.00.024.656 I print_info: file type   = Q2_K - Medium
0.00.024.657 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.329 I load: special tokens cache size = 25
0.00.049.114 I load: token to piece cache size = 0.2984 MB
0.00.049.117 I print_info: arch             = gptneox
0.00.049.117 I print_info: vocab_only       = 0
0.00.049.117 I print_info: n_ctx_train      = 2048
0.00.049.118 I print_info: n_embd           = 2048
0.00.049.118 I print_info: n_layer          = 24
0.00.049.121 I print_info: n_head           = 16
0.00.049.122 I print_info: n_head_kv        = 16
0.00.049.122 I print_info: n_rot            = 32
0.00.049.122 I print_info: n_swa            = 0
0.00.049.122 I print_info: n_embd_head_k    = 128
0.00.049.123 I print_info: n_embd_head_v    = 128
0.00.049.123 I print_info: n_gqa            = 1
0.00.049.124 I print_info: n_embd_k_gqa     = 2048
0.00.049.125 I print_info: n_embd_v_gqa     = 2048
0.00.049.125 I print_info: f_norm_eps       = 1.0e-05
0.00.049.126 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.126 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.126 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.126 I print_info: f_logit_scale    = 0.0e+00
0.00.049.127 I print_info: n_ff             = 8192
0.00.049.127 I print_info: n_expert         = 0
0.00.049.127 I print_info: n_expert_used    = 0
0.00.049.128 I print_info: causal attn      = 1
0.00.049.128 I print_info: pooling type     = 0
0.00.049.130 I print_info: rope type        = 2
0.00.049.131 I print_info: rope scaling     = linear
0.00.049.131 I print_info: freq_base_train  = 10000.0
0.00.049.131 I print_info: freq_scale_train = 1
0.00.049.131 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.132 I print_info: rope_finetuned   = unknown
0.00.049.132 I print_info: ssm_d_conv       = 0
0.00.049.132 I print_info: ssm_d_inner      = 0
0.00.049.132 I print_info: ssm_d_state      = 0
0.00.049.132 I print_info: ssm_dt_rank      = 0
0.00.049.133 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.133 I print_info: model type       = 1.4B
0.00.049.133 I print_info: model params     = 1.41 B
0.00.049.133 I print_info: general.name     = 1.4B
0.00.049.134 I print_info: vocab type       = BPE
0.00.049.134 I print_info: n_vocab          = 50304
0.00.049.134 I print_info: n_merges         = 50009
0.00.049.134 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.135 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.135 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.135 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.137 I print_info: LF token         = 128 'Ä'
0.00.049.137 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.137 I print_info: max token length = 1024
0.00.050.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.893 I load_tensors: offloading output layer to GPU
0.00.050.894 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.899 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.900 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.171 I llama_init_from_model: n_seq_max     = 1
0.00.051.172 I llama_init_from_model: n_ctx         = 2048
0.00.051.172 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.172 I llama_init_from_model: n_batch       = 2048
0.00.051.172 I llama_init_from_model: n_ubatch      = 512
0.00.051.172 I llama_init_from_model: flash_attn    = 0
0.00.051.173 I llama_init_from_model: freq_base     = 10000.0
0.00.051.173 I llama_init_from_model: freq_scale    = 1
0.00.051.174 I ggml_metal_init: allocating
0.00.051.177 I ggml_metal_init: found device: Apple M4
0.00.051.179 I ggml_metal_init: picking default device: Apple M4
0.00.051.748 I ggml_metal_init: using embedded metal library
0.00.054.109 I ggml_metal_init: GPU name:   Apple M4
0.00.054.110 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.111 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.111 I ggml_metal_init: simdgroup reduction   = true
0.00.054.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.112 I ggml_metal_init: has bfloat            = true
0.00.054.112 I ggml_metal_init: use bfloat            = true
0.00.054.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.113 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.820 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.692 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.698 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.716 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.792 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.793 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.794 I llama_init_from_model: graph nodes  = 967
0.00.084.794 I llama_init_from_model: graph splits = 2
0.00.084.797 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.898 I main: llama threadpool init, n_threads = 4
0.00.493.944 I 
0.00.493.984 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.493.984 I 
0.00.494.217 I sampler seed: 1234
0.00.494.222 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.494.233 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.494.233 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.494.233 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.174.524 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64604.19 tokens per second)
0.01.174.525 I llama_perf_context_print:        load time =     484.53 ms
0.01.174.526 I llama_perf_context_print: prompt eval time =      35.79 ms /     7 tokens (    5.11 ms per token,   195.59 tokens per second)
0.01.174.527 I llama_perf_context_print:        eval time =     641.67 ms /    63 runs   (   10.19 ms per token,    98.18 tokens per second)
0.01.174.527 I llama_perf_context_print:       total time =     680.63 ms /    70 tokens
0.01.174.763 I ggml_metal_free: deallocating

real	0m1.193s
user	0m0.108s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.686 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.347 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.352 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.352 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.627 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.628 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.628 I llama_model_loader: - type  f32:  194 tensors
0.00.024.629 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.629 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.629 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.630 I print_info: file format = GGUF V3 (latest)
0.00.024.630 I print_info: file type   = Q2_K - Medium
0.00.024.631 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.277 I load: special tokens cache size = 25
0.00.049.332 I load: token to piece cache size = 0.2984 MB
0.00.049.335 I print_info: arch             = gptneox
0.00.049.336 I print_info: vocab_only       = 0
0.00.049.336 I print_info: n_ctx_train      = 2048
0.00.049.336 I print_info: n_embd           = 2048
0.00.049.336 I print_info: n_layer          = 24
0.00.049.339 I print_info: n_head           = 16
0.00.049.340 I print_info: n_head_kv        = 16
0.00.049.340 I print_info: n_rot            = 32
0.00.049.341 I print_info: n_swa            = 0
0.00.049.341 I print_info: n_embd_head_k    = 128
0.00.049.341 I print_info: n_embd_head_v    = 128
0.00.049.342 I print_info: n_gqa            = 1
0.00.049.342 I print_info: n_embd_k_gqa     = 2048
0.00.049.343 I print_info: n_embd_v_gqa     = 2048
0.00.049.344 I print_info: f_norm_eps       = 1.0e-05
0.00.049.344 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.344 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.344 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.344 I print_info: f_logit_scale    = 0.0e+00
0.00.049.345 I print_info: n_ff             = 8192
0.00.049.345 I print_info: n_expert         = 0
0.00.049.345 I print_info: n_expert_used    = 0
0.00.049.345 I print_info: causal attn      = 1
0.00.049.346 I print_info: pooling type     = 0
0.00.049.346 I print_info: rope type        = 2
0.00.049.346 I print_info: rope scaling     = linear
0.00.049.346 I print_info: freq_base_train  = 10000.0
0.00.049.347 I print_info: freq_scale_train = 1
0.00.049.347 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.347 I print_info: rope_finetuned   = unknown
0.00.049.347 I print_info: ssm_d_conv       = 0
0.00.049.347 I print_info: ssm_d_inner      = 0
0.00.049.349 I print_info: ssm_d_state      = 0
0.00.049.349 I print_info: ssm_dt_rank      = 0
0.00.049.349 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.349 I print_info: model type       = 1.4B
0.00.049.350 I print_info: model params     = 1.41 B
0.00.049.350 I print_info: general.name     = 1.4B
0.00.049.350 I print_info: vocab type       = BPE
0.00.049.351 I print_info: n_vocab          = 50304
0.00.049.352 I print_info: n_merges         = 50009
0.00.049.353 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.353 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.353 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.353 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.353 I print_info: LF token         = 128 'Ä'
0.00.049.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.354 I print_info: max token length = 1024
0.00.051.205 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.206 I load_tensors: offloading output layer to GPU
0.00.051.206 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.217 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.218 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.501 I llama_init_from_model: n_seq_max     = 1
0.00.051.502 I llama_init_from_model: n_ctx         = 128
0.00.051.502 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.502 I llama_init_from_model: n_batch       = 128
0.00.051.502 I llama_init_from_model: n_ubatch      = 128
0.00.051.502 I llama_init_from_model: flash_attn    = 0
0.00.051.503 I llama_init_from_model: freq_base     = 10000.0
0.00.051.503 I llama_init_from_model: freq_scale    = 1
0.00.051.503 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.504 I ggml_metal_init: allocating
0.00.051.507 I ggml_metal_init: found device: Apple M4
0.00.051.509 I ggml_metal_init: picking default device: Apple M4
0.00.052.045 I ggml_metal_init: using embedded metal library
0.00.054.375 I ggml_metal_init: GPU name:   Apple M4
0.00.054.376 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.377 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.377 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.377 I ggml_metal_init: simdgroup reduction   = true
0.00.054.377 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.378 I ggml_metal_init: has bfloat            = true
0.00.054.378 I ggml_metal_init: use bfloat            = true
0.00.054.378 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.976 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.249 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.251 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.266 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.139 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.140 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.140 I llama_init_from_model: graph nodes  = 967
0.00.066.141 I llama_init_from_model: graph splits = 2
0.00.066.142 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.940 I 
0.00.438.976 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.438.981 I perplexity: tokenizing the input ..
0.00.446.450 I perplexity: tokenization took 7.467 ms
0.00.446.454 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.579.243 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.580.656 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.580.688 I llama_perf_context_print:        load time =     429.25 ms
0.00.580.689 I llama_perf_context_print: prompt eval time =     132.53 ms /   128 tokens (    1.04 ms per token,   965.80 tokens per second)
0.00.580.689 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.580.690 I llama_perf_context_print:       total time =     141.75 ms /   129 tokens
0.00.581.206 I ggml_metal_free: deallocating

real	0m0.596s
user	0m0.075s
sys	0m0.067s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.111 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.758 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.764 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.770 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.770 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.771 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.772 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.772 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.773 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.773 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.773 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.775 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.776 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.777 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.778 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.670 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.684 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.526 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.527 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.529 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.529 I llama_model_loader: - type  f32:  194 tensors
0.00.025.530 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.530 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.530 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.530 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.531 I print_info: file format = GGUF V3 (latest)
0.00.025.531 I print_info: file type   = Q3_K - Medium
0.00.025.532 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.971 I load: special tokens cache size = 25
0.00.051.047 I load: token to piece cache size = 0.2984 MB
0.00.051.050 I print_info: arch             = gptneox
0.00.051.050 I print_info: vocab_only       = 0
0.00.051.050 I print_info: n_ctx_train      = 2048
0.00.051.050 I print_info: n_embd           = 2048
0.00.051.051 I print_info: n_layer          = 24
0.00.051.054 I print_info: n_head           = 16
0.00.051.055 I print_info: n_head_kv        = 16
0.00.051.055 I print_info: n_rot            = 32
0.00.051.055 I print_info: n_swa            = 0
0.00.051.055 I print_info: n_embd_head_k    = 128
0.00.051.056 I print_info: n_embd_head_v    = 128
0.00.051.056 I print_info: n_gqa            = 1
0.00.051.057 I print_info: n_embd_k_gqa     = 2048
0.00.051.061 I print_info: n_embd_v_gqa     = 2048
0.00.051.063 I print_info: f_norm_eps       = 1.0e-05
0.00.051.064 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.065 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.065 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.065 I print_info: f_logit_scale    = 0.0e+00
0.00.051.066 I print_info: n_ff             = 8192
0.00.051.066 I print_info: n_expert         = 0
0.00.051.066 I print_info: n_expert_used    = 0
0.00.051.067 I print_info: causal attn      = 1
0.00.051.067 I print_info: pooling type     = 0
0.00.051.067 I print_info: rope type        = 2
0.00.051.067 I print_info: rope scaling     = linear
0.00.051.067 I print_info: freq_base_train  = 10000.0
0.00.051.068 I print_info: freq_scale_train = 1
0.00.051.068 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.068 I print_info: rope_finetuned   = unknown
0.00.051.069 I print_info: ssm_d_conv       = 0
0.00.051.070 I print_info: ssm_d_inner      = 0
0.00.051.070 I print_info: ssm_d_state      = 0
0.00.051.070 I print_info: ssm_dt_rank      = 0
0.00.051.070 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.070 I print_info: model type       = 1.4B
0.00.051.071 I print_info: model params     = 1.41 B
0.00.051.071 I print_info: general.name     = 1.4B
0.00.051.071 I print_info: vocab type       = BPE
0.00.051.072 I print_info: n_vocab          = 50304
0.00.051.072 I print_info: n_merges         = 50009
0.00.051.072 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.072 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.072 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.072 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.073 I print_info: LF token         = 128 'Ä'
0.00.051.073 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.073 I print_info: max token length = 1024
0.00.053.017 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.018 I load_tensors: offloading output layer to GPU
0.00.053.018 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.029 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.030 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.315 I llama_init_from_model: n_seq_max     = 1
0.00.053.316 I llama_init_from_model: n_ctx         = 2048
0.00.053.316 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.316 I llama_init_from_model: n_batch       = 2048
0.00.053.316 I llama_init_from_model: n_ubatch      = 512
0.00.053.316 I llama_init_from_model: flash_attn    = 0
0.00.053.317 I llama_init_from_model: freq_base     = 10000.0
0.00.053.317 I llama_init_from_model: freq_scale    = 1
0.00.053.317 I ggml_metal_init: allocating
0.00.053.321 I ggml_metal_init: found device: Apple M4
0.00.053.323 I ggml_metal_init: picking default device: Apple M4
0.00.053.947 I ggml_metal_init: using embedded metal library
0.00.056.325 I ggml_metal_init: GPU name:   Apple M4
0.00.056.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.327 I ggml_metal_init: simdgroup reduction   = true
0.00.056.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.328 I ggml_metal_init: has bfloat            = true
0.00.056.328 I ggml_metal_init: use bfloat            = true
0.00.056.328 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.252 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.319 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.328 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.360 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.270 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.271 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.271 I llama_init_from_model: graph nodes  = 967
0.00.086.272 I llama_init_from_model: graph splits = 2
0.00.086.275 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.403 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.404 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.593.896 I main: llama threadpool init, n_threads = 4
0.00.593.929 I 
0.00.593.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.593.949 I 
0.00.594.188 I sampler seed: 1234
0.00.594.193 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.594.217 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.594.218 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.594.219 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.344.629 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.344.629 I llama_perf_context_print:        load time =     584.78 ms
0.01.344.630 I llama_perf_context_print: prompt eval time =      44.39 ms /     7 tokens (    6.34 ms per token,   157.70 tokens per second)
0.01.344.631 I llama_perf_context_print:        eval time =     703.03 ms /    63 runs   (   11.16 ms per token,    89.61 tokens per second)
0.01.344.631 I llama_perf_context_print:       total time =     750.73 ms /    70 tokens
0.01.344.833 I ggml_metal_free: deallocating

real	0m1.360s
user	0m0.110s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.000 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.065 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.073 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.073 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.074 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.075 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.075 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.076 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.081 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.082 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.613 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.614 I llama_model_loader: - type  f32:  194 tensors
0.00.024.615 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.615 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.615 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.615 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.615 I print_info: file format = GGUF V3 (latest)
0.00.024.616 I print_info: file type   = Q3_K - Medium
0.00.024.616 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.199 I load: special tokens cache size = 25
0.00.049.137 I load: token to piece cache size = 0.2984 MB
0.00.049.140 I print_info: arch             = gptneox
0.00.049.140 I print_info: vocab_only       = 0
0.00.049.140 I print_info: n_ctx_train      = 2048
0.00.049.140 I print_info: n_embd           = 2048
0.00.049.141 I print_info: n_layer          = 24
0.00.049.144 I print_info: n_head           = 16
0.00.049.146 I print_info: n_head_kv        = 16
0.00.049.146 I print_info: n_rot            = 32
0.00.049.146 I print_info: n_swa            = 0
0.00.049.146 I print_info: n_embd_head_k    = 128
0.00.049.148 I print_info: n_embd_head_v    = 128
0.00.049.149 I print_info: n_gqa            = 1
0.00.049.150 I print_info: n_embd_k_gqa     = 2048
0.00.049.151 I print_info: n_embd_v_gqa     = 2048
0.00.049.151 I print_info: f_norm_eps       = 1.0e-05
0.00.049.153 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.153 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.153 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.153 I print_info: f_logit_scale    = 0.0e+00
0.00.049.154 I print_info: n_ff             = 8192
0.00.049.154 I print_info: n_expert         = 0
0.00.049.154 I print_info: n_expert_used    = 0
0.00.049.154 I print_info: causal attn      = 1
0.00.049.155 I print_info: pooling type     = 0
0.00.049.156 I print_info: rope type        = 2
0.00.049.156 I print_info: rope scaling     = linear
0.00.049.157 I print_info: freq_base_train  = 10000.0
0.00.049.157 I print_info: freq_scale_train = 1
0.00.049.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.158 I print_info: rope_finetuned   = unknown
0.00.049.158 I print_info: ssm_d_conv       = 0
0.00.049.158 I print_info: ssm_d_inner      = 0
0.00.049.158 I print_info: ssm_d_state      = 0
0.00.049.158 I print_info: ssm_dt_rank      = 0
0.00.049.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.159 I print_info: model type       = 1.4B
0.00.049.159 I print_info: model params     = 1.41 B
0.00.049.159 I print_info: general.name     = 1.4B
0.00.049.160 I print_info: vocab type       = BPE
0.00.049.160 I print_info: n_vocab          = 50304
0.00.049.160 I print_info: n_merges         = 50009
0.00.049.160 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.165 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.165 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.165 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.165 I print_info: LF token         = 128 'Ä'
0.00.049.167 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.167 I print_info: max token length = 1024
0.00.051.099 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.099 I load_tensors: offloading output layer to GPU
0.00.051.099 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.110 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.111 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.484 I llama_init_from_model: n_seq_max     = 1
0.00.051.485 I llama_init_from_model: n_ctx         = 128
0.00.051.485 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.485 I llama_init_from_model: n_batch       = 128
0.00.051.485 I llama_init_from_model: n_ubatch      = 128
0.00.051.485 I llama_init_from_model: flash_attn    = 0
0.00.051.486 I llama_init_from_model: freq_base     = 10000.0
0.00.051.486 I llama_init_from_model: freq_scale    = 1
0.00.051.486 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.487 I ggml_metal_init: allocating
0.00.051.489 I ggml_metal_init: found device: Apple M4
0.00.051.491 I ggml_metal_init: picking default device: Apple M4
0.00.052.056 I ggml_metal_init: using embedded metal library
0.00.054.377 I ggml_metal_init: GPU name:   Apple M4
0.00.054.379 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.380 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.380 I ggml_metal_init: simdgroup reduction   = true
0.00.054.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.380 I ggml_metal_init: has bfloat            = true
0.00.054.380 I ggml_metal_init: use bfloat            = true
0.00.054.381 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.904 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.118 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.122 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.135 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.071 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.072 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.072 I llama_init_from_model: graph nodes  = 967
0.00.066.073 I llama_init_from_model: graph splits = 2
0.00.066.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.074 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.526.470 I 
0.00.526.508 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.526.519 I perplexity: tokenizing the input ..
0.00.534.221 I perplexity: tokenization took 7.7 ms
0.00.534.224 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.666.440 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.667.608 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.667.635 I llama_perf_context_print:        load time =     517.47 ms
0.00.667.636 I llama_perf_context_print: prompt eval time =     131.99 ms /   128 tokens (    1.03 ms per token,   969.79 tokens per second)
0.00.667.637 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.667.637 I llama_perf_context_print:       total time =     141.16 ms /   129 tokens
0.00.668.140 I ggml_metal_free: deallocating

real	0m0.683s
user	0m0.077s
sys	0m0.081s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.680 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.442 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.447 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.449 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.454 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.454 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.454 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.455 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.458 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.459 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.459 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.459 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.460 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.460 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.461 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.465 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.276 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.029 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.030 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.032 I llama_model_loader: - type  f32:  194 tensors
0.00.026.032 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.033 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.033 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.033 I print_info: file format = GGUF V3 (latest)
0.00.026.034 I print_info: file type   = Q4_K - Medium
0.00.026.035 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.436 I load: special tokens cache size = 25
0.00.051.584 I load: token to piece cache size = 0.2984 MB
0.00.051.587 I print_info: arch             = gptneox
0.00.051.588 I print_info: vocab_only       = 0
0.00.051.588 I print_info: n_ctx_train      = 2048
0.00.051.588 I print_info: n_embd           = 2048
0.00.051.588 I print_info: n_layer          = 24
0.00.051.591 I print_info: n_head           = 16
0.00.051.592 I print_info: n_head_kv        = 16
0.00.051.592 I print_info: n_rot            = 32
0.00.051.592 I print_info: n_swa            = 0
0.00.051.593 I print_info: n_embd_head_k    = 128
0.00.051.593 I print_info: n_embd_head_v    = 128
0.00.051.593 I print_info: n_gqa            = 1
0.00.051.594 I print_info: n_embd_k_gqa     = 2048
0.00.051.595 I print_info: n_embd_v_gqa     = 2048
0.00.051.595 I print_info: f_norm_eps       = 1.0e-05
0.00.051.596 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.596 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.596 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.596 I print_info: f_logit_scale    = 0.0e+00
0.00.051.597 I print_info: n_ff             = 8192
0.00.051.597 I print_info: n_expert         = 0
0.00.051.599 I print_info: n_expert_used    = 0
0.00.051.601 I print_info: causal attn      = 1
0.00.051.601 I print_info: pooling type     = 0
0.00.051.601 I print_info: rope type        = 2
0.00.051.601 I print_info: rope scaling     = linear
0.00.051.602 I print_info: freq_base_train  = 10000.0
0.00.051.602 I print_info: freq_scale_train = 1
0.00.051.602 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.602 I print_info: rope_finetuned   = unknown
0.00.051.602 I print_info: ssm_d_conv       = 0
0.00.051.603 I print_info: ssm_d_inner      = 0
0.00.051.603 I print_info: ssm_d_state      = 0
0.00.051.604 I print_info: ssm_dt_rank      = 0
0.00.051.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.604 I print_info: model type       = 1.4B
0.00.051.605 I print_info: model params     = 1.41 B
0.00.051.605 I print_info: general.name     = 1.4B
0.00.051.605 I print_info: vocab type       = BPE
0.00.051.606 I print_info: n_vocab          = 50304
0.00.051.606 I print_info: n_merges         = 50009
0.00.051.606 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.606 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.608 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.608 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.608 I print_info: LF token         = 128 'Ä'
0.00.051.609 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.609 I print_info: max token length = 1024
0.00.053.554 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.555 I load_tensors: offloading output layer to GPU
0.00.053.555 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.566 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.567 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.866 I llama_init_from_model: n_seq_max     = 1
0.00.053.866 I llama_init_from_model: n_ctx         = 2048
0.00.053.866 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.867 I llama_init_from_model: n_batch       = 2048
0.00.053.867 I llama_init_from_model: n_ubatch      = 512
0.00.053.867 I llama_init_from_model: flash_attn    = 0
0.00.053.867 I llama_init_from_model: freq_base     = 10000.0
0.00.053.867 I llama_init_from_model: freq_scale    = 1
0.00.053.868 I ggml_metal_init: allocating
0.00.053.871 I ggml_metal_init: found device: Apple M4
0.00.053.873 I ggml_metal_init: picking default device: Apple M4
0.00.054.488 I ggml_metal_init: using embedded metal library
0.00.056.826 I ggml_metal_init: GPU name:   Apple M4
0.00.056.828 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.828 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.828 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.829 I ggml_metal_init: simdgroup reduction   = true
0.00.056.829 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.829 I ggml_metal_init: has bfloat            = true
0.00.056.829 I ggml_metal_init: use bfloat            = true
0.00.056.830 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.830 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.663 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.074 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.083 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.106 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.011 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.012 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.012 I llama_init_from_model: graph nodes  = 967
0.00.086.012 I llama_init_from_model: graph splits = 2
0.00.086.015 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.110 I main: llama threadpool init, n_threads = 4
0.00.612.147 I 
0.00.612.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.168 I 
0.00.612.393 I sampler seed: 1234
0.00.612.397 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.612.407 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.612.407 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.612.408 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.937 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.376.939 I llama_perf_context_print:        load time =     602.43 ms
0.01.376.940 I llama_perf_context_print: prompt eval time =      50.01 ms /     7 tokens (    7.14 ms per token,   139.97 tokens per second)
0.01.376.941 I llama_perf_context_print:        eval time =     711.41 ms /    63 runs   (   11.29 ms per token,    88.56 tokens per second)
0.01.376.941 I llama_perf_context_print:       total time =     764.83 ms /    70 tokens
0.01.377.182 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.110s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.843 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.545 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.549 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.552 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.300 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.079 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.080 I llama_model_loader: - type  f32:  194 tensors
0.00.025.080 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.080 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.081 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.081 I print_info: file format = GGUF V3 (latest)
0.00.025.082 I print_info: file type   = Q4_K - Medium
0.00.025.083 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.577 I load: special tokens cache size = 25
0.00.049.385 I load: token to piece cache size = 0.2984 MB
0.00.049.388 I print_info: arch             = gptneox
0.00.049.388 I print_info: vocab_only       = 0
0.00.049.388 I print_info: n_ctx_train      = 2048
0.00.049.388 I print_info: n_embd           = 2048
0.00.049.389 I print_info: n_layer          = 24
0.00.049.391 I print_info: n_head           = 16
0.00.049.392 I print_info: n_head_kv        = 16
0.00.049.394 I print_info: n_rot            = 32
0.00.049.394 I print_info: n_swa            = 0
0.00.049.394 I print_info: n_embd_head_k    = 128
0.00.049.394 I print_info: n_embd_head_v    = 128
0.00.049.395 I print_info: n_gqa            = 1
0.00.049.396 I print_info: n_embd_k_gqa     = 2048
0.00.049.401 I print_info: n_embd_v_gqa     = 2048
0.00.049.402 I print_info: f_norm_eps       = 1.0e-05
0.00.049.402 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.403 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.403 I print_info: f_logit_scale    = 0.0e+00
0.00.049.404 I print_info: n_ff             = 8192
0.00.049.404 I print_info: n_expert         = 0
0.00.049.404 I print_info: n_expert_used    = 0
0.00.049.404 I print_info: causal attn      = 1
0.00.049.404 I print_info: pooling type     = 0
0.00.049.404 I print_info: rope type        = 2
0.00.049.405 I print_info: rope scaling     = linear
0.00.049.405 I print_info: freq_base_train  = 10000.0
0.00.049.405 I print_info: freq_scale_train = 1
0.00.049.406 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.406 I print_info: rope_finetuned   = unknown
0.00.049.406 I print_info: ssm_d_conv       = 0
0.00.049.406 I print_info: ssm_d_inner      = 0
0.00.049.406 I print_info: ssm_d_state      = 0
0.00.049.406 I print_info: ssm_dt_rank      = 0
0.00.049.406 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.407 I print_info: model type       = 1.4B
0.00.049.407 I print_info: model params     = 1.41 B
0.00.049.407 I print_info: general.name     = 1.4B
0.00.049.408 I print_info: vocab type       = BPE
0.00.049.408 I print_info: n_vocab          = 50304
0.00.049.408 I print_info: n_merges         = 50009
0.00.049.408 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.409 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.409 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.409 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.409 I print_info: LF token         = 128 'Ä'
0.00.049.409 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.410 I print_info: max token length = 1024
0.00.051.353 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.354 I load_tensors: offloading output layer to GPU
0.00.051.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.364 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.366 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.643 I llama_init_from_model: n_seq_max     = 1
0.00.051.643 I llama_init_from_model: n_ctx         = 128
0.00.051.643 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.644 I llama_init_from_model: n_batch       = 128
0.00.051.644 I llama_init_from_model: n_ubatch      = 128
0.00.051.644 I llama_init_from_model: flash_attn    = 0
0.00.051.644 I llama_init_from_model: freq_base     = 10000.0
0.00.051.644 I llama_init_from_model: freq_scale    = 1
0.00.051.645 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.645 I ggml_metal_init: allocating
0.00.051.648 I ggml_metal_init: found device: Apple M4
0.00.051.650 I ggml_metal_init: picking default device: Apple M4
0.00.052.222 I ggml_metal_init: using embedded metal library
0.00.054.549 I ggml_metal_init: GPU name:   Apple M4
0.00.054.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.551 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.552 I ggml_metal_init: simdgroup reduction   = true
0.00.054.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.552 I ggml_metal_init: has bfloat            = true
0.00.054.552 I ggml_metal_init: use bfloat            = true
0.00.054.553 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.553 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.069 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.324 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.326 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.339 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.266 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.267 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.267 I llama_init_from_model: graph nodes  = 967
0.00.066.267 I llama_init_from_model: graph splits = 2
0.00.066.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.354 I 
0.00.543.381 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.385 I perplexity: tokenizing the input ..
0.00.550.399 I perplexity: tokenization took 7.012 ms
0.00.550.403 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.684.750 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.686.046 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.686.080 I llama_perf_context_print:        load time =     533.51 ms
0.00.686.081 I llama_perf_context_print: prompt eval time =     134.12 ms /   128 tokens (    1.05 ms per token,   954.34 tokens per second)
0.00.686.082 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.686.082 I llama_perf_context_print:       total time =     142.73 ms /   129 tokens
0.00.686.613 I ggml_metal_free: deallocating

real	0m0.702s
user	0m0.076s
sys	0m0.092s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.319 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.244 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.246 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.247 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.252 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.253 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.188 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.008 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.008 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.009 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.009 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.009 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.010 I llama_model_loader: - type  f32:  194 tensors
0.00.028.010 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.010 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.011 I print_info: file format = GGUF V3 (latest)
0.00.028.012 I print_info: file type   = Q5_K - Medium
0.00.028.013 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.048.523 I load: special tokens cache size = 25
0.00.054.582 I load: token to piece cache size = 0.2984 MB
0.00.054.587 I print_info: arch             = gptneox
0.00.054.587 I print_info: vocab_only       = 0
0.00.054.588 I print_info: n_ctx_train      = 2048
0.00.054.588 I print_info: n_embd           = 2048
0.00.054.588 I print_info: n_layer          = 24
0.00.054.592 I print_info: n_head           = 16
0.00.054.593 I print_info: n_head_kv        = 16
0.00.054.595 I print_info: n_rot            = 32
0.00.054.596 I print_info: n_swa            = 0
0.00.054.596 I print_info: n_embd_head_k    = 128
0.00.054.596 I print_info: n_embd_head_v    = 128
0.00.054.597 I print_info: n_gqa            = 1
0.00.054.597 I print_info: n_embd_k_gqa     = 2048
0.00.054.598 I print_info: n_embd_v_gqa     = 2048
0.00.054.598 I print_info: f_norm_eps       = 1.0e-05
0.00.054.599 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.600 I print_info: f_logit_scale    = 0.0e+00
0.00.054.600 I print_info: n_ff             = 8192
0.00.054.600 I print_info: n_expert         = 0
0.00.054.600 I print_info: n_expert_used    = 0
0.00.054.601 I print_info: causal attn      = 1
0.00.054.601 I print_info: pooling type     = 0
0.00.054.602 I print_info: rope type        = 2
0.00.054.603 I print_info: rope scaling     = linear
0.00.054.605 I print_info: freq_base_train  = 10000.0
0.00.054.605 I print_info: freq_scale_train = 1
0.00.054.605 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.605 I print_info: rope_finetuned   = unknown
0.00.054.606 I print_info: ssm_d_conv       = 0
0.00.054.606 I print_info: ssm_d_inner      = 0
0.00.054.606 I print_info: ssm_d_state      = 0
0.00.054.606 I print_info: ssm_dt_rank      = 0
0.00.054.606 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.606 I print_info: model type       = 1.4B
0.00.054.607 I print_info: model params     = 1.41 B
0.00.054.607 I print_info: general.name     = 1.4B
0.00.054.607 I print_info: vocab type       = BPE
0.00.054.608 I print_info: n_vocab          = 50304
0.00.054.608 I print_info: n_merges         = 50009
0.00.054.608 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.608 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.608 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.608 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.609 I print_info: LF token         = 128 'Ä'
0.00.054.609 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.609 I print_info: max token length = 1024
0.00.056.587 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.587 I load_tensors: offloading output layer to GPU
0.00.056.587 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.598 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.056.600 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.987 I llama_init_from_model: n_seq_max     = 1
0.00.056.988 I llama_init_from_model: n_ctx         = 2048
0.00.056.988 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.988 I llama_init_from_model: n_batch       = 2048
0.00.056.988 I llama_init_from_model: n_ubatch      = 512
0.00.056.989 I llama_init_from_model: flash_attn    = 0
0.00.056.989 I llama_init_from_model: freq_base     = 10000.0
0.00.056.989 I llama_init_from_model: freq_scale    = 1
0.00.056.990 I ggml_metal_init: allocating
0.00.056.994 I ggml_metal_init: found device: Apple M4
0.00.056.996 I ggml_metal_init: picking default device: Apple M4
0.00.057.637 I ggml_metal_init: using embedded metal library
0.00.060.114 I ggml_metal_init: GPU name:   Apple M4
0.00.060.116 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.116 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.116 I ggml_metal_init: simdgroup reduction   = true
0.00.060.117 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.117 I ggml_metal_init: has bfloat            = true
0.00.060.117 I ggml_metal_init: use bfloat            = true
0.00.060.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.520 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.626 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.631 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.654 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.585 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.586 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.587 I llama_init_from_model: graph nodes  = 967
0.00.089.587 I llama_init_from_model: graph splits = 2
0.00.089.590 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.705 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.972 I main: llama threadpool init, n_threads = 4
0.00.682.010 I 
0.00.682.032 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.034 I 
0.00.682.188 I sampler seed: 1234
0.00.682.192 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.682.208 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.682.208 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.682.208 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.532.336 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.532.337 I llama_perf_context_print:        load time =     672.65 ms
0.01.532.338 I llama_perf_context_print: prompt eval time =      51.51 ms /     7 tokens (    7.36 ms per token,   135.91 tokens per second)
0.01.532.339 I llama_perf_context_print:        eval time =     795.68 ms /    63 runs   (   12.63 ms per token,    79.18 tokens per second)
0.01.532.339 I llama_perf_context_print:       total time =     850.37 ms /    70 tokens
0.01.532.546 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.113s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.392 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.212 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.216 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.224 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.228 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.229 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.230 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.231 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.008 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.772 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.773 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.774 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.774 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.774 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.774 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.775 I llama_model_loader: - type  f32:  194 tensors
0.00.024.775 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.775 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.776 I print_info: file format = GGUF V3 (latest)
0.00.024.776 I print_info: file type   = Q5_K - Medium
0.00.024.777 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.354 I load: special tokens cache size = 25
0.00.049.353 I load: token to piece cache size = 0.2984 MB
0.00.049.355 I print_info: arch             = gptneox
0.00.049.356 I print_info: vocab_only       = 0
0.00.049.356 I print_info: n_ctx_train      = 2048
0.00.049.356 I print_info: n_embd           = 2048
0.00.049.356 I print_info: n_layer          = 24
0.00.049.359 I print_info: n_head           = 16
0.00.049.360 I print_info: n_head_kv        = 16
0.00.049.360 I print_info: n_rot            = 32
0.00.049.360 I print_info: n_swa            = 0
0.00.049.361 I print_info: n_embd_head_k    = 128
0.00.049.361 I print_info: n_embd_head_v    = 128
0.00.049.361 I print_info: n_gqa            = 1
0.00.049.362 I print_info: n_embd_k_gqa     = 2048
0.00.049.363 I print_info: n_embd_v_gqa     = 2048
0.00.049.363 I print_info: f_norm_eps       = 1.0e-05
0.00.049.364 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.364 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.364 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.364 I print_info: f_logit_scale    = 0.0e+00
0.00.049.365 I print_info: n_ff             = 8192
0.00.049.365 I print_info: n_expert         = 0
0.00.049.365 I print_info: n_expert_used    = 0
0.00.049.365 I print_info: causal attn      = 1
0.00.049.365 I print_info: pooling type     = 0
0.00.049.366 I print_info: rope type        = 2
0.00.049.366 I print_info: rope scaling     = linear
0.00.049.366 I print_info: freq_base_train  = 10000.0
0.00.049.367 I print_info: freq_scale_train = 1
0.00.049.367 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.367 I print_info: rope_finetuned   = unknown
0.00.049.367 I print_info: ssm_d_conv       = 0
0.00.049.368 I print_info: ssm_d_inner      = 0
0.00.049.368 I print_info: ssm_d_state      = 0
0.00.049.368 I print_info: ssm_dt_rank      = 0
0.00.049.368 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.368 I print_info: model type       = 1.4B
0.00.049.369 I print_info: model params     = 1.41 B
0.00.049.369 I print_info: general.name     = 1.4B
0.00.049.369 I print_info: vocab type       = BPE
0.00.049.370 I print_info: n_vocab          = 50304
0.00.049.370 I print_info: n_merges         = 50009
0.00.049.372 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.372 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.372 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.373 I print_info: LF token         = 128 'Ä'
0.00.049.373 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.373 I print_info: max token length = 1024
0.00.051.404 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.404 I load_tensors: offloading output layer to GPU
0.00.051.405 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.415 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.416 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.752 I llama_init_from_model: n_seq_max     = 1
0.00.051.753 I llama_init_from_model: n_ctx         = 128
0.00.051.753 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.753 I llama_init_from_model: n_batch       = 128
0.00.051.754 I llama_init_from_model: n_ubatch      = 128
0.00.051.754 I llama_init_from_model: flash_attn    = 0
0.00.051.754 I llama_init_from_model: freq_base     = 10000.0
0.00.051.754 I llama_init_from_model: freq_scale    = 1
0.00.051.755 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.755 I ggml_metal_init: allocating
0.00.051.758 I ggml_metal_init: found device: Apple M4
0.00.051.760 I ggml_metal_init: picking default device: Apple M4
0.00.052.312 I ggml_metal_init: using embedded metal library
0.00.054.655 I ggml_metal_init: GPU name:   Apple M4
0.00.054.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.658 I ggml_metal_init: simdgroup reduction   = true
0.00.054.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.658 I ggml_metal_init: has bfloat            = true
0.00.054.658 I ggml_metal_init: use bfloat            = true
0.00.054.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.659 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.180 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.415 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.417 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.432 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.372 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.373 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.373 I llama_init_from_model: graph nodes  = 967
0.00.066.374 I llama_init_from_model: graph splits = 2
0.00.066.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.078 I 
0.00.620.118 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.122 I perplexity: tokenizing the input ..
0.00.627.755 I perplexity: tokenization took 7.631 ms
0.00.627.762 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.600 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.769.762 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.769.788 I llama_perf_context_print:        load time =     610.68 ms
0.00.769.789 I llama_perf_context_print: prompt eval time =     140.61 ms /   128 tokens (    1.10 ms per token,   910.31 tokens per second)
0.00.769.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.769.790 I llama_perf_context_print:       total time =     149.71 ms /   129 tokens
0.00.770.207 I ggml_metal_free: deallocating

real	0m0.784s
user	0m0.076s
sys	0m0.106s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.724 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.646 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.648 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.648 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.648 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.649 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.649 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.652 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.653 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.654 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.657 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.657 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.218 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.218 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.218 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.219 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.219 I llama_model_loader: - type  f32:  194 tensors
0.00.026.220 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.220 I print_info: file format = GGUF V3 (latest)
0.00.026.221 I print_info: file type   = Q6_K
0.00.026.222 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.002 I load: special tokens cache size = 25
0.00.051.017 I load: token to piece cache size = 0.2984 MB
0.00.051.020 I print_info: arch             = gptneox
0.00.051.020 I print_info: vocab_only       = 0
0.00.051.020 I print_info: n_ctx_train      = 2048
0.00.051.020 I print_info: n_embd           = 2048
0.00.051.021 I print_info: n_layer          = 24
0.00.051.024 I print_info: n_head           = 16
0.00.051.024 I print_info: n_head_kv        = 16
0.00.051.025 I print_info: n_rot            = 32
0.00.051.025 I print_info: n_swa            = 0
0.00.051.025 I print_info: n_embd_head_k    = 128
0.00.051.025 I print_info: n_embd_head_v    = 128
0.00.051.026 I print_info: n_gqa            = 1
0.00.051.026 I print_info: n_embd_k_gqa     = 2048
0.00.051.027 I print_info: n_embd_v_gqa     = 2048
0.00.051.028 I print_info: f_norm_eps       = 1.0e-05
0.00.051.029 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.030 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.030 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.030 I print_info: f_logit_scale    = 0.0e+00
0.00.051.031 I print_info: n_ff             = 8192
0.00.051.031 I print_info: n_expert         = 0
0.00.051.031 I print_info: n_expert_used    = 0
0.00.051.031 I print_info: causal attn      = 1
0.00.051.033 I print_info: pooling type     = 0
0.00.051.035 I print_info: rope type        = 2
0.00.051.035 I print_info: rope scaling     = linear
0.00.051.035 I print_info: freq_base_train  = 10000.0
0.00.051.036 I print_info: freq_scale_train = 1
0.00.051.036 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.036 I print_info: rope_finetuned   = unknown
0.00.051.036 I print_info: ssm_d_conv       = 0
0.00.051.036 I print_info: ssm_d_inner      = 0
0.00.051.036 I print_info: ssm_d_state      = 0
0.00.051.037 I print_info: ssm_dt_rank      = 0
0.00.051.037 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.037 I print_info: model type       = 1.4B
0.00.051.037 I print_info: model params     = 1.41 B
0.00.051.037 I print_info: general.name     = 1.4B
0.00.051.038 I print_info: vocab type       = BPE
0.00.051.038 I print_info: n_vocab          = 50304
0.00.051.038 I print_info: n_merges         = 50009
0.00.051.038 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.039 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.040 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.040 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.041 I print_info: LF token         = 128 'Ä'
0.00.051.041 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.041 I print_info: max token length = 1024
0.00.053.037 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.037 I load_tensors: offloading output layer to GPU
0.00.053.038 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.048 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.049 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.331 I llama_init_from_model: n_seq_max     = 1
0.00.053.332 I llama_init_from_model: n_ctx         = 2048
0.00.053.332 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.333 I llama_init_from_model: n_batch       = 2048
0.00.053.333 I llama_init_from_model: n_ubatch      = 512
0.00.053.333 I llama_init_from_model: flash_attn    = 0
0.00.053.333 I llama_init_from_model: freq_base     = 10000.0
0.00.053.334 I llama_init_from_model: freq_scale    = 1
0.00.053.334 I ggml_metal_init: allocating
0.00.053.337 I ggml_metal_init: found device: Apple M4
0.00.053.339 I ggml_metal_init: picking default device: Apple M4
0.00.053.930 I ggml_metal_init: using embedded metal library
0.00.056.289 I ggml_metal_init: GPU name:   Apple M4
0.00.056.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.291 I ggml_metal_init: simdgroup reduction   = true
0.00.056.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.292 I ggml_metal_init: has bfloat            = true
0.00.056.292 I ggml_metal_init: use bfloat            = true
0.00.056.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.005 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.702 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.712 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.742 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.844 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.846 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.846 I llama_init_from_model: graph nodes  = 967
0.00.086.847 I llama_init_from_model: graph splits = 2
0.00.086.850 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.975 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.906 I main: llama threadpool init, n_threads = 4
0.00.765.944 I 
0.00.765.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.974 I 
0.00.766.206 I sampler seed: 1234
0.00.766.210 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.222 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.222 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.222 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.638.074 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 60996.56 tokens per second)
0.01.638.074 I llama_perf_context_print:        load time =     756.18 ms
0.01.638.075 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.68 tokens per second)
0.01.638.076 I llama_perf_context_print:        eval time =     814.44 ms /    63 runs   (   12.93 ms per token,    77.35 tokens per second)
0.01.638.076 I llama_perf_context_print:       total time =     872.17 ms /    70 tokens
0.01.638.331 I ggml_metal_free: deallocating

real	0m1.656s
user	0m0.109s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4472 (437e05f7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.921 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.603 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.610 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.611 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.611 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.612 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.614 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.382 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.089 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.090 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.091 I llama_model_loader: - type  f32:  194 tensors
0.00.025.092 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.092 I print_info: file format = GGUF V3 (latest)
0.00.025.092 I print_info: file type   = Q6_K
0.00.025.093 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.653 I load: special tokens cache size = 25
0.00.049.418 I load: token to piece cache size = 0.2984 MB
0.00.049.421 I print_info: arch             = gptneox
0.00.049.421 I print_info: vocab_only       = 0
0.00.049.422 I print_info: n_ctx_train      = 2048
0.00.049.422 I print_info: n_embd           = 2048
0.00.049.422 I print_info: n_layer          = 24
0.00.049.425 I print_info: n_head           = 16
0.00.049.425 I print_info: n_head_kv        = 16
0.00.049.426 I print_info: n_rot            = 32
0.00.049.426 I print_info: n_swa            = 0
0.00.049.426 I print_info: n_embd_head_k    = 128
0.00.049.426 I print_info: n_embd_head_v    = 128
0.00.049.427 I print_info: n_gqa            = 1
0.00.049.428 I print_info: n_embd_k_gqa     = 2048
0.00.049.428 I print_info: n_embd_v_gqa     = 2048
0.00.049.429 I print_info: f_norm_eps       = 1.0e-05
0.00.049.429 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.430 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.430 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.432 I print_info: f_logit_scale    = 0.0e+00
0.00.049.433 I print_info: n_ff             = 8192
0.00.049.433 I print_info: n_expert         = 0
0.00.049.433 I print_info: n_expert_used    = 0
0.00.049.433 I print_info: causal attn      = 1
0.00.049.433 I print_info: pooling type     = 0
0.00.049.434 I print_info: rope type        = 2
0.00.049.435 I print_info: rope scaling     = linear
0.00.049.435 I print_info: freq_base_train  = 10000.0
0.00.049.435 I print_info: freq_scale_train = 1
0.00.049.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.436 I print_info: rope_finetuned   = unknown
0.00.049.436 I print_info: ssm_d_conv       = 0
0.00.049.436 I print_info: ssm_d_inner      = 0
0.00.049.436 I print_info: ssm_d_state      = 0
0.00.049.436 I print_info: ssm_dt_rank      = 0
0.00.049.437 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.437 I print_info: model type       = 1.4B
0.00.049.441 I print_info: model params     = 1.41 B
0.00.049.441 I print_info: general.name     = 1.4B
0.00.049.442 I print_info: vocab type       = BPE
0.00.049.442 I print_info: n_vocab          = 50304
0.00.049.442 I print_info: n_merges         = 50009
0.00.049.442 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.442 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.443 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.443 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.443 I print_info: LF token         = 128 'Ä'
0.00.049.443 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.443 I print_info: max token length = 1024
0.00.051.377 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.377 I load_tensors: offloading output layer to GPU
0.00.051.377 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.387 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.389 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.716 I llama_init_from_model: n_seq_max     = 1
0.00.051.717 I llama_init_from_model: n_ctx         = 128
0.00.051.717 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.717 I llama_init_from_model: n_batch       = 128
0.00.051.717 I llama_init_from_model: n_ubatch      = 128
0.00.051.718 I llama_init_from_model: flash_attn    = 0
0.00.051.718 I llama_init_from_model: freq_base     = 10000.0
0.00.051.718 I llama_init_from_model: freq_scale    = 1
0.00.051.719 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.719 I ggml_metal_init: allocating
0.00.051.722 I ggml_metal_init: found device: Apple M4
0.00.051.724 I ggml_metal_init: picking default device: Apple M4
0.00.052.284 I ggml_metal_init: using embedded metal library
0.00.054.595 I ggml_metal_init: GPU name:   Apple M4
0.00.054.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.598 I ggml_metal_init: simdgroup reduction   = true
0.00.054.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.598 I ggml_metal_init: has bfloat            = true
0.00.054.598 I ggml_metal_init: use bfloat            = true
0.00.054.598 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.061 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.381 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.384 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.401 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.259 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.260 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.260 I llama_init_from_model: graph nodes  = 967
0.00.066.260 I llama_init_from_model: graph splits = 2
0.00.066.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.333.808 I 
0.00.333.840 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.333.843 I perplexity: tokenizing the input ..
0.00.341.260 I perplexity: tokenization took 7.416 ms
0.00.341.265 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.481.316 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.482.476 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.482.501 I llama_perf_context_print:        load time =     323.88 ms
0.00.482.502 I llama_perf_context_print: prompt eval time =     139.80 ms /   128 tokens (    1.09 ms per token,   915.61 tokens per second)
0.00.482.503 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.482.503 I llama_perf_context_print:       total time =     148.70 ms /   129 tokens
0.00.483.013 I ggml_metal_free: deallocating

real	0m0.497s
user	0m0.076s
sys	0m0.072s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4472 (437e05f7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142b0a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142b0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142b0b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142b0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142b0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142b0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142b0cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142b0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142b0d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142b0dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142b0e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142b0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142b0f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142b0f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142b10140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142b10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142b10f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142b116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142b11dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142b12590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142b12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142b133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142b13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142b14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142b14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142b14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142b15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142b15ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142b16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142b167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142b16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142b16f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142b177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142b17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142b17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142b18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142b18920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142b18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142b19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142b19700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142b19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142b1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142b1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142b1a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142b1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142b1b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142b1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142b1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142b1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142b1cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142b1d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142b1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142b1dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142b1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142b1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142b1f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142b1f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142b1f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142b1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142b207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142b20a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142b20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142b213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142b21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142b21d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142b221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142b22650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142b22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142b22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142b23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142b238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142b23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142b24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142b24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142b24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142b25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142b25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142b25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142b261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142b26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142b26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142b271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142b27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142b27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142b281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142b28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142b28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142b291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142b29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142b29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142b2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142b2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142b2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142b2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142b2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142b2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142b2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142b1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142b2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142b2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142b2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142b2d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142b2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142b2e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142b2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142b2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142b2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142b2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142b2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142b302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142b30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142b30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142b312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142b31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142b31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142b320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142b32540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142b329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142b32e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142b33320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142b337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142b33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142b34100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142b345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142b34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142b34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142b35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142b35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142b35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142b36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142b36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142b36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142b36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142b373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142b37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142b37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142b381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142b38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142b38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142b38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142b39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142b398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142b39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142b3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142b3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142b3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142b3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142b3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142b3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142b3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142b3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142b3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142b3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142b3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142b3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142b3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142b3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142b3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142b3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142b3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142b3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142b3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142b3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142b3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142b40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142b407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142b40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142b41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142b415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142b41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142b41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142b423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142b42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142b42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142b43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142b43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142b43ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142b43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142b44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142b448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142b44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142b451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142b45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142b45b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142b45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142b46460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142b46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142b46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142b47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142b476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142b47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142b48020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142b484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142b48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142b48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142b494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142b49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142b49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142b4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142b4a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142b4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142b4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x142b4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142b4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142b4c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142b4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142b4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142b4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142b4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142b4e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142b4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142b4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142b4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142b4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142b4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142b50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142b507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142b50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142b51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142b517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142b51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142b52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142b527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142b52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142b53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142b53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142b53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142b54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142b54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142b54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142b55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142b55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142b55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142b56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142b56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142b56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142b57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142b57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142b57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142b581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142b58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142b58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142b591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142b59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142b59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142b5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142b5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142b5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142b5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142b5b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142b5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142b5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142b5c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142b5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142b5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142b5d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142b5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142b5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142b5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142b5ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142b5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142b5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142b5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142b60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142b606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142b60c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142b61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142b61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142b61aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142b61f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142b623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142b62880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142b62d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142b631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142b63660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142b63b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142b63fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142b64440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142b648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142b64d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142b65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142b656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142b65c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142b66330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142b66a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142b67170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142b67890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142b67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142b68340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142b68600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142b68c10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.144.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142a052a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142a05710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142a05b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142a05ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142a06460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142a068d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142a06d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142a071b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142a07620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142a07a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142a07f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142a085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142a09110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142a098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142a0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142a0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142a0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142a0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142a0bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142a0c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142a0cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142a0d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142a0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142a0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142a0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142a0eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142a0eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142a0f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142a0f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142a0faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142a0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142a10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142a10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142a10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142a110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142a11510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142a11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142a11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142a12470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142a12970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142a12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142a13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142a13870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142a13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142a14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142a146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142a14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142a14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142a15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142a158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142a15d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142a16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142a165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142a16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142a16ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142a176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142a17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142a17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142a18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142a18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142a190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142a19540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142a199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142a19e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142a1a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142a1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142a1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142a1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142a1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142a1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142a1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142a1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142a1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142a1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142a1d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142a1d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142a1dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142a1e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142a1e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142a1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142a1f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142a1f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142a1fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142a20290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142a207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142a20d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142a21280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142a217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142a21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142a22270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142a227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142a22d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142a23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142a237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142a23d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142a24250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142a247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142a24cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142a25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142a25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142a25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142a26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142a26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142a26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142a27220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142a27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142a27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142a28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142a28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142a28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142a29200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142a29750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142a29ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142a2a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142a2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142a2aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142a2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142a2b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142a2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142a2bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142a2c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142a2c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142a2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142a2cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142a2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142a2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142a2dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142a2e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142a2e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142a2eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142a2efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142a2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142a2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142a2fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142a30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142a30ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142a31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142a314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142a31980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142a31e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142a322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142a32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142a32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142a330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142a33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142a339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142a33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142a34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142a347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142a34c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142a35100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142a355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142a35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142a35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142a36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142a36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142a36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142a37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142a37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142a37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142a37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142a383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142a38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142a38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142a391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142a39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142a39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142a39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142a3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142a3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142a3ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142a3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142a3b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142a3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142a3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142a3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142a3c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142a3cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142a3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142a3d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142a3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142a3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142a3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142a3e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142a3ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142a3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142a3f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142a3fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142a400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142a40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142a40a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142a40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142a413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142a41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142a41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142a423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142a426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142a42cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142a432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142a438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142a440c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x142a44560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142a44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142a44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142a45440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142a45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142a460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142a46570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142a46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142a471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142a47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142a47c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142a481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142a48700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142a48c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142a491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142a496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142a49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142a4a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142a4a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142a4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142a4b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142a4b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142a4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142a4c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142a4c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142a4cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142a4d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142a4d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142a4dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142a4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142a4e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142a4ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142a4f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142a4f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142a4fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142a50130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142a50680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142a50bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142a51120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142a51670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142a51bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142a52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142a52660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142a52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142a53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142a53650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142a53ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142a540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142a54640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142a54b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142a550e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142a55630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142a55b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142a560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142a56620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142a56b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142a570c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142a57610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142a57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142a580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142a58600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142a58b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142a590a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142a595f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142a59b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142a59fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142a5a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142a5a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142a5adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142a5b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142a5b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142a5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142a5c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142a5c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142a5c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142a5ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142a5d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142a5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142a5dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142a5e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142a5e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142a5ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142a5f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142a5fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142a60270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142a60530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142a60d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142a60fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142a615f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142905ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142905f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1429063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142906820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142906c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142907100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142907570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1429079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142907e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1429083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142908810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142908e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1429099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14290a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14290a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14290b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14290b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14290bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14290c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14290cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14290d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14290dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14290e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14290ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14290f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14290f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14290f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14290fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14290ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142910430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1429108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142910dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142911240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142911500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142911970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142911de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142912250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1429126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142912b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142912fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142913410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142913880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142913cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142914160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1429145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142914a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142914eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142915320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142915790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142915c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142916070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1429164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142916950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142916dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142917230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1429176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142917c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142918110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142918580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1429189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142918e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1429192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142919740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142919bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14291a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14291a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14291a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14291ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14291b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14291b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14291bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14291bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14291c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14291c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14291cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14291d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14291d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14291d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14291de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14291e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14291e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14291eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14291f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14291f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14291f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14291fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1429201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142920630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142920aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142920f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142921380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1429217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142921c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1429220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142922540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1429229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142922e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142923290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142923700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142923b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142923fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142924450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1429248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142925150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142925410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142925880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142925cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142926160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1429265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142926a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142926eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142927320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142927790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142927c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142928070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1429284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142928950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142928dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142929230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1429296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142929b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142929f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14292a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14292a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14292acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14292b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14292b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14292ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14292be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14292c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14292c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14292cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14292d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14292d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14292d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14292dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14292e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14292e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14292eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14292ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14292f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14292f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14292fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142930120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142930590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142930a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142930e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1429312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142931750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142931bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142932030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1429324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142932910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142932d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1429331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142933660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142933ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142933f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1429343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142934820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142934c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142935100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142935570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1429359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142935e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1429362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142936730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142936ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142937010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142937480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1429378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142937d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1429381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142938640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142938ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142938f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142939390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142939800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142939c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14293a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14293a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14293a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14293ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14293b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14293b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14293bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14293bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14293c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14293c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14293cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14293d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14293d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14293da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14293df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14293e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14293e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14293ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14293f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14293f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14293f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14293fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142940280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1429406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142940b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142940fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142941440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1429418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142941d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142942190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142942600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142943180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142943440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142943700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142943b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142943fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142944450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1429448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142944d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1429451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142945610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142945a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142945ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142946360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1429467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142946c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1429470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142947520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142947990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142947e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142948270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1429486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142948b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142948fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142949430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1429498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142949d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14294a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14294a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14294aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14294aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14294b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14294b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14294bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14294c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14294c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14294c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14294cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14294d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14294d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14294db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14294dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14294e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14294e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14294ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14294f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14294f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14294fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14294feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142950320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142950790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142950c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142951070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1429514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142951950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142951dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142952230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1429526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142952b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142952f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1429533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142953860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142953cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142954140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1429545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142954a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142954e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142955300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142955770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142955be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142956050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1429564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142956930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142956da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142957810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142957f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142958650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142958d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142959030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1429594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142959aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14295a0b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.757s
user	0m0.309s
sys	0m0.267s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4472 (437e05f7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f70d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f70dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f70e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f70e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f70ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f70f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f70f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f70fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f710410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f710e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f711310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f711e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f7125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f712df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f714350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f714a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f715240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f715960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f716080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f7167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f717040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f717760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f717a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f718030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f718ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f7191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f7194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f719c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f71a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f71a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f71ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f71b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f71b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f71ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f71bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f71c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f71ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f71d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f71d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f71d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f71df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f71e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f71ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f71f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f71fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f720060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f720c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f721290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f721a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f721f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f7223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f722680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f722c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f723480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f723be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f724080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f724520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f7249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f724e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f7257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f725c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f7260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f726580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f726a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f726ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f727960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f728ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f7293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f729940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f72a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f72a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f72ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f72b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f72b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f72be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f72c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f72c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f72ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f72d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f72d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f72de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f72e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f72e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f72ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f71eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f72f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f72fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f72ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f730500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f730a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f730fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f7314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f731a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f7324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f732a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f732f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f7334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f733a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f733f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f734410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f7348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f734d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f7351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f735690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f735b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f736470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f736910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f736db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f7376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f737b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f738030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f7384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f738970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f738e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f7392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f739750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f73a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f73a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f73a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f73ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f73b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f73b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f73bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f73c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f73c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f73ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f73ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f73d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f73d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f73dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f73e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f73e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f73ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f73ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f73f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f73f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f73fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f7401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f740650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f740af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f740f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f741430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f7418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f741d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f742210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f7426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f742b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f742ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f743490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f743930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f743dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f744270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f744710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f744bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f745050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f7454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f745990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f7462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f746770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f746c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f7470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f747550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f7479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f747e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f748330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f7487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f748c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f749110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f7495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f749a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f749ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f74a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f74a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f74acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f74b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f74b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f74bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f74c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f74c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f74c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f74cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f74d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f74dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14f74e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14f74e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f74eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f74f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14f74f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f74ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f7503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f750840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f750ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f7519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f751f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f752480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f7529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f752f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f753470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f7539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f753f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f754460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f7549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f754f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f7559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f756440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f756990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f756ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f757430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f757980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f757ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f758420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f758970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f758ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f759410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f759960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f759eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f75a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f75a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f75aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f75b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f75b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f75be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f75c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f75c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f75ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f75d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f75d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f75de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f75e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f75e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f75ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f75f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f75f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f75fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f7603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f7608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f760e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f7618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f761e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f762380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f7628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f762e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f763370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f7638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f763e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14f7642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14f764750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f764bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f765090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f765530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f7659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f766310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f7667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f766c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f7670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f767590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f767a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f767ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f768370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f7688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f768fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f769700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f769e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f76a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f76a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14f76aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f76b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f76b8c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.128 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.132 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x150808840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150808cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150809120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150809590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x150809a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150809e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15080a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15080a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15080abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15080b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15080b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15080bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15080c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15080ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15080d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15080dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15080e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15080ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15080f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15080fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150810140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150810860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x150810f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1508116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150811dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150812080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150812340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1508127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150812c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150813090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150813590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150813aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x150813f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1508141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150814640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150814ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150815010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150815510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150815a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150815f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150816410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150816910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150816e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150817310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150817810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x150817c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1508180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x150818560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1508189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x150818e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1508192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x150819720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150819b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15081a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15081a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15081ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15081b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15081b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15081b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15081c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15081c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15081cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15081cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15081d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15081d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15081dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15081e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15081e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15081eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15081efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15081f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15081f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15081fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150820860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x150820db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150821300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150821850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150821da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1508222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150822840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150822d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1508232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150823830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x150823d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1508242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150824820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x150824d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1508252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150825810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150825d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1508262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150826800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150826d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1508272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1508277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x150827d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x150828290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1508287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x150828d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x150829280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1508297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150829d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15082a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15082a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15082ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15082b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15082b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15082bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15082c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15082c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15082ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15082d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15082d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15082db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15082e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15082e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15082e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15082ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15082f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15082f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15082fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150830080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150830520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1508309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150830e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150831300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1508317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150831c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1508320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150832580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150832a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150832ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150833360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150833800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150833ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150834140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1508345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x150834a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150834f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1508353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150835860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150835d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1508361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150836640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150836ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150836f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150837420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1508378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150837d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150838200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1508386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150838b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150838fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150839480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150839920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150839dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15083a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15083a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15083aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15083b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15083b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15083b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15083be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15083c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15083c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15083cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15083d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15083d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15083d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15083de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15083e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15083e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15083ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15083f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15083f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15083fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15083fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150840380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150840820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150840cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150841160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150841600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150841aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150841f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1508423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150842880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150842d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1508431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150843660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150843b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150843fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150844440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150844990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150844ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150845430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150845980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150845c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150846860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150846e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150847660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x150847b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150847dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1508483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1508489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1508491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150849670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150849b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150849fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15084a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15084acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15084b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15084b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15084bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15084c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15084c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15084cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15084d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15084d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15084dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15084e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15084e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15084ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15084f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15084f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15084fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1508501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150850700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150850c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1508511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1508516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150851c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150852190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1508526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150852c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150853180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1508536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150853c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150854170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1508546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150854c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150855160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1508556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x150855c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150856150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1508566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150856bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150857140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150857690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150857be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150858130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150858680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150858bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150859120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150859670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150859bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15085a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15085a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15085abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15085b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15085b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15085bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15085c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15085c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15085cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15085d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15085d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15085da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15085dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15085e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15085e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15085eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15085f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15085f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15085fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15085ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1508603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150860860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150860d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1508611a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150861640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150861b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1508622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1508629d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1508630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150863810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150863ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1508642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150864580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x150864b90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f76b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f74d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f74cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f74d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f720930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f722940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f74f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f71e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f71f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f71f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f71dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f71fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f716ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f70cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f721550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f722f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f72f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f76aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f719ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f71a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f74f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f74de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f7182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f7185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f718870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f76bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f76bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f76c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f76c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f76c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f76cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f76cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f76d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f76d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f76d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f76d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f76db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f76de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f76e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f76e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f76e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f76e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f76ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f76eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f76f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f76f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f76f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f76f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f76fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f76ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f7701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f7704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f770760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f770a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f770ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f770fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f771260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f771520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f7717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f771aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f771d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f772020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f7722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f7725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f772860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f608510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f6087d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f608c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f609160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f6095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f609a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f609eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f60a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f60a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f60ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f60b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f60b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f60b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f60bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f60c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f60c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f60cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f60cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f60d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f60d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f60dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f60e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f60e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f60ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f60ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f60f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f60f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f60fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f610050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f6104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f610da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f611210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f611680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f611af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f611f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f612680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f612f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f613450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f6139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f613ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f614990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f614ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f615430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f615e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f6162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f616760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f6170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f617540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f6179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f617e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f6187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f618c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f619100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f6195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f619a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f619ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f61a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f61a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f61acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f61b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f61b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f61baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f61bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f61c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f61c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f61cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f61d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f61d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f61db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f61dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f61e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f61e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f61ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f61f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f61f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f61fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f620000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f6204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f620940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f621280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f622060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f622500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f6229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f6232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f623780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f623c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f6240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f624560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f624a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f624ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f6257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f625c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f626120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f6265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f626a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f626f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f6273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f627840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f627ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f628180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f628ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f628f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f629400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f6298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f629d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f62a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f62a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f62ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f62afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f62b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f62b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f62bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f62c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f62c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f62cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f62d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f62d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f62db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f62e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f62e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f62e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f62efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f62f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14f62fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14f630240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f630500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f630b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14f631120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f631910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f631db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f632250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f6326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f632ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f6333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f633940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f633e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f6343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f634930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f634e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f6353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f635920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f635e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f6363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f636910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f636e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f6373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f637900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f637e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f6383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f6388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f638e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f6398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f639e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f63a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f63a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f63ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f63b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f63b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f63be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f63c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f63c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f63ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f63d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f63d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f63ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f63e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f63e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f63ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f63f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f63f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f63fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f640320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f640870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f640dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f641310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f641db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f642300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f642850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f642da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f6432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f643840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f6442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f644830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f644d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f6452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f645820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14f645cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14f646160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f646600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f646aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f646f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f6473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f647880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f647d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f6481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f648660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f648b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f648fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f649440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f6498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f649d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f64a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f64a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f64b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f64b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f64bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f64c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14f64ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f64ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f64d2d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.913s
user	0m0.242s
sys	0m0.129s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.56 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.62 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.18 sec*proc (2 tests)

Total Test time (real) =   1.19 sec
        1.21 real         0.71 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
