Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.591s
user	0m0.916s
sys	0m1.217s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target sha1
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Built target llava
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking C executable ../bin/test-c
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Linking CXX executable ../bin/test-tokenizer-0
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-sampling
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-log
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-gguf
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-quantize-fns
[ 64%] Built target test-backend-ops
[ 64%] Built target llama-batched-bench
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-chat-template
[ 64%] Built target test-gguf
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-autorelease
[ 64%] Built target test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-batched
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-batched
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-infill
[ 73%] Built target llama-embedding
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-bench
[ 73%] Built target llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-perplexity
[ 83%] Built target llama-lookup
[ 83%] Built target llama-cli
[ 83%] Built target llama-passkey
[ 83%] Built target llama-lookup-merge
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-quantize
[ 83%] Built target llama-lookup-stats
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-parallel
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Built target llama-retrieval
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-run
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tts
[ 93%] Built target llama-cvector-generator
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Built target llama-run
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.164s
user	0m6.172s
sys	0m10.008s

main: quantize time =  4408.76 ms
main:    total time =  4408.76 ms

main: quantize time =  2382.95 ms
main:    total time =  2382.95 ms

main: quantize time =  3171.08 ms
main:    total time =  3171.08 ms

main: quantize time =  1967.61 ms
main:    total time =  1967.61 ms

main: quantize time =  2188.55 ms
main:    total time =  2188.55 ms

main: quantize time =  6168.94 ms
main:    total time =  6168.94 ms

main: quantize time =  5966.82 ms
main:    total time =  5966.82 ms

main: quantize time =  6808.18 ms
main:    total time =  6808.18 ms

main: quantize time =  5998.25 ms
main:    total time =  5998.25 ms

main: quantize time =  4833.44 ms
main:    total time =  4833.44 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.125 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.283 I main: llama backend init
0.00.000.289 I main: load the model and apply lora adapter, if any
0.00.104.244 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.116.446 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.116.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.116.464 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.116.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.116.465 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.116.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.116.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.116.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.116.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.116.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.116.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.116.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.116.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.116.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.116.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.116.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.116.475 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.123.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.125.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.133.100 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.133.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.133.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.133.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.133.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.133.112 I llama_model_loader: - type  f32:  194 tensors
0.00.133.112 I llama_model_loader: - type  f16:   98 tensors
0.00.133.116 I print_info: file format = GGUF V3 (latest)
0.00.133.118 I print_info: file type   = all F32 (guessed)
0.00.133.120 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.145.744 I load: special tokens cache size = 25
0.00.155.738 I load: token to piece cache size = 0.2984 MB
0.00.155.741 I print_info: arch             = gptneox
0.00.155.741 I print_info: vocab_only       = 0
0.00.155.742 I print_info: n_ctx_train      = 2048
0.00.155.742 I print_info: n_embd           = 2048
0.00.155.742 I print_info: n_layer          = 24
0.00.155.746 I print_info: n_head           = 16
0.00.155.746 I print_info: n_head_kv        = 16
0.00.155.746 I print_info: n_rot            = 32
0.00.155.747 I print_info: n_swa            = 0
0.00.155.749 I print_info: n_embd_head_k    = 128
0.00.155.749 I print_info: n_embd_head_v    = 128
0.00.155.749 I print_info: n_gqa            = 1
0.00.155.750 I print_info: n_embd_k_gqa     = 2048
0.00.155.751 I print_info: n_embd_v_gqa     = 2048
0.00.155.751 I print_info: f_norm_eps       = 1.0e-05
0.00.155.751 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.155.752 I print_info: f_clamp_kqv      = 0.0e+00
0.00.155.752 I print_info: f_max_alibi_bias = 0.0e+00
0.00.155.752 I print_info: f_logit_scale    = 0.0e+00
0.00.155.753 I print_info: n_ff             = 8192
0.00.155.753 I print_info: n_expert         = 0
0.00.155.753 I print_info: n_expert_used    = 0
0.00.155.753 I print_info: causal attn      = 1
0.00.155.753 I print_info: pooling type     = 0
0.00.155.753 I print_info: rope type        = 2
0.00.155.754 I print_info: rope scaling     = linear
0.00.155.755 I print_info: freq_base_train  = 10000.0
0.00.155.755 I print_info: freq_scale_train = 1
0.00.155.755 I print_info: n_ctx_orig_yarn  = 2048
0.00.155.755 I print_info: rope_finetuned   = unknown
0.00.155.756 I print_info: ssm_d_conv       = 0
0.00.155.756 I print_info: ssm_d_inner      = 0
0.00.155.756 I print_info: ssm_d_state      = 0
0.00.155.756 I print_info: ssm_dt_rank      = 0
0.00.155.756 I print_info: ssm_dt_b_c_rms   = 0
0.00.155.756 I print_info: model type       = 1.4B
0.00.155.756 I print_info: model params     = 1.41 B
0.00.155.757 I print_info: general.name     = 1.4B
0.00.155.757 I print_info: vocab type       = BPE
0.00.155.757 I print_info: n_vocab          = 50304
0.00.155.757 I print_info: n_merges         = 50009
0.00.155.757 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.155.758 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.155.758 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.155.758 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.155.758 I print_info: LF token         = 128 'Ä'
0.00.155.758 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.155.759 I print_info: max token length = 1024
0.00.199.176 I load_tensors: offloading 24 repeating layers to GPU
0.00.199.180 I load_tensors: offloading output layer to GPU
0.00.199.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.199.205 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.199.206 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.199.525 I llama_init_from_model: n_seq_max     = 1
0.00.199.526 I llama_init_from_model: n_ctx         = 2048
0.00.199.526 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.199.527 I llama_init_from_model: n_batch       = 2048
0.00.199.527 I llama_init_from_model: n_ubatch      = 512
0.00.199.527 I llama_init_from_model: flash_attn    = 0
0.00.199.528 I llama_init_from_model: freq_base     = 10000.0
0.00.199.528 I llama_init_from_model: freq_scale    = 1
0.00.199.529 I ggml_metal_init: allocating
0.00.199.556 I ggml_metal_init: found device: Apple M4
0.00.199.562 I ggml_metal_init: picking default device: Apple M4
0.00.200.171 I ggml_metal_init: using embedded metal library
0.00.209.346 I ggml_metal_init: GPU name:   Apple M4
0.00.209.349 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.209.350 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.209.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.209.350 I ggml_metal_init: simdgroup reduction   = true
0.00.209.350 I ggml_metal_init: simdgroup matrix mul. = true
0.00.209.351 I ggml_metal_init: has residency sets    = true
0.00.209.351 I ggml_metal_init: has bfloat            = true
0.00.209.351 I ggml_metal_init: use bfloat            = true
0.00.209.351 I ggml_metal_init: hasUnifiedMemory      = true
0.00.209.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.234.162 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.263.984 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.263.990 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.264.013 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.268.313 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.268.315 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.268.316 I llama_init_from_model: graph nodes  = 967
0.00.268.316 I llama_init_from_model: graph splits = 2
0.00.268.319 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.268.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.268.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.335.127 I main: llama threadpool init, n_threads = 4
0.00.335.174 I 
0.00.335.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.335.206 I 
0.00.335.249 I sampler seed: 1234
0.00.335.253 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.335.278 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.335.280 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.335.280 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.163.251 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 60996.56 tokens per second)
0.02.163.252 I llama_perf_context_print:        load time =     229.86 ms
0.02.163.253 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.50 tokens per second)
0.02.163.254 I llama_perf_context_print:        eval time =    1781.52 ms /    63 runs   (   28.28 ms per token,    35.36 tokens per second)
0.02.163.254 I llama_perf_context_print:       total time =    1829.15 ms /    70 tokens
0.02.163.480 I ggml_metal_free: deallocating

real	0m2.522s
user	0m0.131s
sys	0m0.139s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.816 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.535 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.535 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.537 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.538 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.538 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.454 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.454 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.455 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.455 I llama_model_loader: - type  f32:  194 tensors
0.00.037.456 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.457 I print_info: file format = GGUF V3 (latest)
0.00.037.457 I print_info: file type   = Q8_0
0.00.037.459 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.627 I load: special tokens cache size = 25
0.00.053.189 I load: token to piece cache size = 0.2984 MB
0.00.053.194 I print_info: arch             = gptneox
0.00.053.194 I print_info: vocab_only       = 0
0.00.053.194 I print_info: n_ctx_train      = 2048
0.00.053.195 I print_info: n_embd           = 2048
0.00.053.196 I print_info: n_layer          = 24
0.00.053.202 I print_info: n_head           = 16
0.00.053.202 I print_info: n_head_kv        = 16
0.00.053.203 I print_info: n_rot            = 32
0.00.053.203 I print_info: n_swa            = 0
0.00.053.204 I print_info: n_embd_head_k    = 128
0.00.053.204 I print_info: n_embd_head_v    = 128
0.00.053.205 I print_info: n_gqa            = 1
0.00.053.206 I print_info: n_embd_k_gqa     = 2048
0.00.053.206 I print_info: n_embd_v_gqa     = 2048
0.00.053.207 I print_info: f_norm_eps       = 1.0e-05
0.00.053.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.208 I print_info: f_logit_scale    = 0.0e+00
0.00.053.209 I print_info: n_ff             = 8192
0.00.053.209 I print_info: n_expert         = 0
0.00.053.209 I print_info: n_expert_used    = 0
0.00.053.209 I print_info: causal attn      = 1
0.00.053.210 I print_info: pooling type     = 0
0.00.053.210 I print_info: rope type        = 2
0.00.053.210 I print_info: rope scaling     = linear
0.00.053.211 I print_info: freq_base_train  = 10000.0
0.00.053.212 I print_info: freq_scale_train = 1
0.00.053.213 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.213 I print_info: rope_finetuned   = unknown
0.00.053.213 I print_info: ssm_d_conv       = 0
0.00.053.213 I print_info: ssm_d_inner      = 0
0.00.053.213 I print_info: ssm_d_state      = 0
0.00.053.213 I print_info: ssm_dt_rank      = 0
0.00.053.214 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.214 I print_info: model type       = 1.4B
0.00.053.214 I print_info: model params     = 1.41 B
0.00.053.215 I print_info: general.name     = 1.4B
0.00.053.215 I print_info: vocab type       = BPE
0.00.053.216 I print_info: n_vocab          = 50304
0.00.053.216 I print_info: n_merges         = 50009
0.00.053.216 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.216 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.216 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.216 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.218 I print_info: LF token         = 128 'Ä'
0.00.053.218 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.219 I print_info: max token length = 1024
0.01.080.356 I load_tensors: offloading 24 repeating layers to GPU
0.01.080.361 I load_tensors: offloading output layer to GPU
0.01.080.362 I load_tensors: offloaded 25/25 layers to GPU
0.01.080.386 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.080.387 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.081.047 I llama_init_from_model: n_seq_max     = 1
0.01.081.049 I llama_init_from_model: n_ctx         = 2048
0.01.081.050 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.081.050 I llama_init_from_model: n_batch       = 2048
0.01.081.051 I llama_init_from_model: n_ubatch      = 512
0.01.081.051 I llama_init_from_model: flash_attn    = 0
0.01.081.052 I llama_init_from_model: freq_base     = 10000.0
0.01.081.052 I llama_init_from_model: freq_scale    = 1
0.01.081.053 I ggml_metal_init: allocating
0.01.081.065 I ggml_metal_init: found device: Apple M4
0.01.081.072 I ggml_metal_init: picking default device: Apple M4
0.01.082.253 I ggml_metal_init: using embedded metal library
0.01.087.451 I ggml_metal_init: GPU name:   Apple M4
0.01.087.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.087.455 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.087.456 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.087.456 I ggml_metal_init: simdgroup reduction   = true
0.01.087.456 I ggml_metal_init: simdgroup matrix mul. = true
0.01.087.456 I ggml_metal_init: has residency sets    = true
0.01.087.457 I ggml_metal_init: has bfloat            = true
0.01.087.457 I ggml_metal_init: use bfloat            = true
0.01.087.457 I ggml_metal_init: hasUnifiedMemory      = true
0.01.087.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.104.978 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.159.569 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.159.575 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.159.593 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.164.306 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.164.308 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.164.308 I llama_init_from_model: graph nodes  = 967
0.01.164.308 I llama_init_from_model: graph splits = 2
0.01.164.315 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.164.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.164.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.221.041 I main: llama threadpool init, n_threads = 4
0.01.221.081 I 
0.01.221.103 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.221.103 I 
0.01.221.282 I sampler seed: 1234
0.01.221.286 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.221.321 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.221.325 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.221.325 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.305.730 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.02.305.731 I llama_perf_context_print:        load time =    1210.34 ms
0.02.305.732 I llama_perf_context_print: prompt eval time =      49.21 ms /     7 tokens (    7.03 ms per token,   142.25 tokens per second)
0.02.305.732 I llama_perf_context_print:        eval time =    1032.32 ms /    63 runs   (   16.39 ms per token,    61.03 tokens per second)
0.02.305.733 I llama_perf_context_print:       total time =    1085.57 ms /    70 tokens
0.02.306.037 I ggml_metal_free: deallocating

real	0m2.323s
user	0m0.110s
sys	0m0.249s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.017.170 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.024 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.028 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.029 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.029 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.033 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.036 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.036 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.426 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.752 I llama_model_loader: - type  f32:  194 tensors
0.00.044.752 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.753 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.753 I print_info: file format = GGUF V3 (latest)
0.00.044.758 I print_info: file type   = Q4_0
0.00.044.759 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.054.169 I load: special tokens cache size = 25
0.00.061.836 I load: token to piece cache size = 0.2984 MB
0.00.061.840 I print_info: arch             = gptneox
0.00.061.841 I print_info: vocab_only       = 0
0.00.061.841 I print_info: n_ctx_train      = 2048
0.00.061.841 I print_info: n_embd           = 2048
0.00.061.841 I print_info: n_layer          = 24
0.00.061.846 I print_info: n_head           = 16
0.00.061.847 I print_info: n_head_kv        = 16
0.00.061.847 I print_info: n_rot            = 32
0.00.061.848 I print_info: n_swa            = 0
0.00.061.848 I print_info: n_embd_head_k    = 128
0.00.061.848 I print_info: n_embd_head_v    = 128
0.00.061.849 I print_info: n_gqa            = 1
0.00.061.850 I print_info: n_embd_k_gqa     = 2048
0.00.061.850 I print_info: n_embd_v_gqa     = 2048
0.00.061.851 I print_info: f_norm_eps       = 1.0e-05
0.00.061.852 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.852 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.852 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.852 I print_info: f_logit_scale    = 0.0e+00
0.00.061.853 I print_info: n_ff             = 8192
0.00.061.853 I print_info: n_expert         = 0
0.00.061.853 I print_info: n_expert_used    = 0
0.00.061.853 I print_info: causal attn      = 1
0.00.061.854 I print_info: pooling type     = 0
0.00.061.854 I print_info: rope type        = 2
0.00.061.854 I print_info: rope scaling     = linear
0.00.061.855 I print_info: freq_base_train  = 10000.0
0.00.061.855 I print_info: freq_scale_train = 1
0.00.061.855 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.855 I print_info: rope_finetuned   = unknown
0.00.061.855 I print_info: ssm_d_conv       = 0
0.00.061.856 I print_info: ssm_d_inner      = 0
0.00.061.856 I print_info: ssm_d_state      = 0
0.00.061.856 I print_info: ssm_dt_rank      = 0
0.00.061.856 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.856 I print_info: model type       = 1.4B
0.00.061.857 I print_info: model params     = 1.41 B
0.00.061.857 I print_info: general.name     = 1.4B
0.00.061.858 I print_info: vocab type       = BPE
0.00.061.858 I print_info: n_vocab          = 50304
0.00.061.858 I print_info: n_merges         = 50009
0.00.061.859 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.859 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.859 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.859 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.859 I print_info: LF token         = 128 'Ä'
0.00.061.860 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.860 I print_info: max token length = 1024
0.00.649.649 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.670 I load_tensors: offloading output layer to GPU
0.00.649.671 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.706 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.649.707 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.650.945 I llama_init_from_model: n_seq_max     = 1
0.00.650.952 I llama_init_from_model: n_ctx         = 2048
0.00.650.953 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.953 I llama_init_from_model: n_batch       = 2048
0.00.650.954 I llama_init_from_model: n_ubatch      = 512
0.00.650.954 I llama_init_from_model: flash_attn    = 0
0.00.650.957 I llama_init_from_model: freq_base     = 10000.0
0.00.650.958 I llama_init_from_model: freq_scale    = 1
0.00.650.960 I ggml_metal_init: allocating
0.00.651.035 I ggml_metal_init: found device: Apple M4
0.00.651.050 I ggml_metal_init: picking default device: Apple M4
0.00.652.955 I ggml_metal_init: using embedded metal library
0.00.658.686 I ggml_metal_init: GPU name:   Apple M4
0.00.658.703 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.706 I ggml_metal_init: simdgroup reduction   = true
0.00.658.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.706 I ggml_metal_init: has residency sets    = true
0.00.658.707 I ggml_metal_init: has bfloat            = true
0.00.658.707 I ggml_metal_init: use bfloat            = true
0.00.658.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.679.330 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.695 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.739.703 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.729 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.744.664 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.744.665 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.744.666 I llama_init_from_model: graph nodes  = 967
0.00.744.666 I llama_init_from_model: graph splits = 2
0.00.744.671 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.744.804 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.744.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.245 I main: llama threadpool init, n_threads = 4
0.00.786.283 I 
0.00.786.306 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.306 I 
0.00.786.407 I sampler seed: 1234
0.00.786.412 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.431 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.431 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.493.630 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.01.493.631 I llama_perf_context_print:        load time =     768.19 ms
0.01.493.632 I llama_perf_context_print: prompt eval time =      39.59 ms /     7 tokens (    5.66 ms per token,   176.81 tokens per second)
0.01.493.632 I llama_perf_context_print:        eval time =     664.74 ms /    63 runs   (   10.55 ms per token,    94.77 tokens per second)
0.01.493.633 I llama_perf_context_print:       total time =     708.27 ms /    70 tokens
0.01.493.869 I ggml_metal_free: deallocating

real	0m1.513s
user	0m0.117s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.124 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.764 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.767 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.769 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.771 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.490 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.474 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.207 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.209 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.209 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.210 I llama_model_loader: - type  f32:  194 tensors
0.00.028.210 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.210 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.211 I print_info: file format = GGUF V3 (latest)
0.00.028.211 I print_info: file type   = Q4_1
0.00.028.212 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.048 I load: special tokens cache size = 25
0.00.042.022 I load: token to piece cache size = 0.2984 MB
0.00.042.025 I print_info: arch             = gptneox
0.00.042.025 I print_info: vocab_only       = 0
0.00.042.026 I print_info: n_ctx_train      = 2048
0.00.042.026 I print_info: n_embd           = 2048
0.00.042.026 I print_info: n_layer          = 24
0.00.042.029 I print_info: n_head           = 16
0.00.042.030 I print_info: n_head_kv        = 16
0.00.042.032 I print_info: n_rot            = 32
0.00.042.032 I print_info: n_swa            = 0
0.00.042.032 I print_info: n_embd_head_k    = 128
0.00.042.032 I print_info: n_embd_head_v    = 128
0.00.042.033 I print_info: n_gqa            = 1
0.00.042.034 I print_info: n_embd_k_gqa     = 2048
0.00.042.035 I print_info: n_embd_v_gqa     = 2048
0.00.042.035 I print_info: f_norm_eps       = 1.0e-05
0.00.042.036 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.036 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.036 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.036 I print_info: f_logit_scale    = 0.0e+00
0.00.042.037 I print_info: n_ff             = 8192
0.00.042.037 I print_info: n_expert         = 0
0.00.042.037 I print_info: n_expert_used    = 0
0.00.042.037 I print_info: causal attn      = 1
0.00.042.037 I print_info: pooling type     = 0
0.00.042.038 I print_info: rope type        = 2
0.00.042.038 I print_info: rope scaling     = linear
0.00.042.039 I print_info: freq_base_train  = 10000.0
0.00.042.039 I print_info: freq_scale_train = 1
0.00.042.040 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.040 I print_info: rope_finetuned   = unknown
0.00.042.040 I print_info: ssm_d_conv       = 0
0.00.042.040 I print_info: ssm_d_inner      = 0
0.00.042.040 I print_info: ssm_d_state      = 0
0.00.042.040 I print_info: ssm_dt_rank      = 0
0.00.042.042 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.042 I print_info: model type       = 1.4B
0.00.042.042 I print_info: model params     = 1.41 B
0.00.042.043 I print_info: general.name     = 1.4B
0.00.042.043 I print_info: vocab type       = BPE
0.00.042.043 I print_info: n_vocab          = 50304
0.00.042.044 I print_info: n_merges         = 50009
0.00.042.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.045 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.046 I print_info: LF token         = 128 'Ä'
0.00.042.048 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.048 I print_info: max token length = 1024
0.00.689.004 I load_tensors: offloading 24 repeating layers to GPU
0.00.689.016 I load_tensors: offloading output layer to GPU
0.00.689.017 I load_tensors: offloaded 25/25 layers to GPU
0.00.689.048 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.689.050 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.690.183 I llama_init_from_model: n_seq_max     = 1
0.00.690.190 I llama_init_from_model: n_ctx         = 2048
0.00.690.191 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.690.191 I llama_init_from_model: n_batch       = 2048
0.00.690.192 I llama_init_from_model: n_ubatch      = 512
0.00.690.192 I llama_init_from_model: flash_attn    = 0
0.00.690.194 I llama_init_from_model: freq_base     = 10000.0
0.00.690.195 I llama_init_from_model: freq_scale    = 1
0.00.690.198 I ggml_metal_init: allocating
0.00.690.265 I ggml_metal_init: found device: Apple M4
0.00.690.280 I ggml_metal_init: picking default device: Apple M4
0.00.692.146 I ggml_metal_init: using embedded metal library
0.00.697.740 I ggml_metal_init: GPU name:   Apple M4
0.00.697.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.697.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.697.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.697.747 I ggml_metal_init: simdgroup reduction   = true
0.00.697.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.697.748 I ggml_metal_init: has residency sets    = true
0.00.697.748 I ggml_metal_init: has bfloat            = true
0.00.697.749 I ggml_metal_init: use bfloat            = true
0.00.697.750 I ggml_metal_init: hasUnifiedMemory      = true
0.00.697.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.716.667 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.780.105 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.780.112 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.780.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.786.302 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.786.305 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.786.305 I llama_init_from_model: graph nodes  = 967
0.00.786.305 I llama_init_from_model: graph splits = 2
0.00.786.312 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.786.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.786.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.092 I main: llama threadpool init, n_threads = 4
0.00.836.134 I 
0.00.836.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.160 I 
0.00.836.278 I sampler seed: 1234
0.00.836.283 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.302 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.303 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.303 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.602.523 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.602.524 I llama_perf_context_print:        load time =     826.07 ms
0.01.602.524 I llama_perf_context_print: prompt eval time =      49.55 ms /     7 tokens (    7.08 ms per token,   141.27 tokens per second)
0.01.602.525 I llama_perf_context_print:        eval time =     713.96 ms /    63 runs   (   11.33 ms per token,    88.24 tokens per second)
0.01.602.525 I llama_perf_context_print:       total time =     767.32 ms /    70 tokens
0.01.602.767 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.109s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.340 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.294 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.301 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.302 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.302 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.303 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.304 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.305 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.305 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.306 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.308 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.309 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.310 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.310 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.060 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.734 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.735 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.735 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.736 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.736 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.736 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.737 I llama_model_loader: - type  f32:  194 tensors
0.00.026.737 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.737 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.738 I print_info: file format = GGUF V3 (latest)
0.00.026.738 I print_info: file type   = Q5_0
0.00.026.739 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.630 I load: special tokens cache size = 25
0.00.040.483 I load: token to piece cache size = 0.2984 MB
0.00.040.485 I print_info: arch             = gptneox
0.00.040.486 I print_info: vocab_only       = 0
0.00.040.486 I print_info: n_ctx_train      = 2048
0.00.040.486 I print_info: n_embd           = 2048
0.00.040.486 I print_info: n_layer          = 24
0.00.040.489 I print_info: n_head           = 16
0.00.040.490 I print_info: n_head_kv        = 16
0.00.040.490 I print_info: n_rot            = 32
0.00.040.490 I print_info: n_swa            = 0
0.00.040.490 I print_info: n_embd_head_k    = 128
0.00.040.491 I print_info: n_embd_head_v    = 128
0.00.040.491 I print_info: n_gqa            = 1
0.00.040.492 I print_info: n_embd_k_gqa     = 2048
0.00.040.493 I print_info: n_embd_v_gqa     = 2048
0.00.040.493 I print_info: f_norm_eps       = 1.0e-05
0.00.040.494 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.494 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.494 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.494 I print_info: f_logit_scale    = 0.0e+00
0.00.040.495 I print_info: n_ff             = 8192
0.00.040.495 I print_info: n_expert         = 0
0.00.040.495 I print_info: n_expert_used    = 0
0.00.040.495 I print_info: causal attn      = 1
0.00.040.495 I print_info: pooling type     = 0
0.00.040.496 I print_info: rope type        = 2
0.00.040.496 I print_info: rope scaling     = linear
0.00.040.496 I print_info: freq_base_train  = 10000.0
0.00.040.497 I print_info: freq_scale_train = 1
0.00.040.497 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.497 I print_info: rope_finetuned   = unknown
0.00.040.497 I print_info: ssm_d_conv       = 0
0.00.040.497 I print_info: ssm_d_inner      = 0
0.00.040.498 I print_info: ssm_d_state      = 0
0.00.040.498 I print_info: ssm_dt_rank      = 0
0.00.040.498 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.498 I print_info: model type       = 1.4B
0.00.040.505 I print_info: model params     = 1.41 B
0.00.040.507 I print_info: general.name     = 1.4B
0.00.040.508 I print_info: vocab type       = BPE
0.00.040.508 I print_info: n_vocab          = 50304
0.00.040.508 I print_info: n_merges         = 50009
0.00.040.509 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.509 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.510 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.510 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.510 I print_info: LF token         = 128 'Ä'
0.00.040.510 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.511 I print_info: max token length = 1024
0.00.674.221 I load_tensors: offloading 24 repeating layers to GPU
0.00.674.243 I load_tensors: offloading output layer to GPU
0.00.674.243 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.279 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.674.280 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.675.429 I llama_init_from_model: n_seq_max     = 1
0.00.675.435 I llama_init_from_model: n_ctx         = 2048
0.00.675.435 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.675.436 I llama_init_from_model: n_batch       = 2048
0.00.675.436 I llama_init_from_model: n_ubatch      = 512
0.00.675.436 I llama_init_from_model: flash_attn    = 0
0.00.675.438 I llama_init_from_model: freq_base     = 10000.0
0.00.675.439 I llama_init_from_model: freq_scale    = 1
0.00.675.442 I ggml_metal_init: allocating
0.00.675.521 I ggml_metal_init: found device: Apple M4
0.00.675.537 I ggml_metal_init: picking default device: Apple M4
0.00.677.401 I ggml_metal_init: using embedded metal library
0.00.683.391 I ggml_metal_init: GPU name:   Apple M4
0.00.683.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.683.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.683.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.683.400 I ggml_metal_init: simdgroup reduction   = true
0.00.683.400 I ggml_metal_init: simdgroup matrix mul. = true
0.00.683.400 I ggml_metal_init: has residency sets    = true
0.00.683.401 I ggml_metal_init: has bfloat            = true
0.00.683.401 I ggml_metal_init: use bfloat            = true
0.00.683.402 I ggml_metal_init: hasUnifiedMemory      = true
0.00.683.404 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.612 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.756.797 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.756.803 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.756.831 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.761.152 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.761.154 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.761.154 I llama_init_from_model: graph nodes  = 967
0.00.761.154 I llama_init_from_model: graph splits = 2
0.00.761.160 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.761.284 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.761.284 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.808.988 I main: llama threadpool init, n_threads = 4
0.00.809.032 I 
0.00.809.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.056 I 
0.00.809.171 I sampler seed: 1234
0.00.809.176 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.218 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.222 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.222 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.634.311 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51636.36 tokens per second)
0.01.634.311 I llama_perf_context_print:        load time =     797.78 ms
0.01.634.312 I llama_perf_context_print: prompt eval time =      43.24 ms /     7 tokens (    6.18 ms per token,   161.90 tokens per second)
0.01.634.313 I llama_perf_context_print:        eval time =     778.91 ms /    63 runs   (   12.36 ms per token,    80.88 tokens per second)
0.01.634.313 I llama_perf_context_print:       total time =     826.19 ms /    70 tokens
0.01.634.585 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.111s
sys	0m0.218s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.269 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.805 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.810 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.814 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.815 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.819 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.820 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.820 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.821 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.821 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.822 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.685 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.670 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.424 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.425 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.425 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.426 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.426 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.427 I llama_model_loader: - type  f32:  194 tensors
0.00.026.427 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.427 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.428 I print_info: file format = GGUF V3 (latest)
0.00.026.428 I print_info: file type   = Q5_1
0.00.026.429 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.601 I load: special tokens cache size = 25
0.00.040.594 I load: token to piece cache size = 0.2984 MB
0.00.040.597 I print_info: arch             = gptneox
0.00.040.597 I print_info: vocab_only       = 0
0.00.040.598 I print_info: n_ctx_train      = 2048
0.00.040.598 I print_info: n_embd           = 2048
0.00.040.598 I print_info: n_layer          = 24
0.00.040.601 I print_info: n_head           = 16
0.00.040.602 I print_info: n_head_kv        = 16
0.00.040.604 I print_info: n_rot            = 32
0.00.040.604 I print_info: n_swa            = 0
0.00.040.604 I print_info: n_embd_head_k    = 128
0.00.040.604 I print_info: n_embd_head_v    = 128
0.00.040.605 I print_info: n_gqa            = 1
0.00.040.606 I print_info: n_embd_k_gqa     = 2048
0.00.040.607 I print_info: n_embd_v_gqa     = 2048
0.00.040.607 I print_info: f_norm_eps       = 1.0e-05
0.00.040.608 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.608 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.608 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.608 I print_info: f_logit_scale    = 0.0e+00
0.00.040.609 I print_info: n_ff             = 8192
0.00.040.609 I print_info: n_expert         = 0
0.00.040.609 I print_info: n_expert_used    = 0
0.00.040.609 I print_info: causal attn      = 1
0.00.040.609 I print_info: pooling type     = 0
0.00.040.610 I print_info: rope type        = 2
0.00.040.611 I print_info: rope scaling     = linear
0.00.040.612 I print_info: freq_base_train  = 10000.0
0.00.040.612 I print_info: freq_scale_train = 1
0.00.040.612 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.613 I print_info: rope_finetuned   = unknown
0.00.040.613 I print_info: ssm_d_conv       = 0
0.00.040.613 I print_info: ssm_d_inner      = 0
0.00.040.613 I print_info: ssm_d_state      = 0
0.00.040.613 I print_info: ssm_dt_rank      = 0
0.00.040.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.613 I print_info: model type       = 1.4B
0.00.040.614 I print_info: model params     = 1.41 B
0.00.040.614 I print_info: general.name     = 1.4B
0.00.040.618 I print_info: vocab type       = BPE
0.00.040.619 I print_info: n_vocab          = 50304
0.00.040.619 I print_info: n_merges         = 50009
0.00.040.619 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.621 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.621 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.621 I print_info: LF token         = 128 'Ä'
0.00.040.621 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.621 I print_info: max token length = 1024
0.00.613.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.850 I load_tensors: offloading output layer to GPU
0.00.613.850 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.882 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.613.883 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.615.142 I llama_init_from_model: n_seq_max     = 1
0.00.615.146 I llama_init_from_model: n_ctx         = 2048
0.00.615.146 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.615.146 I llama_init_from_model: n_batch       = 2048
0.00.615.147 I llama_init_from_model: n_ubatch      = 512
0.00.615.147 I llama_init_from_model: flash_attn    = 0
0.00.615.149 I llama_init_from_model: freq_base     = 10000.0
0.00.615.149 I llama_init_from_model: freq_scale    = 1
0.00.615.152 I ggml_metal_init: allocating
0.00.615.170 I ggml_metal_init: found device: Apple M4
0.00.615.182 I ggml_metal_init: picking default device: Apple M4
0.00.616.578 I ggml_metal_init: using embedded metal library
0.00.623.086 I ggml_metal_init: GPU name:   Apple M4
0.00.623.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.092 I ggml_metal_init: simdgroup reduction   = true
0.00.623.093 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.093 I ggml_metal_init: has residency sets    = true
0.00.623.093 I ggml_metal_init: has bfloat            = true
0.00.623.093 I ggml_metal_init: use bfloat            = true
0.00.623.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.690 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.199 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.697.207 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.230 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.701.534 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.701.537 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.701.537 I llama_init_from_model: graph nodes  = 967
0.00.701.537 I llama_init_from_model: graph splits = 2
0.00.701.543 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.701.675 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.701.676 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.099 I main: llama threadpool init, n_threads = 4
0.00.752.143 I 
0.00.752.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.172 I 
0.00.752.306 I sampler seed: 1234
0.00.752.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.353 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.356 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.356 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.644.342 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.644.344 I llama_perf_context_print:        load time =     740.92 ms
0.01.644.345 I llama_perf_context_print: prompt eval time =      52.41 ms /     7 tokens (    7.49 ms per token,   133.57 tokens per second)
0.01.644.346 I llama_perf_context_print:        eval time =     836.56 ms /    63 runs   (   13.28 ms per token,    75.31 tokens per second)
0.01.644.346 I llama_perf_context_print:       total time =     893.15 ms /    70 tokens
0.01.644.588 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.112s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.769 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.709 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.714 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.716 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.716 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.717 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.717 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.718 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.718 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.723 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.208 I llama_model_loader: - type  f32:  194 tensors
0.00.025.208 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.208 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.209 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.209 I print_info: file format = GGUF V3 (latest)
0.00.025.210 I print_info: file type   = Q2_K - Medium
0.00.025.211 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.073 I load: special tokens cache size = 25
0.00.038.861 I load: token to piece cache size = 0.2984 MB
0.00.038.864 I print_info: arch             = gptneox
0.00.038.864 I print_info: vocab_only       = 0
0.00.038.865 I print_info: n_ctx_train      = 2048
0.00.038.865 I print_info: n_embd           = 2048
0.00.038.865 I print_info: n_layer          = 24
0.00.038.868 I print_info: n_head           = 16
0.00.038.869 I print_info: n_head_kv        = 16
0.00.038.869 I print_info: n_rot            = 32
0.00.038.870 I print_info: n_swa            = 0
0.00.038.870 I print_info: n_embd_head_k    = 128
0.00.038.870 I print_info: n_embd_head_v    = 128
0.00.038.871 I print_info: n_gqa            = 1
0.00.038.872 I print_info: n_embd_k_gqa     = 2048
0.00.038.872 I print_info: n_embd_v_gqa     = 2048
0.00.038.873 I print_info: f_norm_eps       = 1.0e-05
0.00.038.873 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.873 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.874 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.874 I print_info: f_logit_scale    = 0.0e+00
0.00.038.874 I print_info: n_ff             = 8192
0.00.038.875 I print_info: n_expert         = 0
0.00.038.875 I print_info: n_expert_used    = 0
0.00.038.875 I print_info: causal attn      = 1
0.00.038.876 I print_info: pooling type     = 0
0.00.038.881 I print_info: rope type        = 2
0.00.038.881 I print_info: rope scaling     = linear
0.00.038.881 I print_info: freq_base_train  = 10000.0
0.00.038.882 I print_info: freq_scale_train = 1
0.00.038.882 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.882 I print_info: rope_finetuned   = unknown
0.00.038.882 I print_info: ssm_d_conv       = 0
0.00.038.882 I print_info: ssm_d_inner      = 0
0.00.038.883 I print_info: ssm_d_state      = 0
0.00.038.883 I print_info: ssm_dt_rank      = 0
0.00.038.883 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.883 I print_info: model type       = 1.4B
0.00.038.883 I print_info: model params     = 1.41 B
0.00.038.883 I print_info: general.name     = 1.4B
0.00.038.884 I print_info: vocab type       = BPE
0.00.038.884 I print_info: n_vocab          = 50304
0.00.038.884 I print_info: n_merges         = 50009
0.00.038.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.885 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.885 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.885 I print_info: LF token         = 128 'Ä'
0.00.038.886 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.886 I print_info: max token length = 1024
0.00.342.725 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.739 I load_tensors: offloading output layer to GPU
0.00.342.740 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.767 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.768 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.343.865 I llama_init_from_model: n_seq_max     = 1
0.00.343.874 I llama_init_from_model: n_ctx         = 2048
0.00.343.874 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.343.875 I llama_init_from_model: n_batch       = 2048
0.00.343.875 I llama_init_from_model: n_ubatch      = 512
0.00.343.875 I llama_init_from_model: flash_attn    = 0
0.00.343.877 I llama_init_from_model: freq_base     = 10000.0
0.00.343.880 I llama_init_from_model: freq_scale    = 1
0.00.343.883 I ggml_metal_init: allocating
0.00.343.929 I ggml_metal_init: found device: Apple M4
0.00.343.941 I ggml_metal_init: picking default device: Apple M4
0.00.345.648 I ggml_metal_init: using embedded metal library
0.00.351.120 I ggml_metal_init: GPU name:   Apple M4
0.00.351.134 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.135 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.136 I ggml_metal_init: simdgroup reduction   = true
0.00.351.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.137 I ggml_metal_init: has residency sets    = true
0.00.351.137 I ggml_metal_init: has bfloat            = true
0.00.351.137 I ggml_metal_init: use bfloat            = true
0.00.351.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.580 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.430.165 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.430.173 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.430.197 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.436.214 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.436.216 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.436.216 I llama_init_from_model: graph nodes  = 967
0.00.436.217 I llama_init_from_model: graph splits = 2
0.00.436.220 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.436.345 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.436.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.831 I main: llama threadpool init, n_threads = 4
0.00.496.880 I 
0.00.496.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.905 I 
0.00.497.071 I sampler seed: 1234
0.00.497.079 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.497.092 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.497.093 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.497.094 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.178.541 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.178.542 I llama_perf_context_print:        load time =     486.10 ms
0.01.178.546 I llama_perf_context_print: prompt eval time =      44.26 ms /     7 tokens (    6.32 ms per token,   158.17 tokens per second)
0.01.178.547 I llama_perf_context_print:        eval time =     634.44 ms /    63 runs   (   10.07 ms per token,    99.30 tokens per second)
0.01.178.547 I llama_perf_context_print:       total time =     682.67 ms /    70 tokens
0.01.178.775 I ggml_metal_free: deallocating

real	0m1.198s
user	0m0.113s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.356 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.357 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.357 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.358 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.359 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.361 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.361 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.251 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.261 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.079 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.080 I llama_model_loader: - type  f32:  194 tensors
0.00.025.080 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.080 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.080 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.081 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.081 I print_info: file format = GGUF V3 (latest)
0.00.025.082 I print_info: file type   = Q3_K - Medium
0.00.025.083 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.316 I load: special tokens cache size = 25
0.00.039.015 I load: token to piece cache size = 0.2984 MB
0.00.039.018 I print_info: arch             = gptneox
0.00.039.018 I print_info: vocab_only       = 0
0.00.039.019 I print_info: n_ctx_train      = 2048
0.00.039.019 I print_info: n_embd           = 2048
0.00.039.019 I print_info: n_layer          = 24
0.00.039.022 I print_info: n_head           = 16
0.00.039.023 I print_info: n_head_kv        = 16
0.00.039.023 I print_info: n_rot            = 32
0.00.039.023 I print_info: n_swa            = 0
0.00.039.023 I print_info: n_embd_head_k    = 128
0.00.039.024 I print_info: n_embd_head_v    = 128
0.00.039.024 I print_info: n_gqa            = 1
0.00.039.025 I print_info: n_embd_k_gqa     = 2048
0.00.039.026 I print_info: n_embd_v_gqa     = 2048
0.00.039.027 I print_info: f_norm_eps       = 1.0e-05
0.00.039.027 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.028 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.028 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.028 I print_info: f_logit_scale    = 0.0e+00
0.00.039.029 I print_info: n_ff             = 8192
0.00.039.029 I print_info: n_expert         = 0
0.00.039.029 I print_info: n_expert_used    = 0
0.00.039.031 I print_info: causal attn      = 1
0.00.039.032 I print_info: pooling type     = 0
0.00.039.032 I print_info: rope type        = 2
0.00.039.033 I print_info: rope scaling     = linear
0.00.039.033 I print_info: freq_base_train  = 10000.0
0.00.039.033 I print_info: freq_scale_train = 1
0.00.039.034 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.035 I print_info: rope_finetuned   = unknown
0.00.039.035 I print_info: ssm_d_conv       = 0
0.00.039.035 I print_info: ssm_d_inner      = 0
0.00.039.035 I print_info: ssm_d_state      = 0
0.00.039.036 I print_info: ssm_dt_rank      = 0
0.00.039.036 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.036 I print_info: model type       = 1.4B
0.00.039.036 I print_info: model params     = 1.41 B
0.00.039.036 I print_info: general.name     = 1.4B
0.00.039.037 I print_info: vocab type       = BPE
0.00.039.037 I print_info: n_vocab          = 50304
0.00.039.037 I print_info: n_merges         = 50009
0.00.039.038 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.038 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.038 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.038 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: LF token         = 128 'Ä'
0.00.039.039 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: max token length = 1024
0.00.444.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.990 I load_tensors: offloading output layer to GPU
0.00.444.991 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.025 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.026 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.446.558 I llama_init_from_model: n_seq_max     = 1
0.00.446.563 I llama_init_from_model: n_ctx         = 2048
0.00.446.564 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.446.564 I llama_init_from_model: n_batch       = 2048
0.00.446.565 I llama_init_from_model: n_ubatch      = 512
0.00.446.565 I llama_init_from_model: flash_attn    = 0
0.00.446.571 I llama_init_from_model: freq_base     = 10000.0
0.00.446.575 I llama_init_from_model: freq_scale    = 1
0.00.446.577 I ggml_metal_init: allocating
0.00.446.649 I ggml_metal_init: found device: Apple M4
0.00.446.664 I ggml_metal_init: picking default device: Apple M4
0.00.448.464 I ggml_metal_init: using embedded metal library
0.00.454.432 I ggml_metal_init: GPU name:   Apple M4
0.00.454.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.441 I ggml_metal_init: simdgroup reduction   = true
0.00.454.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.441 I ggml_metal_init: has residency sets    = true
0.00.454.442 I ggml_metal_init: has bfloat            = true
0.00.454.442 I ggml_metal_init: use bfloat            = true
0.00.454.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.446 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.393 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.530.759 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.530.768 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.530.802 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.534.960 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.534.962 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.534.962 I llama_init_from_model: graph nodes  = 967
0.00.534.963 I llama_init_from_model: graph splits = 2
0.00.534.967 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.535.097 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.535.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.708 I main: llama threadpool init, n_threads = 4
0.00.588.751 I 
0.00.588.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.775 I 
0.00.588.942 I sampler seed: 1234
0.00.588.947 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.588.983 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.588.986 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.588.986 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.343.566 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48530.42 tokens per second)
0.01.343.567 I llama_perf_context_print:        load time =     579.05 ms
0.01.343.568 I llama_perf_context_print: prompt eval time =      50.51 ms /     7 tokens (    7.22 ms per token,   138.58 tokens per second)
0.01.343.569 I llama_perf_context_print:        eval time =     701.55 ms /    63 runs   (   11.14 ms per token,    89.80 tokens per second)
0.01.343.569 I llama_perf_context_print:       total time =     755.73 ms /    70 tokens
0.01.343.876 I ggml_metal_free: deallocating

real	0m1.360s
user	0m0.110s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.281 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.292 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.292 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.293 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.294 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.296 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.042 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.089 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.920 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.921 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.921 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.922 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.922 I llama_model_loader: - type  f32:  194 tensors
0.00.025.923 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.923 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.923 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.924 I print_info: file format = GGUF V3 (latest)
0.00.025.924 I print_info: file type   = Q4_K - Medium
0.00.025.925 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.075 I load: special tokens cache size = 25
0.00.040.017 I load: token to piece cache size = 0.2984 MB
0.00.040.019 I print_info: arch             = gptneox
0.00.040.020 I print_info: vocab_only       = 0
0.00.040.020 I print_info: n_ctx_train      = 2048
0.00.040.020 I print_info: n_embd           = 2048
0.00.040.020 I print_info: n_layer          = 24
0.00.040.024 I print_info: n_head           = 16
0.00.040.024 I print_info: n_head_kv        = 16
0.00.040.025 I print_info: n_rot            = 32
0.00.040.025 I print_info: n_swa            = 0
0.00.040.025 I print_info: n_embd_head_k    = 128
0.00.040.025 I print_info: n_embd_head_v    = 128
0.00.040.026 I print_info: n_gqa            = 1
0.00.040.027 I print_info: n_embd_k_gqa     = 2048
0.00.040.027 I print_info: n_embd_v_gqa     = 2048
0.00.040.028 I print_info: f_norm_eps       = 1.0e-05
0.00.040.028 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.030 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.030 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.030 I print_info: f_logit_scale    = 0.0e+00
0.00.040.031 I print_info: n_ff             = 8192
0.00.040.031 I print_info: n_expert         = 0
0.00.040.031 I print_info: n_expert_used    = 0
0.00.040.031 I print_info: causal attn      = 1
0.00.040.032 I print_info: pooling type     = 0
0.00.040.032 I print_info: rope type        = 2
0.00.040.033 I print_info: rope scaling     = linear
0.00.040.034 I print_info: freq_base_train  = 10000.0
0.00.040.034 I print_info: freq_scale_train = 1
0.00.040.034 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.034 I print_info: rope_finetuned   = unknown
0.00.040.034 I print_info: ssm_d_conv       = 0
0.00.040.034 I print_info: ssm_d_inner      = 0
0.00.040.035 I print_info: ssm_d_state      = 0
0.00.040.036 I print_info: ssm_dt_rank      = 0
0.00.040.037 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.037 I print_info: model type       = 1.4B
0.00.040.037 I print_info: model params     = 1.41 B
0.00.040.038 I print_info: general.name     = 1.4B
0.00.040.038 I print_info: vocab type       = BPE
0.00.040.038 I print_info: n_vocab          = 50304
0.00.040.038 I print_info: n_merges         = 50009
0.00.040.039 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.039 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.039 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.039 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: LF token         = 128 'Ä'
0.00.040.043 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: max token length = 1024
0.00.535.059 I load_tensors: offloading 24 repeating layers to GPU
0.00.535.066 I load_tensors: offloading output layer to GPU
0.00.535.066 I load_tensors: offloaded 25/25 layers to GPU
0.00.535.102 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.535.104 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.536.089 I llama_init_from_model: n_seq_max     = 1
0.00.536.095 I llama_init_from_model: n_ctx         = 2048
0.00.536.095 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.536.096 I llama_init_from_model: n_batch       = 2048
0.00.536.096 I llama_init_from_model: n_ubatch      = 512
0.00.536.096 I llama_init_from_model: flash_attn    = 0
0.00.536.098 I llama_init_from_model: freq_base     = 10000.0
0.00.536.099 I llama_init_from_model: freq_scale    = 1
0.00.536.101 I ggml_metal_init: allocating
0.00.536.217 I ggml_metal_init: found device: Apple M4
0.00.536.238 I ggml_metal_init: picking default device: Apple M4
0.00.538.053 I ggml_metal_init: using embedded metal library
0.00.543.311 I ggml_metal_init: GPU name:   Apple M4
0.00.543.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.322 I ggml_metal_init: simdgroup reduction   = true
0.00.543.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.323 I ggml_metal_init: has residency sets    = true
0.00.543.323 I ggml_metal_init: has bfloat            = true
0.00.543.323 I ggml_metal_init: use bfloat            = true
0.00.543.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.560.146 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.575 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.614.581 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.606 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.632 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.619.634 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.619.634 I llama_init_from_model: graph nodes  = 967
0.00.619.635 I llama_init_from_model: graph splits = 2
0.00.619.640 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.769 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.248 I main: llama threadpool init, n_threads = 4
0.00.672.284 I 
0.00.672.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.305 I 
0.00.672.421 I sampler seed: 1234
0.00.672.426 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.672.462 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.672.467 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.672.467 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.449.037 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46344.65 tokens per second)
0.01.449.038 I llama_perf_context_print:        load time =     661.54 ms
0.01.449.039 I llama_perf_context_print: prompt eval time =      58.60 ms /     7 tokens (    8.37 ms per token,   119.46 tokens per second)
0.01.449.040 I llama_perf_context_print:        eval time =     715.45 ms /    63 runs   (   11.36 ms per token,    88.06 tokens per second)
0.01.449.040 I llama_perf_context_print:       total time =     777.65 ms /    70 tokens
0.01.449.283 I ggml_metal_free: deallocating

real	0m1.467s
user	0m0.107s
sys	0m0.183s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.439 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.030.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.446 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.450 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.453 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.610 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.612 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.612 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.612 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.613 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.613 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.039.613 I llama_model_loader: - type  f32:  194 tensors
0.00.039.614 I llama_model_loader: - type q5_K:   61 tensors
0.00.039.614 I llama_model_loader: - type q6_K:   37 tensors
0.00.039.615 I print_info: file format = GGUF V3 (latest)
0.00.039.615 I print_info: file type   = Q5_K - Medium
0.00.039.616 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.048.765 I load: special tokens cache size = 25
0.00.055.727 I load: token to piece cache size = 0.2984 MB
0.00.055.731 I print_info: arch             = gptneox
0.00.055.731 I print_info: vocab_only       = 0
0.00.055.731 I print_info: n_ctx_train      = 2048
0.00.055.731 I print_info: n_embd           = 2048
0.00.055.731 I print_info: n_layer          = 24
0.00.055.734 I print_info: n_head           = 16
0.00.055.735 I print_info: n_head_kv        = 16
0.00.055.736 I print_info: n_rot            = 32
0.00.055.736 I print_info: n_swa            = 0
0.00.055.736 I print_info: n_embd_head_k    = 128
0.00.055.736 I print_info: n_embd_head_v    = 128
0.00.055.737 I print_info: n_gqa            = 1
0.00.055.738 I print_info: n_embd_k_gqa     = 2048
0.00.055.741 I print_info: n_embd_v_gqa     = 2048
0.00.055.741 I print_info: f_norm_eps       = 1.0e-05
0.00.055.742 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.742 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.742 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.742 I print_info: f_logit_scale    = 0.0e+00
0.00.055.743 I print_info: n_ff             = 8192
0.00.055.745 I print_info: n_expert         = 0
0.00.055.745 I print_info: n_expert_used    = 0
0.00.055.745 I print_info: causal attn      = 1
0.00.055.745 I print_info: pooling type     = 0
0.00.055.745 I print_info: rope type        = 2
0.00.055.746 I print_info: rope scaling     = linear
0.00.055.746 I print_info: freq_base_train  = 10000.0
0.00.055.746 I print_info: freq_scale_train = 1
0.00.055.746 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.747 I print_info: rope_finetuned   = unknown
0.00.055.747 I print_info: ssm_d_conv       = 0
0.00.055.747 I print_info: ssm_d_inner      = 0
0.00.055.747 I print_info: ssm_d_state      = 0
0.00.055.747 I print_info: ssm_dt_rank      = 0
0.00.055.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.748 I print_info: model type       = 1.4B
0.00.055.748 I print_info: model params     = 1.41 B
0.00.055.748 I print_info: general.name     = 1.4B
0.00.055.749 I print_info: vocab type       = BPE
0.00.055.749 I print_info: n_vocab          = 50304
0.00.055.749 I print_info: n_merges         = 50009
0.00.055.749 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.750 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.750 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.750 I print_info: LF token         = 128 'Ä'
0.00.055.751 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.751 I print_info: max token length = 1024
0.00.705.950 I load_tensors: offloading 24 repeating layers to GPU
0.00.705.954 I load_tensors: offloading output layer to GPU
0.00.705.955 I load_tensors: offloaded 25/25 layers to GPU
0.00.705.979 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.705.980 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.707.100 I llama_init_from_model: n_seq_max     = 1
0.00.707.102 I llama_init_from_model: n_ctx         = 2048
0.00.707.103 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.707.103 I llama_init_from_model: n_batch       = 2048
0.00.707.104 I llama_init_from_model: n_ubatch      = 512
0.00.707.104 I llama_init_from_model: flash_attn    = 0
0.00.707.105 I llama_init_from_model: freq_base     = 10000.0
0.00.707.105 I llama_init_from_model: freq_scale    = 1
0.00.707.106 I ggml_metal_init: allocating
0.00.707.122 I ggml_metal_init: found device: Apple M4
0.00.707.133 I ggml_metal_init: picking default device: Apple M4
0.00.708.480 I ggml_metal_init: using embedded metal library
0.00.714.645 I ggml_metal_init: GPU name:   Apple M4
0.00.714.648 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.649 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.650 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.651 I ggml_metal_init: simdgroup reduction   = true
0.00.714.651 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.652 I ggml_metal_init: has residency sets    = true
0.00.714.652 I ggml_metal_init: has bfloat            = true
0.00.714.652 I ggml_metal_init: use bfloat            = true
0.00.714.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.657 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.731.935 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.785.595 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.785.601 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.785.627 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.790.299 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.790.301 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.790.302 I llama_init_from_model: graph nodes  = 967
0.00.790.302 I llama_init_from_model: graph splits = 2
0.00.790.308 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.790.430 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.790.430 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.530 I main: llama threadpool init, n_threads = 4
0.00.855.573 I 
0.00.855.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.598 I 
0.00.855.745 I sampler seed: 1234
0.00.855.750 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.855.774 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.855.775 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.855.776 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.707.315 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.707.316 I llama_perf_context_print:        load time =     845.74 ms
0.01.707.317 I llama_perf_context_print: prompt eval time =      51.50 ms /     7 tokens (    7.36 ms per token,   135.93 tokens per second)
0.01.707.317 I llama_perf_context_print:        eval time =     797.17 ms /    63 runs   (   12.65 ms per token,    79.03 tokens per second)
0.01.707.317 I llama_perf_context_print:       total time =     852.65 ms /    70 tokens
0.01.707.568 I ggml_metal_free: deallocating

real	0m1.725s
user	0m0.112s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.757 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.250 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.251 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.253 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.253 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.255 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.058 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.059 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.060 I llama_model_loader: - type  f32:  194 tensors
0.00.026.060 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.060 I print_info: file format = GGUF V3 (latest)
0.00.026.061 I print_info: file type   = Q6_K
0.00.026.062 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.843 I load: special tokens cache size = 25
0.00.039.758 I load: token to piece cache size = 0.2984 MB
0.00.039.761 I print_info: arch             = gptneox
0.00.039.761 I print_info: vocab_only       = 0
0.00.039.762 I print_info: n_ctx_train      = 2048
0.00.039.762 I print_info: n_embd           = 2048
0.00.039.762 I print_info: n_layer          = 24
0.00.039.765 I print_info: n_head           = 16
0.00.039.765 I print_info: n_head_kv        = 16
0.00.039.766 I print_info: n_rot            = 32
0.00.039.766 I print_info: n_swa            = 0
0.00.039.766 I print_info: n_embd_head_k    = 128
0.00.039.766 I print_info: n_embd_head_v    = 128
0.00.039.767 I print_info: n_gqa            = 1
0.00.039.767 I print_info: n_embd_k_gqa     = 2048
0.00.039.768 I print_info: n_embd_v_gqa     = 2048
0.00.039.769 I print_info: f_norm_eps       = 1.0e-05
0.00.039.769 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.769 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.769 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.769 I print_info: f_logit_scale    = 0.0e+00
0.00.039.770 I print_info: n_ff             = 8192
0.00.039.770 I print_info: n_expert         = 0
0.00.039.771 I print_info: n_expert_used    = 0
0.00.039.771 I print_info: causal attn      = 1
0.00.039.771 I print_info: pooling type     = 0
0.00.039.771 I print_info: rope type        = 2
0.00.039.771 I print_info: rope scaling     = linear
0.00.039.772 I print_info: freq_base_train  = 10000.0
0.00.039.772 I print_info: freq_scale_train = 1
0.00.039.772 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.772 I print_info: rope_finetuned   = unknown
0.00.039.773 I print_info: ssm_d_conv       = 0
0.00.039.773 I print_info: ssm_d_inner      = 0
0.00.039.773 I print_info: ssm_d_state      = 0
0.00.039.773 I print_info: ssm_dt_rank      = 0
0.00.039.773 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.773 I print_info: model type       = 1.4B
0.00.039.774 I print_info: model params     = 1.41 B
0.00.039.776 I print_info: general.name     = 1.4B
0.00.039.777 I print_info: vocab type       = BPE
0.00.039.777 I print_info: n_vocab          = 50304
0.00.039.777 I print_info: n_merges         = 50009
0.00.039.778 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: LF token         = 128 'Ä'
0.00.039.779 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.779 I print_info: max token length = 1024
0.00.649.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.784 I load_tensors: offloading output layer to GPU
0.00.649.785 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.821 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.649.822 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.651.447 I llama_init_from_model: n_seq_max     = 1
0.00.651.450 I llama_init_from_model: n_ctx         = 2048
0.00.651.450 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.451 I llama_init_from_model: n_batch       = 2048
0.00.651.451 I llama_init_from_model: n_ubatch      = 512
0.00.651.452 I llama_init_from_model: flash_attn    = 0
0.00.651.453 I llama_init_from_model: freq_base     = 10000.0
0.00.651.453 I llama_init_from_model: freq_scale    = 1
0.00.651.455 I ggml_metal_init: allocating
0.00.651.503 I ggml_metal_init: found device: Apple M4
0.00.651.521 I ggml_metal_init: picking default device: Apple M4
0.00.652.978 I ggml_metal_init: using embedded metal library
0.00.659.241 I ggml_metal_init: GPU name:   Apple M4
0.00.659.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.247 I ggml_metal_init: simdgroup reduction   = true
0.00.659.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.248 I ggml_metal_init: has residency sets    = true
0.00.659.248 I ggml_metal_init: has bfloat            = true
0.00.659.248 I ggml_metal_init: use bfloat            = true
0.00.659.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.867 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.015 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.729.021 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.729.044 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.733.354 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.733.356 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.733.357 I llama_init_from_model: graph nodes  = 967
0.00.733.357 I llama_init_from_model: graph splits = 2
0.00.733.362 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.733.492 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.733.492 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.076 I main: llama threadpool init, n_threads = 4
0.00.799.120 I 
0.00.799.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.147 I 
0.00.799.301 I sampler seed: 1234
0.00.799.306 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.327 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.327 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.327 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.681.119 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.01.681.120 I llama_perf_context_print:        load time =     788.45 ms
0.01.681.121 I llama_perf_context_print: prompt eval time =      54.01 ms /     7 tokens (    7.72 ms per token,   129.60 tokens per second)
0.01.681.121 I llama_perf_context_print:        eval time =     824.87 ms /    63 runs   (   13.09 ms per token,    76.38 tokens per second)
0.01.681.121 I llama_perf_context_print:       total time =     882.91 ms /    70 tokens
0.01.681.379 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.107s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.499 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.650 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.662 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.663 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.664 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.665 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.667 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.668 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.671 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.675 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.617 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.975 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.094 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.097 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.098 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.099 I llama_model_loader: - type  f32:  194 tensors
0.00.059.099 I llama_model_loader: - type  f16:   98 tensors
0.00.059.100 I print_info: file format = GGUF V3 (latest)
0.00.059.101 I print_info: file type   = all F32 (guessed)
0.00.059.103 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.072.460 I load: special tokens cache size = 25
0.00.080.760 I load: token to piece cache size = 0.2984 MB
0.00.080.763 I print_info: arch             = gptneox
0.00.080.763 I print_info: vocab_only       = 0
0.00.080.763 I print_info: n_ctx_train      = 2048
0.00.080.763 I print_info: n_embd           = 2048
0.00.080.764 I print_info: n_layer          = 24
0.00.080.767 I print_info: n_head           = 16
0.00.080.768 I print_info: n_head_kv        = 16
0.00.080.768 I print_info: n_rot            = 32
0.00.080.768 I print_info: n_swa            = 0
0.00.080.768 I print_info: n_embd_head_k    = 128
0.00.080.768 I print_info: n_embd_head_v    = 128
0.00.080.769 I print_info: n_gqa            = 1
0.00.080.770 I print_info: n_embd_k_gqa     = 2048
0.00.080.771 I print_info: n_embd_v_gqa     = 2048
0.00.080.771 I print_info: f_norm_eps       = 1.0e-05
0.00.080.772 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.080.772 I print_info: f_clamp_kqv      = 0.0e+00
0.00.080.772 I print_info: f_max_alibi_bias = 0.0e+00
0.00.080.772 I print_info: f_logit_scale    = 0.0e+00
0.00.080.773 I print_info: n_ff             = 8192
0.00.080.773 I print_info: n_expert         = 0
0.00.080.773 I print_info: n_expert_used    = 0
0.00.080.773 I print_info: causal attn      = 1
0.00.080.773 I print_info: pooling type     = 0
0.00.080.775 I print_info: rope type        = 2
0.00.080.775 I print_info: rope scaling     = linear
0.00.080.776 I print_info: freq_base_train  = 10000.0
0.00.080.776 I print_info: freq_scale_train = 1
0.00.080.776 I print_info: n_ctx_orig_yarn  = 2048
0.00.080.777 I print_info: rope_finetuned   = unknown
0.00.080.777 I print_info: ssm_d_conv       = 0
0.00.080.777 I print_info: ssm_d_inner      = 0
0.00.080.777 I print_info: ssm_d_state      = 0
0.00.080.777 I print_info: ssm_dt_rank      = 0
0.00.080.777 I print_info: ssm_dt_b_c_rms   = 0
0.00.080.778 I print_info: model type       = 1.4B
0.00.080.780 I print_info: model params     = 1.41 B
0.00.080.780 I print_info: general.name     = 1.4B
0.00.080.780 I print_info: vocab type       = BPE
0.00.080.781 I print_info: n_vocab          = 50304
0.00.080.781 I print_info: n_merges         = 50009
0.00.080.781 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.080.781 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.080.782 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.080.782 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.080.782 I print_info: LF token         = 128 'Ä'
0.00.080.782 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.080.782 I print_info: max token length = 1024
0.01.178.854 I load_tensors: offloading 24 repeating layers to GPU
0.01.178.860 I load_tensors: offloading output layer to GPU
0.01.178.860 I load_tensors: offloaded 25/25 layers to GPU
0.01.178.887 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.178.890 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.180.031 I llama_init_from_model: n_seq_max     = 1
0.01.180.032 I llama_init_from_model: n_ctx         = 128
0.01.180.033 I llama_init_from_model: n_ctx_per_seq = 128
0.01.180.033 I llama_init_from_model: n_batch       = 128
0.01.180.033 I llama_init_from_model: n_ubatch      = 128
0.01.180.033 I llama_init_from_model: flash_attn    = 0
0.01.180.037 I llama_init_from_model: freq_base     = 10000.0
0.01.180.040 I llama_init_from_model: freq_scale    = 1
0.01.180.041 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.180.041 I ggml_metal_init: allocating
0.01.180.084 I ggml_metal_init: found device: Apple M4
0.01.180.090 I ggml_metal_init: picking default device: Apple M4
0.01.181.167 I ggml_metal_init: using embedded metal library
0.01.185.097 I ggml_metal_init: GPU name:   Apple M4
0.01.185.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.185.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.185.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.185.101 I ggml_metal_init: simdgroup reduction   = true
0.01.185.101 I ggml_metal_init: simdgroup matrix mul. = true
0.01.185.101 I ggml_metal_init: has residency sets    = true
0.01.185.101 I ggml_metal_init: has bfloat            = true
0.01.185.102 I ggml_metal_init: use bfloat            = true
0.01.185.102 I ggml_metal_init: hasUnifiedMemory      = true
0.01.185.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.196.190 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.198.041 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.198.043 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.198.060 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.199.756 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.199.757 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.199.758 I llama_init_from_model: graph nodes  = 967
0.01.199.758 I llama_init_from_model: graph splits = 2
0.01.199.759 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.199.759 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.235.396 I 
0.01.235.435 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.235.456 I perplexity: tokenizing the input ..
0.01.240.764 I perplexity: tokenization took 5.306 ms
0.01.240.787 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.359.289 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.360.609 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.360.621 I llama_perf_context_print:        load time =    1210.39 ms
0.01.360.622 I llama_perf_context_print: prompt eval time =     118.24 ms /   128 tokens (    0.92 ms per token,  1082.55 tokens per second)
0.01.360.622 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.360.626 I llama_perf_context_print:       total time =     125.23 ms /   129 tokens
0.01.360.994 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.102s
sys	0m0.236s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.268 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.417 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.348 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.363 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.364 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.368 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.369 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.369 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.370 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.370 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.690 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.699 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.799 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.799 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.800 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.800 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.801 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.802 I llama_model_loader: - type  f32:  194 tensors
0.00.039.802 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.803 I print_info: file format = GGUF V3 (latest)
0.00.039.804 I print_info: file type   = Q8_0
0.00.039.806 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.051.019 I load: special tokens cache size = 25
0.00.059.158 I load: token to piece cache size = 0.2984 MB
0.00.059.161 I print_info: arch             = gptneox
0.00.059.162 I print_info: vocab_only       = 0
0.00.059.162 I print_info: n_ctx_train      = 2048
0.00.059.162 I print_info: n_embd           = 2048
0.00.059.162 I print_info: n_layer          = 24
0.00.059.165 I print_info: n_head           = 16
0.00.059.166 I print_info: n_head_kv        = 16
0.00.059.167 I print_info: n_rot            = 32
0.00.059.167 I print_info: n_swa            = 0
0.00.059.167 I print_info: n_embd_head_k    = 128
0.00.059.170 I print_info: n_embd_head_v    = 128
0.00.059.171 I print_info: n_gqa            = 1
0.00.059.172 I print_info: n_embd_k_gqa     = 2048
0.00.059.173 I print_info: n_embd_v_gqa     = 2048
0.00.059.173 I print_info: f_norm_eps       = 1.0e-05
0.00.059.174 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.174 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.174 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.174 I print_info: f_logit_scale    = 0.0e+00
0.00.059.175 I print_info: n_ff             = 8192
0.00.059.176 I print_info: n_expert         = 0
0.00.059.176 I print_info: n_expert_used    = 0
0.00.059.176 I print_info: causal attn      = 1
0.00.059.176 I print_info: pooling type     = 0
0.00.059.181 I print_info: rope type        = 2
0.00.059.181 I print_info: rope scaling     = linear
0.00.059.182 I print_info: freq_base_train  = 10000.0
0.00.059.182 I print_info: freq_scale_train = 1
0.00.059.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.183 I print_info: rope_finetuned   = unknown
0.00.059.183 I print_info: ssm_d_conv       = 0
0.00.059.183 I print_info: ssm_d_inner      = 0
0.00.059.183 I print_info: ssm_d_state      = 0
0.00.059.183 I print_info: ssm_dt_rank      = 0
0.00.059.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.184 I print_info: model type       = 1.4B
0.00.059.184 I print_info: model params     = 1.41 B
0.00.059.184 I print_info: general.name     = 1.4B
0.00.059.185 I print_info: vocab type       = BPE
0.00.059.185 I print_info: n_vocab          = 50304
0.00.059.185 I print_info: n_merges         = 50009
0.00.059.187 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.187 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.187 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.187 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.188 I print_info: LF token         = 128 'Ä'
0.00.059.188 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.188 I print_info: max token length = 1024
0.00.877.631 I load_tensors: offloading 24 repeating layers to GPU
0.00.877.635 I load_tensors: offloading output layer to GPU
0.00.877.635 I load_tensors: offloaded 25/25 layers to GPU
0.00.877.660 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.877.661 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.878.752 I llama_init_from_model: n_seq_max     = 1
0.00.878.754 I llama_init_from_model: n_ctx         = 128
0.00.878.754 I llama_init_from_model: n_ctx_per_seq = 128
0.00.878.754 I llama_init_from_model: n_batch       = 128
0.00.878.755 I llama_init_from_model: n_ubatch      = 128
0.00.878.759 I llama_init_from_model: flash_attn    = 0
0.00.878.760 I llama_init_from_model: freq_base     = 10000.0
0.00.878.760 I llama_init_from_model: freq_scale    = 1
0.00.878.761 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.878.762 I ggml_metal_init: allocating
0.00.878.823 I ggml_metal_init: found device: Apple M4
0.00.878.832 I ggml_metal_init: picking default device: Apple M4
0.00.880.086 I ggml_metal_init: using embedded metal library
0.00.885.280 I ggml_metal_init: GPU name:   Apple M4
0.00.885.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.885.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.885.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.885.286 I ggml_metal_init: simdgroup reduction   = true
0.00.885.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.885.287 I ggml_metal_init: has residency sets    = true
0.00.885.287 I ggml_metal_init: has bfloat            = true
0.00.885.287 I ggml_metal_init: use bfloat            = true
0.00.885.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.885.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.900.243 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.903.618 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.903.621 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.903.649 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.906.888 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.906.889 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.906.890 I llama_init_from_model: graph nodes  = 967
0.00.906.890 I llama_init_from_model: graph splits = 2
0.00.906.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.906.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.488 I 
0.00.935.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.585 I perplexity: tokenizing the input ..
0.00.942.785 I perplexity: tokenization took 7.196 ms
0.00.942.812 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.081.286 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.082.649 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.082.662 I llama_perf_context_print:        load time =     923.06 ms
0.01.082.662 I llama_perf_context_print: prompt eval time =     137.66 ms /   128 tokens (    1.08 ms per token,   929.83 tokens per second)
0.01.082.663 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.082.664 I llama_perf_context_print:       total time =     147.18 ms /   129 tokens
0.01.083.033 I ggml_metal_free: deallocating

real	0m1.105s
user	0m0.088s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.265 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.505 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.789 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.795 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.797 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.798 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.798 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.798 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.712 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.508 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.508 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.509 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.509 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.510 I llama_model_loader: - type  f32:  194 tensors
0.00.025.510 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.510 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.511 I print_info: file format = GGUF V3 (latest)
0.00.025.512 I print_info: file type   = Q4_0
0.00.025.514 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.302 I load: special tokens cache size = 25
0.00.039.300 I load: token to piece cache size = 0.2984 MB
0.00.039.303 I print_info: arch             = gptneox
0.00.039.303 I print_info: vocab_only       = 0
0.00.039.303 I print_info: n_ctx_train      = 2048
0.00.039.304 I print_info: n_embd           = 2048
0.00.039.304 I print_info: n_layer          = 24
0.00.039.307 I print_info: n_head           = 16
0.00.039.308 I print_info: n_head_kv        = 16
0.00.039.308 I print_info: n_rot            = 32
0.00.039.308 I print_info: n_swa            = 0
0.00.039.309 I print_info: n_embd_head_k    = 128
0.00.039.311 I print_info: n_embd_head_v    = 128
0.00.039.312 I print_info: n_gqa            = 1
0.00.039.313 I print_info: n_embd_k_gqa     = 2048
0.00.039.314 I print_info: n_embd_v_gqa     = 2048
0.00.039.314 I print_info: f_norm_eps       = 1.0e-05
0.00.039.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.315 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.315 I print_info: f_logit_scale    = 0.0e+00
0.00.039.316 I print_info: n_ff             = 8192
0.00.039.316 I print_info: n_expert         = 0
0.00.039.316 I print_info: n_expert_used    = 0
0.00.039.316 I print_info: causal attn      = 1
0.00.039.317 I print_info: pooling type     = 0
0.00.039.317 I print_info: rope type        = 2
0.00.039.317 I print_info: rope scaling     = linear
0.00.039.317 I print_info: freq_base_train  = 10000.0
0.00.039.318 I print_info: freq_scale_train = 1
0.00.039.318 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.318 I print_info: rope_finetuned   = unknown
0.00.039.318 I print_info: ssm_d_conv       = 0
0.00.039.318 I print_info: ssm_d_inner      = 0
0.00.039.318 I print_info: ssm_d_state      = 0
0.00.039.318 I print_info: ssm_dt_rank      = 0
0.00.039.320 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.320 I print_info: model type       = 1.4B
0.00.039.320 I print_info: model params     = 1.41 B
0.00.039.321 I print_info: general.name     = 1.4B
0.00.039.321 I print_info: vocab type       = BPE
0.00.039.321 I print_info: n_vocab          = 50304
0.00.039.323 I print_info: n_merges         = 50009
0.00.039.323 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.323 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.323 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.324 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.324 I print_info: LF token         = 128 'Ä'
0.00.039.324 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.324 I print_info: max token length = 1024
0.00.577.911 I load_tensors: offloading 24 repeating layers to GPU
0.00.577.924 I load_tensors: offloading output layer to GPU
0.00.577.925 I load_tensors: offloaded 25/25 layers to GPU
0.00.577.955 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.577.956 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.579.379 I llama_init_from_model: n_seq_max     = 1
0.00.579.385 I llama_init_from_model: n_ctx         = 128
0.00.579.385 I llama_init_from_model: n_ctx_per_seq = 128
0.00.579.390 I llama_init_from_model: n_batch       = 128
0.00.579.390 I llama_init_from_model: n_ubatch      = 128
0.00.579.391 I llama_init_from_model: flash_attn    = 0
0.00.579.394 I llama_init_from_model: freq_base     = 10000.0
0.00.579.394 I llama_init_from_model: freq_scale    = 1
0.00.579.395 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.579.397 I ggml_metal_init: allocating
0.00.579.495 I ggml_metal_init: found device: Apple M4
0.00.579.509 I ggml_metal_init: picking default device: Apple M4
0.00.581.295 I ggml_metal_init: using embedded metal library
0.00.586.896 I ggml_metal_init: GPU name:   Apple M4
0.00.586.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.586.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.586.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.586.911 I ggml_metal_init: simdgroup reduction   = true
0.00.586.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.586.912 I ggml_metal_init: has residency sets    = true
0.00.586.912 I ggml_metal_init: has bfloat            = true
0.00.586.912 I ggml_metal_init: use bfloat            = true
0.00.586.919 I ggml_metal_init: hasUnifiedMemory      = true
0.00.586.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.607.086 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.610.765 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.610.769 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.610.797 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.614.297 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.614.300 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.614.300 I llama_init_from_model: graph nodes  = 967
0.00.614.301 I llama_init_from_model: graph splits = 2
0.00.614.303 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.614.304 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.424 I 
0.00.641.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.535 I perplexity: tokenizing the input ..
0.00.648.609 I perplexity: tokenization took 7.071 ms
0.00.648.631 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.879 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.786.283 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.786.303 I llama_perf_context_print:        load time =     631.91 ms
0.00.786.304 I llama_perf_context_print: prompt eval time =     135.18 ms /   128 tokens (    1.06 ms per token,   946.91 tokens per second)
0.00.786.305 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.306 I llama_perf_context_print:       total time =     144.88 ms /   129 tokens
0.00.786.692 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.080s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.156 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.435 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.441 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.451 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.451 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.452 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.454 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.455 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.455 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.241 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.330 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.133 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.134 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.134 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.135 I llama_model_loader: - type  f32:  194 tensors
0.00.029.136 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.136 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.137 I print_info: file format = GGUF V3 (latest)
0.00.029.137 I print_info: file type   = Q4_1
0.00.029.142 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.980 I load: special tokens cache size = 25
0.00.042.848 I load: token to piece cache size = 0.2984 MB
0.00.042.851 I print_info: arch             = gptneox
0.00.042.851 I print_info: vocab_only       = 0
0.00.042.851 I print_info: n_ctx_train      = 2048
0.00.042.851 I print_info: n_embd           = 2048
0.00.042.852 I print_info: n_layer          = 24
0.00.042.854 I print_info: n_head           = 16
0.00.042.855 I print_info: n_head_kv        = 16
0.00.042.855 I print_info: n_rot            = 32
0.00.042.855 I print_info: n_swa            = 0
0.00.042.856 I print_info: n_embd_head_k    = 128
0.00.042.856 I print_info: n_embd_head_v    = 128
0.00.042.858 I print_info: n_gqa            = 1
0.00.042.859 I print_info: n_embd_k_gqa     = 2048
0.00.042.860 I print_info: n_embd_v_gqa     = 2048
0.00.042.861 I print_info: f_norm_eps       = 1.0e-05
0.00.042.861 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.861 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.861 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.861 I print_info: f_logit_scale    = 0.0e+00
0.00.042.862 I print_info: n_ff             = 8192
0.00.042.862 I print_info: n_expert         = 0
0.00.042.862 I print_info: n_expert_used    = 0
0.00.042.863 I print_info: causal attn      = 1
0.00.042.863 I print_info: pooling type     = 0
0.00.042.863 I print_info: rope type        = 2
0.00.042.863 I print_info: rope scaling     = linear
0.00.042.864 I print_info: freq_base_train  = 10000.0
0.00.042.864 I print_info: freq_scale_train = 1
0.00.042.864 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.864 I print_info: rope_finetuned   = unknown
0.00.042.865 I print_info: ssm_d_conv       = 0
0.00.042.865 I print_info: ssm_d_inner      = 0
0.00.042.865 I print_info: ssm_d_state      = 0
0.00.042.865 I print_info: ssm_dt_rank      = 0
0.00.042.865 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.865 I print_info: model type       = 1.4B
0.00.042.866 I print_info: model params     = 1.41 B
0.00.042.866 I print_info: general.name     = 1.4B
0.00.042.870 I print_info: vocab type       = BPE
0.00.042.870 I print_info: n_vocab          = 50304
0.00.042.870 I print_info: n_merges         = 50009
0.00.042.871 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.871 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.871 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.871 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.871 I print_info: LF token         = 128 'Ä'
0.00.042.872 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.872 I print_info: max token length = 1024
0.00.636.852 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.865 I load_tensors: offloading output layer to GPU
0.00.636.865 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.900 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.636.902 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.638.298 I llama_init_from_model: n_seq_max     = 1
0.00.638.303 I llama_init_from_model: n_ctx         = 128
0.00.638.304 I llama_init_from_model: n_ctx_per_seq = 128
0.00.638.308 I llama_init_from_model: n_batch       = 128
0.00.638.308 I llama_init_from_model: n_ubatch      = 128
0.00.638.309 I llama_init_from_model: flash_attn    = 0
0.00.638.316 I llama_init_from_model: freq_base     = 10000.0
0.00.638.317 I llama_init_from_model: freq_scale    = 1
0.00.638.317 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.638.319 I ggml_metal_init: allocating
0.00.638.442 I ggml_metal_init: found device: Apple M4
0.00.638.457 I ggml_metal_init: picking default device: Apple M4
0.00.640.323 I ggml_metal_init: using embedded metal library
0.00.646.863 I ggml_metal_init: GPU name:   Apple M4
0.00.646.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.871 I ggml_metal_init: simdgroup reduction   = true
0.00.646.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.871 I ggml_metal_init: has residency sets    = true
0.00.646.872 I ggml_metal_init: has bfloat            = true
0.00.646.872 I ggml_metal_init: use bfloat            = true
0.00.646.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.277 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.649 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.667.653 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.667.679 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.670.952 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.670.954 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.670.955 I llama_init_from_model: graph nodes  = 967
0.00.670.955 I llama_init_from_model: graph splits = 2
0.00.670.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.670.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.663 I 
0.00.699.741 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.760 I perplexity: tokenizing the input ..
0.00.706.884 I perplexity: tokenization took 7.12 ms
0.00.706.903 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.087 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.845.427 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.845.443 I llama_perf_context_print:        load time =     690.50 ms
0.00.845.444 I llama_perf_context_print: prompt eval time =     136.23 ms /   128 tokens (    1.06 ms per token,   939.58 tokens per second)
0.00.845.445 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.445 I llama_perf_context_print:       total time =     145.78 ms /   129 tokens
0.00.845.795 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.079s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.942 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.206 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.211 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.220 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.223 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.223 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.894 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.896 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.896 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.896 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.897 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.897 I llama_model_loader: - type  f32:  194 tensors
0.00.025.898 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.898 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.899 I print_info: file format = GGUF V3 (latest)
0.00.025.899 I print_info: file type   = Q5_0
0.00.025.904 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.783 I load: special tokens cache size = 25
0.00.039.708 I load: token to piece cache size = 0.2984 MB
0.00.039.711 I print_info: arch             = gptneox
0.00.039.711 I print_info: vocab_only       = 0
0.00.039.712 I print_info: n_ctx_train      = 2048
0.00.039.712 I print_info: n_embd           = 2048
0.00.039.715 I print_info: n_layer          = 24
0.00.039.718 I print_info: n_head           = 16
0.00.039.719 I print_info: n_head_kv        = 16
0.00.039.719 I print_info: n_rot            = 32
0.00.039.725 I print_info: n_swa            = 0
0.00.039.726 I print_info: n_embd_head_k    = 128
0.00.039.726 I print_info: n_embd_head_v    = 128
0.00.039.728 I print_info: n_gqa            = 1
0.00.039.729 I print_info: n_embd_k_gqa     = 2048
0.00.039.730 I print_info: n_embd_v_gqa     = 2048
0.00.039.730 I print_info: f_norm_eps       = 1.0e-05
0.00.039.731 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.731 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.731 I print_info: f_logit_scale    = 0.0e+00
0.00.039.732 I print_info: n_ff             = 8192
0.00.039.732 I print_info: n_expert         = 0
0.00.039.732 I print_info: n_expert_used    = 0
0.00.039.732 I print_info: causal attn      = 1
0.00.039.734 I print_info: pooling type     = 0
0.00.039.734 I print_info: rope type        = 2
0.00.039.734 I print_info: rope scaling     = linear
0.00.039.734 I print_info: freq_base_train  = 10000.0
0.00.039.735 I print_info: freq_scale_train = 1
0.00.039.735 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.735 I print_info: rope_finetuned   = unknown
0.00.039.736 I print_info: ssm_d_conv       = 0
0.00.039.737 I print_info: ssm_d_inner      = 0
0.00.039.737 I print_info: ssm_d_state      = 0
0.00.039.737 I print_info: ssm_dt_rank      = 0
0.00.039.737 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.737 I print_info: model type       = 1.4B
0.00.039.738 I print_info: model params     = 1.41 B
0.00.039.738 I print_info: general.name     = 1.4B
0.00.039.738 I print_info: vocab type       = BPE
0.00.039.739 I print_info: n_vocab          = 50304
0.00.039.739 I print_info: n_merges         = 50009
0.00.039.739 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.739 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.739 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.739 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.740 I print_info: LF token         = 128 'Ä'
0.00.039.741 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.741 I print_info: max token length = 1024
0.00.690.970 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.986 I load_tensors: offloading output layer to GPU
0.00.690.986 I load_tensors: offloaded 25/25 layers to GPU
0.00.691.017 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.691.018 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.692.407 I llama_init_from_model: n_seq_max     = 1
0.00.692.412 I llama_init_from_model: n_ctx         = 128
0.00.692.413 I llama_init_from_model: n_ctx_per_seq = 128
0.00.692.414 I llama_init_from_model: n_batch       = 128
0.00.692.414 I llama_init_from_model: n_ubatch      = 128
0.00.692.414 I llama_init_from_model: flash_attn    = 0
0.00.692.416 I llama_init_from_model: freq_base     = 10000.0
0.00.692.417 I llama_init_from_model: freq_scale    = 1
0.00.692.417 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.692.420 I ggml_metal_init: allocating
0.00.692.496 I ggml_metal_init: found device: Apple M4
0.00.692.510 I ggml_metal_init: picking default device: Apple M4
0.00.694.207 I ggml_metal_init: using embedded metal library
0.00.700.874 I ggml_metal_init: GPU name:   Apple M4
0.00.700.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.700.879 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.700.880 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.700.881 I ggml_metal_init: simdgroup reduction   = true
0.00.700.881 I ggml_metal_init: simdgroup matrix mul. = true
0.00.700.882 I ggml_metal_init: has residency sets    = true
0.00.700.882 I ggml_metal_init: has bfloat            = true
0.00.700.882 I ggml_metal_init: use bfloat            = true
0.00.700.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.700.892 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.718.680 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.722.191 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.722.195 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.722.219 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.660 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.725.661 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.725.662 I llama_init_from_model: graph nodes  = 967
0.00.725.662 I llama_init_from_model: graph splits = 2
0.00.725.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.725.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.109 I 
0.00.757.193 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.212 I perplexity: tokenizing the input ..
0.00.764.019 I perplexity: tokenization took 6.802 ms
0.00.764.035 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.910.108 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.911.516 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.911.533 I llama_perf_context_print:        load time =     747.16 ms
0.00.911.533 I llama_perf_context_print: prompt eval time =     145.20 ms /   128 tokens (    1.13 ms per token,   881.57 tokens per second)
0.00.911.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.911.534 I llama_perf_context_print:       total time =     154.43 ms /   129 tokens
0.00.911.885 I ggml_metal_free: deallocating

real	0m0.927s
user	0m0.078s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.308 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.849 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.854 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.859 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.860 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.860 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.861 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.861 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.862 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.862 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.862 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.863 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.863 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.865 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.865 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.867 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.867 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.868 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.639 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.698 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.447 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.449 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.449 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.450 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.450 I llama_model_loader: - type  f32:  194 tensors
0.00.024.450 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.451 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.451 I print_info: file format = GGUF V3 (latest)
0.00.024.452 I print_info: file type   = Q5_1
0.00.024.453 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.643 I load: special tokens cache size = 25
0.00.038.561 I load: token to piece cache size = 0.2984 MB
0.00.038.564 I print_info: arch             = gptneox
0.00.038.564 I print_info: vocab_only       = 0
0.00.038.564 I print_info: n_ctx_train      = 2048
0.00.038.564 I print_info: n_embd           = 2048
0.00.038.565 I print_info: n_layer          = 24
0.00.038.567 I print_info: n_head           = 16
0.00.038.568 I print_info: n_head_kv        = 16
0.00.038.568 I print_info: n_rot            = 32
0.00.038.568 I print_info: n_swa            = 0
0.00.038.568 I print_info: n_embd_head_k    = 128
0.00.038.569 I print_info: n_embd_head_v    = 128
0.00.038.569 I print_info: n_gqa            = 1
0.00.038.570 I print_info: n_embd_k_gqa     = 2048
0.00.038.571 I print_info: n_embd_v_gqa     = 2048
0.00.038.571 I print_info: f_norm_eps       = 1.0e-05
0.00.038.572 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.572 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.573 I print_info: f_logit_scale    = 0.0e+00
0.00.038.574 I print_info: n_ff             = 8192
0.00.038.574 I print_info: n_expert         = 0
0.00.038.575 I print_info: n_expert_used    = 0
0.00.038.575 I print_info: causal attn      = 1
0.00.038.575 I print_info: pooling type     = 0
0.00.038.575 I print_info: rope type        = 2
0.00.038.575 I print_info: rope scaling     = linear
0.00.038.576 I print_info: freq_base_train  = 10000.0
0.00.038.576 I print_info: freq_scale_train = 1
0.00.038.576 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.576 I print_info: rope_finetuned   = unknown
0.00.038.576 I print_info: ssm_d_conv       = 0
0.00.038.577 I print_info: ssm_d_inner      = 0
0.00.038.577 I print_info: ssm_d_state      = 0
0.00.038.577 I print_info: ssm_dt_rank      = 0
0.00.038.577 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.577 I print_info: model type       = 1.4B
0.00.038.578 I print_info: model params     = 1.41 B
0.00.038.578 I print_info: general.name     = 1.4B
0.00.038.578 I print_info: vocab type       = BPE
0.00.038.579 I print_info: n_vocab          = 50304
0.00.038.579 I print_info: n_merges         = 50009
0.00.038.579 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.579 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.579 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.580 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.580 I print_info: LF token         = 128 'Ä'
0.00.038.580 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.580 I print_info: max token length = 1024
0.00.588.648 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.662 I load_tensors: offloading output layer to GPU
0.00.588.663 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.700 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.588.702 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.590.261 I llama_init_from_model: n_seq_max     = 1
0.00.590.266 I llama_init_from_model: n_ctx         = 128
0.00.590.266 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.267 I llama_init_from_model: n_batch       = 128
0.00.590.268 I llama_init_from_model: n_ubatch      = 128
0.00.590.268 I llama_init_from_model: flash_attn    = 0
0.00.590.270 I llama_init_from_model: freq_base     = 10000.0
0.00.590.271 I llama_init_from_model: freq_scale    = 1
0.00.590.271 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.274 I ggml_metal_init: allocating
0.00.590.351 I ggml_metal_init: found device: Apple M4
0.00.590.365 I ggml_metal_init: picking default device: Apple M4
0.00.591.855 I ggml_metal_init: using embedded metal library
0.00.598.248 I ggml_metal_init: GPU name:   Apple M4
0.00.598.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.253 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.254 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.254 I ggml_metal_init: simdgroup reduction   = true
0.00.598.255 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.255 I ggml_metal_init: has residency sets    = true
0.00.598.255 I ggml_metal_init: has bfloat            = true
0.00.598.255 I ggml_metal_init: use bfloat            = true
0.00.598.256 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.068 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.501 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.504 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.532 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.774 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.776 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.777 I llama_init_from_model: graph nodes  = 967
0.00.621.777 I llama_init_from_model: graph splits = 2
0.00.621.780 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.780 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.843 I 
0.00.652.958 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.984 I perplexity: tokenizing the input ..
0.00.660.278 I perplexity: tokenization took 7.29 ms
0.00.660.299 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.159 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.797.487 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.797.504 I llama_perf_context_print:        load time =     643.53 ms
0.00.797.505 I llama_perf_context_print: prompt eval time =     134.90 ms /   128 tokens (    1.05 ms per token,   948.83 tokens per second)
0.00.797.505 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.506 I llama_perf_context_print:       total time =     144.67 ms /   129 tokens
0.00.797.914 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.079s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.131 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.529 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.534 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.540 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.541 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.542 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.544 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.546 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.547 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.547 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.244 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.291 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.956 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.959 I llama_model_loader: - type  f32:  194 tensors
0.00.024.960 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.960 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.960 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.961 I print_info: file format = GGUF V3 (latest)
0.00.024.961 I print_info: file type   = Q2_K - Medium
0.00.024.962 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.081 I load: special tokens cache size = 25
0.00.038.792 I load: token to piece cache size = 0.2984 MB
0.00.038.795 I print_info: arch             = gptneox
0.00.038.795 I print_info: vocab_only       = 0
0.00.038.796 I print_info: n_ctx_train      = 2048
0.00.038.796 I print_info: n_embd           = 2048
0.00.038.796 I print_info: n_layer          = 24
0.00.038.799 I print_info: n_head           = 16
0.00.038.800 I print_info: n_head_kv        = 16
0.00.038.800 I print_info: n_rot            = 32
0.00.038.800 I print_info: n_swa            = 0
0.00.038.800 I print_info: n_embd_head_k    = 128
0.00.038.800 I print_info: n_embd_head_v    = 128
0.00.038.801 I print_info: n_gqa            = 1
0.00.038.802 I print_info: n_embd_k_gqa     = 2048
0.00.038.803 I print_info: n_embd_v_gqa     = 2048
0.00.038.803 I print_info: f_norm_eps       = 1.0e-05
0.00.038.803 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.804 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.806 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.806 I print_info: f_logit_scale    = 0.0e+00
0.00.038.807 I print_info: n_ff             = 8192
0.00.038.807 I print_info: n_expert         = 0
0.00.038.807 I print_info: n_expert_used    = 0
0.00.038.807 I print_info: causal attn      = 1
0.00.038.807 I print_info: pooling type     = 0
0.00.038.807 I print_info: rope type        = 2
0.00.038.808 I print_info: rope scaling     = linear
0.00.038.808 I print_info: freq_base_train  = 10000.0
0.00.038.808 I print_info: freq_scale_train = 1
0.00.038.808 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.809 I print_info: rope_finetuned   = unknown
0.00.038.810 I print_info: ssm_d_conv       = 0
0.00.038.811 I print_info: ssm_d_inner      = 0
0.00.038.811 I print_info: ssm_d_state      = 0
0.00.038.811 I print_info: ssm_dt_rank      = 0
0.00.038.811 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.811 I print_info: model type       = 1.4B
0.00.038.812 I print_info: model params     = 1.41 B
0.00.038.812 I print_info: general.name     = 1.4B
0.00.038.812 I print_info: vocab type       = BPE
0.00.038.812 I print_info: n_vocab          = 50304
0.00.038.813 I print_info: n_merges         = 50009
0.00.038.813 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.813 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.813 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.817 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.817 I print_info: LF token         = 128 'Ä'
0.00.038.817 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.818 I print_info: max token length = 1024
0.00.335.848 I load_tensors: offloading 24 repeating layers to GPU
0.00.335.863 I load_tensors: offloading output layer to GPU
0.00.335.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.335.898 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.335.899 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.337.471 I llama_init_from_model: n_seq_max     = 1
0.00.337.477 I llama_init_from_model: n_ctx         = 128
0.00.337.481 I llama_init_from_model: n_ctx_per_seq = 128
0.00.337.482 I llama_init_from_model: n_batch       = 128
0.00.337.482 I llama_init_from_model: n_ubatch      = 128
0.00.337.482 I llama_init_from_model: flash_attn    = 0
0.00.337.485 I llama_init_from_model: freq_base     = 10000.0
0.00.337.488 I llama_init_from_model: freq_scale    = 1
0.00.337.489 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.337.491 I ggml_metal_init: allocating
0.00.337.586 I ggml_metal_init: found device: Apple M4
0.00.337.607 I ggml_metal_init: picking default device: Apple M4
0.00.339.444 I ggml_metal_init: using embedded metal library
0.00.344.937 I ggml_metal_init: GPU name:   Apple M4
0.00.344.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.952 I ggml_metal_init: simdgroup reduction   = true
0.00.344.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.953 I ggml_metal_init: has residency sets    = true
0.00.344.953 I ggml_metal_init: has bfloat            = true
0.00.344.954 I ggml_metal_init: use bfloat            = true
0.00.344.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.959 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.365.910 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.369.491 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.369.496 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.369.527 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.372.890 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.372.892 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.372.892 I llama_init_from_model: graph nodes  = 967
0.00.372.893 I llama_init_from_model: graph splits = 2
0.00.372.896 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.372.897 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.403.165 I 
0.00.403.249 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.403.271 I perplexity: tokenizing the input ..
0.00.410.280 I perplexity: tokenization took 7.008 ms
0.00.410.292 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.551.301 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.552.639 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.552.652 I llama_perf_context_print:        load time =     393.32 ms
0.00.552.653 I llama_perf_context_print: prompt eval time =     140.78 ms /   128 tokens (    1.10 ms per token,   909.22 tokens per second)
0.00.552.653 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.552.654 I llama_perf_context_print:       total time =     149.49 ms /   129 tokens
0.00.553.032 I ggml_metal_free: deallocating

real	0m0.569s
user	0m0.079s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.760 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.694 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.707 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.708 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.711 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.466 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.196 I llama_model_loader: - type  f32:  194 tensors
0.00.024.197 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.197 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.197 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.197 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.198 I print_info: file format = GGUF V3 (latest)
0.00.024.199 I print_info: file type   = Q3_K - Medium
0.00.024.200 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.003 I load: special tokens cache size = 25
0.00.037.926 I load: token to piece cache size = 0.2984 MB
0.00.037.929 I print_info: arch             = gptneox
0.00.037.929 I print_info: vocab_only       = 0
0.00.037.929 I print_info: n_ctx_train      = 2048
0.00.037.930 I print_info: n_embd           = 2048
0.00.037.930 I print_info: n_layer          = 24
0.00.037.933 I print_info: n_head           = 16
0.00.037.934 I print_info: n_head_kv        = 16
0.00.037.934 I print_info: n_rot            = 32
0.00.037.934 I print_info: n_swa            = 0
0.00.037.934 I print_info: n_embd_head_k    = 128
0.00.037.934 I print_info: n_embd_head_v    = 128
0.00.037.935 I print_info: n_gqa            = 1
0.00.037.936 I print_info: n_embd_k_gqa     = 2048
0.00.037.937 I print_info: n_embd_v_gqa     = 2048
0.00.037.937 I print_info: f_norm_eps       = 1.0e-05
0.00.037.938 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.938 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.938 I print_info: f_logit_scale    = 0.0e+00
0.00.037.939 I print_info: n_ff             = 8192
0.00.037.939 I print_info: n_expert         = 0
0.00.037.939 I print_info: n_expert_used    = 0
0.00.037.939 I print_info: causal attn      = 1
0.00.037.940 I print_info: pooling type     = 0
0.00.037.940 I print_info: rope type        = 2
0.00.037.940 I print_info: rope scaling     = linear
0.00.037.940 I print_info: freq_base_train  = 10000.0
0.00.037.941 I print_info: freq_scale_train = 1
0.00.037.941 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.941 I print_info: rope_finetuned   = unknown
0.00.037.941 I print_info: ssm_d_conv       = 0
0.00.037.942 I print_info: ssm_d_inner      = 0
0.00.037.942 I print_info: ssm_d_state      = 0
0.00.037.942 I print_info: ssm_dt_rank      = 0
0.00.037.942 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.942 I print_info: model type       = 1.4B
0.00.037.943 I print_info: model params     = 1.41 B
0.00.037.943 I print_info: general.name     = 1.4B
0.00.037.943 I print_info: vocab type       = BPE
0.00.037.944 I print_info: n_vocab          = 50304
0.00.037.944 I print_info: n_merges         = 50009
0.00.037.944 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.944 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.946 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.947 I print_info: LF token         = 128 'Ä'
0.00.037.947 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.947 I print_info: max token length = 1024
0.00.443.977 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.988 I load_tensors: offloading output layer to GPU
0.00.443.989 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.024 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.444.026 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.445.464 I llama_init_from_model: n_seq_max     = 1
0.00.445.469 I llama_init_from_model: n_ctx         = 128
0.00.445.470 I llama_init_from_model: n_ctx_per_seq = 128
0.00.445.470 I llama_init_from_model: n_batch       = 128
0.00.445.471 I llama_init_from_model: n_ubatch      = 128
0.00.445.471 I llama_init_from_model: flash_attn    = 0
0.00.445.473 I llama_init_from_model: freq_base     = 10000.0
0.00.445.474 I llama_init_from_model: freq_scale    = 1
0.00.445.478 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.445.490 I ggml_metal_init: allocating
0.00.445.584 I ggml_metal_init: found device: Apple M4
0.00.445.598 I ggml_metal_init: picking default device: Apple M4
0.00.447.432 I ggml_metal_init: using embedded metal library
0.00.453.016 I ggml_metal_init: GPU name:   Apple M4
0.00.453.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.035 I ggml_metal_init: simdgroup reduction   = true
0.00.453.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.036 I ggml_metal_init: has residency sets    = true
0.00.453.036 I ggml_metal_init: has bfloat            = true
0.00.453.037 I ggml_metal_init: use bfloat            = true
0.00.453.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.043 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.464 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.477.055 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.477.062 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.477.114 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.480.614 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.480.616 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.480.617 I llama_init_from_model: graph nodes  = 967
0.00.480.617 I llama_init_from_model: graph splits = 2
0.00.480.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.480.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.511.702 I 
0.00.511.789 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.511.810 I perplexity: tokenizing the input ..
0.00.519.013 I perplexity: tokenization took 7.201 ms
0.00.519.025 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.664.191 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.665.595 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.665.608 I llama_perf_context_print:        load time =     502.93 ms
0.00.665.609 I llama_perf_context_print: prompt eval time =     144.94 ms /   128 tokens (    1.13 ms per token,   883.14 tokens per second)
0.00.665.609 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.610 I llama_perf_context_print:       total time =     153.91 ms /   129 tokens
0.00.665.951 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.078s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.345 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.116 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.122 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.125 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.126 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.127 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.127 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.128 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.128 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.128 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.129 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.131 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.840 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.863 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.630 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.631 I llama_model_loader: - type  f32:  194 tensors
0.00.025.631 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.631 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.631 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.632 I print_info: file format = GGUF V3 (latest)
0.00.025.632 I print_info: file type   = Q4_K - Medium
0.00.025.633 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.454 I load: special tokens cache size = 25
0.00.039.403 I load: token to piece cache size = 0.2984 MB
0.00.039.406 I print_info: arch             = gptneox
0.00.039.406 I print_info: vocab_only       = 0
0.00.039.406 I print_info: n_ctx_train      = 2048
0.00.039.406 I print_info: n_embd           = 2048
0.00.039.406 I print_info: n_layer          = 24
0.00.039.409 I print_info: n_head           = 16
0.00.039.410 I print_info: n_head_kv        = 16
0.00.039.410 I print_info: n_rot            = 32
0.00.039.411 I print_info: n_swa            = 0
0.00.039.411 I print_info: n_embd_head_k    = 128
0.00.039.411 I print_info: n_embd_head_v    = 128
0.00.039.412 I print_info: n_gqa            = 1
0.00.039.412 I print_info: n_embd_k_gqa     = 2048
0.00.039.413 I print_info: n_embd_v_gqa     = 2048
0.00.039.413 I print_info: f_norm_eps       = 1.0e-05
0.00.039.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.414 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.414 I print_info: f_logit_scale    = 0.0e+00
0.00.039.415 I print_info: n_ff             = 8192
0.00.039.415 I print_info: n_expert         = 0
0.00.039.415 I print_info: n_expert_used    = 0
0.00.039.416 I print_info: causal attn      = 1
0.00.039.416 I print_info: pooling type     = 0
0.00.039.416 I print_info: rope type        = 2
0.00.039.416 I print_info: rope scaling     = linear
0.00.039.416 I print_info: freq_base_train  = 10000.0
0.00.039.417 I print_info: freq_scale_train = 1
0.00.039.417 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.417 I print_info: rope_finetuned   = unknown
0.00.039.417 I print_info: ssm_d_conv       = 0
0.00.039.417 I print_info: ssm_d_inner      = 0
0.00.039.417 I print_info: ssm_d_state      = 0
0.00.039.418 I print_info: ssm_dt_rank      = 0
0.00.039.418 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.418 I print_info: model type       = 1.4B
0.00.039.418 I print_info: model params     = 1.41 B
0.00.039.421 I print_info: general.name     = 1.4B
0.00.039.421 I print_info: vocab type       = BPE
0.00.039.422 I print_info: n_vocab          = 50304
0.00.039.422 I print_info: n_merges         = 50009
0.00.039.422 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.422 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.422 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.422 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.423 I print_info: LF token         = 128 'Ä'
0.00.039.423 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.423 I print_info: max token length = 1024
0.00.528.549 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.562 I load_tensors: offloading output layer to GPU
0.00.528.562 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.596 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.598 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.530.086 I llama_init_from_model: n_seq_max     = 1
0.00.530.090 I llama_init_from_model: n_ctx         = 128
0.00.530.091 I llama_init_from_model: n_ctx_per_seq = 128
0.00.530.091 I llama_init_from_model: n_batch       = 128
0.00.530.091 I llama_init_from_model: n_ubatch      = 128
0.00.530.092 I llama_init_from_model: flash_attn    = 0
0.00.530.093 I llama_init_from_model: freq_base     = 10000.0
0.00.530.094 I llama_init_from_model: freq_scale    = 1
0.00.530.094 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.530.097 I ggml_metal_init: allocating
0.00.530.154 I ggml_metal_init: found device: Apple M4
0.00.530.167 I ggml_metal_init: picking default device: Apple M4
0.00.531.929 I ggml_metal_init: using embedded metal library
0.00.538.162 I ggml_metal_init: GPU name:   Apple M4
0.00.538.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.170 I ggml_metal_init: simdgroup reduction   = true
0.00.538.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.171 I ggml_metal_init: has residency sets    = true
0.00.538.171 I ggml_metal_init: has bfloat            = true
0.00.538.171 I ggml_metal_init: use bfloat            = true
0.00.538.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.184 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.557.152 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.560.828 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.560.832 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.560.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.564.239 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.564.241 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.564.241 I llama_init_from_model: graph nodes  = 967
0.00.564.242 I llama_init_from_model: graph splits = 2
0.00.564.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.564.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.745 I 
0.00.596.832 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.857 I perplexity: tokenizing the input ..
0.00.604.092 I perplexity: tokenization took 7.231 ms
0.00.604.112 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.493 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.747.826 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.747.842 I llama_perf_context_print:        load time =     586.39 ms
0.00.747.843 I llama_perf_context_print: prompt eval time =     141.44 ms /   128 tokens (    1.11 ms per token,   904.95 tokens per second)
0.00.747.844 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.845 I llama_perf_context_print:       total time =     151.10 ms /   129 tokens
0.00.748.215 I ggml_metal_free: deallocating

real	0m0.764s
user	0m0.079s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.047 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.056 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.059 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.061 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.942 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.991 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.814 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.814 I llama_model_loader: - type  f32:  194 tensors
0.00.024.814 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.815 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.815 I print_info: file format = GGUF V3 (latest)
0.00.024.816 I print_info: file type   = Q5_K - Medium
0.00.024.817 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.011 I load: special tokens cache size = 25
0.00.038.998 I load: token to piece cache size = 0.2984 MB
0.00.039.001 I print_info: arch             = gptneox
0.00.039.002 I print_info: vocab_only       = 0
0.00.039.002 I print_info: n_ctx_train      = 2048
0.00.039.002 I print_info: n_embd           = 2048
0.00.039.002 I print_info: n_layer          = 24
0.00.039.005 I print_info: n_head           = 16
0.00.039.006 I print_info: n_head_kv        = 16
0.00.039.006 I print_info: n_rot            = 32
0.00.039.006 I print_info: n_swa            = 0
0.00.039.006 I print_info: n_embd_head_k    = 128
0.00.039.006 I print_info: n_embd_head_v    = 128
0.00.039.007 I print_info: n_gqa            = 1
0.00.039.008 I print_info: n_embd_k_gqa     = 2048
0.00.039.009 I print_info: n_embd_v_gqa     = 2048
0.00.039.009 I print_info: f_norm_eps       = 1.0e-05
0.00.039.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.010 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.011 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.012 I print_info: f_logit_scale    = 0.0e+00
0.00.039.012 I print_info: n_ff             = 8192
0.00.039.012 I print_info: n_expert         = 0
0.00.039.013 I print_info: n_expert_used    = 0
0.00.039.013 I print_info: causal attn      = 1
0.00.039.013 I print_info: pooling type     = 0
0.00.039.013 I print_info: rope type        = 2
0.00.039.013 I print_info: rope scaling     = linear
0.00.039.014 I print_info: freq_base_train  = 10000.0
0.00.039.014 I print_info: freq_scale_train = 1
0.00.039.014 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.015 I print_info: rope_finetuned   = unknown
0.00.039.015 I print_info: ssm_d_conv       = 0
0.00.039.015 I print_info: ssm_d_inner      = 0
0.00.039.015 I print_info: ssm_d_state      = 0
0.00.039.017 I print_info: ssm_dt_rank      = 0
0.00.039.017 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.017 I print_info: model type       = 1.4B
0.00.039.018 I print_info: model params     = 1.41 B
0.00.039.018 I print_info: general.name     = 1.4B
0.00.039.018 I print_info: vocab type       = BPE
0.00.039.019 I print_info: n_vocab          = 50304
0.00.039.019 I print_info: n_merges         = 50009
0.00.039.019 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.019 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.019 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.020 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.023 I print_info: LF token         = 128 'Ä'
0.00.039.024 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.024 I print_info: max token length = 1024
0.00.587.291 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.306 I load_tensors: offloading output layer to GPU
0.00.587.306 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.339 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.340 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.588.898 I llama_init_from_model: n_seq_max     = 1
0.00.588.903 I llama_init_from_model: n_ctx         = 128
0.00.588.903 I llama_init_from_model: n_ctx_per_seq = 128
0.00.588.908 I llama_init_from_model: n_batch       = 128
0.00.588.908 I llama_init_from_model: n_ubatch      = 128
0.00.588.909 I llama_init_from_model: flash_attn    = 0
0.00.588.917 I llama_init_from_model: freq_base     = 10000.0
0.00.588.918 I llama_init_from_model: freq_scale    = 1
0.00.588.918 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.921 I ggml_metal_init: allocating
0.00.589.026 I ggml_metal_init: found device: Apple M4
0.00.589.040 I ggml_metal_init: picking default device: Apple M4
0.00.590.583 I ggml_metal_init: using embedded metal library
0.00.596.891 I ggml_metal_init: GPU name:   Apple M4
0.00.596.894 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.895 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.896 I ggml_metal_init: simdgroup reduction   = true
0.00.596.897 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.897 I ggml_metal_init: has residency sets    = true
0.00.596.897 I ggml_metal_init: has bfloat            = true
0.00.596.897 I ggml_metal_init: use bfloat            = true
0.00.596.899 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.149 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.781 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.617.785 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.810 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.964 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.620.966 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.620.966 I llama_init_from_model: graph nodes  = 967
0.00.620.967 I llama_init_from_model: graph splits = 2
0.00.620.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.857 I 
0.00.656.957 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.977 I perplexity: tokenizing the input ..
0.00.664.235 I perplexity: tokenization took 7.252 ms
0.00.664.254 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.575 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.819.938 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.819.953 I llama_perf_context_print:        load time =     647.96 ms
0.00.819.954 I llama_perf_context_print: prompt eval time =     153.36 ms /   128 tokens (    1.20 ms per token,   834.62 tokens per second)
0.00.819.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.955 I llama_perf_context_print:       total time =     163.10 ms /   129 tokens
0.00.820.329 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.079s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.129 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.988 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.992 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.995 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.996 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.996 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.997 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.997 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.998 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.999 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.999 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.999 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.000 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.002 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.798 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.846 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.602 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.603 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.604 I llama_model_loader: - type  f32:  194 tensors
0.00.025.604 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.605 I print_info: file format = GGUF V3 (latest)
0.00.025.605 I print_info: file type   = Q6_K
0.00.025.606 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.685 I load: special tokens cache size = 25
0.00.039.636 I load: token to piece cache size = 0.2984 MB
0.00.039.639 I print_info: arch             = gptneox
0.00.039.639 I print_info: vocab_only       = 0
0.00.039.639 I print_info: n_ctx_train      = 2048
0.00.039.640 I print_info: n_embd           = 2048
0.00.039.640 I print_info: n_layer          = 24
0.00.039.643 I print_info: n_head           = 16
0.00.039.644 I print_info: n_head_kv        = 16
0.00.039.644 I print_info: n_rot            = 32
0.00.039.644 I print_info: n_swa            = 0
0.00.039.644 I print_info: n_embd_head_k    = 128
0.00.039.644 I print_info: n_embd_head_v    = 128
0.00.039.645 I print_info: n_gqa            = 1
0.00.039.646 I print_info: n_embd_k_gqa     = 2048
0.00.039.647 I print_info: n_embd_v_gqa     = 2048
0.00.039.647 I print_info: f_norm_eps       = 1.0e-05
0.00.039.648 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.653 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.653 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.653 I print_info: f_logit_scale    = 0.0e+00
0.00.039.654 I print_info: n_ff             = 8192
0.00.039.654 I print_info: n_expert         = 0
0.00.039.654 I print_info: n_expert_used    = 0
0.00.039.655 I print_info: causal attn      = 1
0.00.039.655 I print_info: pooling type     = 0
0.00.039.655 I print_info: rope type        = 2
0.00.039.655 I print_info: rope scaling     = linear
0.00.039.656 I print_info: freq_base_train  = 10000.0
0.00.039.656 I print_info: freq_scale_train = 1
0.00.039.656 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.656 I print_info: rope_finetuned   = unknown
0.00.039.656 I print_info: ssm_d_conv       = 0
0.00.039.657 I print_info: ssm_d_inner      = 0
0.00.039.657 I print_info: ssm_d_state      = 0
0.00.039.657 I print_info: ssm_dt_rank      = 0
0.00.039.657 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.657 I print_info: model type       = 1.4B
0.00.039.658 I print_info: model params     = 1.41 B
0.00.039.658 I print_info: general.name     = 1.4B
0.00.039.660 I print_info: vocab type       = BPE
0.00.039.660 I print_info: n_vocab          = 50304
0.00.039.660 I print_info: n_merges         = 50009
0.00.039.660 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.661 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.661 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.661 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.661 I print_info: LF token         = 128 'Ä'
0.00.039.661 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.661 I print_info: max token length = 1024
0.00.404.746 I load_tensors: offloading 24 repeating layers to GPU
0.00.404.754 I load_tensors: offloading output layer to GPU
0.00.404.755 I load_tensors: offloaded 25/25 layers to GPU
0.00.404.792 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.404.793 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.406.251 I llama_init_from_model: n_seq_max     = 1
0.00.406.258 I llama_init_from_model: n_ctx         = 128
0.00.406.259 I llama_init_from_model: n_ctx_per_seq = 128
0.00.406.260 I llama_init_from_model: n_batch       = 128
0.00.406.260 I llama_init_from_model: n_ubatch      = 128
0.00.406.261 I llama_init_from_model: flash_attn    = 0
0.00.406.261 I llama_init_from_model: freq_base     = 10000.0
0.00.406.262 I llama_init_from_model: freq_scale    = 1
0.00.406.262 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.406.266 I ggml_metal_init: allocating
0.00.406.348 I ggml_metal_init: found device: Apple M4
0.00.406.363 I ggml_metal_init: picking default device: Apple M4
0.00.408.110 I ggml_metal_init: using embedded metal library
0.00.414.578 I ggml_metal_init: GPU name:   Apple M4
0.00.414.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.414.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.414.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.414.585 I ggml_metal_init: simdgroup reduction   = true
0.00.414.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.414.586 I ggml_metal_init: has residency sets    = true
0.00.414.586 I ggml_metal_init: has bfloat            = true
0.00.414.586 I ggml_metal_init: use bfloat            = true
0.00.414.587 I ggml_metal_init: hasUnifiedMemory      = true
0.00.414.589 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.431.457 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.435.024 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.435.032 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.435.064 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.438.356 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.438.358 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.438.358 I llama_init_from_model: graph nodes  = 967
0.00.438.359 I llama_init_from_model: graph splits = 2
0.00.438.362 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.438.362 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.016 I 
0.00.474.099 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.119 I perplexity: tokenizing the input ..
0.00.479.194 I perplexity: tokenization took 5.074 ms
0.00.479.205 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.618.531 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.619.945 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.619.961 I llama_perf_context_print:        load time =     463.88 ms
0.00.619.963 I llama_perf_context_print: prompt eval time =     139.08 ms /   128 tokens (    1.09 ms per token,   920.37 tokens per second)
0.00.619.963 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.963 I llama_perf_context_print:       total time =     145.95 ms /   129 tokens
0.00.620.351 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.075s
sys	0m0.116s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.262 I build: 4593 (4314e56c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.396 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.410 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.411 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.418 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.418 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.645 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.183 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.185 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.186 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.186 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.187 I llama_model_loader: - type  f32:  194 tensors
0.00.053.187 I llama_model_loader: - type  f16:   98 tensors
0.00.053.188 I print_info: file format = GGUF V3 (latest)
0.00.053.189 I print_info: file type   = all F32 (guessed)
0.00.053.190 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.381 I load: special tokens cache size = 25
0.00.073.182 I load: token to piece cache size = 0.2984 MB
0.00.073.185 I print_info: arch             = gptneox
0.00.073.186 I print_info: vocab_only       = 0
0.00.073.186 I print_info: n_ctx_train      = 2048
0.00.073.186 I print_info: n_embd           = 2048
0.00.073.186 I print_info: n_layer          = 24
0.00.073.189 I print_info: n_head           = 16
0.00.073.190 I print_info: n_head_kv        = 16
0.00.073.190 I print_info: n_rot            = 32
0.00.073.191 I print_info: n_swa            = 0
0.00.073.191 I print_info: n_embd_head_k    = 128
0.00.073.191 I print_info: n_embd_head_v    = 128
0.00.073.192 I print_info: n_gqa            = 1
0.00.073.193 I print_info: n_embd_k_gqa     = 2048
0.00.073.193 I print_info: n_embd_v_gqa     = 2048
0.00.073.194 I print_info: f_norm_eps       = 1.0e-05
0.00.073.194 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.195 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.195 I print_info: f_logit_scale    = 0.0e+00
0.00.073.196 I print_info: n_ff             = 8192
0.00.073.196 I print_info: n_expert         = 0
0.00.073.196 I print_info: n_expert_used    = 0
0.00.073.196 I print_info: causal attn      = 1
0.00.073.196 I print_info: pooling type     = 0
0.00.073.196 I print_info: rope type        = 2
0.00.073.197 I print_info: rope scaling     = linear
0.00.073.197 I print_info: freq_base_train  = 10000.0
0.00.073.197 I print_info: freq_scale_train = 1
0.00.073.198 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.198 I print_info: rope_finetuned   = unknown
0.00.073.198 I print_info: ssm_d_conv       = 0
0.00.073.198 I print_info: ssm_d_inner      = 0
0.00.073.200 I print_info: ssm_d_state      = 0
0.00.073.200 I print_info: ssm_dt_rank      = 0
0.00.073.200 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.201 I print_info: model type       = 1.4B
0.00.073.201 I print_info: model params     = 1.41 B
0.00.073.201 I print_info: general.name     = 1.4B
0.00.073.202 I print_info: vocab type       = BPE
0.00.073.202 I print_info: n_vocab          = 50304
0.00.073.202 I print_info: n_merges         = 50009
0.00.073.202 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.203 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.203 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.207 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.208 I print_info: LF token         = 128 'Ä'
0.00.073.208 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.208 I print_info: max token length = 1024
0.01.305.403 I load_tensors: offloading 24 repeating layers to GPU
0.01.305.406 I load_tensors: offloading output layer to GPU
0.01.305.407 I load_tensors: offloaded 25/25 layers to GPU
0.01.305.435 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.305.437 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.306.540 I llama_init_from_model: n_seq_max     = 1
0.01.306.541 I llama_init_from_model: n_ctx         = 128
0.01.306.541 I llama_init_from_model: n_ctx_per_seq = 128
0.01.306.541 I llama_init_from_model: n_batch       = 128
0.01.306.541 I llama_init_from_model: n_ubatch      = 128
0.01.306.542 I llama_init_from_model: flash_attn    = 0
0.01.306.542 I llama_init_from_model: freq_base     = 10000.0
0.01.306.542 I llama_init_from_model: freq_scale    = 1
0.01.306.543 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.306.547 I ggml_metal_init: allocating
0.01.306.602 I ggml_metal_init: found device: Apple M4
0.01.306.609 I ggml_metal_init: picking default device: Apple M4
0.01.307.647 I ggml_metal_init: using embedded metal library
0.01.311.552 I ggml_metal_init: GPU name:   Apple M4
0.01.311.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.311.555 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.311.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.311.556 I ggml_metal_init: simdgroup reduction   = true
0.01.311.556 I ggml_metal_init: simdgroup matrix mul. = true
0.01.311.556 I ggml_metal_init: has residency sets    = true
0.01.311.557 I ggml_metal_init: has bfloat            = true
0.01.311.557 I ggml_metal_init: use bfloat            = true
0.01.311.558 I ggml_metal_init: hasUnifiedMemory      = true
0.01.311.561 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.323.098 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.324.891 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.324.894 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.324.908 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.326.653 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.326.654 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.326.654 I llama_init_from_model: graph nodes  = 967
0.01.326.655 I llama_init_from_model: graph splits = 2
0.01.326.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.326.657 I 
0.01.326.692 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.326.693 I compute_imatrix: tokenizing the input ..
0.01.331.012 I compute_imatrix: tokenization took 4.318 ms
0.01.331.014 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.598.684 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.601.404 I llama_perf_context_print:        load time =    1576.87 ms
0.01.601.405 I llama_perf_context_print: prompt eval time =     265.94 ms /   128 tokens (    2.08 ms per token,   481.31 tokens per second)
0.01.601.405 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.601.406 I llama_perf_context_print:       total time =    1579.58 ms /   129 tokens
0.01.602.100 I ggml_metal_free: deallocating

real	0m1.811s
user	0m0.128s
sys	0m0.242s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4593 (4314e56c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142604b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1426084b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142608a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142609010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1426095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142609b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14260a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14260a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14260ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14260b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14260b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14260bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14260c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14260ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14260d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14260dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14260e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14260ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14260f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14260fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1426101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1426108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142611010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1426118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142611fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142612290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1426128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142613510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142613a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142613d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1426141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142614470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142614d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142615240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142615500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1426159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142615e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1426162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142616780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1426170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142617560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142617a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142617ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142618160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142618770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142618d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1426196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142619cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14261a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14261a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14261aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14261b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14261bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14261c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14261c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14261cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14261cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14261d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14261dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14261dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14261e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14261e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14261ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14261f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14261f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14261fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142620010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1426204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142620950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142620df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142621290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1426221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142622720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142622c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1426231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142623710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142623c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1426241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142624700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142624c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1426251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1426256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142626190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1426266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142626c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1426276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142627c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142628170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1426286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142628c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142629160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1426296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142619390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142629b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14262a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14262a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14262ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14262b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14262b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14262bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14262c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14262c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14262cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14262d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14262d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14262dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14262e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14262e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14262ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14262f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14262f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14262fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14262ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1426303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142630840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142630ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142631180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142631620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142631ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142631f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142632400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1426328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142632d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1426331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142633680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142633b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142634460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142634900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142634da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1426356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142635b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142636020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1426364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142636e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1426372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142637740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142637be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142638080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142638520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1426389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142638e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142639300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1426397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142639c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14263a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14263a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14263aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14263aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14263b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14263b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14263bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14263c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14263c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14263ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14263cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14263d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14263d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14263dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14263e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14263e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14263eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14263ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14263f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14263f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14263fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142640200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1426406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142640fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142641480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142641920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142642700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142643040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1426434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142643980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142643e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1426442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142644760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142644c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1426450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1426459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142645f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142646480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1426469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142646f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1426471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1426477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142647e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142648410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142648c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1426490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142649360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142649970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142649f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14264a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14264ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14264b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14264b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14264bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14264c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14264c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14264ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14264d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14264d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14264dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14264e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14264e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14264ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14264f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14264f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14264fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142650210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142650760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142650cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142651200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142651750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142651ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1426521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142652740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142652c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1426531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142653730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142653c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1426541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142654720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142654c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1426551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142655710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142655c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1426561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142656700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142656c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1426571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1426576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142657c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142658190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1426586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142658c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142659180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1426596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142659c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14265a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14265a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14265ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14265b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14265b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14265bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14265c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14265c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14265cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14265d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14265d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14265dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14265e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14265e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14265eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14265efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14265f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14265f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14265fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142660240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1426606e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142660b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142661020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1426614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142661960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1426622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142662740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142662be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142663130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142663850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142663f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142664690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142664db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142665860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142665b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142666130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.718.264 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.268 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12dd04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12dd05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12dd056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12dd05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12dd05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12dd06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12dd06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12dd06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12dd07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12dd075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12dd07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12dd08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12dd08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12dd093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12dd09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12dd0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12dd0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12dd0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12dd0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12dd0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12dd0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12dd0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12dd0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12dd0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12dd0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12dd0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12dd0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12dd0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12dd0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12dd0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12dd0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12dd0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12dd10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12dd106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12dd10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12dd10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12dd11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12dd118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12dd11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12dd12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12dd12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12dd12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12dd12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12dd13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12dd137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12dd13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12dd140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12dd14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12dd14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12dd14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12dd15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12dd156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12dd15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12dd15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12dd16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12dd16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12dd16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12dd17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12dd17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12dd17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12dd18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12dd184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12dd18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12dd18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12dd19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12dd19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12dd19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12dd19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12dd1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12dd1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12dd1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12dd1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12dd1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12dd1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12dd1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12dd1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12dd1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12dd1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12dd1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12dd1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12dd1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12dd1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12dd1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12dd1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12dd1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12dd1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12dd1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12dd1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12dd1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12dd20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12dd20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12dd209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12dd20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12dd212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12dd21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12dd21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12dd22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12dd22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12dd228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12dd22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12dd231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12dd23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12dd23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12dd23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12dd24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12dd24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12dd24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12dd250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12dd25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12dd259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12dd25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12dd262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12dd26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12dd26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12dd26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12dd27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12dd278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12dd27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12dd281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12dd28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12dd28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12dd28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12dd29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12dd297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12dd29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12dd2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12dd2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12dd2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12dd2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12dd2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12dd2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12dd2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12dd2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12dd2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12dd2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12dd2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12dd2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12dd2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12dd2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12dd2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12dd2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12dd2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12dd2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12dd2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12dd2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12dd2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12dd2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12dd30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12dd306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12dd30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12dd30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12dd31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12dd31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12dd31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12dd32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12dd325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12dd32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12dd32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12dd33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12dd337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12dd33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12dd34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12dd344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12dd34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12dd34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12dd35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12dd35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12dd36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12dd363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12dd36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12dd36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12dd37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12dd375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12dd37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12dd37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12dd38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12dd38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12dd38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12dd39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12dd394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12dd39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12dd39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12dd3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12dd3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12dd3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12dd3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12dd3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12dd3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12dd3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12dd3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12dd3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12dd3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12dd3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12dd3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12dd3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12dd3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12dd3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12dd3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12dd3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12dd3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12dd3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12dd3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12dd3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12dd400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12dd40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12dd409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12dd40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12dd41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12dd417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12dd41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12dd42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12dd42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12dd430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12dd43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12dd43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12dd441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12dd447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12dd44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12dd45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12dd458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12dd45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12dd46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12dd46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12dd46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12dd475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12dd47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12dd48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12dd486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12dd48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12dd49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12dd49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12dd49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12dd4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12dd4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12dd4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12dd4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12dd4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12dd4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12dd4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12dd4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12dd4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12dd4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12dd4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12dd4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12dd4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12dd4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12dd4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12dd4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12dd4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12dd50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12dd50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12dd510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12dd516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12dd51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12dd52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12dd527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12dd52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12dd53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12dd53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12dd53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12dd544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12dd54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12dd55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12dd555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12dd55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12dd56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12dd56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12dd56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12dd571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12dd576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12dd57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12dd580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12dd585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12dd58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12dd58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12dd594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12dd599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12dd59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12dd5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12dd5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12dd5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12dd5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12dd5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12dd5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12dd5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12dd5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12dd5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12dd5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12dd5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12dd5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12dd5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d3086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d306500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d308d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d309180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d3095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d309ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d30a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d30a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d30acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d30b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d30b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d30bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d30c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d30ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d30d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d30ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d30e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d30ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d30f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d30fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d310200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d310920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d311040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d311760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d311e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d312140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d312750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d312d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d313370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d313b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d314000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d3142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d314b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d315090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d315350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d3157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d315c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d316130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d3165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d316a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d316f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d3173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d317850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d317cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d317fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d3185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d318bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d3191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d3197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d319e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d31a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d31aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d31b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d31b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d31be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d31c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d31c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d31ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d31d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d31d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d31dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d31e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d31e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d31eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d31ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d31f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d31f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d31fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12dd5bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12dd4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12dd4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12dd483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12dd45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12dd552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12dd52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12dd50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12dd4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12dd46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12dd43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12dd48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12dd4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12dd4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12dd4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12dd541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12dd46cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12dd47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12dd4f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12dd513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12dd49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12dd4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12dd50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12dd42db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12dd4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12dd4d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12dd47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12dd489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12dd558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12dd53070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12dd44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12dd4dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12dd43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12dd43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12dd455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12dd55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12dd4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12dd53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12dd49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12dd4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12dd4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12dd472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12dd51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12dd46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12dd54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12dd51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12dd4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12dd569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12dd45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12dd56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12dd444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12dd54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12dd4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12dd50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12dd53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12dd524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12dd4a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12dd5e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12dd083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12dd35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12dd41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12dd04880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12dd5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12dd0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12dd5ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12dd5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12dd5f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12dd5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12dd5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12dd5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12dd5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12dd60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12dd60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12dd60800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12dd60ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12dd60d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12dd61040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12dd61300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12dd615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12dd61880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12dd61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12dd61e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12dd620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12dd62380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12dd62640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12dd62900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12dd62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12dd62e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12dd63140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12dd63400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12dd636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12dd63980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12dd63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12dd63f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12dd641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12dd64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12dd64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12dd64a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12dd64cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12dd64f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12dd65240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12dd65500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12dd657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12dd65a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12dd65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12dd66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12dd662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12dd66580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12dd66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12dd66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12dd66dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12dd67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12dd67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12dd67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12dd678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12dd67b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12dd67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12dd68100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12dd683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12dd68680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12dd68940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12dd68c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12dd68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12dd69180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12dd69440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12dd69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12dd699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12dd69c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12dd69f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12dd6a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12dd6a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12dd6a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12dd6aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12dd6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12dd6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12dd6b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12dd6b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12dd6b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12dd6bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12dd6bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12dd6c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12dd6c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12dd6c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12dd6cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12dd6ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12dd6d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12dd6d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12dd6d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12dd6d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12dd6dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12dd6ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12dd6e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12dd6e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12dd6e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12dd6e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12dd6ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12dd6ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12dd6f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12dd6f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12dd6f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12dd6fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12dd6fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12dd6ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12dd70290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12dd70550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12dd70810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12dd70ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12dd70d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12dd71050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12dd71310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12dd715d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12dd71890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12dd71b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12dd71e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12dd720d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12dd72390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12dd72650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12dd72910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12dd72bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12dd72e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12dd73150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12dd73410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12dd736d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12dd73990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12dd73c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12dd73f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12dd741d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12dd74490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12dd74750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12dd74a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12dd74cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12dd74f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12dd75250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12dd75510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12dd757d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12dd75a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12dd75d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12dd76010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12dd762d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12dd76590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12dd76850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12dd76b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12dd76dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12dd77090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12dd77350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12dd77610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12dd778d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12dd77b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12dd77e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12dd78110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12dd783d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12dd78690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12dd78950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12dd78c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12dd78ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12dd79190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12dd79630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12dd79d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12dd7a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12dd7a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12dd7a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12dd7ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12dd7b2d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.798s
user	0m0.285s
sys	0m0.302s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4593 (4314e56c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d70ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d70f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d70fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d7101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d7107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d710d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d711300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d7118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d711e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d712360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d712860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d712d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d713880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d714030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d714840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d714f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d715680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d715da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d7164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d716c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d7173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d717ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d7181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d718a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d7191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d719470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d719a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d71a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d71ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d71aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d71b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d71b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d71bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d71c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d71c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d71cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d71d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d71d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d71d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d71de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d71e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d71e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d71ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d71f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d71f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d71f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d71ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d720880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d720e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d7214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d721ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d7220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d7226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d722ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d7234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d723970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d723e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d7240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d7246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d724ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d725190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d725ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d725f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d726410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d7268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d726d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d7271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d727690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d727b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d727fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d728470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d728910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d728e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d7293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d729900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d729e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d72a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d72a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d72ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d72b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d72b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d72be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d72c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d72c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d72ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d72d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d72d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d72de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d72e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d72e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d72ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d72f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d72f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d72fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d730340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d730890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d720570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d730d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d7314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d731a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d731f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d7324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d7329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d732f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d733490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d7339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d733f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d734480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d7349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d734f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d735470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d7359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d735e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d7367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d736c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d7370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d737580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d737a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d737ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d738360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d738800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d738ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d739140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d7395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d739a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d739f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d73a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d73a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d73ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d73b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d73b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d73bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d73bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d73c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d73c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d73cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d73d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d73d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d73db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d73dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d73e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d73e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d73edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d73f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d73f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d73fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d740040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d7404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d740980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d740e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d7412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d741760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d741c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d7420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d7429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d742e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d743320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d7437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d743c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d744100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d7445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d744a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d744ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d745380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d745820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d745cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d746160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d746600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d746aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d746f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d7473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d747880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d747d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d7481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d748660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d748b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d748fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d749440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d7498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d749d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d74a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d74a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d74ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d74b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d74b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d74b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d74bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d74c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d74c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d74cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d74d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d74d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d74dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d74e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d74e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d74e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d74efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d74f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d74fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d750280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d750540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d750b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d751160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d751950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d751df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d752290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d752730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d752ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d753980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d753ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d754420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d754970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d754ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d755410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d755960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d755eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d756400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d756950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d756ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d7573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d757940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d757e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d7583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d758930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d758e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d7593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d759920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d75a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d75a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d75ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d75b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d75b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d75be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d75c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d75c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d75ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d75d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d75d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d75de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d75e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d75e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d75ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d75f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d75f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d75fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d760360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d7608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d760e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d761350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d7618a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d761df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d762340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d762890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d762de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d763330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d763880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d763dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d764320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d764870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d764dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d765310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d765860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d765d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d7661a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d766640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d766ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d766f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d767420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d7678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d767d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d768200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d7686a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d768b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d768fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d769480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d769920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d769dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d76a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d76aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d76b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d76b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d76bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d76c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d76ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d76cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d76d310 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.056 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f006200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f006670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f006ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f006f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f0073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f007830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f007ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f008110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f008580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f0089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f008e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f009530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f00a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f00a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f00b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f00b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f00be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f00c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f00cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f00d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f00db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f00e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f00e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f00f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f00f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f00fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f00fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f0101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f010660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f010ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f010f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f011470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f0118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f011ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f012010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f012480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f0128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f012d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f0131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f013640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f013ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f013f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f014390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f014800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f014c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f0150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f015550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f0159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f015e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f0162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f016710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f016b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f016ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f017460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f0178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f017d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f0182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f0187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f018c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f019090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f019500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f019970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f019de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f01a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f01a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f01ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f01afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f01b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f01b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f01bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f01c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f01c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f01ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f01ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f01d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f01d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f01dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f01e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f01e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f01e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f01edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f01f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f01f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f01fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f01ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f0203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f020860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f020cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f021140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f0215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f021a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f021e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f022300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f022770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f022be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f023050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f0234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f023930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f023da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f024210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f024680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f024af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f024f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f0253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f025840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f025cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f026120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f026590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f026a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f026e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f0272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f027750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f027bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f028030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f0284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f028910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f028d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f0291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f029660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f029ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f029f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f02a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f02a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f02ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f02b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f02b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f02b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f02be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f02c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f02c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f02cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f02d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f02d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f02d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f02dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f02e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f02e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f02eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f02ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f02f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f02f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f02fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f0300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f030550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f0309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f030e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f0312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f031710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f031b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f031ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f032460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f0328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f032d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f0331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f033620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f033a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f033f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f034370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f0347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f034c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f0350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f035530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f0359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f035e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f036280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f0366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f037320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f0375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f0378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f038180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f0385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f038ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f0397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f03a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f03a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f03a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f03ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f03b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f03b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f03bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f03bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f03c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f03c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f03ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f03d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f03d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f03da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f03deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f03e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f03e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f03ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f03f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f03f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f03f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f03fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f040230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f0406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f040b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f041070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f041580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f0419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f041e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f0422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f042740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f042c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f043170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f043ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f043fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f044560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f044b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f0450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f0456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f045c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f046220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f0467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f046da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f047360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f047920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f047ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f0484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f048a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f049020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f0495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f049ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f04a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f04a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f04ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f04b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f04b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f04be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f04c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f04c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f04cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f04d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f04dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f04e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f04e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f04ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f04f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f04f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f04fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f050320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f0508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f050ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f051460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f051a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f051fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f0525a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f052b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f053120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f0536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f053ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f054260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f054820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f054de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f0553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f055960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f055f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f0564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f056aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f057060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f057620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f057be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f0581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f0586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f058ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f0590a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f0595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f059aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f059fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f05a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f05a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f05aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f05b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f05b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f05bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f05c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f05c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f05cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f05d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f05ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f05e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f05ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f05eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f05f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f05f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f05ff90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d76cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d74ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d74e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d74f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d722380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d721d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d724390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d750e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d719730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d720220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d720b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d721150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d71f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d721760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d718730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d7249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d730fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d76c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d71b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d71bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d751420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d74f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d719d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d71a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d71a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d76d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d76da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d76dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d76dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d76e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d76e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d76e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d76eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d76ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d76f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d76f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d76f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d76f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d76fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d76fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d7700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d770370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d770630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d7708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d770bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d770e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d771130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d7713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d7716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d771970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d771c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d771ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d7721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d772470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d772730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d7729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d772cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d772f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d773230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d7734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d7737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d773a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d773d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d773ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d7742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d774570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d774830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d774af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d774db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d775070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d775330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d7755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d7758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d775b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d775e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d7760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d7763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d776670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d776930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d776bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d776eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d777170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d777430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d7776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d7779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d777c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d777f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d7781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d7784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d778770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d778a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d778cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d778fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d779270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d779530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d7797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d779ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d779d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d77a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d77a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d77a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d77a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d77ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d77adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d77b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d77b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d77b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d77b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d77bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d77be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d77c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d77c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d77c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d77c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d77cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d77cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d77d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d77d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d77d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d77d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d77dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d77df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d77e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d77e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d77e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d77ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d77ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d77eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d77f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d77f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d77f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d77faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d77fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d780070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d780330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d7805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d7808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d780b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d780e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d7810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d7813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d781670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d781930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d781bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d781eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d782170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d782430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d7826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d7829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d782c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d782f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d7831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d7834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d783770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d783a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d783cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d783fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d784270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d784530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d7847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d784ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d784d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d785030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d7852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d7855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d785870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d785b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d785df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d7860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d786370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d786630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d7868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d786bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d786e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d787130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d7873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d7876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d787970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d787c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d787ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d7881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d788470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d788730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d7889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d788cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d788f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d789230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d7894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d7897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d789a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d789d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d789ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d78a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d78a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d78a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d78aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d78adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d78b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d78b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d78b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d78b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d78bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d78be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d78c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d78c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d78c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d78c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d605100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d605570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d6059e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d606560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d606820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d606ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d606f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d6073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d607830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d607ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d608110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d608580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d6089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d608e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d6092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d609740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d609bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d60a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d60a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d60a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d60ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d60b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d60b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d60bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d60bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d60c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d60c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d60cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d60d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d60d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d60d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d60de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d60e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d60e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d60eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d60f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d60f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d60f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d60fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d6101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d610630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d610aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d610f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d611380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d6117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d611c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d6120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d612540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d6129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d612e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d613290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d613700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d613b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d613fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d614450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d6148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d614d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d6151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d615610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d615a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d615ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d616360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d6167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d616c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d6170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d617520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d617990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d617e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d618270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d6186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d618b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d619430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d6198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d619d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d61a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d61abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d61b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d61ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d61c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d61c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d61c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d61ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d61d490 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.954s
user	0m0.236s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
