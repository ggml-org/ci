Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:44 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.591s
user	0m0.683s
sys	0m0.930s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Built target build_info
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  7%] Built target sha1
[  7%] Built target sha256
[  7%] Built target xxhash
[  8%] Linking CXX shared library libggml-base.dylib
[  8%] Built target ggml-base
[  9%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[  9%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/amx.cpp.o
[ 12%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 12%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/mmq.cpp.o
[ 13%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-cpu
[ 15%] Built target ggml-blas
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Linking CXX executable ../../bin/llama-run
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target llava
[ 32%] Built target test-c
[ 32%] Built target llama-run
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple-chat
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target llama-simple
[ 34%] Built target common
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Built target llava_static
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 38%] Built target llava_shared
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-chat-template
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-chat-template
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Built target test-log
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Built target test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Linking CXX executable ../bin/test-autorelease
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Built target test-llama-grammar
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../../bin/llama-batched-bench
[ 59%] Built target test-backend-ops
[ 59%] Built target test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../../bin/llama-batched
[ 63%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 63%] Built target test-model-load-cancel
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Built target test-barrier
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 66%] Built target test-quantize-perf
[ 66%] Built target llama-batched-bench
[ 66%] Built target test-json-schema-to-grammar
[ 66%] Built target test-rope
[ 66%] Built target llama-batched
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-gguf-split
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 74%] Linking CXX executable ../../bin/llama-imatrix
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Built target llama-gbnf-validator
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Built target llama-gritlm
[ 76%] Built target llama-gguf-split
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Built target llama-bench
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Built target llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-lookup
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 79%] Generating loading.html.hpp
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup-merge
[ 81%] Generating completion.js.hpp
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Built target llama-lookup-create
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating deps_daisyui.min.css.hpp
[ 84%] Built target llama-lookup-stats
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Generating deps_markdown-it.js.hpp
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Built target llama-parallel
[ 87%] Built target llama-passkey
[ 87%] Built target llama-retrieval
[ 87%] Built target llama-cli
[ 87%] Built target llama-perplexity
[ 87%] Generating deps_tailwindcss.js.hpp
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-cvector-generator
[ 92%] Linking CXX executable ../../bin/llama-export-lora
[ 92%] Built target llama-speculative
[ 93%] Generating deps_vue.esm-browser.js.hpp
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-tokenize
[ 95%] Built target llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-export-lora
[ 95%] Built target llama-speculative-simple
[ 96%] Generating index.html.hpp
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.590s
user	0m5.951s
sys	0m8.810s

main: quantize time =  5657.33 ms
main:    total time =  5657.33 ms

main: quantize time =  1941.84 ms
main:    total time =  1941.84 ms

main: quantize time =  1756.23 ms
main:    total time =  1756.23 ms

main: quantize time =  2172.59 ms
main:    total time =  2172.59 ms

main: quantize time =  2623.15 ms
main:    total time =  2623.15 ms

main: quantize time =  5506.90 ms
main:    total time =  5506.90 ms

main: quantize time =  6083.09 ms
main:    total time =  6083.09 ms

main: quantize time =  7113.28 ms
main:    total time =  7113.28 ms

main: quantize time =  6073.38 ms
main:    total time =  6073.38 ms

main: quantize time =  4764.74 ms
main:    total time =  4764.74 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.150 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.278 I main: llama backend init
0.00.000.285 I main: load the model and apply lora adapter, if any
0.00.098.565 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.109.335 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.109.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.109.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.109.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.109.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.109.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.109.352 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.109.354 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.109.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.109.356 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.109.356 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.109.357 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.109.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.109.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.109.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.109.365 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.109.365 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.116.304 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.118.534 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.125.499 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.125.507 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.125.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.125.509 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.125.509 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.125.511 I llama_model_loader: - type  f32:  194 tensors
0.00.125.511 I llama_model_loader: - type  f16:   98 tensors
0.00.165.290 I llm_load_vocab: special tokens cache size = 25
0.00.173.079 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.173.082 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.173.082 I llm_load_print_meta: arch             = gptneox
0.00.173.083 I llm_load_print_meta: vocab type       = BPE
0.00.173.083 I llm_load_print_meta: n_vocab          = 50304
0.00.173.083 I llm_load_print_meta: n_merges         = 50009
0.00.173.083 I llm_load_print_meta: vocab_only       = 0
0.00.173.084 I llm_load_print_meta: n_ctx_train      = 2048
0.00.173.084 I llm_load_print_meta: n_embd           = 2048
0.00.173.084 I llm_load_print_meta: n_layer          = 24
0.00.173.088 I llm_load_print_meta: n_head           = 16
0.00.173.088 I llm_load_print_meta: n_head_kv        = 16
0.00.173.089 I llm_load_print_meta: n_rot            = 32
0.00.173.089 I llm_load_print_meta: n_swa            = 0
0.00.173.089 I llm_load_print_meta: n_embd_head_k    = 128
0.00.173.089 I llm_load_print_meta: n_embd_head_v    = 128
0.00.173.090 I llm_load_print_meta: n_gqa            = 1
0.00.173.091 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.173.091 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.173.092 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.173.092 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.173.092 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.173.092 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.173.093 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.173.093 I llm_load_print_meta: n_ff             = 8192
0.00.173.093 I llm_load_print_meta: n_expert         = 0
0.00.173.094 I llm_load_print_meta: n_expert_used    = 0
0.00.173.094 I llm_load_print_meta: causal attn      = 1
0.00.173.094 I llm_load_print_meta: pooling type     = 0
0.00.173.094 I llm_load_print_meta: rope type        = 2
0.00.173.094 I llm_load_print_meta: rope scaling     = linear
0.00.173.095 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.173.095 I llm_load_print_meta: freq_scale_train = 1
0.00.173.095 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.173.095 I llm_load_print_meta: rope_finetuned   = unknown
0.00.173.096 I llm_load_print_meta: ssm_d_conv       = 0
0.00.173.096 I llm_load_print_meta: ssm_d_inner      = 0
0.00.173.096 I llm_load_print_meta: ssm_d_state      = 0
0.00.173.096 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.173.096 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.173.108 I llm_load_print_meta: model type       = 1.4B
0.00.173.109 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.173.109 I llm_load_print_meta: model params     = 1.41 B
0.00.173.110 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.173.110 I llm_load_print_meta: general.name     = 1.4B
0.00.173.110 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.173.110 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.173.110 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.173.111 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.173.111 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.173.111 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.173.112 I llm_load_print_meta: max token length = 1024
0.00.175.862 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.175.862 I llm_load_tensors: offloading output layer to GPU
0.00.175.862 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.175.880 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.175.881 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.176.891 I llama_new_context_with_model: n_seq_max     = 1
0.00.176.892 I llama_new_context_with_model: n_ctx         = 2048
0.00.176.892 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.176.892 I llama_new_context_with_model: n_batch       = 2048
0.00.176.893 I llama_new_context_with_model: n_ubatch      = 512
0.00.176.893 I llama_new_context_with_model: flash_attn    = 0
0.00.176.893 I llama_new_context_with_model: freq_base     = 10000.0
0.00.176.893 I llama_new_context_with_model: freq_scale    = 1
0.00.176.894 I ggml_metal_init: allocating
0.00.176.897 I ggml_metal_init: found device: Apple M4
0.00.176.899 I ggml_metal_init: picking default device: Apple M4
0.00.177.557 I ggml_metal_init: using embedded metal library
0.00.186.929 I ggml_metal_init: GPU name:   Apple M4
0.00.186.931 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.186.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.186.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.186.932 I ggml_metal_init: simdgroup reduction   = true
0.00.186.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.186.933 I ggml_metal_init: has bfloat            = true
0.00.186.933 I ggml_metal_init: use bfloat            = true
0.00.186.933 I ggml_metal_init: hasUnifiedMemory      = true
0.00.186.934 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.229.886 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.229.893 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.229.916 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.230.909 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.230.911 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.230.911 I llama_new_context_with_model: graph nodes  = 967
0.00.230.911 I llama_new_context_with_model: graph splits = 2
0.00.230.933 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.313.585 I main: llama threadpool init, n_threads = 4
0.00.313.621 I 
0.00.313.652 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.313.653 I 
0.00.313.727 I sampler seed: 1234
0.00.313.731 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.313.765 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.313.767 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.313.767 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.165.404 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.02.165.404 I llama_perf_context_print:        load time =     215.01 ms
0.02.165.405 I llama_perf_context_print: prompt eval time =      37.35 ms /     7 tokens (    5.34 ms per token,   187.41 tokens per second)
0.02.165.406 I llama_perf_context_print:        eval time =    1811.28 ms /    63 runs   (   28.75 ms per token,    34.78 tokens per second)
0.02.165.406 I llama_perf_context_print:       total time =    1851.82 ms /    70 tokens
0.02.165.574 I ggml_metal_free: deallocating

real	0m2.497s
user	0m0.154s
sys	0m0.112s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.135 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.394 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.399 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.404 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.404 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.405 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.405 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.406 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.406 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.406 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.407 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.407 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.409 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.409 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.409 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.680 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.681 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.681 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.682 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.682 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.683 I llama_model_loader: - type  f32:  194 tensors
0.00.028.683 I llama_model_loader: - type q8_0:   98 tensors
0.00.050.615 I llm_load_vocab: special tokens cache size = 25
0.00.056.563 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.568 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.568 I llm_load_print_meta: arch             = gptneox
0.00.056.569 I llm_load_print_meta: vocab type       = BPE
0.00.056.575 I llm_load_print_meta: n_vocab          = 50304
0.00.056.575 I llm_load_print_meta: n_merges         = 50009
0.00.056.575 I llm_load_print_meta: vocab_only       = 0
0.00.056.575 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.575 I llm_load_print_meta: n_embd           = 2048
0.00.056.576 I llm_load_print_meta: n_layer          = 24
0.00.056.581 I llm_load_print_meta: n_head           = 16
0.00.056.582 I llm_load_print_meta: n_head_kv        = 16
0.00.056.582 I llm_load_print_meta: n_rot            = 32
0.00.056.582 I llm_load_print_meta: n_swa            = 0
0.00.056.582 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.582 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.583 I llm_load_print_meta: n_gqa            = 1
0.00.056.584 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.585 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.586 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.586 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.586 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.586 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.587 I llm_load_print_meta: n_ff             = 8192
0.00.056.587 I llm_load_print_meta: n_expert         = 0
0.00.056.587 I llm_load_print_meta: n_expert_used    = 0
0.00.056.588 I llm_load_print_meta: causal attn      = 1
0.00.056.588 I llm_load_print_meta: pooling type     = 0
0.00.056.589 I llm_load_print_meta: rope type        = 2
0.00.056.589 I llm_load_print_meta: rope scaling     = linear
0.00.056.589 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.590 I llm_load_print_meta: freq_scale_train = 1
0.00.056.590 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.590 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.590 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.590 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.590 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.590 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.591 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.599 I llm_load_print_meta: model type       = 1.4B
0.00.056.600 I llm_load_print_meta: model ftype      = Q8_0
0.00.056.600 I llm_load_print_meta: model params     = 1.41 B
0.00.056.601 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.056.601 I llm_load_print_meta: general.name     = 1.4B
0.00.056.601 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.601 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.602 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.602 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.602 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.604 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.604 I llm_load_print_meta: max token length = 1024
0.00.058.684 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.684 I llm_load_tensors: offloading output layer to GPU
0.00.058.684 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.690 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.690 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.059.662 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.663 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.663 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.663 I llama_new_context_with_model: n_batch       = 2048
0.00.059.663 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.663 I llama_new_context_with_model: flash_attn    = 0
0.00.059.664 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.664 I llama_new_context_with_model: freq_scale    = 1
0.00.059.665 I ggml_metal_init: allocating
0.00.059.673 I ggml_metal_init: found device: Apple M4
0.00.059.675 I ggml_metal_init: picking default device: Apple M4
0.00.060.387 I ggml_metal_init: using embedded metal library
0.00.062.586 I ggml_metal_init: GPU name:   Apple M4
0.00.062.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.589 I ggml_metal_init: simdgroup reduction   = true
0.00.062.589 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.589 I ggml_metal_init: has bfloat            = true
0.00.062.589 I ggml_metal_init: use bfloat            = true
0.00.062.590 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.043 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.055 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.325 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.327 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.327 I llama_new_context_with_model: graph nodes  = 967
0.00.096.328 I llama_new_context_with_model: graph splits = 2
0.00.096.344 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.419.904 I main: llama threadpool init, n_threads = 4
0.01.419.949 I 
0.01.419.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.419.973 I 
0.01.420.160 I sampler seed: 1234
0.01.420.164 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.420.186 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.420.186 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.420.186 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.510.070 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.02.510.070 I llama_perf_context_print:        load time =    1409.76 ms
0.02.510.072 I llama_perf_context_print: prompt eval time =      37.13 ms /     7 tokens (    5.30 ms per token,   188.54 tokens per second)
0.02.510.073 I llama_perf_context_print:        eval time =    1049.85 ms /    63 runs   (   16.66 ms per token,    60.01 tokens per second)
0.02.510.073 I llama_perf_context_print:       total time =    1090.17 ms /    70 tokens
0.02.510.264 I ggml_metal_free: deallocating

real	0m2.531s
user	0m0.111s
sys	0m0.219s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.017.552 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.738 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.748 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.748 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.750 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.405 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.218 I llama_model_loader: - type  f32:  194 tensors
0.00.044.218 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.218 I llama_model_loader: - type q6_K:    1 tensors
0.00.071.532 I llm_load_vocab: special tokens cache size = 25
0.00.080.769 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.080.773 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.080.773 I llm_load_print_meta: arch             = gptneox
0.00.080.774 I llm_load_print_meta: vocab type       = BPE
0.00.080.774 I llm_load_print_meta: n_vocab          = 50304
0.00.080.775 I llm_load_print_meta: n_merges         = 50009
0.00.080.775 I llm_load_print_meta: vocab_only       = 0
0.00.080.782 I llm_load_print_meta: n_ctx_train      = 2048
0.00.080.783 I llm_load_print_meta: n_embd           = 2048
0.00.080.783 I llm_load_print_meta: n_layer          = 24
0.00.080.788 I llm_load_print_meta: n_head           = 16
0.00.080.789 I llm_load_print_meta: n_head_kv        = 16
0.00.080.789 I llm_load_print_meta: n_rot            = 32
0.00.080.789 I llm_load_print_meta: n_swa            = 0
0.00.080.789 I llm_load_print_meta: n_embd_head_k    = 128
0.00.080.790 I llm_load_print_meta: n_embd_head_v    = 128
0.00.080.791 I llm_load_print_meta: n_gqa            = 1
0.00.080.791 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.080.792 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.080.793 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.080.793 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.080.796 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.080.796 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.080.796 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.080.797 I llm_load_print_meta: n_ff             = 8192
0.00.080.797 I llm_load_print_meta: n_expert         = 0
0.00.080.797 I llm_load_print_meta: n_expert_used    = 0
0.00.080.800 I llm_load_print_meta: causal attn      = 1
0.00.080.800 I llm_load_print_meta: pooling type     = 0
0.00.080.800 I llm_load_print_meta: rope type        = 2
0.00.080.800 I llm_load_print_meta: rope scaling     = linear
0.00.080.801 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.080.801 I llm_load_print_meta: freq_scale_train = 1
0.00.080.806 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.080.806 I llm_load_print_meta: rope_finetuned   = unknown
0.00.080.806 I llm_load_print_meta: ssm_d_conv       = 0
0.00.080.807 I llm_load_print_meta: ssm_d_inner      = 0
0.00.080.807 I llm_load_print_meta: ssm_d_state      = 0
0.00.080.807 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.080.807 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.080.821 I llm_load_print_meta: model type       = 1.4B
0.00.080.821 I llm_load_print_meta: model ftype      = Q4_0
0.00.080.822 I llm_load_print_meta: model params     = 1.41 B
0.00.080.823 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.080.823 I llm_load_print_meta: general.name     = 1.4B
0.00.080.823 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.080.824 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.080.824 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.080.824 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.080.824 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.080.825 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.080.825 I llm_load_print_meta: max token length = 1024
0.00.083.851 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.852 I llm_load_tensors: offloading output layer to GPU
0.00.083.852 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.863 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.083.865 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.085.431 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.433 I llama_new_context_with_model: n_ctx         = 2048
0.00.085.433 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.085.433 I llama_new_context_with_model: n_batch       = 2048
0.00.085.434 I llama_new_context_with_model: n_ubatch      = 512
0.00.085.434 I llama_new_context_with_model: flash_attn    = 0
0.00.085.435 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.435 I llama_new_context_with_model: freq_scale    = 1
0.00.085.436 I ggml_metal_init: allocating
0.00.085.448 I ggml_metal_init: found device: Apple M4
0.00.085.451 I ggml_metal_init: picking default device: Apple M4
0.00.086.348 I ggml_metal_init: using embedded metal library
0.00.089.586 I ggml_metal_init: GPU name:   Apple M4
0.00.089.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.589 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.589 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.590 I ggml_metal_init: simdgroup reduction   = true
0.00.089.590 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.590 I ggml_metal_init: has bfloat            = true
0.00.089.592 I ggml_metal_init: use bfloat            = true
0.00.089.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.125.207 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.219 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.238 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.338 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.339 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.340 I llama_new_context_with_model: graph nodes  = 967
0.00.126.340 I llama_new_context_with_model: graph splits = 2
0.00.126.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.328 I main: llama threadpool init, n_threads = 4
0.00.809.366 I 
0.00.809.394 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.809.396 I 
0.00.809.655 I sampler seed: 1234
0.00.809.659 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.674 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.676 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.676 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.493.218 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.01.493.218 I llama_perf_context_print:        load time =     791.77 ms
0.01.493.219 I llama_perf_context_print: prompt eval time =      37.89 ms /     7 tokens (    5.41 ms per token,   184.74 tokens per second)
0.01.493.220 I llama_perf_context_print:        eval time =     642.56 ms /    63 runs   (   10.20 ms per token,    98.05 tokens per second)
0.01.493.220 I llama_perf_context_print:       total time =     683.89 ms /    70 tokens
0.01.493.387 I ggml_metal_free: deallocating

real	0m1.515s
user	0m0.127s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.882 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.521 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.526 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.527 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.527 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.528 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.530 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.532 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.492 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.552 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.464 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.465 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.466 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.466 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.466 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.467 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.467 I llama_model_loader: - type  f32:  194 tensors
0.00.025.467 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.468 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.450 I llm_load_vocab: special tokens cache size = 25
0.00.051.325 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.328 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.328 I llm_load_print_meta: arch             = gptneox
0.00.051.328 I llm_load_print_meta: vocab type       = BPE
0.00.051.329 I llm_load_print_meta: n_vocab          = 50304
0.00.051.329 I llm_load_print_meta: n_merges         = 50009
0.00.051.329 I llm_load_print_meta: vocab_only       = 0
0.00.051.329 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.329 I llm_load_print_meta: n_embd           = 2048
0.00.051.330 I llm_load_print_meta: n_layer          = 24
0.00.051.332 I llm_load_print_meta: n_head           = 16
0.00.051.333 I llm_load_print_meta: n_head_kv        = 16
0.00.051.333 I llm_load_print_meta: n_rot            = 32
0.00.051.334 I llm_load_print_meta: n_swa            = 0
0.00.051.334 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.334 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.335 I llm_load_print_meta: n_gqa            = 1
0.00.051.336 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.336 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.337 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.337 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.337 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.338 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.338 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.338 I llm_load_print_meta: n_ff             = 8192
0.00.051.339 I llm_load_print_meta: n_expert         = 0
0.00.051.339 I llm_load_print_meta: n_expert_used    = 0
0.00.051.339 I llm_load_print_meta: causal attn      = 1
0.00.051.339 I llm_load_print_meta: pooling type     = 0
0.00.051.339 I llm_load_print_meta: rope type        = 2
0.00.051.339 I llm_load_print_meta: rope scaling     = linear
0.00.051.340 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.340 I llm_load_print_meta: freq_scale_train = 1
0.00.051.340 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.341 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.341 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.341 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.341 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.341 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.341 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.353 I llm_load_print_meta: model type       = 1.4B
0.00.051.353 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.354 I llm_load_print_meta: model params     = 1.41 B
0.00.051.354 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.355 I llm_load_print_meta: general.name     = 1.4B
0.00.051.355 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.355 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.355 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.355 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.355 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.356 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.356 I llm_load_print_meta: max token length = 1024
0.00.053.283 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.284 I llm_load_tensors: offloading output layer to GPU
0.00.053.284 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.294 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.295 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.178 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.179 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.179 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.179 I llama_new_context_with_model: n_batch       = 2048
0.00.054.179 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.180 I llama_new_context_with_model: flash_attn    = 0
0.00.054.180 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.180 I llama_new_context_with_model: freq_scale    = 1
0.00.054.181 I ggml_metal_init: allocating
0.00.054.184 I ggml_metal_init: found device: Apple M4
0.00.054.186 I ggml_metal_init: picking default device: Apple M4
0.00.054.742 I ggml_metal_init: using embedded metal library
0.00.056.656 I ggml_metal_init: GPU name:   Apple M4
0.00.056.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.658 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.659 I ggml_metal_init: simdgroup reduction   = true
0.00.056.659 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.659 I ggml_metal_init: has bfloat            = true
0.00.056.659 I ggml_metal_init: use bfloat            = true
0.00.056.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.375 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.380 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.399 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.430 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.431 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.432 I llama_new_context_with_model: graph nodes  = 967
0.00.084.432 I llama_new_context_with_model: graph splits = 2
0.00.084.440 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.415 I main: llama threadpool init, n_threads = 4
0.00.689.452 I 
0.00.689.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.689.482 I 
0.00.689.714 I sampler seed: 1234
0.00.689.720 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.736 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.737 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.737 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.411.388 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.411.388 I llama_perf_context_print:        load time =     679.53 ms
0.01.411.389 I llama_perf_context_print: prompt eval time =      38.89 ms /     7 tokens (    5.56 ms per token,   179.99 tokens per second)
0.01.411.390 I llama_perf_context_print:        eval time =     680.26 ms /    63 runs   (   10.80 ms per token,    92.61 tokens per second)
0.01.411.390 I llama_perf_context_print:       total time =     721.97 ms /    70 tokens
0.01.411.593 I ggml_metal_free: deallocating

real	0m1.432s
user	0m0.108s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.018 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.544 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.544 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.544 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.545 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.545 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.545 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.716 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.717 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.718 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.719 I llama_model_loader: - type  f32:  194 tensors
0.00.024.719 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.719 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.813 I llm_load_vocab: special tokens cache size = 25
0.00.050.629 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.632 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.632 I llm_load_print_meta: arch             = gptneox
0.00.050.633 I llm_load_print_meta: vocab type       = BPE
0.00.050.633 I llm_load_print_meta: n_vocab          = 50304
0.00.050.633 I llm_load_print_meta: n_merges         = 50009
0.00.050.633 I llm_load_print_meta: vocab_only       = 0
0.00.050.633 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.634 I llm_load_print_meta: n_embd           = 2048
0.00.050.634 I llm_load_print_meta: n_layer          = 24
0.00.050.636 I llm_load_print_meta: n_head           = 16
0.00.050.637 I llm_load_print_meta: n_head_kv        = 16
0.00.050.637 I llm_load_print_meta: n_rot            = 32
0.00.050.637 I llm_load_print_meta: n_swa            = 0
0.00.050.637 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.638 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.638 I llm_load_print_meta: n_gqa            = 1
0.00.050.639 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.640 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.640 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.641 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.641 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.643 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.643 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.644 I llm_load_print_meta: n_ff             = 8192
0.00.050.644 I llm_load_print_meta: n_expert         = 0
0.00.050.644 I llm_load_print_meta: n_expert_used    = 0
0.00.050.646 I llm_load_print_meta: causal attn      = 1
0.00.050.648 I llm_load_print_meta: pooling type     = 0
0.00.050.648 I llm_load_print_meta: rope type        = 2
0.00.050.648 I llm_load_print_meta: rope scaling     = linear
0.00.050.648 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.649 I llm_load_print_meta: freq_scale_train = 1
0.00.050.649 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.649 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.649 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.649 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.649 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.649 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.650 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.661 I llm_load_print_meta: model type       = 1.4B
0.00.050.662 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.662 I llm_load_print_meta: model params     = 1.41 B
0.00.050.663 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.663 I llm_load_print_meta: general.name     = 1.4B
0.00.050.663 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.663 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.663 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.663 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.664 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.664 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.664 I llm_load_print_meta: max token length = 1024
0.00.052.645 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.645 I llm_load_tensors: offloading output layer to GPU
0.00.052.645 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.655 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.656 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.585 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.586 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.586 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.586 I llama_new_context_with_model: n_batch       = 2048
0.00.053.586 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.587 I llama_new_context_with_model: flash_attn    = 0
0.00.053.587 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.587 I llama_new_context_with_model: freq_scale    = 1
0.00.053.588 I ggml_metal_init: allocating
0.00.053.591 I ggml_metal_init: found device: Apple M4
0.00.053.593 I ggml_metal_init: picking default device: Apple M4
0.00.054.147 I ggml_metal_init: using embedded metal library
0.00.056.049 I ggml_metal_init: GPU name:   Apple M4
0.00.056.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.051 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.051 I ggml_metal_init: simdgroup reduction   = true
0.00.056.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.051 I ggml_metal_init: has bfloat            = true
0.00.056.051 I ggml_metal_init: use bfloat            = true
0.00.056.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.377 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.386 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.405 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.309 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.311 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.311 I llama_new_context_with_model: graph nodes  = 967
0.00.084.312 I llama_new_context_with_model: graph splits = 2
0.00.084.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.209 I main: llama threadpool init, n_threads = 4
0.00.734.244 I 
0.00.734.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.734.272 I 
0.00.734.497 I sampler seed: 1234
0.00.734.502 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.549 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.550 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.550 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.522.369 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.522.369 I llama_perf_context_print:        load time =     725.19 ms
0.01.522.370 I llama_perf_context_print: prompt eval time =      36.56 ms /     7 tokens (    5.22 ms per token,   191.47 tokens per second)
0.01.522.371 I llama_perf_context_print:        eval time =     748.31 ms /    63 runs   (   11.88 ms per token,    84.19 tokens per second)
0.01.522.371 I llama_perf_context_print:       total time =     788.16 ms /    70 tokens
0.01.522.547 I ggml_metal_free: deallocating

real	0m1.540s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.680 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.659 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.663 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.671 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.672 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.672 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.673 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.673 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.676 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.676 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.677 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.678 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.678 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.679 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.683 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.685 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.850 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.867 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.868 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.868 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.868 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.869 I llama_model_loader: - type  f32:  194 tensors
0.00.026.869 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.869 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.728 I llm_load_vocab: special tokens cache size = 25
0.00.053.613 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.616 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.616 I llm_load_print_meta: arch             = gptneox
0.00.053.617 I llm_load_print_meta: vocab type       = BPE
0.00.053.617 I llm_load_print_meta: n_vocab          = 50304
0.00.053.617 I llm_load_print_meta: n_merges         = 50009
0.00.053.617 I llm_load_print_meta: vocab_only       = 0
0.00.053.618 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.618 I llm_load_print_meta: n_embd           = 2048
0.00.053.618 I llm_load_print_meta: n_layer          = 24
0.00.053.620 I llm_load_print_meta: n_head           = 16
0.00.053.621 I llm_load_print_meta: n_head_kv        = 16
0.00.053.621 I llm_load_print_meta: n_rot            = 32
0.00.053.622 I llm_load_print_meta: n_swa            = 0
0.00.053.622 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.622 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.623 I llm_load_print_meta: n_gqa            = 1
0.00.053.624 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.626 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.627 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.627 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.628 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.628 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.628 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.636 I llm_load_print_meta: n_ff             = 8192
0.00.053.638 I llm_load_print_meta: n_expert         = 0
0.00.053.638 I llm_load_print_meta: n_expert_used    = 0
0.00.053.639 I llm_load_print_meta: causal attn      = 1
0.00.053.639 I llm_load_print_meta: pooling type     = 0
0.00.053.639 I llm_load_print_meta: rope type        = 2
0.00.053.639 I llm_load_print_meta: rope scaling     = linear
0.00.053.640 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.640 I llm_load_print_meta: freq_scale_train = 1
0.00.053.640 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.640 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.641 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.641 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.641 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.641 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.641 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.653 I llm_load_print_meta: model type       = 1.4B
0.00.053.654 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.654 I llm_load_print_meta: model params     = 1.41 B
0.00.053.654 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.654 I llm_load_print_meta: general.name     = 1.4B
0.00.053.656 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.656 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.656 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.656 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.657 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.657 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.657 I llm_load_print_meta: max token length = 1024
0.00.055.628 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.628 I llm_load_tensors: offloading output layer to GPU
0.00.055.629 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.638 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.639 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.552 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.553 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.553 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.553 I llama_new_context_with_model: n_batch       = 2048
0.00.056.554 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.554 I llama_new_context_with_model: flash_attn    = 0
0.00.056.554 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.554 I llama_new_context_with_model: freq_scale    = 1
0.00.056.555 I ggml_metal_init: allocating
0.00.056.561 I ggml_metal_init: found device: Apple M4
0.00.056.563 I ggml_metal_init: picking default device: Apple M4
0.00.057.124 I ggml_metal_init: using embedded metal library
0.00.059.040 I ggml_metal_init: GPU name:   Apple M4
0.00.059.041 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.041 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.042 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.042 I ggml_metal_init: simdgroup reduction   = true
0.00.059.042 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.042 I ggml_metal_init: has bfloat            = true
0.00.059.043 I ggml_metal_init: use bfloat            = true
0.00.059.043 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.044 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.532 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.537 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.554 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.470 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.471 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.471 I llama_new_context_with_model: graph nodes  = 967
0.00.086.472 I llama_new_context_with_model: graph splits = 2
0.00.086.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.840.193 I main: llama threadpool init, n_threads = 4
0.00.840.230 I 
0.00.840.277 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.840.278 I 
0.00.840.499 I sampler seed: 1234
0.00.840.504 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.840.551 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.840.553 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.840.553 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.680.370 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.680.371 I llama_perf_context_print:        load time =     829.51 ms
0.01.680.371 I llama_perf_context_print: prompt eval time =      36.61 ms /     7 tokens (    5.23 ms per token,   191.20 tokens per second)
0.01.680.372 I llama_perf_context_print:        eval time =     800.04 ms /    63 runs   (   12.70 ms per token,    78.75 tokens per second)
0.01.680.373 I llama_perf_context_print:       total time =     840.18 ms /    70 tokens
0.01.680.552 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.108s
sys	0m0.182s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.736 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.085 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.089 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.090 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.091 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.091 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.092 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.092 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.092 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.094 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.094 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.095 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.096 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.097 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.097 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.098 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.080 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.080 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.080 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.081 I llama_model_loader: - type  f32:  194 tensors
0.00.026.081 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.081 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.082 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.080 I llm_load_vocab: special tokens cache size = 25
0.00.051.725 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.727 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.728 I llm_load_print_meta: arch             = gptneox
0.00.051.728 I llm_load_print_meta: vocab type       = BPE
0.00.051.728 I llm_load_print_meta: n_vocab          = 50304
0.00.051.729 I llm_load_print_meta: n_merges         = 50009
0.00.051.729 I llm_load_print_meta: vocab_only       = 0
0.00.051.729 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.729 I llm_load_print_meta: n_embd           = 2048
0.00.051.729 I llm_load_print_meta: n_layer          = 24
0.00.051.732 I llm_load_print_meta: n_head           = 16
0.00.051.733 I llm_load_print_meta: n_head_kv        = 16
0.00.051.733 I llm_load_print_meta: n_rot            = 32
0.00.051.733 I llm_load_print_meta: n_swa            = 0
0.00.051.734 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.734 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.735 I llm_load_print_meta: n_gqa            = 1
0.00.051.736 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.736 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.737 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.737 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.737 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.737 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.738 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.738 I llm_load_print_meta: n_ff             = 8192
0.00.051.738 I llm_load_print_meta: n_expert         = 0
0.00.051.739 I llm_load_print_meta: n_expert_used    = 0
0.00.051.739 I llm_load_print_meta: causal attn      = 1
0.00.051.739 I llm_load_print_meta: pooling type     = 0
0.00.051.739 I llm_load_print_meta: rope type        = 2
0.00.051.739 I llm_load_print_meta: rope scaling     = linear
0.00.051.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.740 I llm_load_print_meta: freq_scale_train = 1
0.00.051.740 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.740 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.741 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.741 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.741 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.741 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.741 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.753 I llm_load_print_meta: model type       = 1.4B
0.00.051.753 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.753 I llm_load_print_meta: model params     = 1.41 B
0.00.051.754 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.754 I llm_load_print_meta: general.name     = 1.4B
0.00.051.754 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.755 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: max token length = 1024
0.00.053.636 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.636 I llm_load_tensors: offloading output layer to GPU
0.00.053.636 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.646 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.647 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.591 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.591 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.592 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.592 I llama_new_context_with_model: n_batch       = 2048
0.00.054.592 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.592 I llama_new_context_with_model: flash_attn    = 0
0.00.054.593 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.593 I llama_new_context_with_model: freq_scale    = 1
0.00.054.593 I ggml_metal_init: allocating
0.00.054.596 I ggml_metal_init: found device: Apple M4
0.00.054.598 I ggml_metal_init: picking default device: Apple M4
0.00.055.144 I ggml_metal_init: using embedded metal library
0.00.057.041 I ggml_metal_init: GPU name:   Apple M4
0.00.057.043 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.043 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.043 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.044 I ggml_metal_init: simdgroup reduction   = true
0.00.057.044 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.044 I ggml_metal_init: has bfloat            = true
0.00.057.044 I ggml_metal_init: use bfloat            = true
0.00.057.045 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.045 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.344 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.350 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.367 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.388 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.389 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.389 I llama_new_context_with_model: graph nodes  = 967
0.00.085.390 I llama_new_context_with_model: graph splits = 2
0.00.085.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.318 I main: llama threadpool init, n_threads = 4
0.00.510.356 I 
0.00.510.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.510.382 I 
0.00.510.519 I sampler seed: 1234
0.00.510.523 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.510.571 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.510.575 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.510.575 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.192.229 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.192.230 I llama_perf_context_print:        load time =     500.58 ms
0.01.192.231 I llama_perf_context_print: prompt eval time =      35.68 ms /     7 tokens (    5.10 ms per token,   196.17 tokens per second)
0.01.192.231 I llama_perf_context_print:        eval time =     642.90 ms /    63 runs   (   10.20 ms per token,    97.99 tokens per second)
0.01.192.232 I llama_perf_context_print:       total time =     681.91 ms /    70 tokens
0.01.192.405 I ggml_metal_free: deallocating

real	0m1.213s
user	0m0.109s
sys	0m0.111s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.945 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.514 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.525 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.527 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.742 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.744 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.744 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.745 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.745 I llama_model_loader: - type  f32:  194 tensors
0.00.024.746 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.746 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.746 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.746 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.739 I llm_load_vocab: special tokens cache size = 25
0.00.051.581 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.583 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.584 I llm_load_print_meta: arch             = gptneox
0.00.051.584 I llm_load_print_meta: vocab type       = BPE
0.00.051.584 I llm_load_print_meta: n_vocab          = 50304
0.00.051.584 I llm_load_print_meta: n_merges         = 50009
0.00.051.585 I llm_load_print_meta: vocab_only       = 0
0.00.051.585 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.585 I llm_load_print_meta: n_embd           = 2048
0.00.051.585 I llm_load_print_meta: n_layer          = 24
0.00.051.587 I llm_load_print_meta: n_head           = 16
0.00.051.588 I llm_load_print_meta: n_head_kv        = 16
0.00.051.588 I llm_load_print_meta: n_rot            = 32
0.00.051.588 I llm_load_print_meta: n_swa            = 0
0.00.051.589 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.589 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.590 I llm_load_print_meta: n_gqa            = 1
0.00.051.590 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.591 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.592 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.592 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.592 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.592 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.592 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.593 I llm_load_print_meta: n_ff             = 8192
0.00.051.593 I llm_load_print_meta: n_expert         = 0
0.00.051.594 I llm_load_print_meta: n_expert_used    = 0
0.00.051.594 I llm_load_print_meta: causal attn      = 1
0.00.051.594 I llm_load_print_meta: pooling type     = 0
0.00.051.595 I llm_load_print_meta: rope type        = 2
0.00.051.595 I llm_load_print_meta: rope scaling     = linear
0.00.051.596 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.597 I llm_load_print_meta: freq_scale_train = 1
0.00.051.597 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.597 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.597 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.597 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.597 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.597 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.598 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.609 I llm_load_print_meta: model type       = 1.4B
0.00.051.610 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.610 I llm_load_print_meta: model params     = 1.41 B
0.00.051.611 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.611 I llm_load_print_meta: general.name     = 1.4B
0.00.051.611 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.611 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.611 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.612 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.612 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.612 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.612 I llm_load_print_meta: max token length = 1024
0.00.053.595 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.595 I llm_load_tensors: offloading output layer to GPU
0.00.053.595 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.605 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.606 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.501 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.502 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.502 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.502 I llama_new_context_with_model: n_batch       = 2048
0.00.054.502 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.502 I llama_new_context_with_model: flash_attn    = 0
0.00.054.503 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.503 I llama_new_context_with_model: freq_scale    = 1
0.00.054.503 I ggml_metal_init: allocating
0.00.054.507 I ggml_metal_init: found device: Apple M4
0.00.054.509 I ggml_metal_init: picking default device: Apple M4
0.00.055.066 I ggml_metal_init: using embedded metal library
0.00.057.019 I ggml_metal_init: GPU name:   Apple M4
0.00.057.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.021 I ggml_metal_init: simdgroup reduction   = true
0.00.057.021 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.022 I ggml_metal_init: has bfloat            = true
0.00.057.022 I ggml_metal_init: use bfloat            = true
0.00.057.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.155 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.166 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.188 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.177 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.178 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.178 I llama_new_context_with_model: graph nodes  = 967
0.00.086.179 I llama_new_context_with_model: graph splits = 2
0.00.086.187 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.882 I main: llama threadpool init, n_threads = 4
0.00.652.922 I 
0.00.652.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.652.968 I 
0.00.653.197 I sampler seed: 1234
0.00.653.203 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.653.246 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.653.250 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.653.250 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.397.611 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.01.397.612 I llama_perf_context_print:        load time =     643.93 ms
0.01.397.612 I llama_perf_context_print: prompt eval time =      38.86 ms /     7 tokens (    5.55 ms per token,   180.12 tokens per second)
0.01.397.613 I llama_perf_context_print:        eval time =     702.59 ms /    63 runs   (   11.15 ms per token,    89.67 tokens per second)
0.01.397.613 I llama_perf_context_print:       total time =     744.73 ms /    70 tokens
0.01.397.790 I ggml_metal_free: deallocating

real	0m1.413s
user	0m0.109s
sys	0m0.140s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.324 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.460 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.461 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.462 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.462 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.462 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.463 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.463 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.463 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.464 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.466 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.606 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.792 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.793 I llama_model_loader: - type  f32:  194 tensors
0.00.024.793 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.793 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.794 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.831 I llm_load_vocab: special tokens cache size = 25
0.00.050.660 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.663 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.663 I llm_load_print_meta: arch             = gptneox
0.00.050.664 I llm_load_print_meta: vocab type       = BPE
0.00.050.664 I llm_load_print_meta: n_vocab          = 50304
0.00.050.664 I llm_load_print_meta: n_merges         = 50009
0.00.050.664 I llm_load_print_meta: vocab_only       = 0
0.00.050.664 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.665 I llm_load_print_meta: n_embd           = 2048
0.00.050.665 I llm_load_print_meta: n_layer          = 24
0.00.050.667 I llm_load_print_meta: n_head           = 16
0.00.050.668 I llm_load_print_meta: n_head_kv        = 16
0.00.050.668 I llm_load_print_meta: n_rot            = 32
0.00.050.668 I llm_load_print_meta: n_swa            = 0
0.00.050.668 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.669 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.669 I llm_load_print_meta: n_gqa            = 1
0.00.050.670 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.671 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.671 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.672 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.672 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.672 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.672 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.673 I llm_load_print_meta: n_ff             = 8192
0.00.050.673 I llm_load_print_meta: n_expert         = 0
0.00.050.673 I llm_load_print_meta: n_expert_used    = 0
0.00.050.673 I llm_load_print_meta: causal attn      = 1
0.00.050.674 I llm_load_print_meta: pooling type     = 0
0.00.050.674 I llm_load_print_meta: rope type        = 2
0.00.050.674 I llm_load_print_meta: rope scaling     = linear
0.00.050.677 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.677 I llm_load_print_meta: freq_scale_train = 1
0.00.050.677 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.677 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.678 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.678 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.678 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.678 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.678 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.686 I llm_load_print_meta: model type       = 1.4B
0.00.050.687 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.688 I llm_load_print_meta: model params     = 1.41 B
0.00.050.688 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.688 I llm_load_print_meta: general.name     = 1.4B
0.00.050.688 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.690 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.690 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.690 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.690 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.691 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.691 I llm_load_print_meta: max token length = 1024
0.00.052.461 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.461 I llm_load_tensors: offloading output layer to GPU
0.00.052.462 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.467 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.467 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.379 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.380 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.380 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.380 I llama_new_context_with_model: n_batch       = 2048
0.00.053.380 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.381 I llama_new_context_with_model: flash_attn    = 0
0.00.053.381 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.381 I llama_new_context_with_model: freq_scale    = 1
0.00.053.382 I ggml_metal_init: allocating
0.00.053.387 I ggml_metal_init: found device: Apple M4
0.00.053.389 I ggml_metal_init: picking default device: Apple M4
0.00.053.967 I ggml_metal_init: using embedded metal library
0.00.055.929 I ggml_metal_init: GPU name:   Apple M4
0.00.055.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.931 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.935 I ggml_metal_init: simdgroup reduction   = true
0.00.055.935 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.935 I ggml_metal_init: has bfloat            = true
0.00.055.935 I ggml_metal_init: use bfloat            = true
0.00.055.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.938 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.667 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.677 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.697 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.632 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.633 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.634 I llama_new_context_with_model: graph nodes  = 967
0.00.083.634 I llama_new_context_with_model: graph splits = 2
0.00.083.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.955 I main: llama threadpool init, n_threads = 4
0.00.610.995 I 
0.00.611.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.611.024 I 
0.00.611.244 I sampler seed: 1234
0.00.611.249 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.611.283 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.611.285 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.611.285 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.365.815 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50750.54 tokens per second)
0.01.365.816 I llama_perf_context_print:        load time =     601.63 ms
0.01.365.816 I llama_perf_context_print: prompt eval time =      36.42 ms /     7 tokens (    5.20 ms per token,   192.19 tokens per second)
0.01.365.817 I llama_perf_context_print:        eval time =     715.34 ms /    63 runs   (   11.35 ms per token,    88.07 tokens per second)
0.01.365.818 I llama_perf_context_print:       total time =     754.86 ms /    70 tokens
0.01.365.995 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.108s
sys	0m0.137s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.845 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.943 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.947 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.954 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.954 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.108 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.319 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.320 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.320 I llama_model_loader: - type  f32:  194 tensors
0.00.025.321 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.321 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.222 I llm_load_vocab: special tokens cache size = 25
0.00.051.938 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.941 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.941 I llm_load_print_meta: arch             = gptneox
0.00.051.942 I llm_load_print_meta: vocab type       = BPE
0.00.051.942 I llm_load_print_meta: n_vocab          = 50304
0.00.051.942 I llm_load_print_meta: n_merges         = 50009
0.00.051.943 I llm_load_print_meta: vocab_only       = 0
0.00.051.943 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.943 I llm_load_print_meta: n_embd           = 2048
0.00.051.943 I llm_load_print_meta: n_layer          = 24
0.00.051.945 I llm_load_print_meta: n_head           = 16
0.00.051.946 I llm_load_print_meta: n_head_kv        = 16
0.00.051.946 I llm_load_print_meta: n_rot            = 32
0.00.051.946 I llm_load_print_meta: n_swa            = 0
0.00.051.947 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.947 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.948 I llm_load_print_meta: n_gqa            = 1
0.00.051.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.949 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.950 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.950 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.950 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.951 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.951 I llm_load_print_meta: n_ff             = 8192
0.00.051.951 I llm_load_print_meta: n_expert         = 0
0.00.051.953 I llm_load_print_meta: n_expert_used    = 0
0.00.051.956 I llm_load_print_meta: causal attn      = 1
0.00.051.958 I llm_load_print_meta: pooling type     = 0
0.00.051.958 I llm_load_print_meta: rope type        = 2
0.00.051.958 I llm_load_print_meta: rope scaling     = linear
0.00.051.958 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.959 I llm_load_print_meta: freq_scale_train = 1
0.00.051.959 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.959 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.959 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.959 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.960 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.960 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.963 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.975 I llm_load_print_meta: model type       = 1.4B
0.00.051.975 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.975 I llm_load_print_meta: model params     = 1.41 B
0.00.051.976 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.977 I llm_load_print_meta: general.name     = 1.4B
0.00.051.978 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.978 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.979 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.979 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.979 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.979 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.979 I llm_load_print_meta: max token length = 1024
0.00.054.008 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.008 I llm_load_tensors: offloading output layer to GPU
0.00.054.008 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.019 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.020 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.966 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.967 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.968 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.968 I llama_new_context_with_model: n_batch       = 2048
0.00.054.968 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.968 I llama_new_context_with_model: flash_attn    = 0
0.00.054.969 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.969 I llama_new_context_with_model: freq_scale    = 1
0.00.054.969 I ggml_metal_init: allocating
0.00.054.977 I ggml_metal_init: found device: Apple M4
0.00.054.979 I ggml_metal_init: picking default device: Apple M4
0.00.055.529 I ggml_metal_init: using embedded metal library
0.00.057.502 I ggml_metal_init: GPU name:   Apple M4
0.00.057.503 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.505 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.505 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.505 I ggml_metal_init: simdgroup reduction   = true
0.00.057.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.506 I ggml_metal_init: has bfloat            = true
0.00.057.506 I ggml_metal_init: use bfloat            = true
0.00.057.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.705 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.715 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.735 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.689 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.690 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.690 I llama_new_context_with_model: graph nodes  = 967
0.00.085.691 I llama_new_context_with_model: graph splits = 2
0.00.085.698 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.404 I main: llama threadpool init, n_threads = 4
0.00.707.490 I 
0.00.707.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.707.527 I 
0.00.707.753 I sampler seed: 1234
0.00.707.758 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.803 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.807 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.807 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.544.169 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.544.169 I llama_perf_context_print:        load time =     698.55 ms
0.01.544.170 I llama_perf_context_print: prompt eval time =      38.64 ms /     7 tokens (    5.52 ms per token,   181.17 tokens per second)
0.01.544.171 I llama_perf_context_print:        eval time =     794.78 ms /    63 runs   (   12.62 ms per token,    79.27 tokens per second)
0.01.544.171 I llama_perf_context_print:       total time =     836.77 ms /    70 tokens
0.01.544.352 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.909 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.273 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.274 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.278 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.278 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.278 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.293 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.294 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.296 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.219 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.220 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.220 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.221 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.221 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.221 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.222 I llama_model_loader: - type  f32:  194 tensors
0.00.025.222 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.215 I llm_load_vocab: special tokens cache size = 25
0.00.051.181 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.184 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.184 I llm_load_print_meta: arch             = gptneox
0.00.051.184 I llm_load_print_meta: vocab type       = BPE
0.00.051.184 I llm_load_print_meta: n_vocab          = 50304
0.00.051.185 I llm_load_print_meta: n_merges         = 50009
0.00.051.185 I llm_load_print_meta: vocab_only       = 0
0.00.051.185 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.185 I llm_load_print_meta: n_embd           = 2048
0.00.051.185 I llm_load_print_meta: n_layer          = 24
0.00.051.188 I llm_load_print_meta: n_head           = 16
0.00.051.189 I llm_load_print_meta: n_head_kv        = 16
0.00.051.189 I llm_load_print_meta: n_rot            = 32
0.00.051.189 I llm_load_print_meta: n_swa            = 0
0.00.051.189 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.190 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.191 I llm_load_print_meta: n_gqa            = 1
0.00.051.191 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.192 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.193 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.193 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.193 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.193 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.193 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.194 I llm_load_print_meta: n_ff             = 8192
0.00.051.194 I llm_load_print_meta: n_expert         = 0
0.00.051.194 I llm_load_print_meta: n_expert_used    = 0
0.00.051.195 I llm_load_print_meta: causal attn      = 1
0.00.051.195 I llm_load_print_meta: pooling type     = 0
0.00.051.195 I llm_load_print_meta: rope type        = 2
0.00.051.195 I llm_load_print_meta: rope scaling     = linear
0.00.051.195 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.196 I llm_load_print_meta: freq_scale_train = 1
0.00.051.196 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.196 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.196 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.197 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.197 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.197 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.197 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.209 I llm_load_print_meta: model type       = 1.4B
0.00.051.209 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.210 I llm_load_print_meta: model params     = 1.41 B
0.00.051.210 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.210 I llm_load_print_meta: general.name     = 1.4B
0.00.051.210 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.210 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.211 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: max token length = 1024
0.00.053.274 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.274 I llm_load_tensors: offloading output layer to GPU
0.00.053.275 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.285 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.286 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.181 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.182 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.182 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.182 I llama_new_context_with_model: n_batch       = 2048
0.00.054.183 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.183 I llama_new_context_with_model: flash_attn    = 0
0.00.054.183 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.183 I llama_new_context_with_model: freq_scale    = 1
0.00.054.184 I ggml_metal_init: allocating
0.00.054.187 I ggml_metal_init: found device: Apple M4
0.00.054.189 I ggml_metal_init: picking default device: Apple M4
0.00.054.734 I ggml_metal_init: using embedded metal library
0.00.056.639 I ggml_metal_init: GPU name:   Apple M4
0.00.056.641 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.641 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.642 I ggml_metal_init: simdgroup reduction   = true
0.00.056.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.642 I ggml_metal_init: has bfloat            = true
0.00.056.642 I ggml_metal_init: use bfloat            = true
0.00.056.643 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.537 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.542 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.558 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.563 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.565 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.565 I llama_new_context_with_model: graph nodes  = 967
0.00.084.565 I llama_new_context_with_model: graph splits = 2
0.00.084.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.476 I main: llama threadpool init, n_threads = 4
0.00.766.510 I 
0.00.766.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.766.541 I 
0.00.766.777 I sampler seed: 1234
0.00.766.781 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.817 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.818 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.818 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.634.979 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.634.979 I llama_perf_context_print:        load time =     756.56 ms
0.01.634.980 I llama_perf_context_print: prompt eval time =      38.68 ms /     7 tokens (    5.53 ms per token,   180.96 tokens per second)
0.01.634.981 I llama_perf_context_print:        eval time =     826.54 ms /    63 runs   (   13.12 ms per token,    76.22 tokens per second)
0.01.634.981 I llama_perf_context_print:       total time =     868.50 ms /    70 tokens
0.01.635.156 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.108s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.711 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.890 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.069 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.077 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.079 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.079 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.079 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.081 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.086 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.813 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.608 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.161 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.161 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.162 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.162 I llama_model_loader: - type  f32:  194 tensors
0.00.049.163 I llama_model_loader: - type  f16:   98 tensors
0.00.076.335 I llm_load_vocab: special tokens cache size = 25
0.00.082.509 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.512 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.513 I llm_load_print_meta: arch             = gptneox
0.00.082.513 I llm_load_print_meta: vocab type       = BPE
0.00.082.513 I llm_load_print_meta: n_vocab          = 50304
0.00.082.513 I llm_load_print_meta: n_merges         = 50009
0.00.082.514 I llm_load_print_meta: vocab_only       = 0
0.00.082.514 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.514 I llm_load_print_meta: n_embd           = 2048
0.00.082.514 I llm_load_print_meta: n_layer          = 24
0.00.082.517 I llm_load_print_meta: n_head           = 16
0.00.082.518 I llm_load_print_meta: n_head_kv        = 16
0.00.082.518 I llm_load_print_meta: n_rot            = 32
0.00.082.518 I llm_load_print_meta: n_swa            = 0
0.00.082.519 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.519 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.520 I llm_load_print_meta: n_gqa            = 1
0.00.082.520 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.521 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.522 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.522 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.522 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.522 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.522 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.523 I llm_load_print_meta: n_ff             = 8192
0.00.082.523 I llm_load_print_meta: n_expert         = 0
0.00.082.523 I llm_load_print_meta: n_expert_used    = 0
0.00.082.524 I llm_load_print_meta: causal attn      = 1
0.00.082.524 I llm_load_print_meta: pooling type     = 0
0.00.082.524 I llm_load_print_meta: rope type        = 2
0.00.082.524 I llm_load_print_meta: rope scaling     = linear
0.00.082.525 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.525 I llm_load_print_meta: freq_scale_train = 1
0.00.082.525 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.525 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.525 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.526 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.526 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.526 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.526 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.538 I llm_load_print_meta: model type       = 1.4B
0.00.082.538 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.082.539 I llm_load_print_meta: model params     = 1.41 B
0.00.082.539 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.082.539 I llm_load_print_meta: general.name     = 1.4B
0.00.082.540 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.540 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.540 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.541 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.541 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.082.541 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.541 I llm_load_print_meta: max token length = 1024
0.00.084.988 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.084.988 I llm_load_tensors: offloading output layer to GPU
0.00.084.988 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.084.998 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.084.999 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.085.974 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.975 I llama_new_context_with_model: n_ctx         = 128
0.00.085.975 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.085.975 I llama_new_context_with_model: n_batch       = 128
0.00.085.976 I llama_new_context_with_model: n_ubatch      = 128
0.00.085.976 I llama_new_context_with_model: flash_attn    = 0
0.00.085.976 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.976 I llama_new_context_with_model: freq_scale    = 1
0.00.085.977 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.085.977 I ggml_metal_init: allocating
0.00.085.980 I ggml_metal_init: found device: Apple M4
0.00.085.982 I ggml_metal_init: picking default device: Apple M4
0.00.086.533 I ggml_metal_init: using embedded metal library
0.00.088.583 I ggml_metal_init: GPU name:   Apple M4
0.00.088.585 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.585 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.586 I ggml_metal_init: simdgroup reduction   = true
0.00.088.586 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.586 I ggml_metal_init: has bfloat            = true
0.00.088.586 I ggml_metal_init: use bfloat            = true
0.00.088.587 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.524 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.097.526 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.097.539 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.475 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.098.476 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.098.476 I llama_new_context_with_model: graph nodes  = 967
0.00.098.477 I llama_new_context_with_model: graph splits = 2
0.00.098.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.839.448 I 
0.00.839.526 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.839.535 I perplexity: tokenizing the input ..
0.00.852.673 I perplexity: tokenization took 13.138 ms
0.00.852.683 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.971.042 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.972.455 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.972.484 I llama_perf_context_print:        load time =     818.55 ms
0.00.972.485 I llama_perf_context_print: prompt eval time =     118.08 ms /   128 tokens (    0.92 ms per token,  1084.02 tokens per second)
0.00.972.486 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.972.486 I llama_perf_context_print:       total time =     133.04 ms /   129 tokens
0.00.972.820 I ggml_metal_free: deallocating

real	0m1.163s
user	0m0.113s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.243 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.105 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.108 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.108 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.109 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.109 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.110 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.308 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.543 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.543 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.544 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.545 I llama_model_loader: - type  f32:  194 tensors
0.00.024.545 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.879 I llm_load_vocab: special tokens cache size = 25
0.00.056.113 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.116 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.117 I llm_load_print_meta: arch             = gptneox
0.00.056.117 I llm_load_print_meta: vocab type       = BPE
0.00.056.117 I llm_load_print_meta: n_vocab          = 50304
0.00.056.118 I llm_load_print_meta: n_merges         = 50009
0.00.056.118 I llm_load_print_meta: vocab_only       = 0
0.00.056.118 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.118 I llm_load_print_meta: n_embd           = 2048
0.00.056.118 I llm_load_print_meta: n_layer          = 24
0.00.056.122 I llm_load_print_meta: n_head           = 16
0.00.056.123 I llm_load_print_meta: n_head_kv        = 16
0.00.056.123 I llm_load_print_meta: n_rot            = 32
0.00.056.123 I llm_load_print_meta: n_swa            = 0
0.00.056.124 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.126 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.127 I llm_load_print_meta: n_gqa            = 1
0.00.056.128 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.129 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.130 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.130 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.130 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.130 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.131 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.131 I llm_load_print_meta: n_ff             = 8192
0.00.056.131 I llm_load_print_meta: n_expert         = 0
0.00.056.132 I llm_load_print_meta: n_expert_used    = 0
0.00.056.132 I llm_load_print_meta: causal attn      = 1
0.00.056.132 I llm_load_print_meta: pooling type     = 0
0.00.056.132 I llm_load_print_meta: rope type        = 2
0.00.056.132 I llm_load_print_meta: rope scaling     = linear
0.00.056.132 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.133 I llm_load_print_meta: freq_scale_train = 1
0.00.056.133 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.133 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.135 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.135 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.135 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.135 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.135 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.148 I llm_load_print_meta: model type       = 1.4B
0.00.056.148 I llm_load_print_meta: model ftype      = Q8_0
0.00.056.149 I llm_load_print_meta: model params     = 1.41 B
0.00.056.149 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.056.149 I llm_load_print_meta: general.name     = 1.4B
0.00.056.149 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.150 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.150 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.150 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.150 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.150 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.151 I llm_load_print_meta: max token length = 1024
0.00.058.400 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.401 I llm_load_tensors: offloading output layer to GPU
0.00.058.401 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.411 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.412 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.059.438 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.439 I llama_new_context_with_model: n_ctx         = 128
0.00.059.439 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.059.439 I llama_new_context_with_model: n_batch       = 128
0.00.059.439 I llama_new_context_with_model: n_ubatch      = 128
0.00.059.440 I llama_new_context_with_model: flash_attn    = 0
0.00.059.440 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.440 I llama_new_context_with_model: freq_scale    = 1
0.00.059.441 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.059.441 I ggml_metal_init: allocating
0.00.059.445 I ggml_metal_init: found device: Apple M4
0.00.059.447 I ggml_metal_init: picking default device: Apple M4
0.00.060.040 I ggml_metal_init: using embedded metal library
0.00.062.489 I ggml_metal_init: GPU name:   Apple M4
0.00.062.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.493 I ggml_metal_init: simdgroup reduction   = true
0.00.062.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.493 I ggml_metal_init: has bfloat            = true
0.00.062.494 I ggml_metal_init: use bfloat            = true
0.00.062.494 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.626 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.637 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.659 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.555 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.556 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.556 I llama_new_context_with_model: graph nodes  = 967
0.00.072.556 I llama_new_context_with_model: graph splits = 2
0.00.072.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.838.599 I 
0.00.838.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.838.631 I perplexity: tokenizing the input ..
0.00.847.192 I perplexity: tokenization took 8.558 ms
0.00.847.196 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.968.393 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.969.530 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.969.559 I llama_perf_context_print:        load time =     829.35 ms
0.00.969.562 I llama_perf_context_print: prompt eval time =     120.96 ms /   128 tokens (    0.95 ms per token,  1058.19 tokens per second)
0.00.969.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.969.563 I llama_perf_context_print:       total time =     130.96 ms /   129 tokens
0.00.970.070 I ggml_metal_free: deallocating

real	0m0.985s
user	0m0.084s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.522 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.313 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.317 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.319 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.321 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.321 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.322 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.323 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.323 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.324 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.324 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.326 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.327 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.327 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.158 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.196 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.129 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.130 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.131 I llama_model_loader: - type  f32:  194 tensors
0.00.024.131 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.131 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.063 I llm_load_vocab: special tokens cache size = 25
0.00.049.888 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.890 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.891 I llm_load_print_meta: arch             = gptneox
0.00.049.891 I llm_load_print_meta: vocab type       = BPE
0.00.049.891 I llm_load_print_meta: n_vocab          = 50304
0.00.049.891 I llm_load_print_meta: n_merges         = 50009
0.00.049.892 I llm_load_print_meta: vocab_only       = 0
0.00.049.892 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.892 I llm_load_print_meta: n_embd           = 2048
0.00.049.892 I llm_load_print_meta: n_layer          = 24
0.00.049.894 I llm_load_print_meta: n_head           = 16
0.00.049.895 I llm_load_print_meta: n_head_kv        = 16
0.00.049.895 I llm_load_print_meta: n_rot            = 32
0.00.049.895 I llm_load_print_meta: n_swa            = 0
0.00.049.896 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.898 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.899 I llm_load_print_meta: n_gqa            = 1
0.00.049.900 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.901 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.901 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.902 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.902 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.902 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.902 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.903 I llm_load_print_meta: n_ff             = 8192
0.00.049.903 I llm_load_print_meta: n_expert         = 0
0.00.049.903 I llm_load_print_meta: n_expert_used    = 0
0.00.049.903 I llm_load_print_meta: causal attn      = 1
0.00.049.903 I llm_load_print_meta: pooling type     = 0
0.00.049.903 I llm_load_print_meta: rope type        = 2
0.00.049.904 I llm_load_print_meta: rope scaling     = linear
0.00.049.905 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.905 I llm_load_print_meta: freq_scale_train = 1
0.00.049.905 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.905 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.906 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.906 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.906 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.906 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.906 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.918 I llm_load_print_meta: model type       = 1.4B
0.00.049.918 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.919 I llm_load_print_meta: model params     = 1.41 B
0.00.049.919 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.919 I llm_load_print_meta: general.name     = 1.4B
0.00.049.919 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.920 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: max token length = 1024
0.00.051.848 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.848 I llm_load_tensors: offloading output layer to GPU
0.00.051.849 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.859 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.860 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.736 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.737 I llama_new_context_with_model: n_ctx         = 128
0.00.052.737 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.737 I llama_new_context_with_model: n_batch       = 128
0.00.052.737 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.738 I llama_new_context_with_model: flash_attn    = 0
0.00.052.738 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.738 I llama_new_context_with_model: freq_scale    = 1
0.00.052.739 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.739 I ggml_metal_init: allocating
0.00.052.745 I ggml_metal_init: found device: Apple M4
0.00.052.747 I ggml_metal_init: picking default device: Apple M4
0.00.053.286 I ggml_metal_init: using embedded metal library
0.00.055.256 I ggml_metal_init: GPU name:   Apple M4
0.00.055.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.258 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.258 I ggml_metal_init: simdgroup reduction   = true
0.00.055.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.259 I ggml_metal_init: has bfloat            = true
0.00.055.259 I ggml_metal_init: use bfloat            = true
0.00.055.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.562 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.568 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.582 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.431 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.432 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.432 I llama_new_context_with_model: graph nodes  = 967
0.00.065.433 I llama_new_context_with_model: graph splits = 2
0.00.065.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.014 I 
0.00.623.058 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.623.062 I perplexity: tokenizing the input ..
0.00.631.182 I perplexity: tokenization took 8.116 ms
0.00.631.186 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.527 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.754.690 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.754.709 I llama_perf_context_print:        load time =     613.48 ms
0.00.754.710 I llama_perf_context_print: prompt eval time =     122.12 ms /   128 tokens (    0.95 ms per token,  1048.17 tokens per second)
0.00.754.711 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.711 I llama_perf_context_print:       total time =     131.70 ms /   129 tokens
0.00.755.053 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.077s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.344 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.145 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.149 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.151 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.152 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.152 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.152 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.153 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.153 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.154 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.154 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.155 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.155 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.156 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.159 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.159 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.159 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.167 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.289 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.290 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.292 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.292 I llama_model_loader: - type  f32:  194 tensors
0.00.024.293 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.293 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.032 I llm_load_vocab: special tokens cache size = 25
0.00.050.987 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.990 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.990 I llm_load_print_meta: arch             = gptneox
0.00.050.991 I llm_load_print_meta: vocab type       = BPE
0.00.050.991 I llm_load_print_meta: n_vocab          = 50304
0.00.050.991 I llm_load_print_meta: n_merges         = 50009
0.00.050.991 I llm_load_print_meta: vocab_only       = 0
0.00.050.991 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.992 I llm_load_print_meta: n_embd           = 2048
0.00.050.992 I llm_load_print_meta: n_layer          = 24
0.00.050.994 I llm_load_print_meta: n_head           = 16
0.00.050.995 I llm_load_print_meta: n_head_kv        = 16
0.00.050.995 I llm_load_print_meta: n_rot            = 32
0.00.050.995 I llm_load_print_meta: n_swa            = 0
0.00.050.995 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.996 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.999 I llm_load_print_meta: n_gqa            = 1
0.00.051.000 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.000 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.001 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.001 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.003 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.003 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.003 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.004 I llm_load_print_meta: n_ff             = 8192
0.00.051.004 I llm_load_print_meta: n_expert         = 0
0.00.051.004 I llm_load_print_meta: n_expert_used    = 0
0.00.051.004 I llm_load_print_meta: causal attn      = 1
0.00.051.004 I llm_load_print_meta: pooling type     = 0
0.00.051.004 I llm_load_print_meta: rope type        = 2
0.00.051.005 I llm_load_print_meta: rope scaling     = linear
0.00.051.005 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.005 I llm_load_print_meta: freq_scale_train = 1
0.00.051.005 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.006 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.006 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.006 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.006 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.006 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.006 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.013 I llm_load_print_meta: model type       = 1.4B
0.00.051.014 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.014 I llm_load_print_meta: model params     = 1.41 B
0.00.051.015 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.015 I llm_load_print_meta: general.name     = 1.4B
0.00.051.015 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.015 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.015 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.016 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: max token length = 1024
0.00.052.803 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.803 I llm_load_tensors: offloading output layer to GPU
0.00.052.803 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.808 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.808 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.730 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.731 I llama_new_context_with_model: n_ctx         = 128
0.00.053.732 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.732 I llama_new_context_with_model: n_batch       = 128
0.00.053.732 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.732 I llama_new_context_with_model: flash_attn    = 0
0.00.053.732 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.733 I llama_new_context_with_model: freq_scale    = 1
0.00.053.733 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.733 I ggml_metal_init: allocating
0.00.053.739 I ggml_metal_init: found device: Apple M4
0.00.053.742 I ggml_metal_init: picking default device: Apple M4
0.00.054.279 I ggml_metal_init: using embedded metal library
0.00.056.301 I ggml_metal_init: GPU name:   Apple M4
0.00.056.302 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.303 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.305 I ggml_metal_init: simdgroup reduction   = true
0.00.056.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.305 I ggml_metal_init: has bfloat            = true
0.00.056.305 I ggml_metal_init: use bfloat            = true
0.00.056.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.276 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.280 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.293 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.110 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.111 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.111 I llama_new_context_with_model: graph nodes  = 967
0.00.066.111 I llama_new_context_with_model: graph splits = 2
0.00.066.119 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.494 I 
0.00.633.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.633.592 I perplexity: tokenizing the input ..
0.00.641.541 I perplexity: tokenization took 7.949 ms
0.00.641.545 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.233 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.765.491 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.765.524 I llama_perf_context_print:        load time =     624.14 ms
0.00.765.525 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.30 tokens per second)
0.00.765.526 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.526 I llama_perf_context_print:       total time =     132.04 ms /   129 tokens
0.00.766.021 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.077s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.990 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.024 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.030 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.030 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.031 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.031 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.032 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.038 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.038 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.039 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.039 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.044 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.044 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.089 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.090 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.091 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.092 I llama_model_loader: - type  f32:  194 tensors
0.00.025.092 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.092 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.105 I llm_load_vocab: special tokens cache size = 25
0.00.050.955 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.958 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.958 I llm_load_print_meta: arch             = gptneox
0.00.050.959 I llm_load_print_meta: vocab type       = BPE
0.00.050.959 I llm_load_print_meta: n_vocab          = 50304
0.00.050.959 I llm_load_print_meta: n_merges         = 50009
0.00.050.959 I llm_load_print_meta: vocab_only       = 0
0.00.050.960 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.960 I llm_load_print_meta: n_embd           = 2048
0.00.050.960 I llm_load_print_meta: n_layer          = 24
0.00.050.963 I llm_load_print_meta: n_head           = 16
0.00.050.964 I llm_load_print_meta: n_head_kv        = 16
0.00.050.964 I llm_load_print_meta: n_rot            = 32
0.00.050.964 I llm_load_print_meta: n_swa            = 0
0.00.050.964 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.964 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.967 I llm_load_print_meta: n_gqa            = 1
0.00.050.968 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.969 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.969 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.970 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.977 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.980 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.980 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.985 I llm_load_print_meta: n_ff             = 8192
0.00.050.986 I llm_load_print_meta: n_expert         = 0
0.00.050.986 I llm_load_print_meta: n_expert_used    = 0
0.00.050.986 I llm_load_print_meta: causal attn      = 1
0.00.050.987 I llm_load_print_meta: pooling type     = 0
0.00.050.987 I llm_load_print_meta: rope type        = 2
0.00.050.987 I llm_load_print_meta: rope scaling     = linear
0.00.050.987 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.987 I llm_load_print_meta: freq_scale_train = 1
0.00.050.988 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.988 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.988 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.989 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.989 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.989 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.990 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.002 I llm_load_print_meta: model type       = 1.4B
0.00.051.002 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.003 I llm_load_print_meta: model params     = 1.41 B
0.00.051.003 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.004 I llm_load_print_meta: general.name     = 1.4B
0.00.051.004 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.004 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.004 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.004 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.005 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.005 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.005 I llm_load_print_meta: max token length = 1024
0.00.052.989 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.989 I llm_load_tensors: offloading output layer to GPU
0.00.052.989 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.000 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.001 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.929 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.930 I llama_new_context_with_model: n_ctx         = 128
0.00.053.930 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.930 I llama_new_context_with_model: n_batch       = 128
0.00.053.930 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.930 I llama_new_context_with_model: flash_attn    = 0
0.00.053.931 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.931 I llama_new_context_with_model: freq_scale    = 1
0.00.053.931 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.931 I ggml_metal_init: allocating
0.00.053.936 I ggml_metal_init: found device: Apple M4
0.00.053.938 I ggml_metal_init: picking default device: Apple M4
0.00.054.492 I ggml_metal_init: using embedded metal library
0.00.056.424 I ggml_metal_init: GPU name:   Apple M4
0.00.056.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.426 I ggml_metal_init: simdgroup reduction   = true
0.00.056.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.428 I ggml_metal_init: has bfloat            = true
0.00.056.428 I ggml_metal_init: use bfloat            = true
0.00.056.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.633 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.636 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.650 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.574 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.575 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.575 I llama_new_context_with_model: graph nodes  = 967
0.00.066.576 I llama_new_context_with_model: graph splits = 2
0.00.066.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.840 I 
0.00.677.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.677.883 I perplexity: tokenizing the input ..
0.00.685.883 I perplexity: tokenization took 7.999 ms
0.00.685.887 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.249 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.822.516 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.822.541 I llama_perf_context_print:        load time =     667.85 ms
0.00.822.542 I llama_perf_context_print: prompt eval time =     135.14 ms /   128 tokens (    1.06 ms per token,   947.19 tokens per second)
0.00.822.543 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.822.543 I llama_perf_context_print:       total time =     144.70 ms /   129 tokens
0.00.823.000 I ggml_metal_free: deallocating

real	0m0.839s
user	0m0.076s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.793 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.687 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.693 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.694 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.694 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.698 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.699 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.699 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.699 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.699 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.702 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.702 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.738 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.737 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.740 I llama_model_loader: - type  f32:  194 tensors
0.00.023.740 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.740 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.776 I llm_load_vocab: special tokens cache size = 25
0.00.050.674 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.676 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.677 I llm_load_print_meta: arch             = gptneox
0.00.050.677 I llm_load_print_meta: vocab type       = BPE
0.00.050.677 I llm_load_print_meta: n_vocab          = 50304
0.00.050.677 I llm_load_print_meta: n_merges         = 50009
0.00.050.678 I llm_load_print_meta: vocab_only       = 0
0.00.050.678 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.678 I llm_load_print_meta: n_embd           = 2048
0.00.050.678 I llm_load_print_meta: n_layer          = 24
0.00.050.681 I llm_load_print_meta: n_head           = 16
0.00.050.682 I llm_load_print_meta: n_head_kv        = 16
0.00.050.682 I llm_load_print_meta: n_rot            = 32
0.00.050.682 I llm_load_print_meta: n_swa            = 0
0.00.050.682 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.685 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.686 I llm_load_print_meta: n_gqa            = 1
0.00.050.687 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.687 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.688 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.689 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.689 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.690 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.690 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.690 I llm_load_print_meta: n_ff             = 8192
0.00.050.692 I llm_load_print_meta: n_expert         = 0
0.00.050.692 I llm_load_print_meta: n_expert_used    = 0
0.00.050.692 I llm_load_print_meta: causal attn      = 1
0.00.050.692 I llm_load_print_meta: pooling type     = 0
0.00.050.692 I llm_load_print_meta: rope type        = 2
0.00.050.692 I llm_load_print_meta: rope scaling     = linear
0.00.050.693 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.693 I llm_load_print_meta: freq_scale_train = 1
0.00.050.693 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.693 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.694 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.694 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.694 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.694 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.694 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.706 I llm_load_print_meta: model type       = 1.4B
0.00.050.706 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.707 I llm_load_print_meta: model params     = 1.41 B
0.00.050.707 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.708 I llm_load_print_meta: general.name     = 1.4B
0.00.050.709 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.709 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.709 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.709 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.709 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.709 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.710 I llm_load_print_meta: max token length = 1024
0.00.052.743 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.743 I llm_load_tensors: offloading output layer to GPU
0.00.052.743 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.753 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.754 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.711 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.712 I llama_new_context_with_model: n_ctx         = 128
0.00.053.713 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.713 I llama_new_context_with_model: n_batch       = 128
0.00.053.713 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.713 I llama_new_context_with_model: flash_attn    = 0
0.00.053.713 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.714 I llama_new_context_with_model: freq_scale    = 1
0.00.053.714 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.714 I ggml_metal_init: allocating
0.00.053.718 I ggml_metal_init: found device: Apple M4
0.00.053.720 I ggml_metal_init: picking default device: Apple M4
0.00.054.266 I ggml_metal_init: using embedded metal library
0.00.056.191 I ggml_metal_init: GPU name:   Apple M4
0.00.056.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.193 I ggml_metal_init: simdgroup reduction   = true
0.00.056.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.194 I ggml_metal_init: has bfloat            = true
0.00.056.194 I ggml_metal_init: use bfloat            = true
0.00.056.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.671 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.675 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.689 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.646 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.647 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.647 I llama_new_context_with_model: graph nodes  = 967
0.00.066.647 I llama_new_context_with_model: graph splits = 2
0.00.066.660 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.475 I 
0.00.792.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.792.646 I perplexity: tokenizing the input ..
0.00.810.733 I perplexity: tokenization took 18.083 ms
0.00.810.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.963.675 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.965.095 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.965.141 I llama_perf_context_print:        load time =     783.66 ms
0.00.965.143 I llama_perf_context_print: prompt eval time =     151.99 ms /   128 tokens (    1.19 ms per token,   842.17 tokens per second)
0.00.965.145 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.965.145 I llama_perf_context_print:       total time =     172.68 ms /   129 tokens
0.00.965.831 I ggml_metal_free: deallocating

real	0m0.981s
user	0m0.091s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.663 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.668 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.670 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.671 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.671 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.672 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.673 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.660 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.512 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.514 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.514 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.515 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.515 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.515 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.028.516 I llama_model_loader: - type  f32:  194 tensors
0.00.028.516 I llama_model_loader: - type q2_K:   49 tensors
0.00.028.516 I llama_model_loader: - type q3_K:   48 tensors
0.00.028.517 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.191 I llm_load_vocab: special tokens cache size = 25
0.00.056.780 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.783 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.783 I llm_load_print_meta: arch             = gptneox
0.00.056.783 I llm_load_print_meta: vocab type       = BPE
0.00.056.784 I llm_load_print_meta: n_vocab          = 50304
0.00.056.784 I llm_load_print_meta: n_merges         = 50009
0.00.056.784 I llm_load_print_meta: vocab_only       = 0
0.00.056.784 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.784 I llm_load_print_meta: n_embd           = 2048
0.00.056.784 I llm_load_print_meta: n_layer          = 24
0.00.056.787 I llm_load_print_meta: n_head           = 16
0.00.056.788 I llm_load_print_meta: n_head_kv        = 16
0.00.056.788 I llm_load_print_meta: n_rot            = 32
0.00.056.788 I llm_load_print_meta: n_swa            = 0
0.00.056.789 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.789 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.790 I llm_load_print_meta: n_gqa            = 1
0.00.056.792 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.792 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.793 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.793 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.793 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.794 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.794 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.794 I llm_load_print_meta: n_ff             = 8192
0.00.056.794 I llm_load_print_meta: n_expert         = 0
0.00.056.795 I llm_load_print_meta: n_expert_used    = 0
0.00.056.795 I llm_load_print_meta: causal attn      = 1
0.00.056.795 I llm_load_print_meta: pooling type     = 0
0.00.056.795 I llm_load_print_meta: rope type        = 2
0.00.056.795 I llm_load_print_meta: rope scaling     = linear
0.00.056.796 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.796 I llm_load_print_meta: freq_scale_train = 1
0.00.056.796 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.797 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.797 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.797 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.797 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.797 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.797 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.809 I llm_load_print_meta: model type       = 1.4B
0.00.056.810 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.056.810 I llm_load_print_meta: model params     = 1.41 B
0.00.056.811 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.056.811 I llm_load_print_meta: general.name     = 1.4B
0.00.056.811 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.811 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.811 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.811 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.812 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.812 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.812 I llm_load_print_meta: max token length = 1024
0.00.058.678 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.679 I llm_load_tensors: offloading output layer to GPU
0.00.058.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.689 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.058.690 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.059.589 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.590 I llama_new_context_with_model: n_ctx         = 128
0.00.059.590 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.059.590 I llama_new_context_with_model: n_batch       = 128
0.00.059.590 I llama_new_context_with_model: n_ubatch      = 128
0.00.059.591 I llama_new_context_with_model: flash_attn    = 0
0.00.059.591 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.591 I llama_new_context_with_model: freq_scale    = 1
0.00.059.591 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.059.592 I ggml_metal_init: allocating
0.00.059.597 I ggml_metal_init: found device: Apple M4
0.00.059.600 I ggml_metal_init: picking default device: Apple M4
0.00.060.118 I ggml_metal_init: using embedded metal library
0.00.062.063 I ggml_metal_init: GPU name:   Apple M4
0.00.062.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.064 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.065 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.065 I ggml_metal_init: simdgroup reduction   = true
0.00.062.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.065 I ggml_metal_init: has bfloat            = true
0.00.062.065 I ggml_metal_init: use bfloat            = true
0.00.062.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.067 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.036 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.051 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.905 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.906 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.906 I llama_new_context_with_model: graph nodes  = 967
0.00.071.907 I llama_new_context_with_model: graph splits = 2
0.00.071.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.392 I 
0.00.477.426 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.477.431 I perplexity: tokenizing the input ..
0.00.485.604 I perplexity: tokenization took 8.172 ms
0.00.485.607 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.617.784 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.619.220 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.619.275 I llama_perf_context_print:        load time =     466.59 ms
0.00.619.275 I llama_perf_context_print: prompt eval time =     131.94 ms /   128 tokens (    1.03 ms per token,   970.12 tokens per second)
0.00.619.276 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.277 I llama_perf_context_print:       total time =     141.88 ms /   129 tokens
0.00.619.708 I ggml_metal_free: deallocating

real	0m0.637s
user	0m0.083s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.732 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.779 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.783 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.787 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.787 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.788 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.920 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.064 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.182 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.183 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.184 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.184 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.184 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.185 I llama_model_loader: - type  f32:  194 tensors
0.00.024.185 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.186 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.186 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.186 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.887 I llm_load_vocab: special tokens cache size = 25
0.00.051.013 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.017 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.017 I llm_load_print_meta: arch             = gptneox
0.00.051.018 I llm_load_print_meta: vocab type       = BPE
0.00.051.018 I llm_load_print_meta: n_vocab          = 50304
0.00.051.018 I llm_load_print_meta: n_merges         = 50009
0.00.051.018 I llm_load_print_meta: vocab_only       = 0
0.00.051.018 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.018 I llm_load_print_meta: n_embd           = 2048
0.00.051.019 I llm_load_print_meta: n_layer          = 24
0.00.051.024 I llm_load_print_meta: n_head           = 16
0.00.051.024 I llm_load_print_meta: n_head_kv        = 16
0.00.051.024 I llm_load_print_meta: n_rot            = 32
0.00.051.025 I llm_load_print_meta: n_swa            = 0
0.00.051.025 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.025 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.026 I llm_load_print_meta: n_gqa            = 1
0.00.051.026 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.027 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.027 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.028 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.028 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.028 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.028 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.029 I llm_load_print_meta: n_ff             = 8192
0.00.051.029 I llm_load_print_meta: n_expert         = 0
0.00.051.029 I llm_load_print_meta: n_expert_used    = 0
0.00.051.029 I llm_load_print_meta: causal attn      = 1
0.00.051.029 I llm_load_print_meta: pooling type     = 0
0.00.051.029 I llm_load_print_meta: rope type        = 2
0.00.051.029 I llm_load_print_meta: rope scaling     = linear
0.00.051.030 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.030 I llm_load_print_meta: freq_scale_train = 1
0.00.051.030 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.030 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.031 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.031 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.032 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.032 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.032 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.045 I llm_load_print_meta: model type       = 1.4B
0.00.051.045 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.046 I llm_load_print_meta: model params     = 1.41 B
0.00.051.046 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.046 I llm_load_print_meta: general.name     = 1.4B
0.00.051.046 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.046 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.047 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.047 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.047 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.047 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.048 I llm_load_print_meta: max token length = 1024
0.00.052.907 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.907 I llm_load_tensors: offloading output layer to GPU
0.00.052.907 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.917 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.919 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.954 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.955 I llama_new_context_with_model: n_ctx         = 128
0.00.053.955 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.955 I llama_new_context_with_model: n_batch       = 128
0.00.053.955 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.955 I llama_new_context_with_model: flash_attn    = 0
0.00.053.956 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.956 I llama_new_context_with_model: freq_scale    = 1
0.00.053.957 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.957 I ggml_metal_init: allocating
0.00.053.965 I ggml_metal_init: found device: Apple M4
0.00.053.967 I ggml_metal_init: picking default device: Apple M4
0.00.054.565 I ggml_metal_init: using embedded metal library
0.00.056.768 I ggml_metal_init: GPU name:   Apple M4
0.00.056.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.771 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.771 I ggml_metal_init: simdgroup reduction   = true
0.00.056.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.772 I ggml_metal_init: has bfloat            = true
0.00.056.772 I ggml_metal_init: use bfloat            = true
0.00.056.773 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.759 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.775 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.682 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.683 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.683 I llama_new_context_with_model: graph nodes  = 967
0.00.066.684 I llama_new_context_with_model: graph splits = 2
0.00.066.696 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.263 I 
0.00.586.291 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.586.298 I perplexity: tokenizing the input ..
0.00.594.474 I perplexity: tokenization took 8.178 ms
0.00.594.477 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.889 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.727.077 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.727.105 I llama_perf_context_print:        load time =     577.52 ms
0.00.727.106 I llama_perf_context_print: prompt eval time =     131.19 ms /   128 tokens (    1.02 ms per token,   975.69 tokens per second)
0.00.727.107 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.107 I llama_perf_context_print:       total time =     140.84 ms /   129 tokens
0.00.727.586 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.079s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.997 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.802 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.807 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.810 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.813 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.813 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.816 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.817 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.817 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.860 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.991 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.994 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.994 I llama_model_loader: - type  f32:  194 tensors
0.00.024.995 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.995 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.995 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.882 I llm_load_vocab: special tokens cache size = 25
0.00.051.852 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.855 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.856 I llm_load_print_meta: arch             = gptneox
0.00.051.856 I llm_load_print_meta: vocab type       = BPE
0.00.051.856 I llm_load_print_meta: n_vocab          = 50304
0.00.051.856 I llm_load_print_meta: n_merges         = 50009
0.00.051.856 I llm_load_print_meta: vocab_only       = 0
0.00.051.857 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.857 I llm_load_print_meta: n_embd           = 2048
0.00.051.857 I llm_load_print_meta: n_layer          = 24
0.00.051.860 I llm_load_print_meta: n_head           = 16
0.00.051.861 I llm_load_print_meta: n_head_kv        = 16
0.00.051.861 I llm_load_print_meta: n_rot            = 32
0.00.051.861 I llm_load_print_meta: n_swa            = 0
0.00.051.861 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.864 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.864 I llm_load_print_meta: n_gqa            = 1
0.00.051.865 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.866 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.866 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.869 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.869 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.869 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.869 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.870 I llm_load_print_meta: n_ff             = 8192
0.00.051.870 I llm_load_print_meta: n_expert         = 0
0.00.051.870 I llm_load_print_meta: n_expert_used    = 0
0.00.051.870 I llm_load_print_meta: causal attn      = 1
0.00.051.870 I llm_load_print_meta: pooling type     = 0
0.00.051.870 I llm_load_print_meta: rope type        = 2
0.00.051.871 I llm_load_print_meta: rope scaling     = linear
0.00.051.871 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.871 I llm_load_print_meta: freq_scale_train = 1
0.00.051.872 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.872 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.872 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.872 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.872 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.872 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.873 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.885 I llm_load_print_meta: model type       = 1.4B
0.00.051.886 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.886 I llm_load_print_meta: model params     = 1.41 B
0.00.051.886 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.887 I llm_load_print_meta: general.name     = 1.4B
0.00.051.887 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.887 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.887 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.887 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.888 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.888 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.888 I llm_load_print_meta: max token length = 1024
0.00.053.830 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.830 I llm_load_tensors: offloading output layer to GPU
0.00.053.830 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.840 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.841 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.784 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.785 I llama_new_context_with_model: n_ctx         = 128
0.00.054.785 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.785 I llama_new_context_with_model: n_batch       = 128
0.00.054.786 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.786 I llama_new_context_with_model: flash_attn    = 0
0.00.054.786 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.786 I llama_new_context_with_model: freq_scale    = 1
0.00.054.787 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.787 I ggml_metal_init: allocating
0.00.054.793 I ggml_metal_init: found device: Apple M4
0.00.054.795 I ggml_metal_init: picking default device: Apple M4
0.00.055.312 I ggml_metal_init: using embedded metal library
0.00.057.203 I ggml_metal_init: GPU name:   Apple M4
0.00.057.204 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.205 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.205 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.205 I ggml_metal_init: simdgroup reduction   = true
0.00.057.205 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.205 I ggml_metal_init: has bfloat            = true
0.00.057.206 I ggml_metal_init: use bfloat            = true
0.00.057.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.206 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.063 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.068 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.084 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.932 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.933 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.933 I llama_new_context_with_model: graph nodes  = 967
0.00.066.933 I llama_new_context_with_model: graph splits = 2
0.00.066.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.653 I 
0.00.578.693 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.578.699 I perplexity: tokenizing the input ..
0.00.586.308 I perplexity: tokenization took 7.607 ms
0.00.586.311 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.669 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.721.821 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.721.850 I llama_perf_context_print:        load time =     568.65 ms
0.00.721.851 I llama_perf_context_print: prompt eval time =     134.13 ms /   128 tokens (    1.05 ms per token,   954.28 tokens per second)
0.00.721.852 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.721.853 I llama_perf_context_print:       total time =     143.20 ms /   129 tokens
0.00.722.330 I ggml_metal_free: deallocating

real	0m0.739s
user	0m0.078s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.774 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.605 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.605 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.605 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.606 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.607 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.667 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.769 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.770 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.770 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.771 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.771 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.771 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.771 I llama_model_loader: - type  f32:  194 tensors
0.00.023.772 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.772 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.308 I llm_load_vocab: special tokens cache size = 25
0.00.050.301 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.304 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.304 I llm_load_print_meta: arch             = gptneox
0.00.050.304 I llm_load_print_meta: vocab type       = BPE
0.00.050.305 I llm_load_print_meta: n_vocab          = 50304
0.00.050.305 I llm_load_print_meta: n_merges         = 50009
0.00.050.305 I llm_load_print_meta: vocab_only       = 0
0.00.050.305 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.305 I llm_load_print_meta: n_embd           = 2048
0.00.050.306 I llm_load_print_meta: n_layer          = 24
0.00.050.309 I llm_load_print_meta: n_head           = 16
0.00.050.310 I llm_load_print_meta: n_head_kv        = 16
0.00.050.310 I llm_load_print_meta: n_rot            = 32
0.00.050.310 I llm_load_print_meta: n_swa            = 0
0.00.050.310 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.310 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.311 I llm_load_print_meta: n_gqa            = 1
0.00.050.313 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.314 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.314 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.315 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.315 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.315 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.315 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.316 I llm_load_print_meta: n_ff             = 8192
0.00.050.316 I llm_load_print_meta: n_expert         = 0
0.00.050.316 I llm_load_print_meta: n_expert_used    = 0
0.00.050.316 I llm_load_print_meta: causal attn      = 1
0.00.050.317 I llm_load_print_meta: pooling type     = 0
0.00.050.317 I llm_load_print_meta: rope type        = 2
0.00.050.317 I llm_load_print_meta: rope scaling     = linear
0.00.050.317 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.318 I llm_load_print_meta: freq_scale_train = 1
0.00.050.318 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.318 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.318 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.318 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.319 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.319 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.319 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.330 I llm_load_print_meta: model type       = 1.4B
0.00.050.331 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.331 I llm_load_print_meta: model params     = 1.41 B
0.00.050.332 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.332 I llm_load_print_meta: general.name     = 1.4B
0.00.050.332 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.332 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.332 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.332 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.333 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.333 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.333 I llm_load_print_meta: max token length = 1024
0.00.051.891 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.891 I llm_load_tensors: offloading output layer to GPU
0.00.051.892 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.901 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.902 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.743 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.744 I llama_new_context_with_model: n_ctx         = 128
0.00.052.744 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.744 I llama_new_context_with_model: n_batch       = 128
0.00.052.744 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.744 I llama_new_context_with_model: flash_attn    = 0
0.00.052.745 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.745 I llama_new_context_with_model: freq_scale    = 1
0.00.052.745 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.746 I ggml_metal_init: allocating
0.00.052.751 I ggml_metal_init: found device: Apple M4
0.00.052.753 I ggml_metal_init: picking default device: Apple M4
0.00.053.297 I ggml_metal_init: using embedded metal library
0.00.055.213 I ggml_metal_init: GPU name:   Apple M4
0.00.055.215 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.216 I ggml_metal_init: simdgroup reduction   = true
0.00.055.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.216 I ggml_metal_init: has bfloat            = true
0.00.055.216 I ggml_metal_init: use bfloat            = true
0.00.055.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.383 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.385 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.398 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.227 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.228 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.228 I llama_new_context_with_model: graph nodes  = 967
0.00.065.229 I llama_new_context_with_model: graph splits = 2
0.00.065.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.586 I 
0.00.657.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.657.621 I perplexity: tokenizing the input ..
0.00.665.934 I perplexity: tokenization took 8.311 ms
0.00.665.937 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.975 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.807.140 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.807.167 I llama_perf_context_print:        load time =     648.81 ms
0.00.807.168 I llama_perf_context_print: prompt eval time =     139.81 ms /   128 tokens (    1.09 ms per token,   915.52 tokens per second)
0.00.807.168 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.169 I llama_perf_context_print:       total time =     149.58 ms /   129 tokens
0.00.807.530 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.078s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.908 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.486 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.496 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.496 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.497 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.498 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.499 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.424 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.452 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.513 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.514 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.514 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.514 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.515 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.515 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.516 I llama_model_loader: - type  f32:  194 tensors
0.00.024.516 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.306 I llm_load_vocab: special tokens cache size = 25
0.00.051.261 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.264 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.264 I llm_load_print_meta: arch             = gptneox
0.00.051.264 I llm_load_print_meta: vocab type       = BPE
0.00.051.265 I llm_load_print_meta: n_vocab          = 50304
0.00.051.265 I llm_load_print_meta: n_merges         = 50009
0.00.051.265 I llm_load_print_meta: vocab_only       = 0
0.00.051.265 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.265 I llm_load_print_meta: n_embd           = 2048
0.00.051.265 I llm_load_print_meta: n_layer          = 24
0.00.051.268 I llm_load_print_meta: n_head           = 16
0.00.051.269 I llm_load_print_meta: n_head_kv        = 16
0.00.051.269 I llm_load_print_meta: n_rot            = 32
0.00.051.270 I llm_load_print_meta: n_swa            = 0
0.00.051.270 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.270 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.271 I llm_load_print_meta: n_gqa            = 1
0.00.051.272 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.272 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.273 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.273 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.273 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.273 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.276 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.276 I llm_load_print_meta: n_ff             = 8192
0.00.051.277 I llm_load_print_meta: n_expert         = 0
0.00.051.277 I llm_load_print_meta: n_expert_used    = 0
0.00.051.277 I llm_load_print_meta: causal attn      = 1
0.00.051.277 I llm_load_print_meta: pooling type     = 0
0.00.051.277 I llm_load_print_meta: rope type        = 2
0.00.051.278 I llm_load_print_meta: rope scaling     = linear
0.00.051.278 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.278 I llm_load_print_meta: freq_scale_train = 1
0.00.051.279 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.279 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.279 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.280 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.280 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.281 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.281 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.287 I llm_load_print_meta: model type       = 1.4B
0.00.051.288 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.288 I llm_load_print_meta: model params     = 1.41 B
0.00.051.289 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.289 I llm_load_print_meta: general.name     = 1.4B
0.00.051.289 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.289 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.289 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.290 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.290 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.291 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.291 I llm_load_print_meta: max token length = 1024
0.00.053.065 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.066 I llm_load_tensors: offloading output layer to GPU
0.00.053.066 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.071 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.071 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.010 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.011 I llama_new_context_with_model: n_ctx         = 128
0.00.054.011 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.011 I llama_new_context_with_model: n_batch       = 128
0.00.054.011 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.011 I llama_new_context_with_model: flash_attn    = 0
0.00.054.012 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.012 I llama_new_context_with_model: freq_scale    = 1
0.00.054.012 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.013 I ggml_metal_init: allocating
0.00.054.018 I ggml_metal_init: found device: Apple M4
0.00.054.020 I ggml_metal_init: picking default device: Apple M4
0.00.054.536 I ggml_metal_init: using embedded metal library
0.00.056.445 I ggml_metal_init: GPU name:   Apple M4
0.00.056.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.447 I ggml_metal_init: simdgroup reduction   = true
0.00.056.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.447 I ggml_metal_init: has bfloat            = true
0.00.056.448 I ggml_metal_init: use bfloat            = true
0.00.056.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.449 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.307 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.310 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.323 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.157 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.158 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.158 I llama_new_context_with_model: graph nodes  = 967
0.00.066.158 I llama_new_context_with_model: graph splits = 2
0.00.066.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.212.780 I 
0.00.212.814 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.212.821 I perplexity: tokenizing the input ..
0.00.220.416 I perplexity: tokenization took 7.594 ms
0.00.220.423 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.360.855 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.362.006 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.362.031 I llama_perf_context_print:        load time =     202.87 ms
0.00.362.032 I llama_perf_context_print: prompt eval time =     140.19 ms /   128 tokens (    1.10 ms per token,   913.06 tokens per second)
0.00.362.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.033 I llama_perf_context_print:       total time =     149.25 ms /   129 tokens
0.00.362.490 I ggml_metal_free: deallocating

real	0m0.378s
user	0m0.077s
sys	0m0.051s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.266 I build: 4231 (43957ef2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.099 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.886 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.175 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.024 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.898 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.902 I llama_model_loader: - type  f32:  194 tensors
0.00.052.903 I llama_model_loader: - type  f16:   98 tensors
0.00.079.613 I llm_load_vocab: special tokens cache size = 25
0.00.086.113 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.116 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.116 I llm_load_print_meta: arch             = gptneox
0.00.086.117 I llm_load_print_meta: vocab type       = BPE
0.00.086.117 I llm_load_print_meta: n_vocab          = 50304
0.00.086.117 I llm_load_print_meta: n_merges         = 50009
0.00.086.117 I llm_load_print_meta: vocab_only       = 0
0.00.086.117 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.117 I llm_load_print_meta: n_embd           = 2048
0.00.086.118 I llm_load_print_meta: n_layer          = 24
0.00.086.120 I llm_load_print_meta: n_head           = 16
0.00.086.121 I llm_load_print_meta: n_head_kv        = 16
0.00.086.121 I llm_load_print_meta: n_rot            = 32
0.00.086.122 I llm_load_print_meta: n_swa            = 0
0.00.086.122 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.122 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.123 I llm_load_print_meta: n_gqa            = 1
0.00.086.124 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.125 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.125 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.125 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.126 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.126 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.126 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.126 I llm_load_print_meta: n_ff             = 8192
0.00.086.127 I llm_load_print_meta: n_expert         = 0
0.00.086.127 I llm_load_print_meta: n_expert_used    = 0
0.00.086.127 I llm_load_print_meta: causal attn      = 1
0.00.086.127 I llm_load_print_meta: pooling type     = 0
0.00.086.127 I llm_load_print_meta: rope type        = 2
0.00.086.127 I llm_load_print_meta: rope scaling     = linear
0.00.086.128 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.128 I llm_load_print_meta: freq_scale_train = 1
0.00.086.128 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.128 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.128 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.128 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.128 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.129 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.129 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.135 I llm_load_print_meta: model type       = 1.4B
0.00.086.137 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.137 I llm_load_print_meta: model params     = 1.41 B
0.00.086.138 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.138 I llm_load_print_meta: general.name     = 1.4B
0.00.086.138 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.138 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.138 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.139 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.139 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.139 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.139 I llm_load_print_meta: max token length = 1024
0.00.088.609 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.610 I llm_load_tensors: offloading output layer to GPU
0.00.088.610 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.620 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.621 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.746 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.747 I llama_new_context_with_model: n_ctx         = 128
0.00.089.748 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.748 I llama_new_context_with_model: n_batch       = 128
0.00.089.748 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.748 I llama_new_context_with_model: flash_attn    = 0
0.00.089.749 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.749 I llama_new_context_with_model: freq_scale    = 1
0.00.089.749 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.749 I ggml_metal_init: allocating
0.00.089.753 I ggml_metal_init: found device: Apple M4
0.00.089.755 I ggml_metal_init: picking default device: Apple M4
0.00.090.308 I ggml_metal_init: using embedded metal library
0.00.092.401 I ggml_metal_init: GPU name:   Apple M4
0.00.092.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.404 I ggml_metal_init: simdgroup reduction   = true
0.00.092.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.404 I ggml_metal_init: has bfloat            = true
0.00.092.405 I ggml_metal_init: use bfloat            = true
0.00.092.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.865 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.869 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.885 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.775 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.777 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.777 I llama_new_context_with_model: graph nodes  = 967
0.00.101.777 I llama_new_context_with_model: graph splits = 2
0.00.101.790 I 
0.00.101.821 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.101.822 I compute_imatrix: tokenizing the input ..
0.00.108.753 I compute_imatrix: tokenization took 6.93 ms
0.00.108.755 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.455.912 I compute_imatrix: 1.35 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.459.157 I llama_perf_context_print:        load time =    1432.81 ms
0.01.459.158 I llama_perf_context_print: prompt eval time =    1346.50 ms /   128 tokens (   10.52 ms per token,    95.06 tokens per second)
0.01.459.160 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.459.161 I llama_perf_context_print:       total time =    1436.05 ms /   129 tokens
0.01.459.733 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.167s
sys	0m0.224s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4231 (43957ef2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1532049d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1532051d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153205780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153205d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1532062e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153206890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153206e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1532073f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1532079a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153207ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1532083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1532088a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1532093c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153209b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15320a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15320aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15320b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15320b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15320c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15320c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15320cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15320d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15320dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15320e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15320ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15320efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15320f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153210230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153210770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153210a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153210ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153211190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153211a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153211f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153212220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1532126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153212b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153213000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1532134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153213940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153213de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153214280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153214720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153214bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153214e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153215490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153215aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1532163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1532169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153216fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1532175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153217c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153218210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153218820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153219010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1532194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153219950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153219c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15321a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15321aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15321acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15321b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15321b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15321bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15321bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15321c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15321c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15321cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15321d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15321d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15321db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15321dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15321e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15321e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15321ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15321f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15321f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15321fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153220010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1532204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153220950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153220df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153221290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153221730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153221bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153222070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153222510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1532229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153222e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1532232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153223790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153223c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1532240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153224570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153224a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153224eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153225350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1532160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1532259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153225e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1532262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153226780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153226c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1532270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153227560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153227a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153227ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153228340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1532287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153228c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153229120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1532295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153229a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153229f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15322a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15322a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15322ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15322b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15322b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15322bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15322bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15322c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15322c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15322cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15322d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15322d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15322db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15322dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15322e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15322e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15322eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15322f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15322f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15322fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153230020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1532304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153230960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153230e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1532312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153231740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153231be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153232080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153232520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1532329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153232e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153233300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1532337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153233c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1532340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153234580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153234a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153234ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153235360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1532358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153235e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153236350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1532368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153236b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153237170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153237780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153237d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1532383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1532389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1532391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153239640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153239ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153239f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15323a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15323ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15323b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15323b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15323bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15323c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15323c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15323cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15323d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15323d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15323dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15323e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15323e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15323ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15323f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15323f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15323fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153240180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1532406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153240c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153241170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1532416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153241c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153242160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1532426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153242c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153243150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1532436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153243bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153244140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153244690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153244be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153245130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153245680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153245bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153246120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153246670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153246bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153247110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153247660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153247bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153248100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153248650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153248ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1532490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153249640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153249b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15324a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15324a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15324ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15324b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15324b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15324bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15324c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15324c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15324cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15324d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15324d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15324d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15324de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15324e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15324e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15324ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15324f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15324f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15324fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15324fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153250390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153250830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153250cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153251220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153251940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153252060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153252780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153252ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153253160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153253770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153253d80 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.109.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15340a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15340aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15340b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15340b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15340b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15340be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15340c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15340c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15340cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15340d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15340d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15340db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15340e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15340ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15340f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15340fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153410450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153410b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153411290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153411a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153412180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1534128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153412fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1534136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153413e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1534140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153414380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1534147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153414c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1534150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1534155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153415ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153415f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153416210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153416680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153416af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153417050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153417550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153417a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153417f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153418450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153418950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153418e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153419350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153419850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153419cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15341a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15341a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15341aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15341ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15341b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15341b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15341bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15341c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15341c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15341cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15341d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15341d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15341d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15341e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15341e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15341eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15341efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15341f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15341f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15341fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153420240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1534206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153420b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153421020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1534214c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153421960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153421e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1534222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153422740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153422be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153423080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153423520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1534239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153423e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153424300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1534247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153424c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1534250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153425580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153425a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153425ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153426360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153426800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153426ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153427140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1534275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153427a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153427f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1534283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153428860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153428d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1534291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153429640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153429ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153429f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15342a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15342a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15342ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15342b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15342b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15342bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15342bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15342c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15342c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15342cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15342d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15342d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15342dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15342e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15342e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15342e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15342ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15342f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15342f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15342fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1534300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153430540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1534309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153430e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153431320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1534317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153431c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153432100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1534325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153432a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153432ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153433380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153433820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153433cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153434160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153434600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153434aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153434f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1534353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153435880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153435d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1534361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153436660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153436b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153436fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153437440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1534378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153437d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153438220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1534386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153438b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153439000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153439550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153439aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153439ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15343a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15343a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15343ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15343b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15343ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15343c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15343c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15343ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15343d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15343d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15343dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15343e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15343e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15343ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15343f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15343f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15343fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1534403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153440900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153440e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1534413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1534418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153441e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153442390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1534428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153442e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153443380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1534438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153443e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153444370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1534448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153444e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153445360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1534458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153445e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153446350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1534468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153446df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153447340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153447890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153447de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153448330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153448880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153448dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153449320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153449870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153449dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15344a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15344a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15344adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15344b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15344b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15344bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15344c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15344c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15344cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15344d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15344d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15344dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15344e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15344e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15344ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15344f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15344f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15344fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1534502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153450800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153450d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1534511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153451690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153451b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153451fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153452470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153452910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153452db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153453250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1534536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153453b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153454030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1534544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153454970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153454ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1534555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153455d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153456420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153456b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153456e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153457410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153457a20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15340a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15340aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15340b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15340b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15340b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15340be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15340c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15340c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15340cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15340d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15340d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15340da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15340e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15340eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15340f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15340f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1534100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153410790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153410e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153411800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153411ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1534125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153412cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1534133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153413ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153413f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153414390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153414800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153414c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1534150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153415550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1534159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153415e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1534160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153416560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1534169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153416e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1534172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153417720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153417b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153418000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153418470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1534188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153418d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1534191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153419630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153419aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153419f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15341a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15341a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15341ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15341b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15341b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15341b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15341be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15341c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15341c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15341cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15341cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15341d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15341d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15341dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15341e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15341e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15341ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15341eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15341f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15341f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15341fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1534200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153420520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153420990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153420e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153421270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1534216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153421b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153421fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153422430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1534228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153422d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153423180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1534235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153423a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153423ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153424340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1534247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153424c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153425090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153425500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153425970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153425de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153426250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1534266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153426b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153426fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153427410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153427880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153427cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153428160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1534285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153428a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153428eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153429320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153429790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153429c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15342a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15342a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15342a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15342adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15342b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15342b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15342bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15342bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15342c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15342c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15342ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15342d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15342d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15342da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15342de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15342e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15342e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15342ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15342f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15342f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15342f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15342fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153430210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153430680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153430af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153430f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1534313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153431840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153431cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153432120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153432590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153432a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153432e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1534332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153433750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153433bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153434030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1534344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153434910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153434d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1534351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153435660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153435ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153435f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1534363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153436820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153436c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153437100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153437570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1534379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153437e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1534382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153438730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153438ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153439010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153439480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1534398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153439d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15343a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15343a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15343aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15343af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15343b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15343bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15343bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15343c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15343c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15343ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15343d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15343d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15343da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15343de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15343e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15343e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15343ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15343f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15343f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15343f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15343fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153440210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153440680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153440af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153440f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1534413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153441840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153441cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153442120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153442590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153442a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153442e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1534432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153443750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153443bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153444030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1534444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153444910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153444d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1534451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153445660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153445ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153445f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1534463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153446820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153446c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153447100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153447570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1534479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153447e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1534482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153448730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153448ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153449010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153449480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1534498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153449d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15344a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15344a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15344aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15344af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15344b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15344b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15344bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15344c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15344c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15344c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15344ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15344d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15344d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15344db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15344dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15344e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15344e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15344ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15344f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15344fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153450210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153450900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153450d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1534511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153451650 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.752s
user	0m0.272s
sys	0m0.276s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4231 (43957ef2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c60b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c60ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c60c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c60c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c60cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c60d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c60d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c60dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c60e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c60e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c60ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c60f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c60fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c610420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c610c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c611350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c611a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c612190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c6128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c613080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c6137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c613ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c6145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c614e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c6155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c615860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c615e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c616ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c617020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c6172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c617780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c617a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c6182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c618810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c618ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c618f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c619410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c6198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c619d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c61a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c61a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c61ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c61afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c61b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c61b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c61bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c61c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c61cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c61d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c61d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c61dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c61e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c61eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c61f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c61f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c61fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c620200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c6204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c620ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c6212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c621580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c621a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c621ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c622360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c622800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c622ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c623140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c6235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c623a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c6243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c624860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c624d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c6251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c625640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c625ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c625f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c626420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c6268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c626d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c627200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c6276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c627b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c627fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c628480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c628dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c629260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c629700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c629ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c62a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c62a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c62a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c62ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c62b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c62b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c62bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c61c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c62c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c62c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c62cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c62d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c62d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c62d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c62de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c62e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c62e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c62ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c62f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c62f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c62f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c62fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c630310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c6307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c6310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c6335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c6343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c6351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c6368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c6376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c63a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c63a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c63a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c63ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c63b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c63b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c63bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c63c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c63c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c63cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c63d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c63d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c63da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c63e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c63e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c63ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c63f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c63fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c63fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c640390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c640830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c640fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c641530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c641a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c641fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c642520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c642a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c642fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c643510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c643a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c643fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c644500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c644a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c644fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c6454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c645a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c6464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c646a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c646f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c6474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c647a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c647f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c6484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c648a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c6494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c649a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c649f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c64a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c64a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c64af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c64b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c64b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c64bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c64c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c64c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c64cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c64d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c64d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c64df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c64e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c64e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c64ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c64f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c64f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c64fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c650440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c650990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c650ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c651430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c651980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c651ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c652420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c652970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c652ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c653410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c653960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c653e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c6542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c654740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c654be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c655080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c655520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c6559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c655e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c656300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c6567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c656c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c6570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c657580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c657ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c6581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c658910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c659030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c659750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c659a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c65a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c65a630 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.083.842 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d8090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d80ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d80b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d80bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d80c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d80cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d80d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d80d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d80e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d80e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d80eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d80ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d80f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d80f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d80fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d8101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d8111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d8123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d8130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d8139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d8142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d8158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d8161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d8170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d8186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d81a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d81aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d81aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d81b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d81b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d81bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d81c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d81c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d81c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d81cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d81d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d81db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d81df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d81e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d81e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d81ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d81f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d81f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d81fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d81fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d8214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d8233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d8252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d8264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d8283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d8299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d82a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d82a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d82abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d82b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d82b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d82b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d82bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d82c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d82c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d82cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d82cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d82d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d82d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d82dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d82e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d82e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d82e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d82ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d82f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d82f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d82fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d8308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d8311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d8327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d8330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d8339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d835450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d835fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d8362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d836560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d8369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d836e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d8372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d837720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d837b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d838000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d838470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d8388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d838d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d8391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d839630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d839aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d839f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d83a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d83a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d83ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d83b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d83b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d83b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d83be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d83c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d83c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d83cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d83cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d83d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d83d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d83dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d83e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d83e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d83ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d83eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d83f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d83f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d83fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d8400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d840520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d840990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d840e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d841270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d8416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d841b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d841fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d842430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d8428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d842d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d843180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d8435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d843a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d843ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d844340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d8447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d844c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d845090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d845500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d845970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d845de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d8466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d846b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d846fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d847410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d847880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d847cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d848160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d8485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d848a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d848eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d849320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d849e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d84a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d84aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d84b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d84b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d84b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d84bdb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d807d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d808610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d808d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d809570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d809c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d80a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d80aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d80b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d80bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d80c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d80c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d80cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d80d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d80dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d80e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d80e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d80eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d80ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d80f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d80f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d80fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d8100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d8103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d810810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d810c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d8110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d811560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d8119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d811e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d8122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d812720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d812b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d813000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d813470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d8138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d813d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d8141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d814630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d814aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d814f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d815380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d8157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d815c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d8160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d816540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d8169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d816e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d817290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d817700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d817b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d817fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d818450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d8188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d818d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d8191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d819610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d819a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d819ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d81a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d81a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d81ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d81b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d81b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d81b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d81be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d81c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d81c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d81cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d81cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d81d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d81d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d81dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d81e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d81e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d81ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d81eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d81f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d81f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d81fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d820090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d820500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d820970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d820de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d821250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d8216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d821b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d821fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d822410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d822880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d822cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d823160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d8235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d823a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d823eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d824320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d824790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d824c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d825070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d8254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d825950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d825dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d826230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d8266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d826b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d826f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d8273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d827860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d827cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d828140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d8285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d828a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d828e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d829300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d829770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d829be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d82a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d82a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d82a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d82ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d82b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d82b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d82baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d82bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d82c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d82c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d82ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d82d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d82d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d82da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d82de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d82e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d82e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d82ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d82f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d82f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d82f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d82fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d8301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d830660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d830ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d830f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d8313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d831820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d831c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d832100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d832570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d8329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d832e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d8332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d833730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d833ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d834010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d834480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d8348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d834d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d8351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d835950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d835dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d836230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d8366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d836b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d836f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d8373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d837860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d837cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d838140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d8385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d838a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d838e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d839300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d839770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d839be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d83a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d83a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d83a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d83ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d83b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d83b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d83baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d83bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d83c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d83c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d83ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d83d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d83d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d83da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d83de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d83e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d83e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d83ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d83f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d83f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d83f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d83fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d8401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d840660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d840ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d840f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d8413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d841820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d841c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d842100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d842570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d8429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d8432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d843730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d843ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d844010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d844480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d8448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d844d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d8451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d845640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d845ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d845f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d846390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d846800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d846c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d8470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d847550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d8479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d847e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d8482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d848710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d848b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d848ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d8496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d849dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d84a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d84abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d84b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d84b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d84b900 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.948s
user	0m0.240s
sys	0m0.153s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
