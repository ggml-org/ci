Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.620s
user	0m0.938s
sys	0m1.264s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  6%] Built target sha256
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library ../../bin/libggml.dylib
[ 15%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking C executable ../bin/test-c
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 36%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX executable ../../bin/llama-simple
[ 36%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-chat
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Built target test-grammar-parser
[ 51%] Built target test-llama-grammar
[ 51%] Built target test-chat
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-grammar-integration
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../bin/test-barrier
[ 63%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-gguf
[ 63%] Built target test-arg-parser
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-backend-ops
[ 64%] Built target test-chat-template
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Built target test-quantize-perf
[ 66%] Built target test-barrier
[ 66%] Built target test-rope
[ 66%] Built target test-model-load-cancel
[ 66%] Built target test-autorelease
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-eval-callback
[ 74%] Built target llama-embedding
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-batched
[ 74%] Built target llama-gbnf-validator
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-bench
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-cli
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Built target llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookahead
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-merge
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-perplexity
[ 82%] Built target llama-cli
[ 82%] Built target llama-parallel
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-passkey
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Built target llama-quantize
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-run
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-retrieval
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-speculative
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-gen-docs
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.111s
user	0m6.449s
sys	0m10.050s

main: quantize time =  2176.61 ms
main:    total time =  2176.61 ms

main: quantize time =  1356.69 ms
main:    total time =  1356.69 ms

main: quantize time =  4081.15 ms
main:    total time =  4081.15 ms

main: quantize time =  1324.71 ms
main:    total time =  1324.71 ms

main: quantize time =  1406.67 ms
main:    total time =  1406.67 ms

main: quantize time =  4912.40 ms
main:    total time =  4912.40 ms

main: quantize time =  5597.15 ms
main:    total time =  5597.15 ms

main: quantize time =  6595.23 ms
main:    total time =  6595.23 ms

main: quantize time =  5987.10 ms
main:    total time =  5987.10 ms

main: quantize time =  4661.63 ms
main:    total time =  4661.63 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.201 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.352 I main: llama backend init
0.00.000.360 I main: load the model and apply lora adapter, if any
0.00.051.767 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.064.687 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.064.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.064.711 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.064.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.064.713 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.064.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.064.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.064.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.064.717 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.064.718 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.064.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.064.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.064.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.064.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.064.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.064.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.064.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.071.747 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.073.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.083.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.083.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.083.093 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.083.094 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.083.094 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.083.095 I llama_model_loader: - type  f32:  194 tensors
0.00.083.096 I llama_model_loader: - type  f16:   98 tensors
0.00.083.105 I print_info: file format = GGUF V3 (latest)
0.00.083.107 I print_info: file type   = all F32 (guessed)
0.00.083.109 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.099.653 I load: special tokens cache size = 25
0.00.109.410 I load: token to piece cache size = 0.2984 MB
0.00.109.413 I print_info: arch             = gptneox
0.00.109.414 I print_info: vocab_only       = 0
0.00.109.414 I print_info: n_ctx_train      = 2048
0.00.109.414 I print_info: n_embd           = 2048
0.00.109.414 I print_info: n_layer          = 24
0.00.109.419 I print_info: n_head           = 16
0.00.109.420 I print_info: n_head_kv        = 16
0.00.109.420 I print_info: n_rot            = 32
0.00.109.421 I print_info: n_swa            = 0
0.00.109.421 I print_info: n_embd_head_k    = 128
0.00.109.421 I print_info: n_embd_head_v    = 128
0.00.109.422 I print_info: n_gqa            = 1
0.00.109.423 I print_info: n_embd_k_gqa     = 2048
0.00.109.424 I print_info: n_embd_v_gqa     = 2048
0.00.109.424 I print_info: f_norm_eps       = 1.0e-05
0.00.109.425 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.109.425 I print_info: f_clamp_kqv      = 0.0e+00
0.00.109.425 I print_info: f_max_alibi_bias = 0.0e+00
0.00.109.425 I print_info: f_logit_scale    = 0.0e+00
0.00.109.426 I print_info: n_ff             = 8192
0.00.109.426 I print_info: n_expert         = 0
0.00.109.427 I print_info: n_expert_used    = 0
0.00.109.427 I print_info: causal attn      = 1
0.00.109.427 I print_info: pooling type     = 0
0.00.109.427 I print_info: rope type        = 2
0.00.109.427 I print_info: rope scaling     = linear
0.00.109.431 I print_info: freq_base_train  = 10000.0
0.00.109.431 I print_info: freq_scale_train = 1
0.00.109.431 I print_info: n_ctx_orig_yarn  = 2048
0.00.109.432 I print_info: rope_finetuned   = unknown
0.00.109.432 I print_info: ssm_d_conv       = 0
0.00.109.432 I print_info: ssm_d_inner      = 0
0.00.109.432 I print_info: ssm_d_state      = 0
0.00.109.432 I print_info: ssm_dt_rank      = 0
0.00.109.432 I print_info: ssm_dt_b_c_rms   = 0
0.00.109.433 I print_info: model type       = 1.4B
0.00.109.433 I print_info: model params     = 1.41 B
0.00.109.433 I print_info: general.name     = 1.4B
0.00.109.434 I print_info: vocab type       = BPE
0.00.109.434 I print_info: n_vocab          = 50304
0.00.109.434 I print_info: n_merges         = 50009
0.00.109.434 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.109.435 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.109.435 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.109.435 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.109.435 I print_info: LF token         = 187 'Ċ'
0.00.109.436 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.109.436 I print_info: max token length = 1024
0.00.152.527 I load_tensors: offloading 24 repeating layers to GPU
0.00.152.529 I load_tensors: offloading output layer to GPU
0.00.152.530 I load_tensors: offloaded 25/25 layers to GPU
0.00.152.555 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.152.556 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.153.080 I llama_init_from_model: n_seq_max     = 1
0.00.153.081 I llama_init_from_model: n_ctx         = 2048
0.00.153.081 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.153.081 I llama_init_from_model: n_batch       = 2048
0.00.153.082 I llama_init_from_model: n_ubatch      = 512
0.00.153.082 I llama_init_from_model: flash_attn    = 0
0.00.153.082 I llama_init_from_model: freq_base     = 10000.0
0.00.153.083 I llama_init_from_model: freq_scale    = 1
0.00.153.083 I ggml_metal_init: allocating
0.00.153.103 I ggml_metal_init: found device: Apple M4
0.00.153.110 I ggml_metal_init: picking default device: Apple M4
0.00.153.688 I ggml_metal_init: using embedded metal library
0.00.196.452 I ggml_metal_init: GPU name:   Apple M4
0.00.196.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.196.455 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.196.455 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.196.455 I ggml_metal_init: simdgroup reduction   = true
0.00.196.456 I ggml_metal_init: simdgroup matrix mul. = true
0.00.196.456 I ggml_metal_init: has residency sets    = true
0.00.196.456 I ggml_metal_init: has bfloat            = true
0.00.196.456 I ggml_metal_init: use bfloat            = true
0.00.196.456 I ggml_metal_init: hasUnifiedMemory      = true
0.00.196.457 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.122 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.382.947 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.382.953 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.382.977 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.387.017 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.387.019 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.387.019 I llama_init_from_model: graph nodes  = 967
0.00.387.020 I llama_init_from_model: graph splits = 2
0.00.387.024 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.387.160 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.387.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.454.963 I main: llama threadpool init, n_threads = 4
0.00.455.006 I 
0.00.455.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.455.039 I 
0.00.455.237 I sampler seed: 1234
0.00.455.244 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.455.270 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.455.271 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.455.271 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.293.134 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.02.293.135 I llama_perf_context_print:        load time =     402.02 ms
0.02.293.135 I llama_perf_context_print: prompt eval time =      44.32 ms /     7 tokens (    6.33 ms per token,   157.95 tokens per second)
0.02.293.136 I llama_perf_context_print:        eval time =    1790.56 ms /    63 runs   (   28.42 ms per token,    35.18 tokens per second)
0.02.293.136 I llama_perf_context_print:       total time =    1839.34 ms /    70 tokens
0.02.293.361 I ggml_metal_free: deallocating

real	0m2.617s
user	0m0.145s
sys	0m0.147s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.106 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.114 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.115 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.116 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.118 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.119 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.119 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.120 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.120 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.123 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.125 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.813 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.815 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.816 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.816 I llama_model_loader: - type  f32:  194 tensors
0.00.033.816 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.817 I print_info: file format = GGUF V3 (latest)
0.00.033.818 I print_info: file type   = Q8_0
0.00.033.819 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.119 I load: special tokens cache size = 25
0.00.048.234 I load: token to piece cache size = 0.2984 MB
0.00.048.238 I print_info: arch             = gptneox
0.00.048.239 I print_info: vocab_only       = 0
0.00.048.239 I print_info: n_ctx_train      = 2048
0.00.048.239 I print_info: n_embd           = 2048
0.00.048.242 I print_info: n_layer          = 24
0.00.048.247 I print_info: n_head           = 16
0.00.048.248 I print_info: n_head_kv        = 16
0.00.048.250 I print_info: n_rot            = 32
0.00.048.250 I print_info: n_swa            = 0
0.00.048.250 I print_info: n_embd_head_k    = 128
0.00.048.250 I print_info: n_embd_head_v    = 128
0.00.048.251 I print_info: n_gqa            = 1
0.00.048.252 I print_info: n_embd_k_gqa     = 2048
0.00.048.253 I print_info: n_embd_v_gqa     = 2048
0.00.048.253 I print_info: f_norm_eps       = 1.0e-05
0.00.048.253 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.254 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.254 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.258 I print_info: f_logit_scale    = 0.0e+00
0.00.048.260 I print_info: n_ff             = 8192
0.00.048.260 I print_info: n_expert         = 0
0.00.048.260 I print_info: n_expert_used    = 0
0.00.048.260 I print_info: causal attn      = 1
0.00.048.260 I print_info: pooling type     = 0
0.00.048.260 I print_info: rope type        = 2
0.00.048.261 I print_info: rope scaling     = linear
0.00.048.264 I print_info: freq_base_train  = 10000.0
0.00.048.264 I print_info: freq_scale_train = 1
0.00.048.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.265 I print_info: rope_finetuned   = unknown
0.00.048.266 I print_info: ssm_d_conv       = 0
0.00.048.266 I print_info: ssm_d_inner      = 0
0.00.048.267 I print_info: ssm_d_state      = 0
0.00.048.267 I print_info: ssm_dt_rank      = 0
0.00.048.267 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.267 I print_info: model type       = 1.4B
0.00.048.268 I print_info: model params     = 1.41 B
0.00.048.269 I print_info: general.name     = 1.4B
0.00.048.269 I print_info: vocab type       = BPE
0.00.048.271 I print_info: n_vocab          = 50304
0.00.048.271 I print_info: n_merges         = 50009
0.00.048.271 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.271 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.271 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.272 I print_info: LF token         = 187 'Ċ'
0.00.048.272 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.272 I print_info: max token length = 1024
0.01.219.649 I load_tensors: offloading 24 repeating layers to GPU
0.01.219.655 I load_tensors: offloading output layer to GPU
0.01.219.656 I load_tensors: offloaded 25/25 layers to GPU
0.01.219.684 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.219.686 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.220.419 I llama_init_from_model: n_seq_max     = 1
0.01.220.421 I llama_init_from_model: n_ctx         = 2048
0.01.220.425 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.220.426 I llama_init_from_model: n_batch       = 2048
0.01.220.426 I llama_init_from_model: n_ubatch      = 512
0.01.220.426 I llama_init_from_model: flash_attn    = 0
0.01.220.427 I llama_init_from_model: freq_base     = 10000.0
0.01.220.431 I llama_init_from_model: freq_scale    = 1
0.01.220.433 I ggml_metal_init: allocating
0.01.220.480 I ggml_metal_init: found device: Apple M4
0.01.220.489 I ggml_metal_init: picking default device: Apple M4
0.01.221.821 I ggml_metal_init: using embedded metal library
0.01.227.544 I ggml_metal_init: GPU name:   Apple M4
0.01.227.548 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.227.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.227.550 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.227.550 I ggml_metal_init: simdgroup reduction   = true
0.01.227.550 I ggml_metal_init: simdgroup matrix mul. = true
0.01.227.551 I ggml_metal_init: has residency sets    = true
0.01.227.551 I ggml_metal_init: has bfloat            = true
0.01.227.551 I ggml_metal_init: use bfloat            = true
0.01.227.552 I ggml_metal_init: hasUnifiedMemory      = true
0.01.227.561 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.243.256 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.299.968 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.299.975 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.299.998 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.304.611 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.304.613 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.304.614 I llama_init_from_model: graph nodes  = 967
0.01.304.614 I llama_init_from_model: graph splits = 2
0.01.304.620 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.304.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.304.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.352.105 I main: llama threadpool init, n_threads = 4
0.01.352.143 I 
0.01.352.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.352.165 I 
0.01.352.284 I sampler seed: 1234
0.01.352.289 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.352.325 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.352.328 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.352.329 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.448.307 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.02.448.308 I llama_perf_context_print:        load time =    1341.31 ms
0.02.448.308 I llama_perf_context_print: prompt eval time =      49.50 ms /     7 tokens (    7.07 ms per token,   141.42 tokens per second)
0.02.448.310 I llama_perf_context_print:        eval time =    1043.80 ms /    63 runs   (   16.57 ms per token,    60.36 tokens per second)
0.02.448.310 I llama_perf_context_print:       total time =    1097.11 ms /    70 tokens
0.02.448.578 I ggml_metal_free: deallocating

real	0m2.469s
user	0m0.107s
sys	0m0.256s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.016.548 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.542 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.543 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.545 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.547 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.547 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.547 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.548 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.548 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.550 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.550 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.551 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.014 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.534 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.536 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.536 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.537 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.538 I llama_model_loader: - type  f32:  194 tensors
0.00.045.538 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.538 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.543 I print_info: file format = GGUF V3 (latest)
0.00.045.544 I print_info: file type   = Q4_0
0.00.045.545 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.055.472 I load: special tokens cache size = 25
0.00.063.342 I load: token to piece cache size = 0.2984 MB
0.00.063.345 I print_info: arch             = gptneox
0.00.063.345 I print_info: vocab_only       = 0
0.00.063.345 I print_info: n_ctx_train      = 2048
0.00.063.346 I print_info: n_embd           = 2048
0.00.063.346 I print_info: n_layer          = 24
0.00.063.350 I print_info: n_head           = 16
0.00.063.353 I print_info: n_head_kv        = 16
0.00.063.353 I print_info: n_rot            = 32
0.00.063.353 I print_info: n_swa            = 0
0.00.063.353 I print_info: n_embd_head_k    = 128
0.00.063.354 I print_info: n_embd_head_v    = 128
0.00.063.354 I print_info: n_gqa            = 1
0.00.063.355 I print_info: n_embd_k_gqa     = 2048
0.00.063.356 I print_info: n_embd_v_gqa     = 2048
0.00.063.356 I print_info: f_norm_eps       = 1.0e-05
0.00.063.357 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.357 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.357 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.357 I print_info: f_logit_scale    = 0.0e+00
0.00.063.358 I print_info: n_ff             = 8192
0.00.063.358 I print_info: n_expert         = 0
0.00.063.358 I print_info: n_expert_used    = 0
0.00.063.359 I print_info: causal attn      = 1
0.00.063.359 I print_info: pooling type     = 0
0.00.063.359 I print_info: rope type        = 2
0.00.063.359 I print_info: rope scaling     = linear
0.00.063.360 I print_info: freq_base_train  = 10000.0
0.00.063.360 I print_info: freq_scale_train = 1
0.00.063.360 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.360 I print_info: rope_finetuned   = unknown
0.00.063.361 I print_info: ssm_d_conv       = 0
0.00.063.361 I print_info: ssm_d_inner      = 0
0.00.063.361 I print_info: ssm_d_state      = 0
0.00.063.361 I print_info: ssm_dt_rank      = 0
0.00.063.361 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.362 I print_info: model type       = 1.4B
0.00.063.362 I print_info: model params     = 1.41 B
0.00.063.362 I print_info: general.name     = 1.4B
0.00.063.363 I print_info: vocab type       = BPE
0.00.063.363 I print_info: n_vocab          = 50304
0.00.063.363 I print_info: n_merges         = 50009
0.00.063.368 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.368 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.368 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.369 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.369 I print_info: LF token         = 187 'Ċ'
0.00.063.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.371 I print_info: max token length = 1024
0.00.628.244 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.256 I load_tensors: offloading output layer to GPU
0.00.628.257 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.287 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.628.288 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.629.688 I llama_init_from_model: n_seq_max     = 1
0.00.629.696 I llama_init_from_model: n_ctx         = 2048
0.00.629.696 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.629.697 I llama_init_from_model: n_batch       = 2048
0.00.629.697 I llama_init_from_model: n_ubatch      = 512
0.00.629.698 I llama_init_from_model: flash_attn    = 0
0.00.629.698 I llama_init_from_model: freq_base     = 10000.0
0.00.629.699 I llama_init_from_model: freq_scale    = 1
0.00.629.701 I ggml_metal_init: allocating
0.00.629.751 I ggml_metal_init: found device: Apple M4
0.00.629.780 I ggml_metal_init: picking default device: Apple M4
0.00.631.389 I ggml_metal_init: using embedded metal library
0.00.637.040 I ggml_metal_init: GPU name:   Apple M4
0.00.637.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.048 I ggml_metal_init: simdgroup reduction   = true
0.00.637.049 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.049 I ggml_metal_init: has residency sets    = true
0.00.637.049 I ggml_metal_init: has bfloat            = true
0.00.637.049 I ggml_metal_init: use bfloat            = true
0.00.637.050 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.052 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.102 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.075 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.083 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.108 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.277 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.279 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.280 I llama_init_from_model: graph nodes  = 967
0.00.720.280 I llama_init_from_model: graph splits = 2
0.00.720.284 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.513 I main: llama threadpool init, n_threads = 4
0.00.778.557 I 
0.00.778.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.582 I 
0.00.778.745 I sampler seed: 1234
0.00.778.749 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.790 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.793 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.793 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.465.260 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46802.90 tokens per second)
0.01.465.261 I llama_perf_context_print:        load time =     761.05 ms
0.01.465.261 I llama_perf_context_print: prompt eval time =      49.30 ms /     7 tokens (    7.04 ms per token,   141.98 tokens per second)
0.01.465.263 I llama_perf_context_print:        eval time =     634.60 ms /    63 runs   (   10.07 ms per token,    99.28 tokens per second)
0.01.465.264 I llama_perf_context_print:       total time =     687.66 ms /    70 tokens
0.01.465.495 I ggml_metal_free: deallocating

real	0m1.493s
user	0m0.117s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.916 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.065 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.076 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.076 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.076 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.077 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.078 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.078 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.078 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.079 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.079 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.079 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.080 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.081 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.082 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.082 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.876 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.610 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.610 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.611 I llama_model_loader: - type  f32:  194 tensors
0.00.034.611 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.611 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.612 I print_info: file format = GGUF V3 (latest)
0.00.034.612 I print_info: file type   = Q4_1
0.00.034.613 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.755 I load: special tokens cache size = 25
0.00.049.742 I load: token to piece cache size = 0.2984 MB
0.00.049.745 I print_info: arch             = gptneox
0.00.049.745 I print_info: vocab_only       = 0
0.00.049.745 I print_info: n_ctx_train      = 2048
0.00.049.745 I print_info: n_embd           = 2048
0.00.049.745 I print_info: n_layer          = 24
0.00.049.748 I print_info: n_head           = 16
0.00.049.749 I print_info: n_head_kv        = 16
0.00.049.749 I print_info: n_rot            = 32
0.00.049.749 I print_info: n_swa            = 0
0.00.049.750 I print_info: n_embd_head_k    = 128
0.00.049.751 I print_info: n_embd_head_v    = 128
0.00.049.751 I print_info: n_gqa            = 1
0.00.049.752 I print_info: n_embd_k_gqa     = 2048
0.00.049.753 I print_info: n_embd_v_gqa     = 2048
0.00.049.753 I print_info: f_norm_eps       = 1.0e-05
0.00.049.754 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.755 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.756 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.756 I print_info: f_logit_scale    = 0.0e+00
0.00.049.756 I print_info: n_ff             = 8192
0.00.049.757 I print_info: n_expert         = 0
0.00.049.757 I print_info: n_expert_used    = 0
0.00.049.757 I print_info: causal attn      = 1
0.00.049.757 I print_info: pooling type     = 0
0.00.049.757 I print_info: rope type        = 2
0.00.049.757 I print_info: rope scaling     = linear
0.00.049.758 I print_info: freq_base_train  = 10000.0
0.00.049.758 I print_info: freq_scale_train = 1
0.00.049.758 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.760 I print_info: rope_finetuned   = unknown
0.00.049.760 I print_info: ssm_d_conv       = 0
0.00.049.760 I print_info: ssm_d_inner      = 0
0.00.049.760 I print_info: ssm_d_state      = 0
0.00.049.760 I print_info: ssm_dt_rank      = 0
0.00.049.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.761 I print_info: model type       = 1.4B
0.00.049.761 I print_info: model params     = 1.41 B
0.00.049.761 I print_info: general.name     = 1.4B
0.00.049.762 I print_info: vocab type       = BPE
0.00.049.762 I print_info: n_vocab          = 50304
0.00.049.762 I print_info: n_merges         = 50009
0.00.049.762 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.763 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.763 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.763 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.763 I print_info: LF token         = 187 'Ċ'
0.00.049.764 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.764 I print_info: max token length = 1024
0.00.747.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.747.783 I load_tensors: offloading output layer to GPU
0.00.747.783 I load_tensors: offloaded 25/25 layers to GPU
0.00.747.809 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.747.810 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.749.035 I llama_init_from_model: n_seq_max     = 1
0.00.749.043 I llama_init_from_model: n_ctx         = 2048
0.00.749.043 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.749.043 I llama_init_from_model: n_batch       = 2048
0.00.749.044 I llama_init_from_model: n_ubatch      = 512
0.00.749.044 I llama_init_from_model: flash_attn    = 0
0.00.749.047 I llama_init_from_model: freq_base     = 10000.0
0.00.749.047 I llama_init_from_model: freq_scale    = 1
0.00.749.055 I ggml_metal_init: allocating
0.00.749.117 I ggml_metal_init: found device: Apple M4
0.00.749.131 I ggml_metal_init: picking default device: Apple M4
0.00.750.746 I ggml_metal_init: using embedded metal library
0.00.756.131 I ggml_metal_init: GPU name:   Apple M4
0.00.756.143 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.756.144 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.756.145 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.756.146 I ggml_metal_init: simdgroup reduction   = true
0.00.756.146 I ggml_metal_init: simdgroup matrix mul. = true
0.00.756.146 I ggml_metal_init: has residency sets    = true
0.00.756.146 I ggml_metal_init: has bfloat            = true
0.00.756.146 I ggml_metal_init: use bfloat            = true
0.00.756.148 I ggml_metal_init: hasUnifiedMemory      = true
0.00.756.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.776.408 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.838.652 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.838.658 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.838.682 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.843.483 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.843.484 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.843.484 I llama_init_from_model: graph nodes  = 967
0.00.843.485 I llama_init_from_model: graph splits = 2
0.00.843.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.843.618 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.843.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.900.099 I main: llama threadpool init, n_threads = 4
0.00.900.141 I 
0.00.900.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.900.167 I 
0.00.900.343 I sampler seed: 1234
0.00.900.349 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.900.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.900.363 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.900.364 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.632.670 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.01.632.671 I llama_perf_context_print:        load time =     890.24 ms
0.01.632.672 I llama_perf_context_print: prompt eval time =      48.78 ms /     7 tokens (    6.97 ms per token,   143.50 tokens per second)
0.01.632.672 I llama_perf_context_print:        eval time =     680.78 ms /    63 runs   (   10.81 ms per token,    92.54 tokens per second)
0.01.632.674 I llama_perf_context_print:       total time =     733.51 ms /    70 tokens
0.01.632.891 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.111s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.722 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.284 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.996 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.999 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.999 I llama_model_loader: - type  f32:  194 tensors
0.00.024.000 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.000 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.001 I print_info: file format = GGUF V3 (latest)
0.00.024.001 I print_info: file type   = Q5_0
0.00.024.002 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.031.839 I load: special tokens cache size = 25
0.00.037.825 I load: token to piece cache size = 0.2984 MB
0.00.037.828 I print_info: arch             = gptneox
0.00.037.828 I print_info: vocab_only       = 0
0.00.037.829 I print_info: n_ctx_train      = 2048
0.00.037.829 I print_info: n_embd           = 2048
0.00.037.829 I print_info: n_layer          = 24
0.00.037.832 I print_info: n_head           = 16
0.00.037.833 I print_info: n_head_kv        = 16
0.00.037.833 I print_info: n_rot            = 32
0.00.037.833 I print_info: n_swa            = 0
0.00.037.834 I print_info: n_embd_head_k    = 128
0.00.037.836 I print_info: n_embd_head_v    = 128
0.00.037.837 I print_info: n_gqa            = 1
0.00.037.838 I print_info: n_embd_k_gqa     = 2048
0.00.037.838 I print_info: n_embd_v_gqa     = 2048
0.00.037.839 I print_info: f_norm_eps       = 1.0e-05
0.00.037.839 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.839 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.840 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.840 I print_info: f_logit_scale    = 0.0e+00
0.00.037.841 I print_info: n_ff             = 8192
0.00.037.841 I print_info: n_expert         = 0
0.00.037.841 I print_info: n_expert_used    = 0
0.00.037.841 I print_info: causal attn      = 1
0.00.037.841 I print_info: pooling type     = 0
0.00.037.841 I print_info: rope type        = 2
0.00.037.842 I print_info: rope scaling     = linear
0.00.037.842 I print_info: freq_base_train  = 10000.0
0.00.037.842 I print_info: freq_scale_train = 1
0.00.037.843 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.843 I print_info: rope_finetuned   = unknown
0.00.037.843 I print_info: ssm_d_conv       = 0
0.00.037.843 I print_info: ssm_d_inner      = 0
0.00.037.844 I print_info: ssm_d_state      = 0
0.00.037.845 I print_info: ssm_dt_rank      = 0
0.00.037.845 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.845 I print_info: model type       = 1.4B
0.00.037.845 I print_info: model params     = 1.41 B
0.00.037.845 I print_info: general.name     = 1.4B
0.00.037.846 I print_info: vocab type       = BPE
0.00.037.846 I print_info: n_vocab          = 50304
0.00.037.846 I print_info: n_merges         = 50009
0.00.037.847 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.847 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.847 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.847 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.851 I print_info: LF token         = 187 'Ċ'
0.00.037.851 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.852 I print_info: max token length = 1024
0.00.816.367 I load_tensors: offloading 24 repeating layers to GPU
0.00.816.381 I load_tensors: offloading output layer to GPU
0.00.816.382 I load_tensors: offloaded 25/25 layers to GPU
0.00.816.413 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.816.415 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.817.863 I llama_init_from_model: n_seq_max     = 1
0.00.817.870 I llama_init_from_model: n_ctx         = 2048
0.00.817.870 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.817.870 I llama_init_from_model: n_batch       = 2048
0.00.817.871 I llama_init_from_model: n_ubatch      = 512
0.00.817.871 I llama_init_from_model: flash_attn    = 0
0.00.817.873 I llama_init_from_model: freq_base     = 10000.0
0.00.817.874 I llama_init_from_model: freq_scale    = 1
0.00.817.880 I ggml_metal_init: allocating
0.00.817.987 I ggml_metal_init: found device: Apple M4
0.00.818.001 I ggml_metal_init: picking default device: Apple M4
0.00.819.797 I ggml_metal_init: using embedded metal library
0.00.826.444 I ggml_metal_init: GPU name:   Apple M4
0.00.826.449 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.826.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.826.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.826.451 I ggml_metal_init: simdgroup reduction   = true
0.00.826.451 I ggml_metal_init: simdgroup matrix mul. = true
0.00.826.452 I ggml_metal_init: has residency sets    = true
0.00.826.452 I ggml_metal_init: has bfloat            = true
0.00.826.452 I ggml_metal_init: use bfloat            = true
0.00.826.453 I ggml_metal_init: hasUnifiedMemory      = true
0.00.826.462 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.843.916 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.896.988 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.896.996 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.897.020 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.901.830 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.901.832 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.901.832 I llama_init_from_model: graph nodes  = 967
0.00.901.832 I llama_init_from_model: graph splits = 2
0.00.901.837 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.901.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.901.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.962.546 I main: llama threadpool init, n_threads = 4
0.00.962.582 I 
0.00.962.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.962.606 I 
0.00.962.782 I sampler seed: 1234
0.00.962.786 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.962.797 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.962.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.962.797 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.764.856 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.764.857 I llama_perf_context_print:        load time =     952.84 ms
0.01.764.858 I llama_perf_context_print: prompt eval time =      53.93 ms /     7 tokens (    7.70 ms per token,   129.80 tokens per second)
0.01.764.858 I llama_perf_context_print:        eval time =     745.31 ms /    63 runs   (   11.83 ms per token,    84.53 tokens per second)
0.01.764.859 I llama_perf_context_print:       total time =     803.29 ms /    70 tokens
0.01.765.130 I ggml_metal_free: deallocating

real	0m1.784s
user	0m0.108s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.666 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.336 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.342 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.343 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.344 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.345 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.347 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.349 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.351 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.163 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.936 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.938 I llama_model_loader: - type  f32:  194 tensors
0.00.025.938 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.939 I print_info: file format = GGUF V3 (latest)
0.00.025.940 I print_info: file type   = Q5_1
0.00.025.941 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.085 I load: special tokens cache size = 25
0.00.040.171 I load: token to piece cache size = 0.2984 MB
0.00.040.174 I print_info: arch             = gptneox
0.00.040.174 I print_info: vocab_only       = 0
0.00.040.175 I print_info: n_ctx_train      = 2048
0.00.040.175 I print_info: n_embd           = 2048
0.00.040.175 I print_info: n_layer          = 24
0.00.040.178 I print_info: n_head           = 16
0.00.040.178 I print_info: n_head_kv        = 16
0.00.040.178 I print_info: n_rot            = 32
0.00.040.179 I print_info: n_swa            = 0
0.00.040.179 I print_info: n_embd_head_k    = 128
0.00.040.179 I print_info: n_embd_head_v    = 128
0.00.040.182 I print_info: n_gqa            = 1
0.00.040.183 I print_info: n_embd_k_gqa     = 2048
0.00.040.183 I print_info: n_embd_v_gqa     = 2048
0.00.040.184 I print_info: f_norm_eps       = 1.0e-05
0.00.040.186 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.186 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.187 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.187 I print_info: f_logit_scale    = 0.0e+00
0.00.040.187 I print_info: n_ff             = 8192
0.00.040.187 I print_info: n_expert         = 0
0.00.040.188 I print_info: n_expert_used    = 0
0.00.040.188 I print_info: causal attn      = 1
0.00.040.188 I print_info: pooling type     = 0
0.00.040.188 I print_info: rope type        = 2
0.00.040.190 I print_info: rope scaling     = linear
0.00.040.190 I print_info: freq_base_train  = 10000.0
0.00.040.191 I print_info: freq_scale_train = 1
0.00.040.191 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.191 I print_info: rope_finetuned   = unknown
0.00.040.191 I print_info: ssm_d_conv       = 0
0.00.040.191 I print_info: ssm_d_inner      = 0
0.00.040.191 I print_info: ssm_d_state      = 0
0.00.040.191 I print_info: ssm_dt_rank      = 0
0.00.040.192 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.192 I print_info: model type       = 1.4B
0.00.040.192 I print_info: model params     = 1.41 B
0.00.040.192 I print_info: general.name     = 1.4B
0.00.040.193 I print_info: vocab type       = BPE
0.00.040.193 I print_info: n_vocab          = 50304
0.00.040.193 I print_info: n_merges         = 50009
0.00.040.194 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.198 I print_info: LF token         = 187 'Ċ'
0.00.040.199 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.199 I print_info: max token length = 1024
0.00.605.257 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.272 I load_tensors: offloading output layer to GPU
0.00.605.272 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.310 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.605.311 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.606.742 I llama_init_from_model: n_seq_max     = 1
0.00.606.746 I llama_init_from_model: n_ctx         = 2048
0.00.606.746 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.606.746 I llama_init_from_model: n_batch       = 2048
0.00.606.747 I llama_init_from_model: n_ubatch      = 512
0.00.606.747 I llama_init_from_model: flash_attn    = 0
0.00.606.748 I llama_init_from_model: freq_base     = 10000.0
0.00.606.749 I llama_init_from_model: freq_scale    = 1
0.00.606.750 I ggml_metal_init: allocating
0.00.606.779 I ggml_metal_init: found device: Apple M4
0.00.606.793 I ggml_metal_init: picking default device: Apple M4
0.00.608.207 I ggml_metal_init: using embedded metal library
0.00.614.673 I ggml_metal_init: GPU name:   Apple M4
0.00.614.678 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.679 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.680 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.683 I ggml_metal_init: simdgroup reduction   = true
0.00.614.684 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.684 I ggml_metal_init: has residency sets    = true
0.00.614.684 I ggml_metal_init: has bfloat            = true
0.00.614.685 I ggml_metal_init: use bfloat            = true
0.00.614.685 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.113 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.157 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.163 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.638 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.641 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.641 I llama_init_from_model: graph nodes  = 967
0.00.688.641 I llama_init_from_model: graph splits = 2
0.00.688.646 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.776 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.777 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.629 I main: llama threadpool init, n_threads = 4
0.00.744.675 I 
0.00.744.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.703 I 
0.00.744.884 I sampler seed: 1234
0.00.744.888 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.928 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.931 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.931 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.593.638 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50969.13 tokens per second)
0.01.593.638 I llama_perf_context_print:        load time =     733.04 ms
0.01.593.639 I llama_perf_context_print: prompt eval time =      52.11 ms /     7 tokens (    7.44 ms per token,   134.32 tokens per second)
0.01.593.640 I llama_perf_context_print:        eval time =     793.63 ms /    63 runs   (   12.60 ms per token,    79.38 tokens per second)
0.01.593.640 I llama_perf_context_print:       total time =     849.93 ms /    70 tokens
0.01.593.866 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.291 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.298 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.299 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.301 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.302 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.759 I llama_model_loader: - type  f32:  194 tensors
0.00.023.759 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.759 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.760 I print_info: file format = GGUF V3 (latest)
0.00.023.761 I print_info: file type   = Q2_K - Medium
0.00.023.761 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.619 I load: special tokens cache size = 25
0.00.037.359 I load: token to piece cache size = 0.2984 MB
0.00.037.361 I print_info: arch             = gptneox
0.00.037.361 I print_info: vocab_only       = 0
0.00.037.362 I print_info: n_ctx_train      = 2048
0.00.037.362 I print_info: n_embd           = 2048
0.00.037.362 I print_info: n_layer          = 24
0.00.037.364 I print_info: n_head           = 16
0.00.037.365 I print_info: n_head_kv        = 16
0.00.037.365 I print_info: n_rot            = 32
0.00.037.366 I print_info: n_swa            = 0
0.00.037.366 I print_info: n_embd_head_k    = 128
0.00.037.366 I print_info: n_embd_head_v    = 128
0.00.037.367 I print_info: n_gqa            = 1
0.00.037.368 I print_info: n_embd_k_gqa     = 2048
0.00.037.368 I print_info: n_embd_v_gqa     = 2048
0.00.037.369 I print_info: f_norm_eps       = 1.0e-05
0.00.037.369 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.369 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.371 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.371 I print_info: f_logit_scale    = 0.0e+00
0.00.037.372 I print_info: n_ff             = 8192
0.00.037.372 I print_info: n_expert         = 0
0.00.037.373 I print_info: n_expert_used    = 0
0.00.037.374 I print_info: causal attn      = 1
0.00.037.374 I print_info: pooling type     = 0
0.00.037.374 I print_info: rope type        = 2
0.00.037.375 I print_info: rope scaling     = linear
0.00.037.375 I print_info: freq_base_train  = 10000.0
0.00.037.375 I print_info: freq_scale_train = 1
0.00.037.376 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.376 I print_info: rope_finetuned   = unknown
0.00.037.376 I print_info: ssm_d_conv       = 0
0.00.037.376 I print_info: ssm_d_inner      = 0
0.00.037.376 I print_info: ssm_d_state      = 0
0.00.037.376 I print_info: ssm_dt_rank      = 0
0.00.037.376 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.377 I print_info: model type       = 1.4B
0.00.037.377 I print_info: model params     = 1.41 B
0.00.037.377 I print_info: general.name     = 1.4B
0.00.037.378 I print_info: vocab type       = BPE
0.00.037.378 I print_info: n_vocab          = 50304
0.00.037.378 I print_info: n_merges         = 50009
0.00.037.378 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.379 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.379 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.379 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.379 I print_info: LF token         = 187 'Ċ'
0.00.037.379 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.380 I print_info: max token length = 1024
0.00.338.987 I load_tensors: offloading 24 repeating layers to GPU
0.00.339.003 I load_tensors: offloading output layer to GPU
0.00.339.004 I load_tensors: offloaded 25/25 layers to GPU
0.00.339.039 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.339.040 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.340.610 I llama_init_from_model: n_seq_max     = 1
0.00.340.615 I llama_init_from_model: n_ctx         = 2048
0.00.340.615 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.340.615 I llama_init_from_model: n_batch       = 2048
0.00.340.616 I llama_init_from_model: n_ubatch      = 512
0.00.340.616 I llama_init_from_model: flash_attn    = 0
0.00.340.618 I llama_init_from_model: freq_base     = 10000.0
0.00.340.623 I llama_init_from_model: freq_scale    = 1
0.00.340.632 I ggml_metal_init: allocating
0.00.340.731 I ggml_metal_init: found device: Apple M4
0.00.340.746 I ggml_metal_init: picking default device: Apple M4
0.00.342.556 I ggml_metal_init: using embedded metal library
0.00.348.133 I ggml_metal_init: GPU name:   Apple M4
0.00.348.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.348.153 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.348.153 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.348.154 I ggml_metal_init: simdgroup reduction   = true
0.00.348.154 I ggml_metal_init: simdgroup matrix mul. = true
0.00.348.155 I ggml_metal_init: has residency sets    = true
0.00.348.155 I ggml_metal_init: has bfloat            = true
0.00.348.155 I ggml_metal_init: use bfloat            = true
0.00.348.159 I ggml_metal_init: hasUnifiedMemory      = true
0.00.348.164 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.369.299 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.430.745 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.430.754 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.430.826 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.435.242 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.435.244 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.435.244 I llama_init_from_model: graph nodes  = 967
0.00.435.244 I llama_init_from_model: graph splits = 2
0.00.435.250 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.435.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.435.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.931 I main: llama threadpool init, n_threads = 4
0.00.496.975 I 
0.00.497.000 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.001 I 
0.00.497.175 I sampler seed: 1234
0.00.497.179 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.497.190 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.497.192 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.497.192 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.181.585 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.181.585 I llama_perf_context_print:        load time =     487.29 ms
0.01.181.586 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.10 tokens per second)
0.01.181.587 I llama_perf_context_print:        eval time =     637.85 ms /    63 runs   (   10.12 ms per token,    98.77 tokens per second)
0.01.181.587 I llama_perf_context_print:       total time =     685.56 ms /    70 tokens
0.01.181.834 I ggml_metal_free: deallocating

real	0m1.198s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.472 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.479 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.480 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.483 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.488 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.970 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.971 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.971 I llama_model_loader: - type  f32:  194 tensors
0.00.023.971 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.972 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.972 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.972 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.973 I print_info: file format = GGUF V3 (latest)
0.00.023.973 I print_info: file type   = Q3_K - Medium
0.00.023.974 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.114 I load: special tokens cache size = 25
0.00.038.287 I load: token to piece cache size = 0.2984 MB
0.00.038.290 I print_info: arch             = gptneox
0.00.038.290 I print_info: vocab_only       = 0
0.00.038.290 I print_info: n_ctx_train      = 2048
0.00.038.290 I print_info: n_embd           = 2048
0.00.038.291 I print_info: n_layer          = 24
0.00.038.294 I print_info: n_head           = 16
0.00.038.294 I print_info: n_head_kv        = 16
0.00.038.295 I print_info: n_rot            = 32
0.00.038.295 I print_info: n_swa            = 0
0.00.038.295 I print_info: n_embd_head_k    = 128
0.00.038.295 I print_info: n_embd_head_v    = 128
0.00.038.296 I print_info: n_gqa            = 1
0.00.038.297 I print_info: n_embd_k_gqa     = 2048
0.00.038.297 I print_info: n_embd_v_gqa     = 2048
0.00.038.298 I print_info: f_norm_eps       = 1.0e-05
0.00.038.298 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.298 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.299 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.299 I print_info: f_logit_scale    = 0.0e+00
0.00.038.301 I print_info: n_ff             = 8192
0.00.038.301 I print_info: n_expert         = 0
0.00.038.301 I print_info: n_expert_used    = 0
0.00.038.301 I print_info: causal attn      = 1
0.00.038.301 I print_info: pooling type     = 0
0.00.038.301 I print_info: rope type        = 2
0.00.038.303 I print_info: rope scaling     = linear
0.00.038.304 I print_info: freq_base_train  = 10000.0
0.00.038.304 I print_info: freq_scale_train = 1
0.00.038.304 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.304 I print_info: rope_finetuned   = unknown
0.00.038.304 I print_info: ssm_d_conv       = 0
0.00.038.306 I print_info: ssm_d_inner      = 0
0.00.038.306 I print_info: ssm_d_state      = 0
0.00.038.306 I print_info: ssm_dt_rank      = 0
0.00.038.306 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.306 I print_info: model type       = 1.4B
0.00.038.307 I print_info: model params     = 1.41 B
0.00.038.308 I print_info: general.name     = 1.4B
0.00.038.308 I print_info: vocab type       = BPE
0.00.038.309 I print_info: n_vocab          = 50304
0.00.038.309 I print_info: n_merges         = 50009
0.00.038.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.310 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.311 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.311 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.311 I print_info: LF token         = 187 'Ċ'
0.00.038.311 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.311 I print_info: max token length = 1024
0.00.436.864 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.878 I load_tensors: offloading output layer to GPU
0.00.436.879 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.914 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.915 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.438.448 I llama_init_from_model: n_seq_max     = 1
0.00.438.453 I llama_init_from_model: n_ctx         = 2048
0.00.438.453 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.438.454 I llama_init_from_model: n_batch       = 2048
0.00.438.454 I llama_init_from_model: n_ubatch      = 512
0.00.438.455 I llama_init_from_model: flash_attn    = 0
0.00.438.458 I llama_init_from_model: freq_base     = 10000.0
0.00.438.458 I llama_init_from_model: freq_scale    = 1
0.00.438.461 I ggml_metal_init: allocating
0.00.438.567 I ggml_metal_init: found device: Apple M4
0.00.438.581 I ggml_metal_init: picking default device: Apple M4
0.00.440.367 I ggml_metal_init: using embedded metal library
0.00.445.808 I ggml_metal_init: GPU name:   Apple M4
0.00.445.821 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.823 I ggml_metal_init: simdgroup reduction   = true
0.00.445.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.824 I ggml_metal_init: has residency sets    = true
0.00.445.824 I ggml_metal_init: has bfloat            = true
0.00.445.825 I ggml_metal_init: use bfloat            = true
0.00.445.827 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.831 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.052 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.519.613 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.519.622 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.519.649 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.524.307 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.524.309 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.524.309 I llama_init_from_model: graph nodes  = 967
0.00.524.310 I llama_init_from_model: graph splits = 2
0.00.524.314 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.524.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.524.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.919 I main: llama threadpool init, n_threads = 4
0.00.581.959 I 
0.00.581.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.983 I 
0.00.582.158 I sampler seed: 1234
0.00.582.163 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.582.174 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.582.174 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.582.174 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.335.935 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.335.935 I llama_perf_context_print:        load time =     572.18 ms
0.01.335.940 I llama_perf_context_print: prompt eval time =      49.70 ms /     7 tokens (    7.10 ms per token,   140.86 tokens per second)
0.01.335.941 I llama_perf_context_print:        eval time =     701.10 ms /    63 runs   (   11.13 ms per token,    89.86 tokens per second)
0.01.335.941 I llama_perf_context_print:       total time =     754.93 ms /    70 tokens
0.01.336.201 I ggml_metal_free: deallocating

real	0m1.353s
user	0m0.110s
sys	0m0.178s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.432 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.892 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.899 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.900 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.900 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.900 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.904 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.904 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.904 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.905 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.905 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.905 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.910 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.911 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.912 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.670 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.673 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.425 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.426 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.426 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.427 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.427 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.427 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.427 I llama_model_loader: - type  f32:  194 tensors
0.00.024.428 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.428 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.428 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.429 I print_info: file format = GGUF V3 (latest)
0.00.024.429 I print_info: file type   = Q4_K - Medium
0.00.024.429 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.286 I load: special tokens cache size = 25
0.00.038.572 I load: token to piece cache size = 0.2984 MB
0.00.038.575 I print_info: arch             = gptneox
0.00.038.575 I print_info: vocab_only       = 0
0.00.038.575 I print_info: n_ctx_train      = 2048
0.00.038.575 I print_info: n_embd           = 2048
0.00.038.575 I print_info: n_layer          = 24
0.00.038.578 I print_info: n_head           = 16
0.00.038.579 I print_info: n_head_kv        = 16
0.00.038.579 I print_info: n_rot            = 32
0.00.038.579 I print_info: n_swa            = 0
0.00.038.581 I print_info: n_embd_head_k    = 128
0.00.038.581 I print_info: n_embd_head_v    = 128
0.00.038.582 I print_info: n_gqa            = 1
0.00.038.583 I print_info: n_embd_k_gqa     = 2048
0.00.038.588 I print_info: n_embd_v_gqa     = 2048
0.00.038.590 I print_info: f_norm_eps       = 1.0e-05
0.00.038.590 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.590 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.590 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.591 I print_info: f_logit_scale    = 0.0e+00
0.00.038.593 I print_info: n_ff             = 8192
0.00.038.593 I print_info: n_expert         = 0
0.00.038.594 I print_info: n_expert_used    = 0
0.00.038.594 I print_info: causal attn      = 1
0.00.038.594 I print_info: pooling type     = 0
0.00.038.594 I print_info: rope type        = 2
0.00.038.594 I print_info: rope scaling     = linear
0.00.038.595 I print_info: freq_base_train  = 10000.0
0.00.038.596 I print_info: freq_scale_train = 1
0.00.038.596 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.597 I print_info: rope_finetuned   = unknown
0.00.038.597 I print_info: ssm_d_conv       = 0
0.00.038.597 I print_info: ssm_d_inner      = 0
0.00.038.597 I print_info: ssm_d_state      = 0
0.00.038.598 I print_info: ssm_dt_rank      = 0
0.00.038.598 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.598 I print_info: model type       = 1.4B
0.00.038.599 I print_info: model params     = 1.41 B
0.00.038.599 I print_info: general.name     = 1.4B
0.00.038.600 I print_info: vocab type       = BPE
0.00.038.600 I print_info: n_vocab          = 50304
0.00.038.600 I print_info: n_merges         = 50009
0.00.038.600 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.600 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.600 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.601 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.601 I print_info: LF token         = 187 'Ċ'
0.00.038.601 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.601 I print_info: max token length = 1024
0.00.531.547 I load_tensors: offloading 24 repeating layers to GPU
0.00.531.563 I load_tensors: offloading output layer to GPU
0.00.531.564 I load_tensors: offloaded 25/25 layers to GPU
0.00.531.601 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.531.602 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.533.274 I llama_init_from_model: n_seq_max     = 1
0.00.533.279 I llama_init_from_model: n_ctx         = 2048
0.00.533.280 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.533.280 I llama_init_from_model: n_batch       = 2048
0.00.533.280 I llama_init_from_model: n_ubatch      = 512
0.00.533.281 I llama_init_from_model: flash_attn    = 0
0.00.533.282 I llama_init_from_model: freq_base     = 10000.0
0.00.533.287 I llama_init_from_model: freq_scale    = 1
0.00.533.289 I ggml_metal_init: allocating
0.00.533.395 I ggml_metal_init: found device: Apple M4
0.00.533.409 I ggml_metal_init: picking default device: Apple M4
0.00.535.166 I ggml_metal_init: using embedded metal library
0.00.541.641 I ggml_metal_init: GPU name:   Apple M4
0.00.541.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.541.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.541.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.541.647 I ggml_metal_init: simdgroup reduction   = true
0.00.541.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.541.648 I ggml_metal_init: has residency sets    = true
0.00.541.648 I ggml_metal_init: has bfloat            = true
0.00.541.649 I ggml_metal_init: use bfloat            = true
0.00.541.649 I ggml_metal_init: hasUnifiedMemory      = true
0.00.541.651 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.559.329 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.613.973 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.613.979 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.006 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.431 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.619.433 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.619.434 I llama_init_from_model: graph nodes  = 967
0.00.619.434 I llama_init_from_model: graph splits = 2
0.00.619.439 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.545 I main: llama threadpool init, n_threads = 4
0.00.678.591 I 
0.00.678.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.619 I 
0.00.678.789 I sampler seed: 1234
0.00.678.794 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.678.817 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.678.818 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.678.819 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.451.275 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.451.276 I llama_perf_context_print:        load time =     668.19 ms
0.01.451.277 I llama_perf_context_print: prompt eval time =      57.40 ms /     7 tokens (    8.20 ms per token,   121.96 tokens per second)
0.01.451.277 I llama_perf_context_print:        eval time =     712.08 ms /    63 runs   (   11.30 ms per token,    88.47 tokens per second)
0.01.451.278 I llama_perf_context_print:       total time =     773.66 ms /    70 tokens
0.01.451.547 I ggml_metal_free: deallocating

real	0m1.470s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.967 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.672 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.681 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.682 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.684 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.508 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.508 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.509 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.509 I llama_model_loader: - type  f32:  194 tensors
0.00.024.510 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.510 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.511 I print_info: file format = GGUF V3 (latest)
0.00.024.511 I print_info: file type   = Q5_K - Medium
0.00.024.512 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.830 I load: special tokens cache size = 25
0.00.039.005 I load: token to piece cache size = 0.2984 MB
0.00.039.012 I print_info: arch             = gptneox
0.00.039.012 I print_info: vocab_only       = 0
0.00.039.013 I print_info: n_ctx_train      = 2048
0.00.039.013 I print_info: n_embd           = 2048
0.00.039.013 I print_info: n_layer          = 24
0.00.039.018 I print_info: n_head           = 16
0.00.039.018 I print_info: n_head_kv        = 16
0.00.039.019 I print_info: n_rot            = 32
0.00.039.019 I print_info: n_swa            = 0
0.00.039.019 I print_info: n_embd_head_k    = 128
0.00.039.019 I print_info: n_embd_head_v    = 128
0.00.039.020 I print_info: n_gqa            = 1
0.00.039.020 I print_info: n_embd_k_gqa     = 2048
0.00.039.022 I print_info: n_embd_v_gqa     = 2048
0.00.039.022 I print_info: f_norm_eps       = 1.0e-05
0.00.039.023 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.023 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.023 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.023 I print_info: f_logit_scale    = 0.0e+00
0.00.039.024 I print_info: n_ff             = 8192
0.00.039.024 I print_info: n_expert         = 0
0.00.039.024 I print_info: n_expert_used    = 0
0.00.039.025 I print_info: causal attn      = 1
0.00.039.025 I print_info: pooling type     = 0
0.00.039.025 I print_info: rope type        = 2
0.00.039.025 I print_info: rope scaling     = linear
0.00.039.025 I print_info: freq_base_train  = 10000.0
0.00.039.026 I print_info: freq_scale_train = 1
0.00.039.026 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.027 I print_info: rope_finetuned   = unknown
0.00.039.027 I print_info: ssm_d_conv       = 0
0.00.039.027 I print_info: ssm_d_inner      = 0
0.00.039.027 I print_info: ssm_d_state      = 0
0.00.039.027 I print_info: ssm_dt_rank      = 0
0.00.039.027 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.027 I print_info: model type       = 1.4B
0.00.039.028 I print_info: model params     = 1.41 B
0.00.039.028 I print_info: general.name     = 1.4B
0.00.039.028 I print_info: vocab type       = BPE
0.00.039.028 I print_info: n_vocab          = 50304
0.00.039.029 I print_info: n_merges         = 50009
0.00.039.029 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.029 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.029 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.029 I print_info: LF token         = 187 'Ċ'
0.00.039.030 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.030 I print_info: max token length = 1024
0.00.574.096 I load_tensors: offloading 24 repeating layers to GPU
0.00.574.101 I load_tensors: offloading output layer to GPU
0.00.574.102 I load_tensors: offloaded 25/25 layers to GPU
0.00.574.120 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.574.121 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.575.008 I llama_init_from_model: n_seq_max     = 1
0.00.575.012 I llama_init_from_model: n_ctx         = 2048
0.00.575.012 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.575.013 I llama_init_from_model: n_batch       = 2048
0.00.575.013 I llama_init_from_model: n_ubatch      = 512
0.00.575.013 I llama_init_from_model: flash_attn    = 0
0.00.575.015 I llama_init_from_model: freq_base     = 10000.0
0.00.575.015 I llama_init_from_model: freq_scale    = 1
0.00.575.016 I ggml_metal_init: allocating
0.00.575.066 I ggml_metal_init: found device: Apple M4
0.00.575.078 I ggml_metal_init: picking default device: Apple M4
0.00.576.135 I ggml_metal_init: using embedded metal library
0.00.580.300 I ggml_metal_init: GPU name:   Apple M4
0.00.580.305 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.580.306 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.580.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.580.307 I ggml_metal_init: simdgroup reduction   = true
0.00.580.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.580.308 I ggml_metal_init: has residency sets    = true
0.00.580.308 I ggml_metal_init: has bfloat            = true
0.00.580.308 I ggml_metal_init: use bfloat            = true
0.00.580.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.580.312 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.592.608 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.202 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.625.207 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.625.231 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.629.505 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.629.507 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.629.507 I llama_init_from_model: graph nodes  = 967
0.00.629.507 I llama_init_from_model: graph splits = 2
0.00.629.514 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.629.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.629.644 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.329 I main: llama threadpool init, n_threads = 4
0.00.686.364 I 
0.00.686.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.388 I 
0.00.686.495 I sampler seed: 1234
0.00.686.500 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.517 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.517 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.517 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.536.057 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50750.54 tokens per second)
0.01.536.058 I llama_perf_context_print:        load time =     676.44 ms
0.01.536.059 I llama_perf_context_print: prompt eval time =      51.14 ms /     7 tokens (    7.31 ms per token,   136.87 tokens per second)
0.01.536.059 I llama_perf_context_print:        eval time =     795.95 ms /    63 runs   (   12.63 ms per token,    79.15 tokens per second)
0.01.536.060 I llama_perf_context_print:       total time =     850.65 ms /    70 tokens
0.01.536.356 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.103s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.008.746 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.581 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.582 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.582 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.582 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.328 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.329 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.330 I llama_model_loader: - type  f32:  194 tensors
0.00.024.330 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.331 I print_info: file format = GGUF V3 (latest)
0.00.024.331 I print_info: file type   = Q6_K
0.00.024.332 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.862 I load: special tokens cache size = 25
0.00.039.056 I load: token to piece cache size = 0.2984 MB
0.00.039.060 I print_info: arch             = gptneox
0.00.039.060 I print_info: vocab_only       = 0
0.00.039.060 I print_info: n_ctx_train      = 2048
0.00.039.061 I print_info: n_embd           = 2048
0.00.039.061 I print_info: n_layer          = 24
0.00.039.065 I print_info: n_head           = 16
0.00.039.065 I print_info: n_head_kv        = 16
0.00.039.065 I print_info: n_rot            = 32
0.00.039.066 I print_info: n_swa            = 0
0.00.039.066 I print_info: n_embd_head_k    = 128
0.00.039.066 I print_info: n_embd_head_v    = 128
0.00.039.067 I print_info: n_gqa            = 1
0.00.039.068 I print_info: n_embd_k_gqa     = 2048
0.00.039.070 I print_info: n_embd_v_gqa     = 2048
0.00.039.071 I print_info: f_norm_eps       = 1.0e-05
0.00.039.071 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.071 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.071 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.071 I print_info: f_logit_scale    = 0.0e+00
0.00.039.072 I print_info: n_ff             = 8192
0.00.039.072 I print_info: n_expert         = 0
0.00.039.072 I print_info: n_expert_used    = 0
0.00.039.073 I print_info: causal attn      = 1
0.00.039.073 I print_info: pooling type     = 0
0.00.039.075 I print_info: rope type        = 2
0.00.039.075 I print_info: rope scaling     = linear
0.00.039.076 I print_info: freq_base_train  = 10000.0
0.00.039.076 I print_info: freq_scale_train = 1
0.00.039.076 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.076 I print_info: rope_finetuned   = unknown
0.00.039.077 I print_info: ssm_d_conv       = 0
0.00.039.077 I print_info: ssm_d_inner      = 0
0.00.039.077 I print_info: ssm_d_state      = 0
0.00.039.077 I print_info: ssm_dt_rank      = 0
0.00.039.077 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.077 I print_info: model type       = 1.4B
0.00.039.077 I print_info: model params     = 1.41 B
0.00.039.079 I print_info: general.name     = 1.4B
0.00.039.079 I print_info: vocab type       = BPE
0.00.039.079 I print_info: n_vocab          = 50304
0.00.039.079 I print_info: n_merges         = 50009
0.00.039.079 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.080 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.080 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.080 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.080 I print_info: LF token         = 187 'Ċ'
0.00.039.080 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.081 I print_info: max token length = 1024
0.00.628.968 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.974 I load_tensors: offloading output layer to GPU
0.00.628.974 I load_tensors: offloaded 25/25 layers to GPU
0.00.629.003 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.629.005 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.630.358 I llama_init_from_model: n_seq_max     = 1
0.00.630.360 I llama_init_from_model: n_ctx         = 2048
0.00.630.361 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.630.361 I llama_init_from_model: n_batch       = 2048
0.00.630.362 I llama_init_from_model: n_ubatch      = 512
0.00.630.362 I llama_init_from_model: flash_attn    = 0
0.00.630.363 I llama_init_from_model: freq_base     = 10000.0
0.00.630.364 I llama_init_from_model: freq_scale    = 1
0.00.630.365 I ggml_metal_init: allocating
0.00.630.444 I ggml_metal_init: found device: Apple M4
0.00.630.461 I ggml_metal_init: picking default device: Apple M4
0.00.631.943 I ggml_metal_init: using embedded metal library
0.00.638.075 I ggml_metal_init: GPU name:   Apple M4
0.00.638.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.080 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.081 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.081 I ggml_metal_init: simdgroup reduction   = true
0.00.638.081 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.082 I ggml_metal_init: has residency sets    = true
0.00.638.082 I ggml_metal_init: has bfloat            = true
0.00.638.082 I ggml_metal_init: use bfloat            = true
0.00.638.083 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.085 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.946 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.055 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.708.061 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.708.084 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.323 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.712.326 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.712.326 I llama_init_from_model: graph nodes  = 967
0.00.712.326 I llama_init_from_model: graph splits = 2
0.00.712.331 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.712.461 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.462 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.134 I main: llama threadpool init, n_threads = 4
0.00.774.175 I 
0.00.774.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.200 I 
0.00.774.375 I sampler seed: 1234
0.00.774.380 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.450 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.453 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.454 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.656.147 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.656.148 I llama_perf_context_print:        load time =     764.47 ms
0.01.656.149 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.67 tokens per second)
0.01.656.150 I llama_perf_context_print:        eval time =     824.36 ms /    63 runs   (   13.09 ms per token,    76.42 tokens per second)
0.01.656.150 I llama_perf_context_print:       total time =     882.93 ms /    70 tokens
0.01.656.423 I ggml_metal_free: deallocating

real	0m1.676s
user	0m0.109s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.554 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.213 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.514 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.521 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.522 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.523 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.523 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.523 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.526 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.526 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.527 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.528 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.833 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.833 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.835 I llama_model_loader: - type  f32:  194 tensors
0.00.054.835 I llama_model_loader: - type  f16:   98 tensors
0.00.054.836 I print_info: file format = GGUF V3 (latest)
0.00.054.836 I print_info: file type   = all F32 (guessed)
0.00.054.838 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.455 I load: special tokens cache size = 25
0.00.075.741 I load: token to piece cache size = 0.2984 MB
0.00.075.745 I print_info: arch             = gptneox
0.00.075.745 I print_info: vocab_only       = 0
0.00.075.745 I print_info: n_ctx_train      = 2048
0.00.075.745 I print_info: n_embd           = 2048
0.00.075.746 I print_info: n_layer          = 24
0.00.075.749 I print_info: n_head           = 16
0.00.075.750 I print_info: n_head_kv        = 16
0.00.075.751 I print_info: n_rot            = 32
0.00.075.751 I print_info: n_swa            = 0
0.00.075.751 I print_info: n_embd_head_k    = 128
0.00.075.753 I print_info: n_embd_head_v    = 128
0.00.075.754 I print_info: n_gqa            = 1
0.00.075.755 I print_info: n_embd_k_gqa     = 2048
0.00.075.755 I print_info: n_embd_v_gqa     = 2048
0.00.075.756 I print_info: f_norm_eps       = 1.0e-05
0.00.075.756 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.757 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.757 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.757 I print_info: f_logit_scale    = 0.0e+00
0.00.075.758 I print_info: n_ff             = 8192
0.00.075.758 I print_info: n_expert         = 0
0.00.075.758 I print_info: n_expert_used    = 0
0.00.075.758 I print_info: causal attn      = 1
0.00.075.758 I print_info: pooling type     = 0
0.00.075.759 I print_info: rope type        = 2
0.00.075.759 I print_info: rope scaling     = linear
0.00.075.759 I print_info: freq_base_train  = 10000.0
0.00.075.760 I print_info: freq_scale_train = 1
0.00.075.760 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.760 I print_info: rope_finetuned   = unknown
0.00.075.760 I print_info: ssm_d_conv       = 0
0.00.075.760 I print_info: ssm_d_inner      = 0
0.00.075.761 I print_info: ssm_d_state      = 0
0.00.075.761 I print_info: ssm_dt_rank      = 0
0.00.075.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.763 I print_info: model type       = 1.4B
0.00.075.764 I print_info: model params     = 1.41 B
0.00.075.764 I print_info: general.name     = 1.4B
0.00.075.764 I print_info: vocab type       = BPE
0.00.075.764 I print_info: n_vocab          = 50304
0.00.075.766 I print_info: n_merges         = 50009
0.00.075.766 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.766 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.767 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.767 I print_info: LF token         = 187 'Ċ'
0.00.075.767 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.767 I print_info: max token length = 1024
0.01.452.518 I load_tensors: offloading 24 repeating layers to GPU
0.01.452.522 I load_tensors: offloading output layer to GPU
0.01.452.523 I load_tensors: offloaded 25/25 layers to GPU
0.01.452.548 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.452.550 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.453.351 I llama_init_from_model: n_seq_max     = 1
0.01.453.352 I llama_init_from_model: n_ctx         = 128
0.01.453.353 I llama_init_from_model: n_ctx_per_seq = 128
0.01.453.353 I llama_init_from_model: n_batch       = 128
0.01.453.353 I llama_init_from_model: n_ubatch      = 128
0.01.453.353 I llama_init_from_model: flash_attn    = 0
0.01.453.357 I llama_init_from_model: freq_base     = 10000.0
0.01.453.360 I llama_init_from_model: freq_scale    = 1
0.01.453.361 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.453.361 I ggml_metal_init: allocating
0.01.453.402 I ggml_metal_init: found device: Apple M4
0.01.453.407 I ggml_metal_init: picking default device: Apple M4
0.01.454.638 I ggml_metal_init: using embedded metal library
0.01.459.122 I ggml_metal_init: GPU name:   Apple M4
0.01.459.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.459.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.459.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.459.126 I ggml_metal_init: simdgroup reduction   = true
0.01.459.126 I ggml_metal_init: simdgroup matrix mul. = true
0.01.459.126 I ggml_metal_init: has residency sets    = true
0.01.459.126 I ggml_metal_init: has bfloat            = true
0.01.459.126 I ggml_metal_init: use bfloat            = true
0.01.459.127 I ggml_metal_init: hasUnifiedMemory      = true
0.01.459.128 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.469.901 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.471.635 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.471.638 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.471.651 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.473.331 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.473.332 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.473.333 I llama_init_from_model: graph nodes  = 967
0.01.473.333 I llama_init_from_model: graph splits = 2
0.01.473.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.473.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.508.986 I 
0.01.509.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.509.057 I perplexity: tokenizing the input ..
0.01.514.417 I perplexity: tokenization took 5.358 ms
0.01.514.421 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.633.232 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.634.577 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.634.612 I llama_perf_context_print:        load time =    1484.76 ms
0.01.634.613 I llama_perf_context_print: prompt eval time =     118.51 ms /   128 tokens (    0.93 ms per token,  1080.10 tokens per second)
0.01.634.614 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.634.615 I llama_perf_context_print:       total time =     125.63 ms /   129 tokens
0.01.635.053 I ggml_metal_free: deallocating

real	0m1.826s
user	0m0.097s
sys	0m0.269s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.238 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.397 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.401 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.402 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.403 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.403 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.406 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.406 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.406 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.406 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.407 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.410 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.410 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.256 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.275 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.131 I llama_model_loader: - type  f32:  194 tensors
0.00.025.132 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.132 I print_info: file format = GGUF V3 (latest)
0.00.025.133 I print_info: file type   = Q8_0
0.00.025.134 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.534 I load: special tokens cache size = 25
0.00.039.645 I load: token to piece cache size = 0.2984 MB
0.00.039.650 I print_info: arch             = gptneox
0.00.039.650 I print_info: vocab_only       = 0
0.00.039.650 I print_info: n_ctx_train      = 2048
0.00.039.650 I print_info: n_embd           = 2048
0.00.039.650 I print_info: n_layer          = 24
0.00.039.654 I print_info: n_head           = 16
0.00.039.655 I print_info: n_head_kv        = 16
0.00.039.655 I print_info: n_rot            = 32
0.00.039.655 I print_info: n_swa            = 0
0.00.039.657 I print_info: n_embd_head_k    = 128
0.00.039.657 I print_info: n_embd_head_v    = 128
0.00.039.658 I print_info: n_gqa            = 1
0.00.039.659 I print_info: n_embd_k_gqa     = 2048
0.00.039.659 I print_info: n_embd_v_gqa     = 2048
0.00.039.660 I print_info: f_norm_eps       = 1.0e-05
0.00.039.661 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.661 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.663 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.663 I print_info: f_logit_scale    = 0.0e+00
0.00.039.663 I print_info: n_ff             = 8192
0.00.039.665 I print_info: n_expert         = 0
0.00.039.665 I print_info: n_expert_used    = 0
0.00.039.665 I print_info: causal attn      = 1
0.00.039.665 I print_info: pooling type     = 0
0.00.039.666 I print_info: rope type        = 2
0.00.039.667 I print_info: rope scaling     = linear
0.00.039.667 I print_info: freq_base_train  = 10000.0
0.00.039.668 I print_info: freq_scale_train = 1
0.00.039.669 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.669 I print_info: rope_finetuned   = unknown
0.00.039.669 I print_info: ssm_d_conv       = 0
0.00.039.669 I print_info: ssm_d_inner      = 0
0.00.039.669 I print_info: ssm_d_state      = 0
0.00.039.670 I print_info: ssm_dt_rank      = 0
0.00.039.671 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.672 I print_info: model type       = 1.4B
0.00.039.672 I print_info: model params     = 1.41 B
0.00.039.672 I print_info: general.name     = 1.4B
0.00.039.672 I print_info: vocab type       = BPE
0.00.039.673 I print_info: n_vocab          = 50304
0.00.039.673 I print_info: n_merges         = 50009
0.00.039.673 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.674 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.674 I print_info: LF token         = 187 'Ċ'
0.00.039.674 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.674 I print_info: max token length = 1024
0.00.875.372 I load_tensors: offloading 24 repeating layers to GPU
0.00.875.379 I load_tensors: offloading output layer to GPU
0.00.875.379 I load_tensors: offloaded 25/25 layers to GPU
0.00.875.407 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.875.409 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.876.695 I llama_init_from_model: n_seq_max     = 1
0.00.876.697 I llama_init_from_model: n_ctx         = 128
0.00.876.698 I llama_init_from_model: n_ctx_per_seq = 128
0.00.876.698 I llama_init_from_model: n_batch       = 128
0.00.876.698 I llama_init_from_model: n_ubatch      = 128
0.00.876.699 I llama_init_from_model: flash_attn    = 0
0.00.876.699 I llama_init_from_model: freq_base     = 10000.0
0.00.876.700 I llama_init_from_model: freq_scale    = 1
0.00.876.700 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.876.701 I ggml_metal_init: allocating
0.00.876.764 I ggml_metal_init: found device: Apple M4
0.00.876.775 I ggml_metal_init: picking default device: Apple M4
0.00.878.023 I ggml_metal_init: using embedded metal library
0.00.883.369 I ggml_metal_init: GPU name:   Apple M4
0.00.883.373 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.883.373 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.883.374 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.883.375 I ggml_metal_init: simdgroup reduction   = true
0.00.883.375 I ggml_metal_init: simdgroup matrix mul. = true
0.00.883.375 I ggml_metal_init: has residency sets    = true
0.00.883.375 I ggml_metal_init: has bfloat            = true
0.00.883.375 I ggml_metal_init: use bfloat            = true
0.00.883.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.883.378 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.898.465 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.901.813 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.901.822 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.901.868 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.905.056 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.905.057 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.905.058 I llama_init_from_model: graph nodes  = 967
0.00.905.058 I llama_init_from_model: graph splits = 2
0.00.905.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.905.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.040 I 
0.00.933.122 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.140 I perplexity: tokenizing the input ..
0.00.940.147 I perplexity: tokenization took 7.004 ms
0.00.940.155 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.078.987 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.080.324 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.080.348 I llama_perf_context_print:        load time =     923.79 ms
0.01.080.349 I llama_perf_context_print: prompt eval time =     137.86 ms /   128 tokens (    1.08 ms per token,   928.48 tokens per second)
0.01.080.350 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.080.350 I llama_perf_context_print:       total time =     147.31 ms /   129 tokens
0.01.080.762 I ggml_metal_free: deallocating

real	0m1.096s
user	0m0.076s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.114 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.942 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.949 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.949 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.950 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.957 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.958 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.959 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.960 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.960 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.962 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.962 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.962 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.493 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.494 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.494 I llama_model_loader: - type  f32:  194 tensors
0.00.025.495 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.495 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.495 I print_info: file format = GGUF V3 (latest)
0.00.025.496 I print_info: file type   = Q4_0
0.00.025.496 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.285 I load: special tokens cache size = 25
0.00.039.278 I load: token to piece cache size = 0.2984 MB
0.00.039.280 I print_info: arch             = gptneox
0.00.039.281 I print_info: vocab_only       = 0
0.00.039.281 I print_info: n_ctx_train      = 2048
0.00.039.281 I print_info: n_embd           = 2048
0.00.039.281 I print_info: n_layer          = 24
0.00.039.284 I print_info: n_head           = 16
0.00.039.285 I print_info: n_head_kv        = 16
0.00.039.285 I print_info: n_rot            = 32
0.00.039.286 I print_info: n_swa            = 0
0.00.039.286 I print_info: n_embd_head_k    = 128
0.00.039.286 I print_info: n_embd_head_v    = 128
0.00.039.287 I print_info: n_gqa            = 1
0.00.039.288 I print_info: n_embd_k_gqa     = 2048
0.00.039.288 I print_info: n_embd_v_gqa     = 2048
0.00.039.289 I print_info: f_norm_eps       = 1.0e-05
0.00.039.289 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.289 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.291 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.292 I print_info: f_logit_scale    = 0.0e+00
0.00.039.292 I print_info: n_ff             = 8192
0.00.039.292 I print_info: n_expert         = 0
0.00.039.292 I print_info: n_expert_used    = 0
0.00.039.293 I print_info: causal attn      = 1
0.00.039.293 I print_info: pooling type     = 0
0.00.039.293 I print_info: rope type        = 2
0.00.039.293 I print_info: rope scaling     = linear
0.00.039.293 I print_info: freq_base_train  = 10000.0
0.00.039.298 I print_info: freq_scale_train = 1
0.00.039.298 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.299 I print_info: rope_finetuned   = unknown
0.00.039.299 I print_info: ssm_d_conv       = 0
0.00.039.299 I print_info: ssm_d_inner      = 0
0.00.039.299 I print_info: ssm_d_state      = 0
0.00.039.299 I print_info: ssm_dt_rank      = 0
0.00.039.299 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.299 I print_info: model type       = 1.4B
0.00.039.300 I print_info: model params     = 1.41 B
0.00.039.300 I print_info: general.name     = 1.4B
0.00.039.301 I print_info: vocab type       = BPE
0.00.039.301 I print_info: n_vocab          = 50304
0.00.039.301 I print_info: n_merges         = 50009
0.00.039.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.302 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.302 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.302 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.303 I print_info: LF token         = 187 'Ċ'
0.00.039.303 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.303 I print_info: max token length = 1024
0.00.598.349 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.366 I load_tensors: offloading output layer to GPU
0.00.598.367 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.402 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.598.404 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.599.773 I llama_init_from_model: n_seq_max     = 1
0.00.599.778 I llama_init_from_model: n_ctx         = 128
0.00.599.779 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.783 I llama_init_from_model: n_batch       = 128
0.00.599.784 I llama_init_from_model: n_ubatch      = 128
0.00.599.784 I llama_init_from_model: flash_attn    = 0
0.00.599.792 I llama_init_from_model: freq_base     = 10000.0
0.00.599.792 I llama_init_from_model: freq_scale    = 1
0.00.599.793 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.795 I ggml_metal_init: allocating
0.00.599.877 I ggml_metal_init: found device: Apple M4
0.00.599.891 I ggml_metal_init: picking default device: Apple M4
0.00.601.688 I ggml_metal_init: using embedded metal library
0.00.607.186 I ggml_metal_init: GPU name:   Apple M4
0.00.607.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.205 I ggml_metal_init: simdgroup reduction   = true
0.00.607.205 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.205 I ggml_metal_init: has residency sets    = true
0.00.607.205 I ggml_metal_init: has bfloat            = true
0.00.607.206 I ggml_metal_init: use bfloat            = true
0.00.607.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.880 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.467 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.477 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.515 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.906 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.908 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.908 I llama_init_from_model: graph nodes  = 967
0.00.633.908 I llama_init_from_model: graph splits = 2
0.00.633.911 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.912 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.465 I 
0.00.659.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.584 I perplexity: tokenizing the input ..
0.00.666.468 I perplexity: tokenization took 6.88 ms
0.00.666.475 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.157 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.791.482 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.791.509 I llama_perf_context_print:        load time =     649.34 ms
0.00.791.511 I llama_perf_context_print: prompt eval time =     122.76 ms /   128 tokens (    0.96 ms per token,  1042.66 tokens per second)
0.00.791.512 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.512 I llama_perf_context_print:       total time =     132.05 ms /   129 tokens
0.00.791.875 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.079s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.944 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.102 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.013 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.025 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.885 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.885 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.886 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.887 I llama_model_loader: - type  f32:  194 tensors
0.00.024.887 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.888 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.888 I print_info: file format = GGUF V3 (latest)
0.00.024.889 I print_info: file type   = Q4_1
0.00.024.890 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.274 I load: special tokens cache size = 25
0.00.039.501 I load: token to piece cache size = 0.2984 MB
0.00.039.504 I print_info: arch             = gptneox
0.00.039.504 I print_info: vocab_only       = 0
0.00.039.504 I print_info: n_ctx_train      = 2048
0.00.039.505 I print_info: n_embd           = 2048
0.00.039.505 I print_info: n_layer          = 24
0.00.039.509 I print_info: n_head           = 16
0.00.039.510 I print_info: n_head_kv        = 16
0.00.039.510 I print_info: n_rot            = 32
0.00.039.510 I print_info: n_swa            = 0
0.00.039.510 I print_info: n_embd_head_k    = 128
0.00.039.511 I print_info: n_embd_head_v    = 128
0.00.039.511 I print_info: n_gqa            = 1
0.00.039.512 I print_info: n_embd_k_gqa     = 2048
0.00.039.513 I print_info: n_embd_v_gqa     = 2048
0.00.039.513 I print_info: f_norm_eps       = 1.0e-05
0.00.039.514 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.514 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.514 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.514 I print_info: f_logit_scale    = 0.0e+00
0.00.039.515 I print_info: n_ff             = 8192
0.00.039.515 I print_info: n_expert         = 0
0.00.039.515 I print_info: n_expert_used    = 0
0.00.039.516 I print_info: causal attn      = 1
0.00.039.516 I print_info: pooling type     = 0
0.00.039.516 I print_info: rope type        = 2
0.00.039.516 I print_info: rope scaling     = linear
0.00.039.516 I print_info: freq_base_train  = 10000.0
0.00.039.517 I print_info: freq_scale_train = 1
0.00.039.517 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.517 I print_info: rope_finetuned   = unknown
0.00.039.517 I print_info: ssm_d_conv       = 0
0.00.039.521 I print_info: ssm_d_inner      = 0
0.00.039.521 I print_info: ssm_d_state      = 0
0.00.039.521 I print_info: ssm_dt_rank      = 0
0.00.039.521 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.521 I print_info: model type       = 1.4B
0.00.039.522 I print_info: model params     = 1.41 B
0.00.039.522 I print_info: general.name     = 1.4B
0.00.039.522 I print_info: vocab type       = BPE
0.00.039.523 I print_info: n_vocab          = 50304
0.00.039.523 I print_info: n_merges         = 50009
0.00.039.523 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.523 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.524 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.525 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.525 I print_info: LF token         = 187 'Ċ'
0.00.039.525 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.525 I print_info: max token length = 1024
0.00.627.856 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.870 I load_tensors: offloading output layer to GPU
0.00.627.870 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.903 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.627.905 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.629.521 I llama_init_from_model: n_seq_max     = 1
0.00.629.525 I llama_init_from_model: n_ctx         = 128
0.00.629.526 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.527 I llama_init_from_model: n_batch       = 128
0.00.629.527 I llama_init_from_model: n_ubatch      = 128
0.00.629.527 I llama_init_from_model: flash_attn    = 0
0.00.629.529 I llama_init_from_model: freq_base     = 10000.0
0.00.629.530 I llama_init_from_model: freq_scale    = 1
0.00.629.530 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.533 I ggml_metal_init: allocating
0.00.629.610 I ggml_metal_init: found device: Apple M4
0.00.629.624 I ggml_metal_init: picking default device: Apple M4
0.00.631.353 I ggml_metal_init: using embedded metal library
0.00.637.697 I ggml_metal_init: GPU name:   Apple M4
0.00.637.701 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.704 I ggml_metal_init: simdgroup reduction   = true
0.00.637.704 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.704 I ggml_metal_init: has residency sets    = true
0.00.637.705 I ggml_metal_init: has bfloat            = true
0.00.637.705 I ggml_metal_init: use bfloat            = true
0.00.637.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.751 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.659.340 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.659.348 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.659.381 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.662.695 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.662.696 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.662.697 I llama_init_from_model: graph nodes  = 967
0.00.662.697 I llama_init_from_model: graph splits = 2
0.00.662.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.662.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.141 I 
0.00.692.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.251 I perplexity: tokenizing the input ..
0.00.699.676 I perplexity: tokenization took 7.422 ms
0.00.699.690 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.625 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.838.933 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.838.954 I llama_perf_context_print:        load time =     683.19 ms
0.00.838.955 I llama_perf_context_print: prompt eval time =     137.02 ms /   128 tokens (    1.07 ms per token,   934.14 tokens per second)
0.00.838.955 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.956 I llama_perf_context_print:       total time =     146.82 ms /   129 tokens
0.00.839.347 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.080s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.710 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.843 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.849 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.856 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.856 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.857 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.857 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.857 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.858 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.859 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.860 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.860 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.861 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.863 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.863 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.864 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.620 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.363 I llama_model_loader: - type  f32:  194 tensors
0.00.024.363 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.363 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.364 I print_info: file format = GGUF V3 (latest)
0.00.024.364 I print_info: file type   = Q5_0
0.00.024.365 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.484 I load: special tokens cache size = 25
0.00.038.486 I load: token to piece cache size = 0.2984 MB
0.00.038.489 I print_info: arch             = gptneox
0.00.038.489 I print_info: vocab_only       = 0
0.00.038.490 I print_info: n_ctx_train      = 2048
0.00.038.490 I print_info: n_embd           = 2048
0.00.038.490 I print_info: n_layer          = 24
0.00.038.493 I print_info: n_head           = 16
0.00.038.494 I print_info: n_head_kv        = 16
0.00.038.494 I print_info: n_rot            = 32
0.00.038.495 I print_info: n_swa            = 0
0.00.038.495 I print_info: n_embd_head_k    = 128
0.00.038.495 I print_info: n_embd_head_v    = 128
0.00.038.496 I print_info: n_gqa            = 1
0.00.038.497 I print_info: n_embd_k_gqa     = 2048
0.00.038.497 I print_info: n_embd_v_gqa     = 2048
0.00.038.502 I print_info: f_norm_eps       = 1.0e-05
0.00.038.503 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.503 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.503 I print_info: f_logit_scale    = 0.0e+00
0.00.038.504 I print_info: n_ff             = 8192
0.00.038.504 I print_info: n_expert         = 0
0.00.038.504 I print_info: n_expert_used    = 0
0.00.038.505 I print_info: causal attn      = 1
0.00.038.505 I print_info: pooling type     = 0
0.00.038.505 I print_info: rope type        = 2
0.00.038.505 I print_info: rope scaling     = linear
0.00.038.506 I print_info: freq_base_train  = 10000.0
0.00.038.506 I print_info: freq_scale_train = 1
0.00.038.506 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.506 I print_info: rope_finetuned   = unknown
0.00.038.507 I print_info: ssm_d_conv       = 0
0.00.038.507 I print_info: ssm_d_inner      = 0
0.00.038.507 I print_info: ssm_d_state      = 0
0.00.038.507 I print_info: ssm_dt_rank      = 0
0.00.038.507 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.508 I print_info: model type       = 1.4B
0.00.038.508 I print_info: model params     = 1.41 B
0.00.038.508 I print_info: general.name     = 1.4B
0.00.038.509 I print_info: vocab type       = BPE
0.00.038.509 I print_info: n_vocab          = 50304
0.00.038.509 I print_info: n_merges         = 50009
0.00.038.509 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.510 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.510 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.510 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.510 I print_info: LF token         = 187 'Ċ'
0.00.038.511 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.511 I print_info: max token length = 1024
0.00.679.600 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.615 I load_tensors: offloading output layer to GPU
0.00.679.616 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.649 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.679.651 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.681.182 I llama_init_from_model: n_seq_max     = 1
0.00.681.185 I llama_init_from_model: n_ctx         = 128
0.00.681.186 I llama_init_from_model: n_ctx_per_seq = 128
0.00.681.186 I llama_init_from_model: n_batch       = 128
0.00.681.187 I llama_init_from_model: n_ubatch      = 128
0.00.681.187 I llama_init_from_model: flash_attn    = 0
0.00.681.188 I llama_init_from_model: freq_base     = 10000.0
0.00.681.189 I llama_init_from_model: freq_scale    = 1
0.00.681.190 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.681.191 I ggml_metal_init: allocating
0.00.681.207 I ggml_metal_init: found device: Apple M4
0.00.681.217 I ggml_metal_init: picking default device: Apple M4
0.00.682.564 I ggml_metal_init: using embedded metal library
0.00.688.867 I ggml_metal_init: GPU name:   Apple M4
0.00.688.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.688.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.688.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.688.874 I ggml_metal_init: simdgroup reduction   = true
0.00.688.874 I ggml_metal_init: simdgroup matrix mul. = true
0.00.688.874 I ggml_metal_init: has residency sets    = true
0.00.688.874 I ggml_metal_init: has bfloat            = true
0.00.688.875 I ggml_metal_init: use bfloat            = true
0.00.688.875 I ggml_metal_init: hasUnifiedMemory      = true
0.00.688.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.763 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.175 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.709.178 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.709.210 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.390 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.712.392 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.712.392 I llama_init_from_model: graph nodes  = 967
0.00.712.393 I llama_init_from_model: graph splits = 2
0.00.712.395 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.712.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.962 I 
0.00.743.034 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.053 I perplexity: tokenizing the input ..
0.00.750.601 I perplexity: tokenization took 7.545 ms
0.00.750.607 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.307 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.901.724 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.901.748 I llama_perf_context_print:        load time =     734.24 ms
0.00.901.749 I llama_perf_context_print: prompt eval time =     148.82 ms /   128 tokens (    1.16 ms per token,   860.12 tokens per second)
0.00.901.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.750 I llama_perf_context_print:       total time =     158.79 ms /   129 tokens
0.00.902.086 I ggml_metal_free: deallocating

real	0m0.917s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.639 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.649 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.651 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.652 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.654 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.655 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.655 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.655 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.656 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.656 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.422 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.266 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.267 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.267 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.267 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.268 I llama_model_loader: - type  f32:  194 tensors
0.00.025.268 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.269 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.269 I print_info: file format = GGUF V3 (latest)
0.00.025.270 I print_info: file type   = Q5_1
0.00.025.274 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.377 I load: special tokens cache size = 25
0.00.039.402 I load: token to piece cache size = 0.2984 MB
0.00.039.404 I print_info: arch             = gptneox
0.00.039.405 I print_info: vocab_only       = 0
0.00.039.405 I print_info: n_ctx_train      = 2048
0.00.039.405 I print_info: n_embd           = 2048
0.00.039.405 I print_info: n_layer          = 24
0.00.039.409 I print_info: n_head           = 16
0.00.039.410 I print_info: n_head_kv        = 16
0.00.039.410 I print_info: n_rot            = 32
0.00.039.410 I print_info: n_swa            = 0
0.00.039.410 I print_info: n_embd_head_k    = 128
0.00.039.410 I print_info: n_embd_head_v    = 128
0.00.039.413 I print_info: n_gqa            = 1
0.00.039.413 I print_info: n_embd_k_gqa     = 2048
0.00.039.414 I print_info: n_embd_v_gqa     = 2048
0.00.039.415 I print_info: f_norm_eps       = 1.0e-05
0.00.039.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.415 I print_info: f_logit_scale    = 0.0e+00
0.00.039.416 I print_info: n_ff             = 8192
0.00.039.416 I print_info: n_expert         = 0
0.00.039.416 I print_info: n_expert_used    = 0
0.00.039.416 I print_info: causal attn      = 1
0.00.039.416 I print_info: pooling type     = 0
0.00.039.417 I print_info: rope type        = 2
0.00.039.417 I print_info: rope scaling     = linear
0.00.039.417 I print_info: freq_base_train  = 10000.0
0.00.039.417 I print_info: freq_scale_train = 1
0.00.039.419 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.420 I print_info: rope_finetuned   = unknown
0.00.039.420 I print_info: ssm_d_conv       = 0
0.00.039.420 I print_info: ssm_d_inner      = 0
0.00.039.420 I print_info: ssm_d_state      = 0
0.00.039.420 I print_info: ssm_dt_rank      = 0
0.00.039.420 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.421 I print_info: model type       = 1.4B
0.00.039.421 I print_info: model params     = 1.41 B
0.00.039.422 I print_info: general.name     = 1.4B
0.00.039.422 I print_info: vocab type       = BPE
0.00.039.423 I print_info: n_vocab          = 50304
0.00.039.423 I print_info: n_merges         = 50009
0.00.039.423 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.423 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.423 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.423 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.424 I print_info: LF token         = 187 'Ċ'
0.00.039.424 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.424 I print_info: max token length = 1024
0.00.612.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.958 I load_tensors: offloading output layer to GPU
0.00.612.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.996 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.612.997 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.614.224 I llama_init_from_model: n_seq_max     = 1
0.00.614.227 I llama_init_from_model: n_ctx         = 128
0.00.614.227 I llama_init_from_model: n_ctx_per_seq = 128
0.00.614.228 I llama_init_from_model: n_batch       = 128
0.00.614.229 I llama_init_from_model: n_ubatch      = 128
0.00.614.229 I llama_init_from_model: flash_attn    = 0
0.00.614.240 I llama_init_from_model: freq_base     = 10000.0
0.00.614.241 I llama_init_from_model: freq_scale    = 1
0.00.614.241 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.614.243 I ggml_metal_init: allocating
0.00.614.324 I ggml_metal_init: found device: Apple M4
0.00.614.339 I ggml_metal_init: picking default device: Apple M4
0.00.616.055 I ggml_metal_init: using embedded metal library
0.00.622.566 I ggml_metal_init: GPU name:   Apple M4
0.00.622.569 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.570 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.571 I ggml_metal_init: simdgroup reduction   = true
0.00.622.572 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.572 I ggml_metal_init: has residency sets    = true
0.00.622.572 I ggml_metal_init: has bfloat            = true
0.00.622.572 I ggml_metal_init: use bfloat            = true
0.00.622.573 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.730 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.643.179 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.643.183 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.643.211 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.374 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.646.376 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.646.377 I llama_init_from_model: graph nodes  = 967
0.00.646.377 I llama_init_from_model: graph splits = 2
0.00.646.380 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.381 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.683 I 
0.00.676.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.783 I perplexity: tokenizing the input ..
0.00.684.275 I perplexity: tokenization took 7.487 ms
0.00.684.283 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.831.929 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.833.269 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.833.298 I llama_perf_context_print:        load time =     666.75 ms
0.00.833.299 I llama_perf_context_print: prompt eval time =     146.70 ms /   128 tokens (    1.15 ms per token,   872.51 tokens per second)
0.00.833.299 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.300 I llama_perf_context_print:       total time =     156.62 ms /   129 tokens
0.00.833.686 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.079s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.333 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.008 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.010 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.010 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.010 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.011 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.011 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.012 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.014 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.724 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.429 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.430 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.431 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.431 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.431 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.432 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.432 I llama_model_loader: - type  f32:  194 tensors
0.00.024.433 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.433 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.433 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.434 I print_info: file format = GGUF V3 (latest)
0.00.024.434 I print_info: file type   = Q2_K - Medium
0.00.024.435 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.169 I load: special tokens cache size = 25
0.00.038.158 I load: token to piece cache size = 0.2984 MB
0.00.038.161 I print_info: arch             = gptneox
0.00.038.161 I print_info: vocab_only       = 0
0.00.038.161 I print_info: n_ctx_train      = 2048
0.00.038.161 I print_info: n_embd           = 2048
0.00.038.162 I print_info: n_layer          = 24
0.00.038.165 I print_info: n_head           = 16
0.00.038.165 I print_info: n_head_kv        = 16
0.00.038.165 I print_info: n_rot            = 32
0.00.038.166 I print_info: n_swa            = 0
0.00.038.168 I print_info: n_embd_head_k    = 128
0.00.038.168 I print_info: n_embd_head_v    = 128
0.00.038.169 I print_info: n_gqa            = 1
0.00.038.170 I print_info: n_embd_k_gqa     = 2048
0.00.038.170 I print_info: n_embd_v_gqa     = 2048
0.00.038.171 I print_info: f_norm_eps       = 1.0e-05
0.00.038.171 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.171 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.171 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.172 I print_info: f_logit_scale    = 0.0e+00
0.00.038.172 I print_info: n_ff             = 8192
0.00.038.172 I print_info: n_expert         = 0
0.00.038.173 I print_info: n_expert_used    = 0
0.00.038.173 I print_info: causal attn      = 1
0.00.038.173 I print_info: pooling type     = 0
0.00.038.173 I print_info: rope type        = 2
0.00.038.173 I print_info: rope scaling     = linear
0.00.038.173 I print_info: freq_base_train  = 10000.0
0.00.038.178 I print_info: freq_scale_train = 1
0.00.038.178 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.178 I print_info: rope_finetuned   = unknown
0.00.038.178 I print_info: ssm_d_conv       = 0
0.00.038.178 I print_info: ssm_d_inner      = 0
0.00.038.178 I print_info: ssm_d_state      = 0
0.00.038.180 I print_info: ssm_dt_rank      = 0
0.00.038.180 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.180 I print_info: model type       = 1.4B
0.00.038.181 I print_info: model params     = 1.41 B
0.00.038.181 I print_info: general.name     = 1.4B
0.00.038.181 I print_info: vocab type       = BPE
0.00.038.181 I print_info: n_vocab          = 50304
0.00.038.181 I print_info: n_merges         = 50009
0.00.038.182 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.182 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.182 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.182 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.182 I print_info: LF token         = 187 'Ċ'
0.00.038.185 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.185 I print_info: max token length = 1024
0.00.338.274 I load_tensors: offloading 24 repeating layers to GPU
0.00.338.288 I load_tensors: offloading output layer to GPU
0.00.338.289 I load_tensors: offloaded 25/25 layers to GPU
0.00.338.322 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.338.324 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.339.840 I llama_init_from_model: n_seq_max     = 1
0.00.339.850 I llama_init_from_model: n_ctx         = 128
0.00.339.850 I llama_init_from_model: n_ctx_per_seq = 128
0.00.339.851 I llama_init_from_model: n_batch       = 128
0.00.339.851 I llama_init_from_model: n_ubatch      = 128
0.00.339.851 I llama_init_from_model: flash_attn    = 0
0.00.339.854 I llama_init_from_model: freq_base     = 10000.0
0.00.339.855 I llama_init_from_model: freq_scale    = 1
0.00.339.855 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.339.858 I ggml_metal_init: allocating
0.00.339.928 I ggml_metal_init: found device: Apple M4
0.00.339.941 I ggml_metal_init: picking default device: Apple M4
0.00.341.617 I ggml_metal_init: using embedded metal library
0.00.347.249 I ggml_metal_init: GPU name:   Apple M4
0.00.347.260 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.261 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.262 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.262 I ggml_metal_init: simdgroup reduction   = true
0.00.347.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.263 I ggml_metal_init: has residency sets    = true
0.00.347.263 I ggml_metal_init: has bfloat            = true
0.00.347.263 I ggml_metal_init: use bfloat            = true
0.00.347.265 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.651 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.372.205 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.372.216 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.372.257 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.375.764 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.375.766 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.375.766 I llama_init_from_model: graph nodes  = 967
0.00.375.767 I llama_init_from_model: graph splits = 2
0.00.375.769 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.375.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.408.459 I 
0.00.408.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.408.567 I perplexity: tokenizing the input ..
0.00.415.360 I perplexity: tokenization took 6.79 ms
0.00.415.368 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.559.826 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.183 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.203 I llama_perf_context_print:        load time =     399.12 ms
0.00.561.204 I llama_perf_context_print: prompt eval time =     143.51 ms /   128 tokens (    1.12 ms per token,   891.94 tokens per second)
0.00.561.205 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.205 I llama_perf_context_print:       total time =     152.75 ms /   129 tokens
0.00.561.583 I ggml_metal_free: deallocating

real	0m0.575s
user	0m0.080s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.721 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.171 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.176 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.178 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.179 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.179 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.182 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.182 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.183 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.183 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.184 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.189 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.190 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.838 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.839 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.840 I llama_model_loader: - type  f32:  194 tensors
0.00.023.840 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.840 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.841 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.841 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.841 I print_info: file format = GGUF V3 (latest)
0.00.023.842 I print_info: file type   = Q3_K - Medium
0.00.023.846 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.608 I load: special tokens cache size = 25
0.00.037.618 I load: token to piece cache size = 0.2984 MB
0.00.037.621 I print_info: arch             = gptneox
0.00.037.621 I print_info: vocab_only       = 0
0.00.037.622 I print_info: n_ctx_train      = 2048
0.00.037.622 I print_info: n_embd           = 2048
0.00.037.622 I print_info: n_layer          = 24
0.00.037.624 I print_info: n_head           = 16
0.00.037.625 I print_info: n_head_kv        = 16
0.00.037.625 I print_info: n_rot            = 32
0.00.037.625 I print_info: n_swa            = 0
0.00.037.626 I print_info: n_embd_head_k    = 128
0.00.037.626 I print_info: n_embd_head_v    = 128
0.00.037.627 I print_info: n_gqa            = 1
0.00.037.627 I print_info: n_embd_k_gqa     = 2048
0.00.037.628 I print_info: n_embd_v_gqa     = 2048
0.00.037.629 I print_info: f_norm_eps       = 1.0e-05
0.00.037.629 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.629 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.630 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.631 I print_info: f_logit_scale    = 0.0e+00
0.00.037.631 I print_info: n_ff             = 8192
0.00.037.633 I print_info: n_expert         = 0
0.00.037.633 I print_info: n_expert_used    = 0
0.00.037.633 I print_info: causal attn      = 1
0.00.037.634 I print_info: pooling type     = 0
0.00.037.634 I print_info: rope type        = 2
0.00.037.634 I print_info: rope scaling     = linear
0.00.037.634 I print_info: freq_base_train  = 10000.0
0.00.037.635 I print_info: freq_scale_train = 1
0.00.037.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.636 I print_info: rope_finetuned   = unknown
0.00.037.636 I print_info: ssm_d_conv       = 0
0.00.037.636 I print_info: ssm_d_inner      = 0
0.00.037.636 I print_info: ssm_d_state      = 0
0.00.037.636 I print_info: ssm_dt_rank      = 0
0.00.037.637 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.637 I print_info: model type       = 1.4B
0.00.037.637 I print_info: model params     = 1.41 B
0.00.037.637 I print_info: general.name     = 1.4B
0.00.037.638 I print_info: vocab type       = BPE
0.00.037.638 I print_info: n_vocab          = 50304
0.00.037.638 I print_info: n_merges         = 50009
0.00.037.639 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.639 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.639 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.642 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.643 I print_info: LF token         = 187 'Ċ'
0.00.037.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.643 I print_info: max token length = 1024
0.00.449.966 I load_tensors: offloading 24 repeating layers to GPU
0.00.449.980 I load_tensors: offloading output layer to GPU
0.00.449.980 I load_tensors: offloaded 25/25 layers to GPU
0.00.450.008 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.450.010 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.451.356 I llama_init_from_model: n_seq_max     = 1
0.00.451.364 I llama_init_from_model: n_ctx         = 128
0.00.451.365 I llama_init_from_model: n_ctx_per_seq = 128
0.00.451.365 I llama_init_from_model: n_batch       = 128
0.00.451.366 I llama_init_from_model: n_ubatch      = 128
0.00.451.366 I llama_init_from_model: flash_attn    = 0
0.00.451.367 I llama_init_from_model: freq_base     = 10000.0
0.00.451.367 I llama_init_from_model: freq_scale    = 1
0.00.451.371 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.451.373 I ggml_metal_init: allocating
0.00.451.423 I ggml_metal_init: found device: Apple M4
0.00.451.436 I ggml_metal_init: picking default device: Apple M4
0.00.453.088 I ggml_metal_init: using embedded metal library
0.00.458.685 I ggml_metal_init: GPU name:   Apple M4
0.00.458.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.458.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.458.701 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.458.701 I ggml_metal_init: simdgroup reduction   = true
0.00.458.702 I ggml_metal_init: simdgroup matrix mul. = true
0.00.458.702 I ggml_metal_init: has residency sets    = true
0.00.458.702 I ggml_metal_init: has bfloat            = true
0.00.458.702 I ggml_metal_init: use bfloat            = true
0.00.458.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.458.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.479.398 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.483.099 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.483.103 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.483.132 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.486.601 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.486.603 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.486.603 I llama_init_from_model: graph nodes  = 967
0.00.486.604 I llama_init_from_model: graph splits = 2
0.00.486.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.486.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.977 I 
0.00.517.048 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.517.067 I perplexity: tokenizing the input ..
0.00.524.139 I perplexity: tokenization took 7.07 ms
0.00.524.146 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.666.998 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.668.334 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.668.359 I llama_perf_context_print:        load time =     508.25 ms
0.00.668.360 I llama_perf_context_print: prompt eval time =     141.87 ms /   128 tokens (    1.11 ms per token,   902.24 tokens per second)
0.00.668.361 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.668.361 I llama_perf_context_print:       total time =     151.39 ms /   129 tokens
0.00.668.754 I ggml_metal_free: deallocating

real	0m0.683s
user	0m0.080s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.096 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.884 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.613 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.283 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.285 I llama_model_loader: - type  f32:  194 tensors
0.00.025.286 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.286 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.286 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.287 I print_info: file format = GGUF V3 (latest)
0.00.025.287 I print_info: file type   = Q4_K - Medium
0.00.025.288 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.451 I load: special tokens cache size = 25
0.00.039.432 I load: token to piece cache size = 0.2984 MB
0.00.039.435 I print_info: arch             = gptneox
0.00.039.435 I print_info: vocab_only       = 0
0.00.039.436 I print_info: n_ctx_train      = 2048
0.00.039.436 I print_info: n_embd           = 2048
0.00.039.436 I print_info: n_layer          = 24
0.00.039.439 I print_info: n_head           = 16
0.00.039.440 I print_info: n_head_kv        = 16
0.00.039.440 I print_info: n_rot            = 32
0.00.039.440 I print_info: n_swa            = 0
0.00.039.440 I print_info: n_embd_head_k    = 128
0.00.039.440 I print_info: n_embd_head_v    = 128
0.00.039.441 I print_info: n_gqa            = 1
0.00.039.442 I print_info: n_embd_k_gqa     = 2048
0.00.039.443 I print_info: n_embd_v_gqa     = 2048
0.00.039.443 I print_info: f_norm_eps       = 1.0e-05
0.00.039.445 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.446 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.446 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.446 I print_info: f_logit_scale    = 0.0e+00
0.00.039.447 I print_info: n_ff             = 8192
0.00.039.447 I print_info: n_expert         = 0
0.00.039.448 I print_info: n_expert_used    = 0
0.00.039.448 I print_info: causal attn      = 1
0.00.039.448 I print_info: pooling type     = 0
0.00.039.448 I print_info: rope type        = 2
0.00.039.448 I print_info: rope scaling     = linear
0.00.039.450 I print_info: freq_base_train  = 10000.0
0.00.039.450 I print_info: freq_scale_train = 1
0.00.039.451 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.451 I print_info: rope_finetuned   = unknown
0.00.039.451 I print_info: ssm_d_conv       = 0
0.00.039.451 I print_info: ssm_d_inner      = 0
0.00.039.451 I print_info: ssm_d_state      = 0
0.00.039.451 I print_info: ssm_dt_rank      = 0
0.00.039.452 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.452 I print_info: model type       = 1.4B
0.00.039.452 I print_info: model params     = 1.41 B
0.00.039.452 I print_info: general.name     = 1.4B
0.00.039.457 I print_info: vocab type       = BPE
0.00.039.457 I print_info: n_vocab          = 50304
0.00.039.457 I print_info: n_merges         = 50009
0.00.039.458 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: LF token         = 187 'Ċ'
0.00.039.459 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.459 I print_info: max token length = 1024
0.00.523.227 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.242 I load_tensors: offloading output layer to GPU
0.00.523.243 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.277 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.279 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.524.839 I llama_init_from_model: n_seq_max     = 1
0.00.524.843 I llama_init_from_model: n_ctx         = 128
0.00.524.844 I llama_init_from_model: n_ctx_per_seq = 128
0.00.524.844 I llama_init_from_model: n_batch       = 128
0.00.524.845 I llama_init_from_model: n_ubatch      = 128
0.00.524.845 I llama_init_from_model: flash_attn    = 0
0.00.524.847 I llama_init_from_model: freq_base     = 10000.0
0.00.524.847 I llama_init_from_model: freq_scale    = 1
0.00.524.848 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.524.850 I ggml_metal_init: allocating
0.00.524.919 I ggml_metal_init: found device: Apple M4
0.00.524.932 I ggml_metal_init: picking default device: Apple M4
0.00.526.666 I ggml_metal_init: using embedded metal library
0.00.533.578 I ggml_metal_init: GPU name:   Apple M4
0.00.533.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.583 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.584 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.588 I ggml_metal_init: simdgroup reduction   = true
0.00.533.588 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.589 I ggml_metal_init: has residency sets    = true
0.00.533.589 I ggml_metal_init: has bfloat            = true
0.00.533.589 I ggml_metal_init: use bfloat            = true
0.00.533.590 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.551.728 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.555.256 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.555.260 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.555.286 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.558.614 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.558.616 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.558.616 I llama_init_from_model: graph nodes  = 967
0.00.558.617 I llama_init_from_model: graph splits = 2
0.00.558.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.558.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.433 I 
0.00.585.513 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.534 I perplexity: tokenizing the input ..
0.00.593.307 I perplexity: tokenization took 7.77 ms
0.00.593.319 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.737 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.730.081 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.730.103 I llama_perf_context_print:        load time =     575.33 ms
0.00.730.103 I llama_perf_context_print: prompt eval time =     134.53 ms /   128 tokens (    1.05 ms per token,   951.47 tokens per second)
0.00.730.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.730.107 I llama_perf_context_print:       total time =     144.67 ms /   129 tokens
0.00.730.476 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.080s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.764 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.766 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.769 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.466 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.522 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.228 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.229 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.230 I llama_model_loader: - type  f32:  194 tensors
0.00.024.230 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.231 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.231 I print_info: file format = GGUF V3 (latest)
0.00.024.232 I print_info: file type   = Q5_K - Medium
0.00.024.234 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.326 I load: special tokens cache size = 25
0.00.038.392 I load: token to piece cache size = 0.2984 MB
0.00.038.395 I print_info: arch             = gptneox
0.00.038.395 I print_info: vocab_only       = 0
0.00.038.395 I print_info: n_ctx_train      = 2048
0.00.038.396 I print_info: n_embd           = 2048
0.00.038.396 I print_info: n_layer          = 24
0.00.038.398 I print_info: n_head           = 16
0.00.038.399 I print_info: n_head_kv        = 16
0.00.038.399 I print_info: n_rot            = 32
0.00.038.399 I print_info: n_swa            = 0
0.00.038.400 I print_info: n_embd_head_k    = 128
0.00.038.400 I print_info: n_embd_head_v    = 128
0.00.038.401 I print_info: n_gqa            = 1
0.00.038.401 I print_info: n_embd_k_gqa     = 2048
0.00.038.402 I print_info: n_embd_v_gqa     = 2048
0.00.038.403 I print_info: f_norm_eps       = 1.0e-05
0.00.038.403 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.403 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.403 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.404 I print_info: f_logit_scale    = 0.0e+00
0.00.038.404 I print_info: n_ff             = 8192
0.00.038.405 I print_info: n_expert         = 0
0.00.038.405 I print_info: n_expert_used    = 0
0.00.038.405 I print_info: causal attn      = 1
0.00.038.405 I print_info: pooling type     = 0
0.00.038.405 I print_info: rope type        = 2
0.00.038.405 I print_info: rope scaling     = linear
0.00.038.408 I print_info: freq_base_train  = 10000.0
0.00.038.408 I print_info: freq_scale_train = 1
0.00.038.408 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.409 I print_info: rope_finetuned   = unknown
0.00.038.409 I print_info: ssm_d_conv       = 0
0.00.038.409 I print_info: ssm_d_inner      = 0
0.00.038.409 I print_info: ssm_d_state      = 0
0.00.038.409 I print_info: ssm_dt_rank      = 0
0.00.038.409 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.409 I print_info: model type       = 1.4B
0.00.038.410 I print_info: model params     = 1.41 B
0.00.038.410 I print_info: general.name     = 1.4B
0.00.038.411 I print_info: vocab type       = BPE
0.00.038.411 I print_info: n_vocab          = 50304
0.00.038.412 I print_info: n_merges         = 50009
0.00.038.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.416 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.416 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.418 I print_info: LF token         = 187 'Ċ'
0.00.038.418 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.418 I print_info: max token length = 1024
0.00.601.404 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.421 I load_tensors: offloading output layer to GPU
0.00.601.422 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.454 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.455 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.602.993 I llama_init_from_model: n_seq_max     = 1
0.00.602.997 I llama_init_from_model: n_ctx         = 128
0.00.602.997 I llama_init_from_model: n_ctx_per_seq = 128
0.00.603.001 I llama_init_from_model: n_batch       = 128
0.00.603.002 I llama_init_from_model: n_ubatch      = 128
0.00.603.002 I llama_init_from_model: flash_attn    = 0
0.00.603.003 I llama_init_from_model: freq_base     = 10000.0
0.00.603.011 I llama_init_from_model: freq_scale    = 1
0.00.603.012 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.018 I ggml_metal_init: allocating
0.00.603.074 I ggml_metal_init: found device: Apple M4
0.00.603.086 I ggml_metal_init: picking default device: Apple M4
0.00.604.542 I ggml_metal_init: using embedded metal library
0.00.610.840 I ggml_metal_init: GPU name:   Apple M4
0.00.610.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.846 I ggml_metal_init: simdgroup reduction   = true
0.00.610.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.847 I ggml_metal_init: has residency sets    = true
0.00.610.847 I ggml_metal_init: has bfloat            = true
0.00.610.847 I ggml_metal_init: use bfloat            = true
0.00.610.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.547 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.551 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.577 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.831 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.833 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.834 I llama_init_from_model: graph nodes  = 967
0.00.634.834 I llama_init_from_model: graph splits = 2
0.00.634.837 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.317 I 
0.00.671.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.419 I perplexity: tokenizing the input ..
0.00.678.684 I perplexity: tokenization took 7.263 ms
0.00.678.692 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.060 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.829.473 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.829.495 I llama_perf_context_print:        load time =     662.51 ms
0.00.829.496 I llama_perf_context_print: prompt eval time =     148.44 ms /   128 tokens (    1.16 ms per token,   862.28 tokens per second)
0.00.829.497 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.497 I llama_perf_context_print:       total time =     158.18 ms /   129 tokens
0.00.829.844 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.079s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.377 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.387 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.387 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.388 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.388 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.388 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.389 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.390 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.391 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.391 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.180 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.864 I llama_model_loader: - type  f32:  194 tensors
0.00.023.864 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.865 I print_info: file format = GGUF V3 (latest)
0.00.023.866 I print_info: file type   = Q6_K
0.00.023.866 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.602 I load: special tokens cache size = 25
0.00.037.669 I load: token to piece cache size = 0.2984 MB
0.00.037.672 I print_info: arch             = gptneox
0.00.037.672 I print_info: vocab_only       = 0
0.00.037.672 I print_info: n_ctx_train      = 2048
0.00.037.672 I print_info: n_embd           = 2048
0.00.037.673 I print_info: n_layer          = 24
0.00.037.676 I print_info: n_head           = 16
0.00.037.676 I print_info: n_head_kv        = 16
0.00.037.677 I print_info: n_rot            = 32
0.00.037.677 I print_info: n_swa            = 0
0.00.037.677 I print_info: n_embd_head_k    = 128
0.00.037.677 I print_info: n_embd_head_v    = 128
0.00.037.678 I print_info: n_gqa            = 1
0.00.037.679 I print_info: n_embd_k_gqa     = 2048
0.00.037.679 I print_info: n_embd_v_gqa     = 2048
0.00.037.680 I print_info: f_norm_eps       = 1.0e-05
0.00.037.680 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.680 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.681 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.681 I print_info: f_logit_scale    = 0.0e+00
0.00.037.681 I print_info: n_ff             = 8192
0.00.037.682 I print_info: n_expert         = 0
0.00.037.682 I print_info: n_expert_used    = 0
0.00.037.682 I print_info: causal attn      = 1
0.00.037.682 I print_info: pooling type     = 0
0.00.037.682 I print_info: rope type        = 2
0.00.037.685 I print_info: rope scaling     = linear
0.00.037.685 I print_info: freq_base_train  = 10000.0
0.00.037.685 I print_info: freq_scale_train = 1
0.00.037.686 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.686 I print_info: rope_finetuned   = unknown
0.00.037.686 I print_info: ssm_d_conv       = 0
0.00.037.686 I print_info: ssm_d_inner      = 0
0.00.037.686 I print_info: ssm_d_state      = 0
0.00.037.686 I print_info: ssm_dt_rank      = 0
0.00.037.687 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.687 I print_info: model type       = 1.4B
0.00.037.687 I print_info: model params     = 1.41 B
0.00.037.688 I print_info: general.name     = 1.4B
0.00.037.688 I print_info: vocab type       = BPE
0.00.037.688 I print_info: n_vocab          = 50304
0.00.037.689 I print_info: n_merges         = 50009
0.00.037.693 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.693 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.693 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.694 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.694 I print_info: LF token         = 187 'Ċ'
0.00.037.695 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.695 I print_info: max token length = 1024
0.00.622.991 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.997 I load_tensors: offloading output layer to GPU
0.00.622.998 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.015 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.623.016 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.623.884 I llama_init_from_model: n_seq_max     = 1
0.00.623.891 I llama_init_from_model: n_ctx         = 128
0.00.623.891 I llama_init_from_model: n_ctx_per_seq = 128
0.00.623.892 I llama_init_from_model: n_batch       = 128
0.00.623.892 I llama_init_from_model: n_ubatch      = 128
0.00.623.892 I llama_init_from_model: flash_attn    = 0
0.00.623.894 I llama_init_from_model: freq_base     = 10000.0
0.00.623.894 I llama_init_from_model: freq_scale    = 1
0.00.623.895 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.623.896 I ggml_metal_init: allocating
0.00.623.936 I ggml_metal_init: found device: Apple M4
0.00.623.947 I ggml_metal_init: picking default device: Apple M4
0.00.624.976 I ggml_metal_init: using embedded metal library
0.00.629.393 I ggml_metal_init: GPU name:   Apple M4
0.00.629.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.400 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.400 I ggml_metal_init: simdgroup reduction   = true
0.00.629.401 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.401 I ggml_metal_init: has residency sets    = true
0.00.629.401 I ggml_metal_init: has bfloat            = true
0.00.629.401 I ggml_metal_init: use bfloat            = true
0.00.629.403 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.320 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.999 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.643.003 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.643.021 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.644.627 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.644.628 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.644.629 I llama_init_from_model: graph nodes  = 967
0.00.644.629 I llama_init_from_model: graph splits = 2
0.00.644.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.643 I 
0.00.678.677 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.687 I perplexity: tokenizing the input ..
0.00.682.569 I perplexity: tokenization took 3.881 ms
0.00.682.573 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.746 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.823.147 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.823.169 I llama_perf_context_print:        load time =     669.77 ms
0.00.823.169 I llama_perf_context_print: prompt eval time =     138.94 ms /   128 tokens (    1.09 ms per token,   921.23 tokens per second)
0.00.823.170 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.172 I llama_perf_context_print:       total time =     144.53 ms /   129 tokens
0.00.823.521 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.066s
sys	0m0.130s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.205 I build: 4599 (8b576b6c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.808 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.778 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.786 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.787 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.619 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.621 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.622 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.622 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.622 I llama_model_loader: - type  f32:  194 tensors
0.00.044.623 I llama_model_loader: - type  f16:   98 tensors
0.00.044.623 I print_info: file format = GGUF V3 (latest)
0.00.044.624 I print_info: file type   = all F32 (guessed)
0.00.044.625 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.055.620 I load: special tokens cache size = 25
0.00.063.882 I load: token to piece cache size = 0.2984 MB
0.00.063.885 I print_info: arch             = gptneox
0.00.063.886 I print_info: vocab_only       = 0
0.00.063.886 I print_info: n_ctx_train      = 2048
0.00.063.886 I print_info: n_embd           = 2048
0.00.063.886 I print_info: n_layer          = 24
0.00.063.889 I print_info: n_head           = 16
0.00.063.890 I print_info: n_head_kv        = 16
0.00.063.891 I print_info: n_rot            = 32
0.00.063.891 I print_info: n_swa            = 0
0.00.063.891 I print_info: n_embd_head_k    = 128
0.00.063.891 I print_info: n_embd_head_v    = 128
0.00.063.892 I print_info: n_gqa            = 1
0.00.063.893 I print_info: n_embd_k_gqa     = 2048
0.00.063.894 I print_info: n_embd_v_gqa     = 2048
0.00.063.894 I print_info: f_norm_eps       = 1.0e-05
0.00.063.895 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.895 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.895 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.895 I print_info: f_logit_scale    = 0.0e+00
0.00.063.896 I print_info: n_ff             = 8192
0.00.063.896 I print_info: n_expert         = 0
0.00.063.896 I print_info: n_expert_used    = 0
0.00.063.897 I print_info: causal attn      = 1
0.00.063.897 I print_info: pooling type     = 0
0.00.063.897 I print_info: rope type        = 2
0.00.063.898 I print_info: rope scaling     = linear
0.00.063.899 I print_info: freq_base_train  = 10000.0
0.00.063.899 I print_info: freq_scale_train = 1
0.00.063.899 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.899 I print_info: rope_finetuned   = unknown
0.00.063.899 I print_info: ssm_d_conv       = 0
0.00.063.900 I print_info: ssm_d_inner      = 0
0.00.063.902 I print_info: ssm_d_state      = 0
0.00.063.902 I print_info: ssm_dt_rank      = 0
0.00.063.902 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.902 I print_info: model type       = 1.4B
0.00.063.903 I print_info: model params     = 1.41 B
0.00.063.903 I print_info: general.name     = 1.4B
0.00.063.903 I print_info: vocab type       = BPE
0.00.063.903 I print_info: n_vocab          = 50304
0.00.063.903 I print_info: n_merges         = 50009
0.00.063.904 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.904 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.904 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.904 I print_info: LF token         = 187 'Ċ'
0.00.063.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.909 I print_info: max token length = 1024
0.01.328.284 I load_tensors: offloading 24 repeating layers to GPU
0.01.328.287 I load_tensors: offloading output layer to GPU
0.01.328.288 I load_tensors: offloaded 25/25 layers to GPU
0.01.328.311 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.328.312 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.329.076 I llama_init_from_model: n_seq_max     = 1
0.01.329.077 I llama_init_from_model: n_ctx         = 128
0.01.329.078 I llama_init_from_model: n_ctx_per_seq = 128
0.01.329.078 I llama_init_from_model: n_batch       = 128
0.01.329.078 I llama_init_from_model: n_ubatch      = 128
0.01.329.078 I llama_init_from_model: flash_attn    = 0
0.01.329.082 I llama_init_from_model: freq_base     = 10000.0
0.01.329.084 I llama_init_from_model: freq_scale    = 1
0.01.329.085 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.329.088 I ggml_metal_init: allocating
0.01.329.146 I ggml_metal_init: found device: Apple M4
0.01.329.152 I ggml_metal_init: picking default device: Apple M4
0.01.330.147 I ggml_metal_init: using embedded metal library
0.01.334.316 I ggml_metal_init: GPU name:   Apple M4
0.01.334.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.334.320 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.334.320 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.334.320 I ggml_metal_init: simdgroup reduction   = true
0.01.334.321 I ggml_metal_init: simdgroup matrix mul. = true
0.01.334.321 I ggml_metal_init: has residency sets    = true
0.01.334.321 I ggml_metal_init: has bfloat            = true
0.01.334.321 I ggml_metal_init: use bfloat            = true
0.01.334.322 I ggml_metal_init: hasUnifiedMemory      = true
0.01.334.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.345.643 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.347.471 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.347.476 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.347.490 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.349.127 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.349.128 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.349.129 I llama_init_from_model: graph nodes  = 967
0.01.349.129 I llama_init_from_model: graph splits = 2
0.01.349.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.349.130 I 
0.01.349.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.349.169 I compute_imatrix: tokenizing the input ..
0.01.353.346 I compute_imatrix: tokenization took 4.176 ms
0.01.353.348 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.619.958 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.622.359 I llama_perf_context_print:        load time =    1600.14 ms
0.01.622.360 I llama_perf_context_print: prompt eval time =     264.85 ms /   128 tokens (    2.07 ms per token,   483.30 tokens per second)
0.01.622.361 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.622.361 I llama_perf_context_print:       total time =    1602.54 ms /   129 tokens
0.01.622.870 I ggml_metal_free: deallocating

real	0m1.804s
user	0m0.116s
sys	0m0.247s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4599 (8b576b6c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121b05790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121b05e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121b064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121b06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121b06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121b092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121b09710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121b09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121b09ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121b0a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121b0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121b0af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121b0ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121b0c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121b0ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121b0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121b0d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121b0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121b0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121b0eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121b0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121b0fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121b10400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121b10ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121b113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121b11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121b11940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121b11db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121b12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121b128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121b12d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121b132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121b13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121b13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121b13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121b14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121b149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121b14e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121b152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121b15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121b15ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121b16010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121b16480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121b168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121b16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121b171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121b17640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121b18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121b18330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121b187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121b18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121b19080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121b194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121b19960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121b19dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121b1a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121b1a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121b1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121b1b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121b1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121b1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121b1bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121b1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121b1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121b1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121b1d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121b1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121b1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121b1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121b1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121b1eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121b1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121b1f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121b1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121b1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121b20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121b20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121b210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121b21650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121b21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121b221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121b22760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121b22d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121b232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121b23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121b23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121b243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121b24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121b24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121b254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121b25a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121b26040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121b265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121b26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121b27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121b27700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121b27cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121b17c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121b28410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121b28880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121b28cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121b292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121b29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121b29e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121b2a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121b2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121b2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121b2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121b2ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121b2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121b2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121b2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121b2d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121b2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121b2dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121b2e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121b2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121b2eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121b2efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121b2f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121b2f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121b2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121b303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121b308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121b30de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121b312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121b317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121b31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121b321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121b326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121b32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121b330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121b335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121b33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121b33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121b344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121b349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121b34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121b353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121b358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121b35de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121b362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121b367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121b36ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121b371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121b376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121b37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121b380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121b385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121b38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121b38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121b394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121b399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121b39ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121b3a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121b3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121b3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121b3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121b3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121b3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121b3c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121b3c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121b3cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121b3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121b3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121b3dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121b3dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121b3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121b3e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121b3eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121b3f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121b3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121b3fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121b402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121b407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121b40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121b411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121b416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121b41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121b420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121b425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121b42ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121b42fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121b434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121b439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121b43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121b443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121b448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121b44de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121b452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121b457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121b45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121b461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121b466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121b46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121b47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121b477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121b47da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121b483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121b489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121b48fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121b497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121b49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121b49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121b4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121b4ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121b4b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121b4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121b4bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121b4c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121b4c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121b4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121b4d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121b4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121b4de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121b4e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121b4e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121b4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121b4f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121b4f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121b4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121b50330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121b50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121b50dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121b51320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121b51870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121b51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121b52310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121b52860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121b52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121b53300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121b53850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121b53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121b542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121b54840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121b54d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121b552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121b55830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121b55d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121b562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121b56820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121b56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121b572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121b57810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121b57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121b582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121b58800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121b58d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121b592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121b597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121b59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121b5a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121b5a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121b5ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121b5b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121b5b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121b5bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121b5c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121b5c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121b5cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121b5d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121b5d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121b5dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121b5e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121b5e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121b5ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121b5f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121b5f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121b5fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121b60020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121b604c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121b60960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121b60e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121b612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121b61740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121b61be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121b62080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121b62520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121b629c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121b62e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121b63300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121b637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121b63cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121b64410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121b64b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121b65250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121b65970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121b65c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121b66420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121b666e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121b66cf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.705.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.705.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121b060c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121b20800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121b20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121b257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121b1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121b279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121b251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121b2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121b2c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121b2bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121b27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121b21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121b2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121b46f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121b26e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121b21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121b24c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121b23580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121b29b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121b469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121b2b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121b268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121b21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121b24690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121b22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121b29560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121b2b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121b26300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121b20db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121b240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121b28fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121b2ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121b25d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121b23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121b2a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121b669a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121b48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121b48c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121b4a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121b10940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121b17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121b14130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121b1a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121b1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121b65ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121b27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121b4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121b49290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121b67150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121b67410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121b676d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121b67990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121b67c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121b67f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121b681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121b68490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121b68750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121b68a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121b68cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121b68f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121b69250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121b69510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121b697d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121b69a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121b69d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121b6a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121b6a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121b6a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121b6a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121b6ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121b6add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121b6b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121b6b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121b6b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121b6b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121b6bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121b6be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121b6c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121b6c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121b6c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121b6c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121b6cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121b6ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121b6d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121b6d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121b6d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121b6d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121b6dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121b6df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121b6e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121b6e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121b6e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121b6ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121b6ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121b6efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121b6f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121b6f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121b6f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121b6fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121b6fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121b70050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121b70310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121b705d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121b70890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121b70b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121b70e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121b710d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121b71390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121b71650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121b71910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121b71bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121b71e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121b72150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121b72410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121b726d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121b72990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121b72c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121b72f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121b731d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121b73490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121b73750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121b73a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121b73cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121b73f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121b74250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121b74510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121b747d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121b74a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121b74d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121b75010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121b752d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121b75590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121b75850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121b75b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121b75dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121b76090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121b76350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121b76610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121b768d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121b76b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121b76e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121b77110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121b773d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121b77690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121b77950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121b77c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121b77ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121b78190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121b78450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121b78710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121b789d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121b78c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121b78f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121b79210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121b794d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121b79790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121b79a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121b79d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121b79fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121b7a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121b7a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121b7a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121b7aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121b7ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121b7b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121b7b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121b7b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121b7b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121b7bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121b7be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121b7c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121b7c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121b7c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121b7c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121b7cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121b7ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121b7d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121b7d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121b7d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121b7d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121b7de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121b7e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121b7e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121b7eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121b7efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121b7f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121b7f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121b7fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121b80180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121b805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121b80a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121b80ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121b81340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121b817b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121b81c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121b82090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121b82500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121b82970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121b82de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121b83250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121b836c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121b83b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121b840f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121b84600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121b84a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121b84ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121b85350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121b857c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121b85ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121b861f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121b86d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121b87020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121b875e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121b87ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121b88160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121b88720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121b88ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121b892a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121b89860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121b89e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121b8a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121b8a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121b8af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121b8b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121b8bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121b8c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121b8c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121b8cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121b8d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121b8d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121b8dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121b8e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121b8e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121b8eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121b8f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121b8fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121b8ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121b905a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121b90b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121b91120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121b916e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121b91ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121b92260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121b92820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121b92de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121b933a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121b93960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121b93f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121b944e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121b94aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121b95060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121b95620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121b95be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121b961a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121b96760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121b96d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121b972e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121b978a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121b97e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121b98420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121b989e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121b98fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121b99560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121b99b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121b9a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121b9a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121b9ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121b9b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121b9b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121b9bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121b9c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121b9c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121b9cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121b9d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121b9d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121b9da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121b9df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121b9e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121b9e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121b9ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121b9f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121b9f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121b9fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121ba0730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121ba0e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121ba1570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121ba1c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121ba1f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121ba2740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121ba2a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121ba3010 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121b9ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121b90e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121b8fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121b8c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121b8a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121b99820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121b96fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121b94d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121b92ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121b8ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121b88420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121b8d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121b8e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121b93c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121b90860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121b986e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121b8b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121b8c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121b93660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121b958e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121b8e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121b8f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121b947a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121b872e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121b913e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121b919a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121b8bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121b8cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121b99de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121b975a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121b88fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121b92520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121b878a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121b87e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121b83df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121b89b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121b9a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121b8f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121b97b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121b8da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121b902a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121b941e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121b8b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121b95ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121b8a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121b98ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121b96460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121b91f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121b9af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121b89560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121b9a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121b889e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121b99260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121b930a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121b95320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121b98120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121b96a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121b8eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121ba3470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121ba3730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121ba39f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121ba3cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121ba3f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121ba4230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121ba44f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121ba47b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121ba4a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121ba4d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121ba4ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121ba52b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121ba5570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121ba5830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121ba5af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121ba5db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121ba6070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121ba6330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121ba65f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121ba68b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121ba6b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121ba6e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121ba70f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121ba73b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121ba7670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121ba7930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121ba7bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121ba7eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121ba8170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121ba8430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121ba86f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121ba89b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121ba8c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121ba8f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121ba91f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121ba94b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121ba9770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121ba9a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121ba9cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121ba9fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121baa270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121baa530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121baa7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121baaab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121baad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121bab030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121bab2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121bab5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121bab870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121babb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121babdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121bac0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121bac370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121bac630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121bac8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121bacbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121bace70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121bad130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121bad3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121bad6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121bad970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121badc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121badef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121bae1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121bae470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121bae730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121bae9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121baecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121baef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121baf230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121baf4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121baf7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121bafa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121bafd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121bafff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121bb02b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121bb0570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121bb0830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121bb0af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121bb0db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121bb1070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121bb1330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121bb15f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121bb18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121bb1b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121bb1e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121bb20f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121bb23b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121bb2670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121bb2930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121bb2bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121bb2eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121bb3170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121bb3430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121bb36f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121bb39b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121bb3c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121bb3f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121bb41f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121bb44b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121bb4770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121bb4a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121bb4cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121bb4fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121bb5270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121bb5530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121bb57f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121bb5ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121bb5d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121bb6030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121bb62f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121bb65b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121bb6870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121bb6b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121bb6df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121bb70b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121bb7370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121bb7630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121bb78f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121bb7bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121bb7e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121bb8130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121bb83f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121bb86b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121bb8970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121bb8c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121bb8ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121bb91b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121bb9470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121bb9730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121bb99f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121bb9cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121bb9f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121bba230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121bba4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121bba7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121bbaa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121bbad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121bbaff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121bbb2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121bbb570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121bbb830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121bbbaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121bbbdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121bbc070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121bbc330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121bbc5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121bbc8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121bbcb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121bbce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121bbd0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121bbd3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121bbd980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121bbdc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121bbdf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121bbe370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121bbe7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121bbec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121bbf0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121bbf530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121bbf9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121bbfe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121bc0280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121bc06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121bc0b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121bc0fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121bc1440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121bc18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121bc1d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121bc2190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121bc2600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121bc2a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121bc2ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121bc3350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121bc37c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121bc3c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121bc40a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121bc4510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121bc4980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121bc4df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121bc5260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121bc56d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121bc5b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121bc5fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121bc6420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121bc6890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121bc6d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121bc7170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121bc75e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121bc7a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121bc7ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121bc8330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121bc87a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121bc8c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121bc9080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121bc94f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121bc9960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121bc9dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121bca240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121bca6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121bcab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121bcaf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121bcb400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121bcb870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121bcbce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121bcc150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121bcc5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121bcca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121bccea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121bcd310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121bcd780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121bcdbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121bce060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121bce4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121bce940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121bcedb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121bcf220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121bcf690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121bcfb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121bcff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121bd03e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121bd0850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121bd0cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121bd1130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121bd15a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121bd2010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121bd2730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121bd2e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121bd3570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121bd3830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121bd3ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121bd42a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121bd48b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.774s
user	0m0.278s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4599 (8b576b6c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15360e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15360edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15360f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15360f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15360fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153610470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153610a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153610fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153611580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153611f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153612480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153612fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153613750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153613f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153614680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153614da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1536154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153615be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1536163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153616ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1536171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153617910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1536181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1536188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153618b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1536191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15361a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15361a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15361aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15361ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15361b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15361bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15361be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15361c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15361c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15361cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15361d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15361d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15361d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15361de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15361e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15361e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15361ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15361f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15361f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15361ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1536205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153620bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1536211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1536217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153621df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153622400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153622bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153623090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153623530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1536237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153623e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1536245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1536248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153624d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1536251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153625690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153625b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153625fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153626470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153626910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153626db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153627250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1536276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153627b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153628030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153628580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153628ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153629020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153629570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153629ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15362a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15362a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15362aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15362b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15362b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15362baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15362bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15362c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15362ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15362cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15362d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15362da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15362dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15362e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15362ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15362efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15362f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15362fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15362ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15361fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153630420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153630bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153631120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153631670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153631bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153632110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153632660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153632bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153633100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153633650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1536340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153634640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153634b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1536350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153635580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153635a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153635ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153636360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153636800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153636ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1536375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153637a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153637f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1536383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153638860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153638d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1536391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153639640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153639ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153639f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15363a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15363a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15363ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15363b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15363b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15363bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15363bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15363c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15363c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15363cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15363d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15363d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15363dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15363e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15363e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15363e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15363ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15363f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15363f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15363fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1536400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153640540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1536409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153640e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153641320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1536417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153641c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1536425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153642a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153642ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153643380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153643820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153643cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153644160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153644600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153644aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153644f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1536453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153645880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153645d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1536461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153646660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153646b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153646fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153647440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1536478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153647d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153648220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1536486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153648b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153649000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1536494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153649940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153649de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15364a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15364a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15364abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15364b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15364b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15364b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15364be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15364c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15364c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15364cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15364d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15364d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15364dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15364e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15364e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15364ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15364f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15364f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15364fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153650880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153651070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153651510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1536519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153651e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153652600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153652b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1536530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1536535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153653b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153654090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1536545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153654b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153655080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1536555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153655b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153656070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1536565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153656b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153657060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1536575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153657b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153658050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1536585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153658af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153659040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153659590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153659ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15365a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15365a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15365aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15365b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15365b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15365bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15365c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15365c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15365cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15365d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15365d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15365daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15365dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15365e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15365ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15365efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15365f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15365fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15365ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153660520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153660a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153660fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153661510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153661a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153661fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153662500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153662a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153662fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1536634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153663a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153663f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1536644e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153664a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153664f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153665420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1536658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153665d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153666200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1536666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153666b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153666fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153667480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153667920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153667dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153668260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153668700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153668ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153669040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1536694e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153669a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15366a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15366a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15366af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15366b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15366b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15366c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15366c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15366ca30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.833 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154808ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154808f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1548097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154809d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15480a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15480a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15480acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15480b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15480b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15480bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15480c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15480c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15480ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15480d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15480dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15480e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15480ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15480f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15480f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1548102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1548109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154811100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154811820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154811f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154812660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154812920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154812f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154813540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154813b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154814340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1548147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154814aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154815330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154815870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154815b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154815fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154816470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154816910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154816db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154817250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1548176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154817b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154818030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1548184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154818790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154818da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1548193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1548199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15481a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15481abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15481b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15481b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15481be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15481c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15481cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15481cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15481d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15481d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15481e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15481e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15481e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15481edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15481f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15481f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15481fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154820070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154820510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1548209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154820e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1548212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154821790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154821c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154822180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1548226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154822c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154823170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1548236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154823c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154824160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1548246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154824c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154825150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1548256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154826140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154826690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x154826be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154827130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154827680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x154827bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154828120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154828670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154828bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154829660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154829bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15482a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15482a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15482aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15482b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15482b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15482bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15482c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15482c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15482cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15482d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15482d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15482db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15482e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15482e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15482eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15482f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15482f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15482f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15482fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154830330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1548307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154830c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154831110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1548315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154831a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154831ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154832390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154832830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154832cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154833170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154833ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154833f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1548343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154834890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154834d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1548351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154835670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154835b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154835fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154836450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1548368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154836d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154837230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1548376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154837b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154838010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1548384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154838950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154839290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154839730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154839bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15483a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15483a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15483a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15483ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15483b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15483b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15483bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15483c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15483c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15483ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15483ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15483d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15483d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15483dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15483e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15483e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15483ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15483ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15483f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15483f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15483fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154840190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154840630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154840ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154840f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154841410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1548418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154841d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1548421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154842690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154842b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154842fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154843470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154843910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154843db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154844250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1548446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154844b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154845030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1548454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154845970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154845e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1548462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154846800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154846d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1548472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1548477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154847ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1548480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1548486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154848ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1548494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154849970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154849c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15484a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15484a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15484b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15484b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15484b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15484be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15484c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15484cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15484d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15484d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15484db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15484e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15484e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15484eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15484f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15484f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15484faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154850040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154850590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154850ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154851030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154851580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154851ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154852020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154852570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154853010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154853560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154853ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154854000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154854550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154854aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154854ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154855540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154855a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154855fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154856530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154856a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154857520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154857a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154857fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154858510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154858a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154858fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154859500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154859a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154859fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15485a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15485aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15485af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15485b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15485ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15485bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15485c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15485ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15485cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15485d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15485da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15485df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15485e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15485ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15485ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15485f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15485f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15485fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1548601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154860670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154860b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154860fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154861450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1548618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154861d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154862230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1548626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154862b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154863010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1548634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154863a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154864120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154864840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154864f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154865680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154865940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154866130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1548663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154866a00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15366c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15364e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15364dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15364e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153621aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153621490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153650530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153618e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15361f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153620260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153620870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15361ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153617e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15360dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1536226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1536240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1536306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15366bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15361b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15361b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153650b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15364efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153619460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153619720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1536199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15366ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15366d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15366d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15366d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15366d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15366dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15366df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15366e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15366e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15366e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15366ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15366ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15366ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15366f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15366f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15366f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15366fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15366fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153670010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1536702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153670590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153670850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153670b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153670dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153671090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153671350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153671610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1536718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153671b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153671e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153672110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1536723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153672690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153672950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153672c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153672ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153673190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153673450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153673710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1536739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153673c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153673f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153674210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1536744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153674790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153674a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153674d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153674fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153675290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153675550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153675810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153675ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153675d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153676050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153676310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1536765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153676890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153676b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153676e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1536770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153677390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153677650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153677910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153677bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153677e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153678150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153678410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1536786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153678990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153678c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153678f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1536791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153679490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153679750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153679a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153679cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153679f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15367a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15367a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15367a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15367aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15367ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15367b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15367b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15367b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15367b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15367bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15367bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15367c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15367c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15367c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15367c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15367cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15367ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15367d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15367d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15367d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15367d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15367dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15367ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15367e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15367e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15367e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15367e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15367ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15367ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15367f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15367f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15367f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15367fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15367fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15367ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153680290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153680550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153680810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153680ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153680d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153681050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153681310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1536815d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153681890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153681b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153681e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1536820d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153682390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153682650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153682910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153682bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153682e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153683150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153683410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1536836d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153683990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153683c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153683f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1536841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153684490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153684750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153684a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153684cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153684f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153685250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153685510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1536857d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153685a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153685d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153686010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1536862d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153686590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153686850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153686b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153686dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153687090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153687350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153687610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1536878d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153687b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153687e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153688110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1536883d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153688690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153688950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153688c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153688ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153689190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153689450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153689710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1536899d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153689c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153689f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15368a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15368a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15368a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15368aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15368ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15368afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15368b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15368b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15368b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15368bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15368bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15368c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15368c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15368c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15368cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15368ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15368d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15368d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15368d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15368d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15368dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15368dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15368e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15368e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15368e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15368e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15368eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15368ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15368f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15368f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15368f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15368fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15368fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15368ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1536902a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153690560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153690820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153690ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153690da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153691060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153691320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1536915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1536918a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153691b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153691e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153692370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1536928c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153692e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153693360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1536938b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153693e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153694350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1536948a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153694df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153695340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153695890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153695de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153696330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153696880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153696dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153697320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153697870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153697dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153698310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153698860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153698db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153699300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153699850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153699da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15369a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15369a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15369a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15369ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15369b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15369b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15369bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15369c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15369c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15369cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15369d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15369d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15369da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15369df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15369e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15369e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15369ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15369f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15369ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1536a06c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1536a0de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1536a10a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1536a1890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1536a1b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1536a2160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.961s
user	0m0.236s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
