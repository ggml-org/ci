Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.46.3)
Requirement already satisfied: gguf>=0.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.15.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)
Requirement already satisfied: torch~=2.2.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2)
Requirement already satisfied: filelock in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.26.2)
Requirement already satisfied: packaging>=20.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)
Requirement already satisfied: requests in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)
Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.3)
Requirement already satisfied: safetensors>=0.4.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.0)
Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)
Requirement already satisfied: sympy in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.3)
Requirement already satisfied: networkx in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)
Requirement already satisfied: jinja2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)
Requirement already satisfied: fsspec in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///Users/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (1.26.4)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (6.0.2)
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (0.2.0)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (4.67.0)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.15.0-py3-none-any.whl size=3463 sha256=1414605d5b6b875c26d7b7072686a7099dadb1fc2d9ac6692b7932f35cbb868c
  Stored in directory: /private/var/folders/3z/y7wjb5257l1dj_dxrng2zq0h0000gn/T/pip-ephem-wheel-cache-3rhvuduq/wheels/57/e8/ad/c90c2fcd445a1780e6969131e27698e4f239a16d34ccd95852
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.15.0
    Uninstalling gguf-0.15.0:
      Successfully uninstalled gguf-0.15.0
Successfully installed gguf-0.15.0
+ gg_run_ctest_debug
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (3.1s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-debug

real	0m3.345s
user	0m0.862s
sys	0m1.493s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_debug-make.log
++ nproc
+ make -j10
[  0%] Generating build details from Git
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Built target build_info
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Linking CXX executable ../../bin/llama-gguf
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 32%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Built target llava
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-simple
[ 37%] Built target llava_shared
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-chat
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-sampling
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-log
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Built target test-grammar-parser
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Built target test-chat
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Built target test-arg-parser
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-autorelease
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-chat-template
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Built target test-barrier
[ 65%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Built target test-quantize-perf
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Built target test-rope
[ 70%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Built target llama-gritlm
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Built target llama-imatrix
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-infill
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookahead
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-cli
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup
[ 82%] Built target llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Built target llama-lookup-stats
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Built target llama-quantize
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-speculative
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Built target llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Built target llama-tts
[ 93%] Built target llama-run
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m4.390s
user	0m10.080s
sys	0m10.592s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.28 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.66 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.26 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.91 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.78 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  195.62 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.91 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.09 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 256.61 sec*proc (29 tests)

Total Test time (real) = 256.63 sec

real	4m16.689s
user	8m40.389s
sys	0m7.285s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.491s
user	0m0.851s
sys	0m1.210s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_release-make.log
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target build_info
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 32%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 34%] Linking C executable ../bin/test-c
[ 35%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Built target llava
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Built target common
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-chat
[ 49%] Linking CXX executable ../bin/test-sampling
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-sampling
[ 50%] Built target test-chat
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-autorelease
[ 63%] Built target test-backend-ops
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-batched-bench
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-infill
[ 73%] Built target llama-embedding
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 74%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-bench
[ 77%] Linking CXX executable ../../bin/llama-perplexity
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-create
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-perplexity
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-parallel
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-tts
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-gen-docs
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tts
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-gen-docs
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Built target llama-run
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m4.189s
user	0m7.996s
sys	0m10.994s
+ '[' -z ']'
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.24 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.16 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.73 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.36 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   31.08 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.41 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.25 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.92 sec*proc (29 tests)

Total Test time (real) =  54.93 sec

real	0m54.940s
user	1m17.831s
sys	0m6.466s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/embd_bge_small.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:43 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:43 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json [711396/711396] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:44 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:44 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:45 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:45 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:46 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:46 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:47 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.542s
user	0m0.860s
sys	0m1.248s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/embd_bge_small-make.log
++ nproc
+ make -j10
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target build_info
[  5%] Built target sha256
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target xxhash
[  6%] Built target sha1
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 31%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 32%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-sampling
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-gguf
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-gguf
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-barrier
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-quantize-perf
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Built target llama-infill
[ 79%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-cli
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-parallel
[ 81%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-tokenize
[ 90%] Built target llama-speculative
[ 90%] Built target llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.203s
user	0m6.521s
sys	0m9.990s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/bge-small --outfile ../models-mnt/bge-small/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: bge-small
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,               torch.float32 --> F16, shape = {384, 30522}
INFO:hf-to-gguf:position_embd.weight,            torch.float32 --> F32, shape = {384, 512}
INFO:hf-to-gguf:token_types.weight,              torch.float32 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.4.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.4.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.4.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.5.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.5.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.5.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.6.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.6.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.6.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.7.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.7.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.7.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.8.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.8.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.8.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.9.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.9.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.9.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.10.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.10.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.10.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.11.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.11.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.11.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 512
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Setting special token type unk to 100
INFO:gguf.vocab:Setting special token type sep to 102
INFO:gguf.vocab:Setting special token type pad to 0
WARNING:gguf.vocab:No handler for special token type cls with id 101 - skipping
INFO:gguf.vocab:Setting special token type mask to 103
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/bge-small/ggml-model-f16.gguf: n_tensors = 197, total_size = 66.9M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/66.9M [00:00<?, ?byte/s]Writing:  35%|███▌      | 23.4M/66.9M [00:00<00:00, 195Mbyte/s]Writing:  65%|██████▍   | 43.2M/66.9M [00:00<00:00, 189Mbyte/s]Writing:  95%|█████████▍| 63.4M/66.9M [00:00<00:00, 193Mbyte/s]Writing: 100%|██████████| 66.9M/66.9M [00:00<00:00, 192Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/bge-small/ggml-model-f16.gguf
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/llama-quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Bge Small
llama_model_loader: - kv   3:                           general.basename str              = bge
llama_model_loader: - kv   4:                         general.size_label str              = small
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                        bert.context_length u32              = 512
llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  124 tensors
llama_model_loader: - type  f16:   73 tensors
[   1/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f32, size =    0.750 MB
[   2/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   3/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   6/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   7/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   8/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   9/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  10/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  11/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  13/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  14/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  16/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  17/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  18/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  19/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  20/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  23/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  24/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  25/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  26/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  27/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  29/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  30/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  32/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  33/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  34/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  35/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  36/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  39/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  40/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  41/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  42/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  43/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  45/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  46/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  48/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  49/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  50/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  51/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  52/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  55/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  56/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  57/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  58/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  59/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  61/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  62/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  64/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  65/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  66/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  67/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  68/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  71/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  72/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  73/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  74/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  75/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  77/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  78/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  80/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  81/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  82/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  83/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  84/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  87/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  88/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  89/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  90/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  91/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  93/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  94/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  96/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  97/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  98/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  99/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 100/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 103/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 104/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 105/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 106/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 107/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 109/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 110/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 112/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 113/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 114/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 115/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 116/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 119/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 120/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 121/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 122/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 123/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 125/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 126/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 128/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 129/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 130/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 131/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 132/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 135/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 136/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 137/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 138/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 139/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 141/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 142/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 144/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 145/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 146/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 147/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 148/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 151/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 152/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 153/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 154/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 155/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 157/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 158/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 160/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 161/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 162/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 163/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 164/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 167/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 168/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 169/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 170/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 171/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 173/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 174/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 176/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 177/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 178/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 179/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 180/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 183/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 184/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 185/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 186/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 187/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 189/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 190/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 192/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 193/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 194/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 195/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 196/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_impl: model size  =    63.84 MB
llama_model_quantize_impl: quant size  =    34.38 MB

main: quantize time =   111.36 ms
main:    total time =   111.36 ms
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/embd_bge_small-tg-f16.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.170 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.145 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.681 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.688 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.691 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.692 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.693 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.693 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.694 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.697 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.698 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.699 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.699 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.700 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.703 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.706 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.707 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.708 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.709 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.709 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.710 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.983 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.397 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.399 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.400 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.400 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.401 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.402 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.033.402 I llama_model_loader: - type  f32:  124 tensors
0.00.033.403 I llama_model_loader: - type  f16:   73 tensors
0.00.033.403 I print_info: file format = GGUF V3 (latest)
0.00.033.410 I print_info: file type   = F16
0.00.033.411 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.037.777 I load: special tokens cache size = 5
0.00.039.932 I load: token to piece cache size = 0.2032 MB
0.00.039.960 I print_info: arch             = bert
0.00.039.961 I print_info: vocab_only       = 0
0.00.039.962 I print_info: n_ctx_train      = 512
0.00.039.962 I print_info: n_embd           = 384
0.00.039.962 I print_info: n_layer          = 12
0.00.039.965 I print_info: n_head           = 12
0.00.039.966 I print_info: n_head_kv        = 12
0.00.039.967 I print_info: n_rot            = 32
0.00.039.967 I print_info: n_swa            = 0
0.00.039.967 I print_info: n_embd_head_k    = 32
0.00.039.967 I print_info: n_embd_head_v    = 32
0.00.039.968 I print_info: n_gqa            = 1
0.00.039.969 I print_info: n_embd_k_gqa     = 384
0.00.039.970 I print_info: n_embd_v_gqa     = 384
0.00.039.971 I print_info: f_norm_eps       = 1.0e-12
0.00.039.972 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.972 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.972 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.972 I print_info: f_logit_scale    = 0.0e+00
0.00.039.973 I print_info: n_ff             = 1536
0.00.039.973 I print_info: n_expert         = 0
0.00.039.974 I print_info: n_expert_used    = 0
0.00.039.974 I print_info: causal attn      = 0
0.00.039.974 I print_info: pooling type     = 2
0.00.039.974 I print_info: rope type        = 2
0.00.039.975 I print_info: rope scaling     = linear
0.00.039.975 I print_info: freq_base_train  = 10000.0
0.00.039.975 I print_info: freq_scale_train = 1
0.00.039.976 I print_info: n_ctx_orig_yarn  = 512
0.00.039.976 I print_info: rope_finetuned   = unknown
0.00.039.976 I print_info: ssm_d_conv       = 0
0.00.039.976 I print_info: ssm_d_inner      = 0
0.00.039.976 I print_info: ssm_d_state      = 0
0.00.039.977 I print_info: ssm_dt_rank      = 0
0.00.039.977 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.989 I print_info: model type       = 33M
0.00.039.992 I print_info: model params     = 33.21 M
0.00.039.993 I print_info: general.name     = Bge Small
0.00.039.993 I print_info: vocab type       = WPM
0.00.039.994 I print_info: n_vocab          = 30522
0.00.039.994 I print_info: n_merges         = 0
0.00.039.994 I print_info: BOS token        = 101 '[CLS]'
0.00.039.995 I print_info: UNK token        = 100 '[UNK]'
0.00.039.995 I print_info: SEP token        = 102 '[SEP]'
0.00.039.997 I print_info: PAD token        = 0 '[PAD]'
0.00.039.997 I print_info: MASK token       = 103 '[MASK]'
0.00.039.997 I print_info: LF token         = 0 '[PAD]'
0.00.039.998 I print_info: max token length = 21
0.00.039.999 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.043.284 I load_tensors: offloading 12 repeating layers to GPU
0.00.043.286 I load_tensors: offloading output layer to GPU
0.00.043.287 I load_tensors: offloaded 13/13 layers to GPU
0.00.043.313 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.316 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.581 I llama_context: constructing llama_context
0.00.043.583 I llama_context: n_seq_max     = 1
0.00.043.583 I llama_context: n_ctx         = 512
0.00.043.583 I llama_context: n_ctx_per_seq = 512
0.00.043.584 I llama_context: n_batch       = 2048
0.00.043.584 I llama_context: n_ubatch      = 2048
0.00.043.584 I llama_context: flash_attn    = 0
0.00.043.585 I llama_context: freq_base     = 10000.0
0.00.043.585 I llama_context: freq_scale    = 1
0.00.043.586 I ggml_metal_init: allocating
0.00.043.592 I ggml_metal_init: found device: Apple M4
0.00.043.597 I ggml_metal_init: picking default device: Apple M4
0.00.044.441 I ggml_metal_init: using embedded metal library
0.00.048.760 I ggml_metal_init: GPU name:   Apple M4
0.00.048.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.048.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.048.764 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.048.765 I ggml_metal_init: simdgroup reduction   = true
0.00.048.765 I ggml_metal_init: simdgroup matrix mul. = true
0.00.048.765 I ggml_metal_init: has residency sets    = true
0.00.048.765 I ggml_metal_init: has bfloat            = true
0.00.048.765 I ggml_metal_init: use bfloat            = true
0.00.048.766 I ggml_metal_init: hasUnifiedMemory      = true
0.00.048.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.794 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.061.961 I init:      Metal compute buffer size =    16.75 MiB
0.00.061.963 I init:        CPU compute buffer size =     2.51 MiB
0.00.061.963 I init: graph nodes  = 441
0.00.061.963 I init: graph splits = 2
0.00.061.965 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.061.965 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.061.965 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.067.026 I 
0.00.067.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.067.669 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.072.551 I llama_perf_context_print:        load time =      45.88 ms
0.00.072.552 I llama_perf_context_print: prompt eval time =       4.74 ms /     9 tokens (    0.53 ms per token,  1900.74 tokens per second)
0.00.072.553 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.072.553 I llama_perf_context_print:       total time =       5.52 ms /    10 tokens
0.00.072.710 I ggml_metal_free: deallocating

real	0m0.254s
user	0m0.051s
sys	0m0.034s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/embd_bge_small-tg-q8_0.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.043 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.643 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.405 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.410 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.410 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.411 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.411 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.411 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.412 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.413 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.413 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.413 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.414 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.417 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.417 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.418 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.418 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.418 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.419 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.974 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.697 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.698 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.698 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.699 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.699 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.699 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.700 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.700 I llama_model_loader: - type  f32:  124 tensors
0.00.015.701 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.701 I print_info: file format = GGUF V3 (latest)
0.00.015.702 I print_info: file type   = Q8_0
0.00.015.702 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.198 I load: special tokens cache size = 5
0.00.019.363 I load: token to piece cache size = 0.2032 MB
0.00.019.371 I print_info: arch             = bert
0.00.019.372 I print_info: vocab_only       = 0
0.00.019.372 I print_info: n_ctx_train      = 512
0.00.019.373 I print_info: n_embd           = 384
0.00.019.373 I print_info: n_layer          = 12
0.00.019.376 I print_info: n_head           = 12
0.00.019.376 I print_info: n_head_kv        = 12
0.00.019.376 I print_info: n_rot            = 32
0.00.019.376 I print_info: n_swa            = 0
0.00.019.377 I print_info: n_embd_head_k    = 32
0.00.019.377 I print_info: n_embd_head_v    = 32
0.00.019.377 I print_info: n_gqa            = 1
0.00.019.378 I print_info: n_embd_k_gqa     = 384
0.00.019.379 I print_info: n_embd_v_gqa     = 384
0.00.019.380 I print_info: f_norm_eps       = 1.0e-12
0.00.019.380 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.380 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.380 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.383 I print_info: f_logit_scale    = 0.0e+00
0.00.019.383 I print_info: n_ff             = 1536
0.00.019.383 I print_info: n_expert         = 0
0.00.019.384 I print_info: n_expert_used    = 0
0.00.019.384 I print_info: causal attn      = 0
0.00.019.384 I print_info: pooling type     = 2
0.00.019.384 I print_info: rope type        = 2
0.00.019.384 I print_info: rope scaling     = linear
0.00.019.384 I print_info: freq_base_train  = 10000.0
0.00.019.385 I print_info: freq_scale_train = 1
0.00.019.385 I print_info: n_ctx_orig_yarn  = 512
0.00.019.385 I print_info: rope_finetuned   = unknown
0.00.019.385 I print_info: ssm_d_conv       = 0
0.00.019.385 I print_info: ssm_d_inner      = 0
0.00.019.385 I print_info: ssm_d_state      = 0
0.00.019.385 I print_info: ssm_dt_rank      = 0
0.00.019.386 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.386 I print_info: model type       = 33M
0.00.019.386 I print_info: model params     = 33.21 M
0.00.019.387 I print_info: general.name     = Bge Small
0.00.019.388 I print_info: vocab type       = WPM
0.00.019.388 I print_info: n_vocab          = 30522
0.00.019.388 I print_info: n_merges         = 0
0.00.019.388 I print_info: BOS token        = 101 '[CLS]'
0.00.019.388 I print_info: UNK token        = 100 '[UNK]'
0.00.019.389 I print_info: SEP token        = 102 '[SEP]'
0.00.019.389 I print_info: PAD token        = 0 '[PAD]'
0.00.019.391 I print_info: MASK token       = 103 '[MASK]'
0.00.019.391 I print_info: LF token         = 0 '[PAD]'
0.00.019.391 I print_info: max token length = 21
0.00.019.392 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.208 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.210 I load_tensors: offloading output layer to GPU
0.00.021.211 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.218 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.220 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.451 I llama_context: constructing llama_context
0.00.021.451 I llama_context: n_seq_max     = 1
0.00.021.452 I llama_context: n_ctx         = 512
0.00.021.452 I llama_context: n_ctx_per_seq = 512
0.00.021.452 I llama_context: n_batch       = 2048
0.00.021.452 I llama_context: n_ubatch      = 2048
0.00.021.453 I llama_context: flash_attn    = 0
0.00.021.453 I llama_context: freq_base     = 10000.0
0.00.021.453 I llama_context: freq_scale    = 1
0.00.021.454 I ggml_metal_init: allocating
0.00.021.461 I ggml_metal_init: found device: Apple M4
0.00.021.464 I ggml_metal_init: picking default device: Apple M4
0.00.021.981 I ggml_metal_init: using embedded metal library
0.00.024.564 I ggml_metal_init: GPU name:   Apple M4
0.00.024.566 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.566 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.567 I ggml_metal_init: simdgroup reduction   = true
0.00.024.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.567 I ggml_metal_init: has residency sets    = true
0.00.024.567 I ggml_metal_init: has bfloat            = true
0.00.024.567 I ggml_metal_init: use bfloat            = true
0.00.024.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.986 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.036.050 I init:      Metal compute buffer size =    16.75 MiB
0.00.036.051 I init:        CPU compute buffer size =     2.51 MiB
0.00.036.051 I init: graph nodes  = 441
0.00.036.052 I init: graph splits = 2
0.00.036.053 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.036.053 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.054 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.532 I 
0.00.039.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.991 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.194 I llama_perf_context_print:        load time =      29.89 ms
0.00.044.195 I llama_perf_context_print: prompt eval time =       4.07 ms /     9 tokens (    0.45 ms per token,  2211.85 tokens per second)
0.00.044.196 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.196 I llama_perf_context_print:       total time =       4.66 ms /    10 tokens
0.00.044.349 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_rerank_tiny
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:56 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:57 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json [2030772/2030772] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:57 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json [1215/1215] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:03:58 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json [280/280] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json:
2025-02-20 10:04:00 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt:
2025-02-20 10:04:00 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json:
2025-02-20 10:04:00 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:04:01 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/1_Pooling https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
+ local out=models-mnt/rerank-tiny/1_Pooling
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/1_Pooling
+ cd models-mnt/rerank-tiny/1_Pooling
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json:
2025-02-20 10:04:01 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/rerank-tiny
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.588s
user	0m0.854s
sys	0m1.290s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-make.log
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target build_info
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 32%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-sampling
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-chat
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Built target test-chat-template
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-batched
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Built target llama-infill
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-cli
[ 81%] Built target llama-parallel
[ 81%] Built target llama-lookup-stats
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-tts
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-retrieval
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-tts
[ 92%] Built target llama-run
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.234s
user	0m6.535s
sys	0m10.245s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/rerank-tiny --outfile ../models-mnt/rerank-tiny/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: rerank-tiny
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,              torch.bfloat16 --> F16, shape = {384, 61056}
INFO:hf-to-gguf:token_types.weight,             torch.bfloat16 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,           torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:cls.weight,                     torch.bfloat16 --> F16, shape = {384, 1}
INFO:hf-to-gguf:cls.bias,                       torch.bfloat16 --> F32, shape = {1}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 39382 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 2
INFO:gguf.vocab:Setting special token type unk to 3
INFO:gguf.vocab:Setting special token type sep to 2
INFO:gguf.vocab:Setting special token type pad to 1
WARNING:gguf.vocab:No handler for special token type cls with id 0 - skipping
INFO:gguf.vocab:Setting special token type mask to 4
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/rerank-tiny/ggml-model-f16.gguf: n_tensors = 70, total_size = 65.8M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/65.8M [00:00<?, ?byte/s]Writing:  77%|███████▋  | 50.4M/65.8M [00:00<00:00, 501Mbyte/s]Writing: 100%|██████████| 65.8M/65.8M [00:00<00:00, 464Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ model_f16=../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-rk-f16.log
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.228 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.011 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.145 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.153 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.154 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.163 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.164 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.164 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.165 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.166 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.167 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.170 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.171 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.174 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.175 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.176 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.046.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.048.401 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.875 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.878 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.878 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.879 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.879 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.880 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.880 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.880 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.881 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.881 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.881 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.052.882 I llama_model_loader: - type  f32:   40 tensors
0.00.052.883 I llama_model_loader: - type  f16:   30 tensors
0.00.052.883 I print_info: file format = GGUF V3 (latest)
0.00.052.884 I print_info: file type   = F16
0.00.052.885 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.057.290 W load: empty token at index 5
0.00.062.404 W load: model vocab missing newline token, using special_pad_id instead
0.00.063.888 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.063.922 I load: special tokens cache size = 5
0.00.334.581 I load: token to piece cache size = 1.5060 MB
0.00.334.616 I print_info: arch             = jina-bert-v2
0.00.334.617 I print_info: vocab_only       = 0
0.00.334.617 I print_info: n_ctx_train      = 8192
0.00.334.617 I print_info: n_embd           = 384
0.00.334.617 I print_info: n_layer          = 4
0.00.334.621 I print_info: n_head           = 12
0.00.334.622 I print_info: n_head_kv        = 12
0.00.334.622 I print_info: n_rot            = 32
0.00.334.622 I print_info: n_swa            = 0
0.00.334.622 I print_info: n_embd_head_k    = 32
0.00.334.622 I print_info: n_embd_head_v    = 32
0.00.334.623 I print_info: n_gqa            = 1
0.00.334.623 I print_info: n_embd_k_gqa     = 384
0.00.334.624 I print_info: n_embd_v_gqa     = 384
0.00.334.624 I print_info: f_norm_eps       = 1.0e-12
0.00.334.625 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.334.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.334.625 I print_info: f_max_alibi_bias = 8.0e+00
0.00.334.625 I print_info: f_logit_scale    = 0.0e+00
0.00.334.626 I print_info: n_ff             = 1536
0.00.334.626 I print_info: n_expert         = 0
0.00.334.626 I print_info: n_expert_used    = 0
0.00.334.626 I print_info: causal attn      = 0
0.00.334.626 I print_info: pooling type     = -1
0.00.334.626 I print_info: rope type        = -1
0.00.334.627 I print_info: rope scaling     = linear
0.00.334.627 I print_info: freq_base_train  = 10000.0
0.00.334.627 I print_info: freq_scale_train = 1
0.00.334.627 I print_info: n_ctx_orig_yarn  = 8192
0.00.334.628 I print_info: rope_finetuned   = unknown
0.00.334.628 I print_info: ssm_d_conv       = 0
0.00.334.628 I print_info: ssm_d_inner      = 0
0.00.334.628 I print_info: ssm_d_state      = 0
0.00.334.628 I print_info: ssm_dt_rank      = 0
0.00.334.628 I print_info: ssm_dt_b_c_rms   = 0
0.00.334.629 I print_info: model type       = 33M
0.00.334.630 I print_info: model params     = 32.90 M
0.00.334.630 I print_info: general.name     = Jina Bert Implementation
0.00.334.630 I print_info: vocab type       = BPE
0.00.334.630 I print_info: n_vocab          = 61056
0.00.334.631 I print_info: n_merges         = 39382
0.00.334.633 I print_info: BOS token        = 0 '<s>'
0.00.334.633 I print_info: EOS token        = 2 '</s>'
0.00.334.633 I print_info: UNK token        = 3 '<unk>'
0.00.334.633 I print_info: SEP token        = 2 '</s>'
0.00.334.633 I print_info: PAD token        = 1 '<pad>'
0.00.334.633 I print_info: MASK token       = 4 '<mask>'
0.00.334.634 I print_info: EOG token        = 2 '</s>'
0.00.334.634 I print_info: max token length = 45
0.00.334.634 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.336.010 I load_tensors: offloading 4 repeating layers to GPU
0.00.336.011 I load_tensors: offloading output layer to GPU
0.00.336.011 I load_tensors: offloaded 5/5 layers to GPU
0.00.336.032 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.336.033 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.336.363 I llama_context: constructing llama_context
0.00.336.363 I llama_context: n_seq_max     = 1
0.00.336.364 I llama_context: n_ctx         = 8192
0.00.336.364 I llama_context: n_ctx_per_seq = 8192
0.00.336.364 I llama_context: n_batch       = 2048
0.00.336.364 I llama_context: n_ubatch      = 2048
0.00.336.364 I llama_context: flash_attn    = 0
0.00.336.365 I llama_context: freq_base     = 10000.0
0.00.336.365 I llama_context: freq_scale    = 1
0.00.336.365 I ggml_metal_init: allocating
0.00.336.371 I ggml_metal_init: found device: Apple M4
0.00.336.376 I ggml_metal_init: picking default device: Apple M4
0.00.336.975 I ggml_metal_init: using embedded metal library
0.00.339.483 I ggml_metal_init: GPU name:   Apple M4
0.00.339.484 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.484 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.485 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.485 I ggml_metal_init: simdgroup reduction   = true
0.00.339.485 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.485 I ggml_metal_init: has residency sets    = true
0.00.339.486 I ggml_metal_init: has bfloat            = true
0.00.339.486 I ggml_metal_init: use bfloat            = true
0.00.339.486 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.348.881 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.356.067 I init:      Metal compute buffer size =   223.01 MiB
0.00.356.070 I init:        CPU compute buffer size =    22.02 MiB
0.00.356.070 I init: graph nodes  = 158
0.00.356.070 I init: graph splits = 2
0.00.356.072 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.356.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.356.073 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.247 I 
0.00.359.262 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.359.667 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.668 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.672 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.673 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.680 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.680 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.359.703 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.934 I llama_perf_context_print:        load time =     334.23 ms
0.00.362.940 I llama_perf_context_print: prompt eval time =       3.22 ms /    62 tokens (    0.05 ms per token, 19236.74 tokens per second)
0.00.362.941 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.942 I llama_perf_context_print:       total time =       3.69 ms /    63 tokens
0.00.363.169 I ggml_metal_free: deallocating

real	0m1.085s
user	0m0.337s
sys	0m0.041s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 0'
+ check_score 'rerank score 0' 'rerank score 0:    0.023' 0.00 0.05
+ qnt='rerank score 0'
++ echo 'rerank score 0:    0.023'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.023
++ echo '0.023 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.023 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 0' 0.023
+ return 0
  - rerank score 0 @ 0.023 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 1'
+ check_score 'rerank score 1' 'rerank score 1:    0.024' 0.00 0.05
+ qnt='rerank score 1'
++ echo 'rerank score 1:    0.024'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.024
++ echo '0.024 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.024 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 1' 0.024
+ return 0
  - rerank score 1 @ 0.024 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 2'
+ check_score 'rerank score 2' 'rerank score 2:    0.199' 0.10 0.30
+ qnt='rerank score 2'
++ echo 'rerank score 2:    0.199'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.199
++ echo '0.199 < 0.10'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.199 > 0.30'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 2' 0.199
+ return 0
  - rerank score 2 @ 0.199 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_pythia_1_4b
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:04:10 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json [570/570] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:04:11 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json [2113710/2113710] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:04:11 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json [396/396] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-02-20 10:04:11 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json [99/99] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /Users/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ head -n 60 models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ path_models=../models-mnt/pythia/1.4B
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.613s
user	0m0.842s
sys	0m1.305s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-make.log
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target xxhash
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Linking CXX executable ../../bin/llama-gguf
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Built target llava
[ 33%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-chat
[ 49%] Linking CXX executable ../bin/test-log
[ 50%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-sampling
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-chat
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched-bench
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-infill
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-perplexity
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookahead
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-cli
[ 81%] Generating loading.html.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Built target llama-parallel
[ 83%] Built target llama-perplexity
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-speculative-simple
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.203s
user	0m6.636s
sys	0m10.548s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/pythia/1.4B --outfile ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: 1.4B
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.0.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.0.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.0.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.0.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.1.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.1.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.1.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.1.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.2.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.2.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.2.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.2.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.3.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.3.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.3.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.3.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.4.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.4.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.4.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.4.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.5.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.5.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.5.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.5.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.6.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.6.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.6.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.6.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.7.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.7.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.7.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.7.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.8.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.8.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.8.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.8.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.9.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.9.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.9.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.9.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.10.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.10.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.10.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.10.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.11.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.11.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.11.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.11.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.12.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.12.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.12.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.12.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.13.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.13.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.13.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.13.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.14.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.14.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.14.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.14.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.15.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.15.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.15.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.15.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.16.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.16.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.16.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.16.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.17.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.17.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.17.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.17.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.18.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.18.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.18.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.18.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.19.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.19.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.19.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.19.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.20.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.20.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.20.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.20.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.21.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.21.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.21.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.21.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.22.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.22.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.22.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.22.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.23.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.23.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.23.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.23.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.bias,          torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output.weight,             torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 50009 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 0
INFO:gguf.vocab:Setting special token type unk to 0
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/pythia/1.4B/ggml-model-f16.gguf: n_tensors = 292, total_size = 2.8G
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/2.83G [00:00<?, ?byte/s]Writing:   7%|▋         | 206M/2.83G [00:01<00:23, 113Mbyte/s]Writing:   8%|▊         | 240M/2.83G [00:01<00:20, 126Mbyte/s]Writing:  10%|▉         | 273M/2.83G [00:02<00:20, 126Mbyte/s]Writing:  11%|█         | 307M/2.83G [00:02<00:22, 114Mbyte/s]Writing:  12%|█▏        | 340M/2.83G [00:02<00:18, 135Mbyte/s]Writing:  13%|█▎        | 374M/2.83G [00:02<00:17, 138Mbyte/s]Writing:  14%|█▍        | 408M/2.83G [00:03<00:18, 129Mbyte/s]Writing:  16%|█▌        | 441M/2.83G [00:03<00:16, 148Mbyte/s]Writing:  17%|█▋        | 475M/2.83G [00:03<00:15, 150Mbyte/s]Writing:  18%|█▊        | 508M/2.83G [00:03<00:13, 170Mbyte/s]Writing:  20%|██        | 576M/2.83G [00:03<00:09, 247Mbyte/s]Writing:  22%|██▏       | 609M/2.83G [00:03<00:08, 259Mbyte/s]Writing:  24%|██▍       | 676M/2.83G [00:04<00:06, 324Mbyte/s]Writing:  26%|██▋       | 744M/2.83G [00:04<00:05, 378Mbyte/s]Writing:  29%|██▊       | 811M/2.83G [00:04<00:05, 400Mbyte/s]Writing:  31%|███       | 878M/2.83G [00:04<00:04, 429Mbyte/s]Writing:  33%|███▎      | 945M/2.83G [00:04<00:04, 457Mbyte/s]Writing:  36%|███▌      | 1.01G/2.83G [00:04<00:04, 453Mbyte/s]Writing:  38%|███▊      | 1.08G/2.83G [00:04<00:03, 476Mbyte/s]Writing:  41%|████      | 1.15G/2.83G [00:05<00:03, 492Mbyte/s]Writing:  43%|████▎     | 1.21G/2.83G [00:05<00:03, 485Mbyte/s]Writing:  45%|████▌     | 1.28G/2.83G [00:05<00:03, 498Mbyte/s]Writing:  48%|████▊     | 1.35G/2.83G [00:05<00:02, 504Mbyte/s]Writing:  50%|████▉     | 1.42G/2.83G [00:05<00:02, 483Mbyte/s]Writing:  52%|█████▏    | 1.48G/2.83G [00:05<00:02, 509Mbyte/s]Writing:  55%|█████▍    | 1.55G/2.83G [00:05<00:02, 517Mbyte/s]Writing:  57%|█████▋    | 1.62G/2.83G [00:05<00:02, 502Mbyte/s]Writing:  59%|█████▉    | 1.68G/2.83G [00:06<00:02, 452Mbyte/s]Writing:  62%|██████▏   | 1.74G/2.83G [00:06<00:02, 482Mbyte/s]Writing:  64%|██████▍   | 1.82G/2.83G [00:06<00:02, 467Mbyte/s]Writing:  67%|██████▋   | 1.89G/2.83G [00:06<00:02, 454Mbyte/s]Writing:  69%|██████▉   | 1.95G/2.83G [00:06<00:01, 474Mbyte/s]Writing:  71%|███████▏  | 2.02G/2.83G [00:06<00:01, 470Mbyte/s]Writing:  74%|███████▎  | 2.09G/2.83G [00:07<00:01, 489Mbyte/s]Writing:  76%|███████▌  | 2.15G/2.83G [00:07<00:01, 504Mbyte/s]Writing:  78%|███████▊  | 2.22G/2.83G [00:07<00:01, 485Mbyte/s]Writing:  81%|████████  | 2.29G/2.83G [00:07<00:01, 495Mbyte/s]Writing:  83%|████████▎ | 2.35G/2.83G [00:07<00:00, 515Mbyte/s]Writing:  86%|████████▌ | 2.42G/2.83G [00:07<00:00, 481Mbyte/s]Writing:  88%|████████▊ | 2.49G/2.83G [00:07<00:00, 480Mbyte/s]Writing:  90%|█████████ | 2.55G/2.83G [00:07<00:00, 489Mbyte/s]Writing:  93%|█████████▎| 2.62G/2.83G [00:08<00:00, 455Mbyte/s]Writing: 100%|██████████| 2.83G/2.83G [00:08<00:00, 373Mbyte/s]Writing: 100%|██████████| 2.83G/2.83G [00:08<00:00, 322Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_f16=../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf
+ wiki_test_60=../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf q8_0
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1435.23 MB

main: quantize time =  9346.41 ms
main:    total time =  9346.41 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf q4_0
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_0 .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   786.31 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf q4_1

main: quantize time =  2452.12 ms
main:    total time =  2452.12 ms
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_1 .. size =   196.50 MiB ->    61.41 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   864.46 MB

main: quantize time =  2189.17 ms
main:    total time =  2189.17 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf q5_0
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_0 .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   942.60 MB

main: quantize time =  2682.34 ms
main:    total time =  2682.34 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf q5_1
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_1 .. size =   196.50 MiB ->    73.69 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1020.74 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf q2_k

main: quantize time =  2142.41 ms
main:    total time =  2142.41 ms
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q2_K .. size =   196.50 MiB ->    32.24 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   542.04 MB

main: quantize time =  5169.47 ms
main:    total time =  5169.47 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf q3_k
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q3_K .. size =   196.50 MiB ->    42.22 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   724.27 MB

main: quantize time =  5776.53 ms
main:    total time =  5776.53 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf q4_k
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_K .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   871.81 MB

main: quantize time =  6918.37 ms
main:    total time =  6918.37 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf q5_k
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_K .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1006.35 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf q6_k

main: quantize time =  5784.48 ms
main:    total time =  5784.48 ms
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1108.64 MB

main: quantize time =  4633.19 ms
main:    total time =  4633.19 ms
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.232 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.409 I main: llama backend init
0.00.000.417 I main: load the model and apply lora adapter, if any
0.00.096.480 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.109.977 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.110.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.110.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.110.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.110.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.110.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.110.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.110.018 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.110.018 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.110.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.110.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.110.021 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.110.022 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.110.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.110.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.110.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.110.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.117.175 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.119.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.126.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.126.710 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.126.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.126.711 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.126.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.126.714 I llama_model_loader: - type  f32:  194 tensors
0.00.126.714 I llama_model_loader: - type  f16:   98 tensors
0.00.126.717 I print_info: file format = GGUF V3 (latest)
0.00.126.719 I print_info: file type   = all F32 (guessed)
0.00.126.741 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.137.870 I load: special tokens cache size = 25
0.00.144.947 I load: token to piece cache size = 0.2984 MB
0.00.144.973 I print_info: arch             = gptneox
0.00.144.974 I print_info: vocab_only       = 0
0.00.144.974 I print_info: n_ctx_train      = 2048
0.00.144.974 I print_info: n_embd           = 2048
0.00.144.975 I print_info: n_layer          = 24
0.00.144.978 I print_info: n_head           = 16
0.00.144.979 I print_info: n_head_kv        = 16
0.00.144.979 I print_info: n_rot            = 32
0.00.144.979 I print_info: n_swa            = 0
0.00.144.979 I print_info: n_embd_head_k    = 128
0.00.144.979 I print_info: n_embd_head_v    = 128
0.00.144.980 I print_info: n_gqa            = 1
0.00.144.981 I print_info: n_embd_k_gqa     = 2048
0.00.144.981 I print_info: n_embd_v_gqa     = 2048
0.00.144.982 I print_info: f_norm_eps       = 1.0e-05
0.00.144.983 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.144.983 I print_info: f_clamp_kqv      = 0.0e+00
0.00.144.983 I print_info: f_max_alibi_bias = 0.0e+00
0.00.144.983 I print_info: f_logit_scale    = 0.0e+00
0.00.144.984 I print_info: n_ff             = 8192
0.00.144.984 I print_info: n_expert         = 0
0.00.144.984 I print_info: n_expert_used    = 0
0.00.144.984 I print_info: causal attn      = 1
0.00.144.984 I print_info: pooling type     = 0
0.00.144.984 I print_info: rope type        = 2
0.00.144.985 I print_info: rope scaling     = linear
0.00.144.985 I print_info: freq_base_train  = 10000.0
0.00.144.985 I print_info: freq_scale_train = 1
0.00.144.985 I print_info: n_ctx_orig_yarn  = 2048
0.00.144.986 I print_info: rope_finetuned   = unknown
0.00.144.986 I print_info: ssm_d_conv       = 0
0.00.144.986 I print_info: ssm_d_inner      = 0
0.00.144.986 I print_info: ssm_d_state      = 0
0.00.144.986 I print_info: ssm_dt_rank      = 0
0.00.144.986 I print_info: ssm_dt_b_c_rms   = 0
0.00.144.987 I print_info: model type       = 1.4B
0.00.144.987 I print_info: model params     = 1.41 B
0.00.144.987 I print_info: general.name     = 1.4B
0.00.144.987 I print_info: vocab type       = BPE
0.00.144.988 I print_info: n_vocab          = 50304
0.00.144.988 I print_info: n_merges         = 50009
0.00.144.988 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.144.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.144.988 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.144.989 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.144.989 I print_info: LF token         = 187 'Ċ'
0.00.144.989 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.144.989 I print_info: max token length = 1024
0.00.144.990 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.180.017 I load_tensors: offloading 24 repeating layers to GPU
0.00.180.019 I load_tensors: offloading output layer to GPU
0.00.180.020 I load_tensors: offloaded 25/25 layers to GPU
0.00.180.039 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.180.041 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.180.384 I llama_context: constructing llama_context
0.00.180.385 I llama_context: n_seq_max     = 1
0.00.180.385 I llama_context: n_ctx         = 2048
0.00.180.386 I llama_context: n_ctx_per_seq = 2048
0.00.180.386 I llama_context: n_batch       = 2048
0.00.180.386 I llama_context: n_ubatch      = 512
0.00.180.386 I llama_context: flash_attn    = 0
0.00.180.387 I llama_context: freq_base     = 10000.0
0.00.180.387 I llama_context: freq_scale    = 1
0.00.180.388 I ggml_metal_init: allocating
0.00.180.405 I ggml_metal_init: found device: Apple M4
0.00.180.411 I ggml_metal_init: picking default device: Apple M4
0.00.181.010 I ggml_metal_init: using embedded metal library
0.00.230.614 I ggml_metal_init: GPU name:   Apple M4
0.00.230.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.230.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.230.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.230.620 I ggml_metal_init: simdgroup reduction   = true
0.00.230.620 I ggml_metal_init: simdgroup matrix mul. = true
0.00.230.621 I ggml_metal_init: has residency sets    = true
0.00.230.621 I ggml_metal_init: has bfloat            = true
0.00.230.621 I ggml_metal_init: use bfloat            = true
0.00.230.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.230.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.278.793 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.283.447 I init:      Metal compute buffer size =   102.25 MiB
0.00.283.449 I init:        CPU compute buffer size =     5.01 MiB
0.00.283.449 I init: graph nodes  = 943
0.00.283.450 I init: graph splits = 2
0.00.283.451 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.283.453 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.283.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.283.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.317.088 I main: llama threadpool init, n_threads = 4
0.00.317.123 I 
0.00.317.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.317.138 I 
0.00.317.181 I sampler seed: 1234
0.00.317.187 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.317.210 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.317.211 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.317.211 I 
I believe the meaning of life is to do.
The following the court, and the second, I'm not have a large, we see and the world, the new and the case where the time, the following day of the presence of a “t-1) = new, a little bit of the first-type="table-type="

0.02.184.387 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47364.91 tokens per second)
0.02.184.388 I llama_perf_context_print:        load time =     219.73 ms
0.02.184.392 I llama_perf_context_print: prompt eval time =      43.93 ms /     7 tokens (    6.28 ms per token,   159.33 tokens per second)
0.02.184.393 I llama_perf_context_print:        eval time =    1820.63 ms /    63 runs   (   28.90 ms per token,    34.60 tokens per second)
0.02.184.394 I llama_perf_context_print:       total time =    1868.16 ms /    70 tokens
0.02.184.650 I ggml_metal_free: deallocating

real	0m2.516s
user	0m0.119s
sys	0m0.081s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.236 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.414 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.415 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.415 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.417 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.418 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.418 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.421 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.404 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.597 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.598 I llama_model_loader: - type  f32:  194 tensors
0.00.034.598 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.599 I print_info: file format = GGUF V3 (latest)
0.00.034.600 I print_info: file type   = Q8_0
0.00.034.601 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.561 I load: special tokens cache size = 25
0.00.048.380 I load: token to piece cache size = 0.2984 MB
0.00.048.394 I print_info: arch             = gptneox
0.00.048.395 I print_info: vocab_only       = 0
0.00.048.395 I print_info: n_ctx_train      = 2048
0.00.048.396 I print_info: n_embd           = 2048
0.00.048.396 I print_info: n_layer          = 24
0.00.048.400 I print_info: n_head           = 16
0.00.048.401 I print_info: n_head_kv        = 16
0.00.048.401 I print_info: n_rot            = 32
0.00.048.401 I print_info: n_swa            = 0
0.00.048.401 I print_info: n_embd_head_k    = 128
0.00.048.402 I print_info: n_embd_head_v    = 128
0.00.048.402 I print_info: n_gqa            = 1
0.00.048.403 I print_info: n_embd_k_gqa     = 2048
0.00.048.404 I print_info: n_embd_v_gqa     = 2048
0.00.048.405 I print_info: f_norm_eps       = 1.0e-05
0.00.048.405 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.405 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.405 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.405 I print_info: f_logit_scale    = 0.0e+00
0.00.048.406 I print_info: n_ff             = 8192
0.00.048.406 I print_info: n_expert         = 0
0.00.048.406 I print_info: n_expert_used    = 0
0.00.048.407 I print_info: causal attn      = 1
0.00.048.407 I print_info: pooling type     = 0
0.00.048.407 I print_info: rope type        = 2
0.00.048.408 I print_info: rope scaling     = linear
0.00.048.409 I print_info: freq_base_train  = 10000.0
0.00.048.409 I print_info: freq_scale_train = 1
0.00.048.409 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.410 I print_info: rope_finetuned   = unknown
0.00.048.410 I print_info: ssm_d_conv       = 0
0.00.048.410 I print_info: ssm_d_inner      = 0
0.00.048.410 I print_info: ssm_d_state      = 0
0.00.048.410 I print_info: ssm_dt_rank      = 0
0.00.048.410 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.411 I print_info: model type       = 1.4B
0.00.048.411 I print_info: model params     = 1.41 B
0.00.048.411 I print_info: general.name     = 1.4B
0.00.048.412 I print_info: vocab type       = BPE
0.00.048.412 I print_info: n_vocab          = 50304
0.00.048.412 I print_info: n_merges         = 50009
0.00.048.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.413 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.413 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.413 I print_info: LF token         = 187 'Ċ'
0.00.048.413 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.413 I print_info: max token length = 1024
0.00.048.415 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.231.269 I load_tensors: offloading 24 repeating layers to GPU
0.01.231.274 I load_tensors: offloading output layer to GPU
0.01.231.276 I load_tensors: offloaded 25/25 layers to GPU
0.01.231.295 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.231.298 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.232.330 I llama_context: constructing llama_context
0.01.232.332 I llama_context: n_seq_max     = 1
0.01.232.333 I llama_context: n_ctx         = 2048
0.01.232.333 I llama_context: n_ctx_per_seq = 2048
0.01.232.333 I llama_context: n_batch       = 2048
0.01.232.333 I llama_context: n_ubatch      = 512
0.01.232.334 I llama_context: flash_attn    = 0
0.01.232.334 I llama_context: freq_base     = 10000.0
0.01.232.335 I llama_context: freq_scale    = 1
0.01.232.335 I ggml_metal_init: allocating
0.01.232.345 I ggml_metal_init: found device: Apple M4
0.01.232.351 I ggml_metal_init: picking default device: Apple M4
0.01.233.636 I ggml_metal_init: using embedded metal library
0.01.239.210 I ggml_metal_init: GPU name:   Apple M4
0.01.239.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.239.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.239.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.239.215 I ggml_metal_init: simdgroup reduction   = true
0.01.239.215 I ggml_metal_init: simdgroup matrix mul. = true
0.01.239.215 I ggml_metal_init: has residency sets    = true
0.01.239.216 I ggml_metal_init: has bfloat            = true
0.01.239.216 I ggml_metal_init: use bfloat            = true
0.01.239.217 I ggml_metal_init: hasUnifiedMemory      = true
0.01.239.218 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.254.681 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.265.996 I init:      Metal compute buffer size =   102.25 MiB
0.01.265.997 I init:        CPU compute buffer size =     5.01 MiB
0.01.265.998 I init: graph nodes  = 943
0.01.265.998 I init: graph splits = 2
0.01.266.000 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.01.266.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.266.176 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.266.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.291.436 I main: llama threadpool init, n_threads = 4
0.01.291.473 I 
0.01.291.488 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.291.488 I 
0.01.291.642 I sampler seed: 1234
0.01.291.647 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.291.657 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.291.657 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.291.657 I 
I believe the meaning of life is to do the way the same time-1.










" to the following:




    }



The results in this year to get him, and the way.








" (3. The

0.02.373.845 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63111.11 tokens per second)
0.02.373.845 I llama_perf_context_print:        load time =    1281.26 ms
0.02.373.846 I llama_perf_context_print: prompt eval time =      48.77 ms /     7 tokens (    6.97 ms per token,   143.53 tokens per second)
0.02.373.847 I llama_perf_context_print:        eval time =    1030.83 ms /    63 runs   (   16.36 ms per token,    61.12 tokens per second)
0.02.373.848 I llama_perf_context_print:       total time =    1083.34 ms /    70 tokens
0.02.374.117 I ggml_metal_free: deallocating

real	0m2.390s
user	0m0.099s
sys	0m0.186s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.920 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.002 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.015 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.017 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.017 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.017 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.018 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.020 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.022 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.023 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.023 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.724 I llama_model_loader: - type  f32:  194 tensors
0.00.038.725 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.726 I print_info: file format = GGUF V3 (latest)
0.00.038.726 I print_info: file type   = Q4_0
0.00.038.727 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.220 I load: special tokens cache size = 25
0.00.055.593 I load: token to piece cache size = 0.2984 MB
0.00.055.609 I print_info: arch             = gptneox
0.00.055.611 I print_info: vocab_only       = 0
0.00.055.611 I print_info: n_ctx_train      = 2048
0.00.055.611 I print_info: n_embd           = 2048
0.00.055.612 I print_info: n_layer          = 24
0.00.055.616 I print_info: n_head           = 16
0.00.055.617 I print_info: n_head_kv        = 16
0.00.055.617 I print_info: n_rot            = 32
0.00.055.617 I print_info: n_swa            = 0
0.00.055.617 I print_info: n_embd_head_k    = 128
0.00.055.618 I print_info: n_embd_head_v    = 128
0.00.055.620 I print_info: n_gqa            = 1
0.00.055.621 I print_info: n_embd_k_gqa     = 2048
0.00.055.622 I print_info: n_embd_v_gqa     = 2048
0.00.055.623 I print_info: f_norm_eps       = 1.0e-05
0.00.055.623 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.623 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.623 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.624 I print_info: f_logit_scale    = 0.0e+00
0.00.055.625 I print_info: n_ff             = 8192
0.00.055.630 I print_info: n_expert         = 0
0.00.055.630 I print_info: n_expert_used    = 0
0.00.055.630 I print_info: causal attn      = 1
0.00.055.630 I print_info: pooling type     = 0
0.00.055.631 I print_info: rope type        = 2
0.00.055.631 I print_info: rope scaling     = linear
0.00.055.631 I print_info: freq_base_train  = 10000.0
0.00.055.636 I print_info: freq_scale_train = 1
0.00.055.637 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.637 I print_info: rope_finetuned   = unknown
0.00.055.637 I print_info: ssm_d_conv       = 0
0.00.055.638 I print_info: ssm_d_inner      = 0
0.00.055.638 I print_info: ssm_d_state      = 0
0.00.055.638 I print_info: ssm_dt_rank      = 0
0.00.055.638 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.638 I print_info: model type       = 1.4B
0.00.055.639 I print_info: model params     = 1.41 B
0.00.055.641 I print_info: general.name     = 1.4B
0.00.055.642 I print_info: vocab type       = BPE
0.00.055.642 I print_info: n_vocab          = 50304
0.00.055.644 I print_info: n_merges         = 50009
0.00.055.644 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.644 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.645 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.655 I print_info: LF token         = 187 'Ċ'
0.00.055.656 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.656 I print_info: max token length = 1024
0.00.055.657 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.708.639 I load_tensors: offloading 24 repeating layers to GPU
0.00.708.655 I load_tensors: offloading output layer to GPU
0.00.708.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.708.696 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.708.697 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.710.090 I llama_context: constructing llama_context
0.00.710.094 I llama_context: n_seq_max     = 1
0.00.710.094 I llama_context: n_ctx         = 2048
0.00.710.095 I llama_context: n_ctx_per_seq = 2048
0.00.710.095 I llama_context: n_batch       = 2048
0.00.710.096 I llama_context: n_ubatch      = 512
0.00.710.096 I llama_context: flash_attn    = 0
0.00.710.099 I llama_context: freq_base     = 10000.0
0.00.710.099 I llama_context: freq_scale    = 1
0.00.710.101 I ggml_metal_init: allocating
0.00.710.227 I ggml_metal_init: found device: Apple M4
0.00.710.242 I ggml_metal_init: picking default device: Apple M4
0.00.712.216 I ggml_metal_init: using embedded metal library
0.00.717.875 I ggml_metal_init: GPU name:   Apple M4
0.00.717.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.882 I ggml_metal_init: simdgroup reduction   = true
0.00.717.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.883 I ggml_metal_init: has residency sets    = true
0.00.717.883 I ggml_metal_init: has bfloat            = true
0.00.717.883 I ggml_metal_init: use bfloat            = true
0.00.717.884 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.737.180 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.748.427 I init:      Metal compute buffer size =   102.25 MiB
0.00.748.429 I init:        CPU compute buffer size =     5.01 MiB
0.00.748.429 I init: graph nodes  = 943
0.00.748.430 I init: graph splits = 2
0.00.748.433 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.748.440 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.975 I main: llama threadpool init, n_threads = 4
0.00.774.016 I 
0.00.774.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.031 I 
0.00.774.205 I sampler seed: 1234
0.00.774.211 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.221 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.222 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.222 I 
I believe the meaning of life is to sell them to


“It’s a large masses.
The first time. Influence, the other people don’s, as possible.com. Aspectate (d
' (Table 6. A A A A A A A A A A A A A.html">
It

0.01.451.754 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.451.754 I llama_perf_context_print:        load time =     763.06 ms
0.01.451.755 I llama_perf_context_print: prompt eval time =      48.99 ms /     7 tokens (    7.00 ms per token,   142.88 tokens per second)
0.01.451.756 I llama_perf_context_print:        eval time =     625.89 ms /    63 runs   (    9.93 ms per token,   100.66 tokens per second)
0.01.451.756 I llama_perf_context_print:       total time =     678.77 ms /    70 tokens
0.01.451.983 I ggml_metal_free: deallocating

real	0m1.477s
user	0m0.107s
sys	0m0.135s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.017.018 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.037.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.554 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.047.433 I llama_model_loader: - type  f32:  194 tensors
0.00.047.433 I llama_model_loader: - type q4_1:   97 tensors
0.00.047.433 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.434 I print_info: file format = GGUF V3 (latest)
0.00.047.434 I print_info: file type   = Q4_1
0.00.047.438 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.056.620 I load: special tokens cache size = 25
0.00.064.959 I load: token to piece cache size = 0.2984 MB
0.00.064.974 I print_info: arch             = gptneox
0.00.064.975 I print_info: vocab_only       = 0
0.00.064.975 I print_info: n_ctx_train      = 2048
0.00.064.976 I print_info: n_embd           = 2048
0.00.064.976 I print_info: n_layer          = 24
0.00.064.979 I print_info: n_head           = 16
0.00.064.980 I print_info: n_head_kv        = 16
0.00.064.980 I print_info: n_rot            = 32
0.00.064.980 I print_info: n_swa            = 0
0.00.064.981 I print_info: n_embd_head_k    = 128
0.00.064.981 I print_info: n_embd_head_v    = 128
0.00.064.982 I print_info: n_gqa            = 1
0.00.064.983 I print_info: n_embd_k_gqa     = 2048
0.00.064.983 I print_info: n_embd_v_gqa     = 2048
0.00.064.984 I print_info: f_norm_eps       = 1.0e-05
0.00.064.984 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.984 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.987 I print_info: f_logit_scale    = 0.0e+00
0.00.064.988 I print_info: n_ff             = 8192
0.00.064.988 I print_info: n_expert         = 0
0.00.064.989 I print_info: n_expert_used    = 0
0.00.064.989 I print_info: causal attn      = 1
0.00.064.989 I print_info: pooling type     = 0
0.00.064.989 I print_info: rope type        = 2
0.00.064.989 I print_info: rope scaling     = linear
0.00.064.990 I print_info: freq_base_train  = 10000.0
0.00.064.990 I print_info: freq_scale_train = 1
0.00.064.990 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.991 I print_info: rope_finetuned   = unknown
0.00.064.991 I print_info: ssm_d_conv       = 0
0.00.064.991 I print_info: ssm_d_inner      = 0
0.00.064.991 I print_info: ssm_d_state      = 0
0.00.064.991 I print_info: ssm_dt_rank      = 0
0.00.064.991 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.992 I print_info: model type       = 1.4B
0.00.064.996 I print_info: model params     = 1.41 B
0.00.064.996 I print_info: general.name     = 1.4B
0.00.064.997 I print_info: vocab type       = BPE
0.00.064.997 I print_info: n_vocab          = 50304
0.00.064.998 I print_info: n_merges         = 50009
0.00.064.998 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.998 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.998 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.999 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.999 I print_info: LF token         = 187 'Ċ'
0.00.064.999 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.001 I print_info: max token length = 1024
0.00.065.001 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.683.250 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.268 I load_tensors: offloading output layer to GPU
0.00.683.269 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.302 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.683.303 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.684.888 I llama_context: constructing llama_context
0.00.684.891 I llama_context: n_seq_max     = 1
0.00.684.892 I llama_context: n_ctx         = 2048
0.00.684.892 I llama_context: n_ctx_per_seq = 2048
0.00.684.893 I llama_context: n_batch       = 2048
0.00.684.893 I llama_context: n_ubatch      = 512
0.00.684.893 I llama_context: flash_attn    = 0
0.00.684.896 I llama_context: freq_base     = 10000.0
0.00.684.903 I llama_context: freq_scale    = 1
0.00.684.905 I ggml_metal_init: allocating
0.00.684.979 I ggml_metal_init: found device: Apple M4
0.00.685.002 I ggml_metal_init: picking default device: Apple M4
0.00.686.932 I ggml_metal_init: using embedded metal library
0.00.692.393 I ggml_metal_init: GPU name:   Apple M4
0.00.692.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.405 I ggml_metal_init: simdgroup reduction   = true
0.00.692.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.405 I ggml_metal_init: has residency sets    = true
0.00.692.405 I ggml_metal_init: has bfloat            = true
0.00.692.406 I ggml_metal_init: use bfloat            = true
0.00.692.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.433 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.724.588 I init:      Metal compute buffer size =   102.25 MiB
0.00.724.590 I init:        CPU compute buffer size =     5.01 MiB
0.00.724.591 I init: graph nodes  = 943
0.00.724.591 I init: graph splits = 2
0.00.724.594 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.724.603 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.811 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.063 I main: llama threadpool init, n_threads = 4
0.00.752.101 I 
0.00.752.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.117 I 
0.00.752.307 I sampler seed: 1234
0.00.752.313 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.323 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.323 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.323 I 
I believe the meaning of life is that.



















"







"







" class='stylesheet.




















0.01.465.846 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.01.465.847 I llama_perf_context_print:        load time =     734.07 ms
0.01.465.848 I llama_perf_context_print: prompt eval time =      49.14 ms /     7 tokens (    7.02 ms per token,   142.46 tokens per second)
0.01.465.851 I llama_perf_context_print:        eval time =     661.94 ms /    63 runs   (   10.51 ms per token,    95.17 tokens per second)
0.01.465.852 I llama_perf_context_print:       total time =     714.76 ms /    70 tokens
0.01.466.056 I ggml_metal_free: deallocating

real	0m1.482s
user	0m0.107s
sys	0m0.136s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.023 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.723 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.724 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.726 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.720 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.516 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.518 I llama_model_loader: - type  f32:  194 tensors
0.00.025.519 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.519 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.519 I print_info: file format = GGUF V3 (latest)
0.00.025.520 I print_info: file type   = Q5_0
0.00.025.524 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.547 I load: special tokens cache size = 25
0.00.039.580 I load: token to piece cache size = 0.2984 MB
0.00.039.595 I print_info: arch             = gptneox
0.00.039.596 I print_info: vocab_only       = 0
0.00.039.596 I print_info: n_ctx_train      = 2048
0.00.039.596 I print_info: n_embd           = 2048
0.00.039.596 I print_info: n_layer          = 24
0.00.039.599 I print_info: n_head           = 16
0.00.039.600 I print_info: n_head_kv        = 16
0.00.039.600 I print_info: n_rot            = 32
0.00.039.600 I print_info: n_swa            = 0
0.00.039.601 I print_info: n_embd_head_k    = 128
0.00.039.601 I print_info: n_embd_head_v    = 128
0.00.039.602 I print_info: n_gqa            = 1
0.00.039.602 I print_info: n_embd_k_gqa     = 2048
0.00.039.603 I print_info: n_embd_v_gqa     = 2048
0.00.039.604 I print_info: f_norm_eps       = 1.0e-05
0.00.039.604 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.606 I print_info: f_logit_scale    = 0.0e+00
0.00.039.607 I print_info: n_ff             = 8192
0.00.039.607 I print_info: n_expert         = 0
0.00.039.607 I print_info: n_expert_used    = 0
0.00.039.607 I print_info: causal attn      = 1
0.00.039.607 I print_info: pooling type     = 0
0.00.039.609 I print_info: rope type        = 2
0.00.039.609 I print_info: rope scaling     = linear
0.00.039.609 I print_info: freq_base_train  = 10000.0
0.00.039.612 I print_info: freq_scale_train = 1
0.00.039.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.613 I print_info: rope_finetuned   = unknown
0.00.039.613 I print_info: ssm_d_conv       = 0
0.00.039.613 I print_info: ssm_d_inner      = 0
0.00.039.615 I print_info: ssm_d_state      = 0
0.00.039.615 I print_info: ssm_dt_rank      = 0
0.00.039.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.615 I print_info: model type       = 1.4B
0.00.039.616 I print_info: model params     = 1.41 B
0.00.039.616 I print_info: general.name     = 1.4B
0.00.039.616 I print_info: vocab type       = BPE
0.00.039.616 I print_info: n_vocab          = 50304
0.00.039.616 I print_info: n_merges         = 50009
0.00.039.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.617 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: LF token         = 187 'Ċ'
0.00.039.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: max token length = 1024
0.00.039.623 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.711.096 I load_tensors: offloading 24 repeating layers to GPU
0.00.711.112 I load_tensors: offloading output layer to GPU
0.00.711.113 I load_tensors: offloaded 25/25 layers to GPU
0.00.711.146 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.711.147 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.712.600 I llama_context: constructing llama_context
0.00.712.604 I llama_context: n_seq_max     = 1
0.00.712.605 I llama_context: n_ctx         = 2048
0.00.712.606 I llama_context: n_ctx_per_seq = 2048
0.00.712.606 I llama_context: n_batch       = 2048
0.00.712.606 I llama_context: n_ubatch      = 512
0.00.712.607 I llama_context: flash_attn    = 0
0.00.712.609 I llama_context: freq_base     = 10000.0
0.00.712.610 I llama_context: freq_scale    = 1
0.00.712.612 I ggml_metal_init: allocating
0.00.712.692 I ggml_metal_init: found device: Apple M4
0.00.712.716 I ggml_metal_init: picking default device: Apple M4
0.00.714.286 I ggml_metal_init: using embedded metal library
0.00.720.625 I ggml_metal_init: GPU name:   Apple M4
0.00.720.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.720.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.720.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.720.631 I ggml_metal_init: simdgroup reduction   = true
0.00.720.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.720.632 I ggml_metal_init: has residency sets    = true
0.00.720.632 I ggml_metal_init: has bfloat            = true
0.00.720.632 I ggml_metal_init: use bfloat            = true
0.00.720.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.720.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.737.994 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.750.564 I init:      Metal compute buffer size =   102.25 MiB
0.00.750.566 I init:        CPU compute buffer size =     5.01 MiB
0.00.750.567 I init: graph nodes  = 943
0.00.750.567 I init: graph splits = 2
0.00.750.570 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.750.581 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.799 I main: llama threadpool init, n_threads = 4
0.00.777.844 I 
0.00.777.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.858 I 
0.00.778.015 I sampler seed: 1234
0.00.778.021 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.031 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.031 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.032 I 
I believe the meaning of life is to have a result is a little on the



<div>



<c^2d-on and a new (E_2


<tr> <http://www.


<T_t_i= = 0. He also has been introduced

0.01.541.521 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.541.522 I llama_perf_context_print:        load time =     767.86 ms
0.01.541.524 I llama_perf_context_print: prompt eval time =      42.92 ms /     7 tokens (    6.13 ms per token,   163.11 tokens per second)
0.01.541.524 I llama_perf_context_print:        eval time =     718.03 ms /    63 runs   (   11.40 ms per token,    87.74 tokens per second)
0.01.541.525 I llama_perf_context_print:       total time =     764.64 ms /    70 tokens
0.01.541.798 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.100s
sys	0m0.159s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.637 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.334 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.335 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.335 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.337 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.337 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.338 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.338 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.339 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.392 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.198 I llama_model_loader: - type  f32:  194 tensors
0.00.025.198 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.199 I print_info: file format = GGUF V3 (latest)
0.00.025.199 I print_info: file type   = Q5_1
0.00.025.200 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.309 I load: special tokens cache size = 25
0.00.039.458 I load: token to piece cache size = 0.2984 MB
0.00.039.472 I print_info: arch             = gptneox
0.00.039.473 I print_info: vocab_only       = 0
0.00.039.474 I print_info: n_ctx_train      = 2048
0.00.039.474 I print_info: n_embd           = 2048
0.00.039.474 I print_info: n_layer          = 24
0.00.039.477 I print_info: n_head           = 16
0.00.039.478 I print_info: n_head_kv        = 16
0.00.039.478 I print_info: n_rot            = 32
0.00.039.480 I print_info: n_swa            = 0
0.00.039.481 I print_info: n_embd_head_k    = 128
0.00.039.481 I print_info: n_embd_head_v    = 128
0.00.039.482 I print_info: n_gqa            = 1
0.00.039.483 I print_info: n_embd_k_gqa     = 2048
0.00.039.483 I print_info: n_embd_v_gqa     = 2048
0.00.039.484 I print_info: f_norm_eps       = 1.0e-05
0.00.039.484 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.484 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.484 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.484 I print_info: f_logit_scale    = 0.0e+00
0.00.039.485 I print_info: n_ff             = 8192
0.00.039.485 I print_info: n_expert         = 0
0.00.039.485 I print_info: n_expert_used    = 0
0.00.039.486 I print_info: causal attn      = 1
0.00.039.486 I print_info: pooling type     = 0
0.00.039.486 I print_info: rope type        = 2
0.00.039.486 I print_info: rope scaling     = linear
0.00.039.487 I print_info: freq_base_train  = 10000.0
0.00.039.487 I print_info: freq_scale_train = 1
0.00.039.487 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.487 I print_info: rope_finetuned   = unknown
0.00.039.487 I print_info: ssm_d_conv       = 0
0.00.039.487 I print_info: ssm_d_inner      = 0
0.00.039.488 I print_info: ssm_d_state      = 0
0.00.039.488 I print_info: ssm_dt_rank      = 0
0.00.039.488 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.488 I print_info: model type       = 1.4B
0.00.039.489 I print_info: model params     = 1.41 B
0.00.039.489 I print_info: general.name     = 1.4B
0.00.039.489 I print_info: vocab type       = BPE
0.00.039.489 I print_info: n_vocab          = 50304
0.00.039.490 I print_info: n_merges         = 50009
0.00.039.490 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.491 I print_info: LF token         = 187 'Ċ'
0.00.039.491 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.491 I print_info: max token length = 1024
0.00.039.491 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.145 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.158 I load_tensors: offloading output layer to GPU
0.00.620.159 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.187 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.620.188 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.621.689 I llama_context: constructing llama_context
0.00.621.696 I llama_context: n_seq_max     = 1
0.00.621.697 I llama_context: n_ctx         = 2048
0.00.621.697 I llama_context: n_ctx_per_seq = 2048
0.00.621.698 I llama_context: n_batch       = 2048
0.00.621.698 I llama_context: n_ubatch      = 512
0.00.621.699 I llama_context: flash_attn    = 0
0.00.621.700 I llama_context: freq_base     = 10000.0
0.00.621.700 I llama_context: freq_scale    = 1
0.00.621.703 I ggml_metal_init: allocating
0.00.621.752 I ggml_metal_init: found device: Apple M4
0.00.621.774 I ggml_metal_init: picking default device: Apple M4
0.00.623.551 I ggml_metal_init: using embedded metal library
0.00.630.185 I ggml_metal_init: GPU name:   Apple M4
0.00.630.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.192 I ggml_metal_init: simdgroup reduction   = true
0.00.630.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.193 I ggml_metal_init: has residency sets    = true
0.00.630.193 I ggml_metal_init: has bfloat            = true
0.00.630.193 I ggml_metal_init: use bfloat            = true
0.00.630.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.792 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.659.655 I init:      Metal compute buffer size =   102.25 MiB
0.00.659.657 I init:        CPU compute buffer size =     5.01 MiB
0.00.659.657 I init: graph nodes  = 943
0.00.659.657 I init: graph splits = 2
0.00.659.660 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.659.672 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.659.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.659.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.532 I main: llama threadpool init, n_threads = 4
0.00.686.574 I 
0.00.686.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.592 I 
0.00.686.798 I sampler seed: 1234
0.00.686.805 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.815 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.815 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.816 I 
I believe the meaning of life is to do is an end of the next day, and then the only a lot of the following the point for it is the first.



"







A) in a result in the first in the most likely to the following a lot of the case is the most recently

0.01.502.563 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.502.564 I llama_perf_context_print:        load time =     676.98 ms
0.01.502.565 I llama_perf_context_print: prompt eval time =      42.08 ms /     7 tokens (    6.01 ms per token,   166.37 tokens per second)
0.01.502.565 I llama_perf_context_print:        eval time =     771.03 ms /    63 runs   (   12.24 ms per token,    81.71 tokens per second)
0.01.502.566 I llama_perf_context_print:       total time =     816.95 ms /    70 tokens
0.01.502.785 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.102s
sys	0m0.151s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.521 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.074 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.081 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.085 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.086 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.087 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.089 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.026 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.007 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.007 I llama_model_loader: - type  f32:  194 tensors
0.00.025.008 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.008 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.008 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.009 I print_info: file format = GGUF V3 (latest)
0.00.025.009 I print_info: file type   = Q2_K - Medium
0.00.025.010 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.856 I load: special tokens cache size = 25
0.00.038.901 I load: token to piece cache size = 0.2984 MB
0.00.038.915 I print_info: arch             = gptneox
0.00.038.916 I print_info: vocab_only       = 0
0.00.038.916 I print_info: n_ctx_train      = 2048
0.00.038.917 I print_info: n_embd           = 2048
0.00.038.917 I print_info: n_layer          = 24
0.00.038.924 I print_info: n_head           = 16
0.00.038.925 I print_info: n_head_kv        = 16
0.00.038.925 I print_info: n_rot            = 32
0.00.038.925 I print_info: n_swa            = 0
0.00.038.927 I print_info: n_embd_head_k    = 128
0.00.038.927 I print_info: n_embd_head_v    = 128
0.00.038.927 I print_info: n_gqa            = 1
0.00.038.928 I print_info: n_embd_k_gqa     = 2048
0.00.038.929 I print_info: n_embd_v_gqa     = 2048
0.00.038.929 I print_info: f_norm_eps       = 1.0e-05
0.00.038.929 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.930 I print_info: f_logit_scale    = 0.0e+00
0.00.038.930 I print_info: n_ff             = 8192
0.00.038.931 I print_info: n_expert         = 0
0.00.038.931 I print_info: n_expert_used    = 0
0.00.038.931 I print_info: causal attn      = 1
0.00.038.931 I print_info: pooling type     = 0
0.00.038.931 I print_info: rope type        = 2
0.00.038.933 I print_info: rope scaling     = linear
0.00.038.933 I print_info: freq_base_train  = 10000.0
0.00.038.933 I print_info: freq_scale_train = 1
0.00.038.933 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.933 I print_info: rope_finetuned   = unknown
0.00.038.933 I print_info: ssm_d_conv       = 0
0.00.038.934 I print_info: ssm_d_inner      = 0
0.00.038.935 I print_info: ssm_d_state      = 0
0.00.038.935 I print_info: ssm_dt_rank      = 0
0.00.038.935 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.935 I print_info: model type       = 1.4B
0.00.038.935 I print_info: model params     = 1.41 B
0.00.038.936 I print_info: general.name     = 1.4B
0.00.038.936 I print_info: vocab type       = BPE
0.00.038.936 I print_info: n_vocab          = 50304
0.00.038.937 I print_info: n_merges         = 50009
0.00.038.937 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: LF token         = 187 'Ċ'
0.00.038.938 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.939 I print_info: max token length = 1024
0.00.038.939 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.349.217 I load_tensors: offloading 24 repeating layers to GPU
0.00.349.235 I load_tensors: offloading output layer to GPU
0.00.349.236 I load_tensors: offloaded 25/25 layers to GPU
0.00.349.270 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.349.271 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.350.991 I llama_context: constructing llama_context
0.00.350.997 I llama_context: n_seq_max     = 1
0.00.350.998 I llama_context: n_ctx         = 2048
0.00.350.999 I llama_context: n_ctx_per_seq = 2048
0.00.350.999 I llama_context: n_batch       = 2048
0.00.350.999 I llama_context: n_ubatch      = 512
0.00.350.999 I llama_context: flash_attn    = 0
0.00.351.001 I llama_context: freq_base     = 10000.0
0.00.351.002 I llama_context: freq_scale    = 1
0.00.351.004 I ggml_metal_init: allocating
0.00.351.099 I ggml_metal_init: found device: Apple M4
0.00.351.128 I ggml_metal_init: picking default device: Apple M4
0.00.353.064 I ggml_metal_init: using embedded metal library
0.00.358.655 I ggml_metal_init: GPU name:   Apple M4
0.00.358.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.358.677 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.358.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.358.679 I ggml_metal_init: simdgroup reduction   = true
0.00.358.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.358.680 I ggml_metal_init: has residency sets    = true
0.00.358.680 I ggml_metal_init: has bfloat            = true
0.00.358.680 I ggml_metal_init: use bfloat            = true
0.00.358.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.358.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.379.592 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.390.472 I init:      Metal compute buffer size =   102.25 MiB
0.00.390.474 I init:        CPU compute buffer size =     5.01 MiB
0.00.390.475 I init: graph nodes  = 943
0.00.390.476 I init: graph splits = 2
0.00.390.478 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.390.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.390.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.390.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.569 I main: llama threadpool init, n_threads = 4
0.00.416.610 I 
0.00.416.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.627 I 
0.00.416.815 I sampler seed: 1234
0.00.416.820 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.416.830 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.416.831 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.416.831 I 
I believe the meaning of life is the stal?!</__?                                                                                                                                                        }
1}

?</}\|\|\?
-??
_1????                                } \__yy|2???!
                                }
\:???</:1}.

0.01.079.774 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.079.775 I llama_perf_context_print:        load time =     406.14 ms
0.01.079.776 I llama_perf_context_print: prompt eval time =      35.61 ms /     7 tokens (    5.09 ms per token,   196.58 tokens per second)
0.01.079.776 I llama_perf_context_print:        eval time =     624.74 ms /    63 runs   (    9.92 ms per token,   100.84 tokens per second)
0.01.079.777 I llama_perf_context_print:       total time =     664.12 ms /    70 tokens
0.01.079.967 I ggml_metal_free: deallocating

real	0m1.096s
user	0m0.103s
sys	0m0.101s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.162 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.166 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.173 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.173 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.174 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.119 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.119 I llama_model_loader: - type  f32:  194 tensors
0.00.025.119 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.120 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.120 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.120 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.121 I print_info: file format = GGUF V3 (latest)
0.00.025.121 I print_info: file type   = Q3_K - Medium
0.00.025.122 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.999 I load: special tokens cache size = 25
0.00.038.795 I load: token to piece cache size = 0.2984 MB
0.00.038.809 I print_info: arch             = gptneox
0.00.038.810 I print_info: vocab_only       = 0
0.00.038.810 I print_info: n_ctx_train      = 2048
0.00.038.810 I print_info: n_embd           = 2048
0.00.038.811 I print_info: n_layer          = 24
0.00.038.813 I print_info: n_head           = 16
0.00.038.814 I print_info: n_head_kv        = 16
0.00.038.814 I print_info: n_rot            = 32
0.00.038.814 I print_info: n_swa            = 0
0.00.038.815 I print_info: n_embd_head_k    = 128
0.00.038.815 I print_info: n_embd_head_v    = 128
0.00.038.816 I print_info: n_gqa            = 1
0.00.038.816 I print_info: n_embd_k_gqa     = 2048
0.00.038.817 I print_info: n_embd_v_gqa     = 2048
0.00.038.818 I print_info: f_norm_eps       = 1.0e-05
0.00.038.818 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.818 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.820 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.820 I print_info: f_logit_scale    = 0.0e+00
0.00.038.821 I print_info: n_ff             = 8192
0.00.038.821 I print_info: n_expert         = 0
0.00.038.821 I print_info: n_expert_used    = 0
0.00.038.823 I print_info: causal attn      = 1
0.00.038.824 I print_info: pooling type     = 0
0.00.038.824 I print_info: rope type        = 2
0.00.038.824 I print_info: rope scaling     = linear
0.00.038.824 I print_info: freq_base_train  = 10000.0
0.00.038.828 I print_info: freq_scale_train = 1
0.00.038.828 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.828 I print_info: rope_finetuned   = unknown
0.00.038.828 I print_info: ssm_d_conv       = 0
0.00.038.829 I print_info: ssm_d_inner      = 0
0.00.038.829 I print_info: ssm_d_state      = 0
0.00.038.829 I print_info: ssm_dt_rank      = 0
0.00.038.829 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.829 I print_info: model type       = 1.4B
0.00.038.830 I print_info: model params     = 1.41 B
0.00.038.830 I print_info: general.name     = 1.4B
0.00.038.831 I print_info: vocab type       = BPE
0.00.038.831 I print_info: n_vocab          = 50304
0.00.038.831 I print_info: n_merges         = 50009
0.00.038.832 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.832 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.832 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.832 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.833 I print_info: LF token         = 187 'Ċ'
0.00.038.834 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.834 I print_info: max token length = 1024
0.00.038.834 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.434.673 I load_tensors: offloading 24 repeating layers to GPU
0.00.434.689 I load_tensors: offloading output layer to GPU
0.00.434.690 I load_tensors: offloaded 25/25 layers to GPU
0.00.434.724 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.434.725 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.436.427 I llama_context: constructing llama_context
0.00.436.430 I llama_context: n_seq_max     = 1
0.00.436.431 I llama_context: n_ctx         = 2048
0.00.436.431 I llama_context: n_ctx_per_seq = 2048
0.00.436.432 I llama_context: n_batch       = 2048
0.00.436.432 I llama_context: n_ubatch      = 512
0.00.436.432 I llama_context: flash_attn    = 0
0.00.436.435 I llama_context: freq_base     = 10000.0
0.00.436.436 I llama_context: freq_scale    = 1
0.00.436.438 I ggml_metal_init: allocating
0.00.436.512 I ggml_metal_init: found device: Apple M4
0.00.436.538 I ggml_metal_init: picking default device: Apple M4
0.00.438.475 I ggml_metal_init: using embedded metal library
0.00.444.952 I ggml_metal_init: GPU name:   Apple M4
0.00.444.957 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.960 I ggml_metal_init: simdgroup reduction   = true
0.00.444.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.960 I ggml_metal_init: has residency sets    = true
0.00.444.961 I ggml_metal_init: has bfloat            = true
0.00.444.961 I ggml_metal_init: use bfloat            = true
0.00.444.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.831 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.474.800 I init:      Metal compute buffer size =   102.25 MiB
0.00.474.802 I init:        CPU compute buffer size =     5.01 MiB
0.00.474.802 I init: graph nodes  = 943
0.00.474.803 I init: graph splits = 2
0.00.474.805 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.474.812 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.474.992 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.474.993 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.087 I main: llama threadpool init, n_threads = 4
0.00.501.132 I 
0.00.501.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.501.152 I 
0.00.501.331 I sampler seed: 1234
0.00.501.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.351 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.351 I 
I believe the meaning of life is that are


















The U.






	if (or




3 -i}\k - - 2x2.







The variable.
The

0.01.232.760 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.232.761 I llama_perf_context_print:        load time =     491.14 ms
0.01.232.762 I llama_perf_context_print: prompt eval time =      49.40 ms /     7 tokens (    7.06 ms per token,   141.69 tokens per second)
0.01.232.764 I llama_perf_context_print:        eval time =     679.51 ms /    63 runs   (   10.79 ms per token,    92.71 tokens per second)
0.01.232.764 I llama_perf_context_print:       total time =     732.75 ms /    70 tokens
0.01.233.031 I ggml_metal_free: deallocating

real	0m1.247s
user	0m0.104s
sys	0m0.108s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.115 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.939 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.940 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.909 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.046 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.905 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.905 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.906 I llama_model_loader: - type  f32:  194 tensors
0.00.025.906 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.906 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.906 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.907 I print_info: file format = GGUF V3 (latest)
0.00.025.907 I print_info: file type   = Q4_K - Medium
0.00.025.908 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.832 I load: special tokens cache size = 25
0.00.039.651 I load: token to piece cache size = 0.2984 MB
0.00.039.665 I print_info: arch             = gptneox
0.00.039.666 I print_info: vocab_only       = 0
0.00.039.666 I print_info: n_ctx_train      = 2048
0.00.039.666 I print_info: n_embd           = 2048
0.00.039.666 I print_info: n_layer          = 24
0.00.039.669 I print_info: n_head           = 16
0.00.039.670 I print_info: n_head_kv        = 16
0.00.039.670 I print_info: n_rot            = 32
0.00.039.670 I print_info: n_swa            = 0
0.00.039.671 I print_info: n_embd_head_k    = 128
0.00.039.671 I print_info: n_embd_head_v    = 128
0.00.039.671 I print_info: n_gqa            = 1
0.00.039.672 I print_info: n_embd_k_gqa     = 2048
0.00.039.674 I print_info: n_embd_v_gqa     = 2048
0.00.039.675 I print_info: f_norm_eps       = 1.0e-05
0.00.039.675 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.675 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.676 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.676 I print_info: f_logit_scale    = 0.0e+00
0.00.039.676 I print_info: n_ff             = 8192
0.00.039.676 I print_info: n_expert         = 0
0.00.039.677 I print_info: n_expert_used    = 0
0.00.039.677 I print_info: causal attn      = 1
0.00.039.678 I print_info: pooling type     = 0
0.00.039.679 I print_info: rope type        = 2
0.00.039.679 I print_info: rope scaling     = linear
0.00.039.679 I print_info: freq_base_train  = 10000.0
0.00.039.680 I print_info: freq_scale_train = 1
0.00.039.680 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.680 I print_info: rope_finetuned   = unknown
0.00.039.680 I print_info: ssm_d_conv       = 0
0.00.039.680 I print_info: ssm_d_inner      = 0
0.00.039.681 I print_info: ssm_d_state      = 0
0.00.039.681 I print_info: ssm_dt_rank      = 0
0.00.039.681 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.681 I print_info: model type       = 1.4B
0.00.039.681 I print_info: model params     = 1.41 B
0.00.039.682 I print_info: general.name     = 1.4B
0.00.039.682 I print_info: vocab type       = BPE
0.00.039.682 I print_info: n_vocab          = 50304
0.00.039.682 I print_info: n_merges         = 50009
0.00.039.682 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: LF token         = 187 'Ċ'
0.00.039.684 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: max token length = 1024
0.00.039.685 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.548.781 I load_tensors: offloading 24 repeating layers to GPU
0.00.548.798 I load_tensors: offloading output layer to GPU
0.00.548.798 I load_tensors: offloaded 25/25 layers to GPU
0.00.548.830 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.548.831 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.550.165 I llama_context: constructing llama_context
0.00.550.170 I llama_context: n_seq_max     = 1
0.00.550.171 I llama_context: n_ctx         = 2048
0.00.550.171 I llama_context: n_ctx_per_seq = 2048
0.00.550.171 I llama_context: n_batch       = 2048
0.00.550.172 I llama_context: n_ubatch      = 512
0.00.550.172 I llama_context: flash_attn    = 0
0.00.550.175 I llama_context: freq_base     = 10000.0
0.00.550.175 I llama_context: freq_scale    = 1
0.00.550.177 I ggml_metal_init: allocating
0.00.550.246 I ggml_metal_init: found device: Apple M4
0.00.550.292 I ggml_metal_init: picking default device: Apple M4
0.00.552.138 I ggml_metal_init: using embedded metal library
0.00.558.011 I ggml_metal_init: GPU name:   Apple M4
0.00.558.017 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.019 I ggml_metal_init: simdgroup reduction   = true
0.00.558.019 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.019 I ggml_metal_init: has residency sets    = true
0.00.558.019 I ggml_metal_init: has bfloat            = true
0.00.558.020 I ggml_metal_init: use bfloat            = true
0.00.558.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.025 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.573.864 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.584.453 I init:      Metal compute buffer size =   102.25 MiB
0.00.584.456 I init:        CPU compute buffer size =     5.01 MiB
0.00.584.456 I init: graph nodes  = 943
0.00.584.456 I init: graph splits = 2
0.00.584.459 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.584.468 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.584.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.584.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.102 I main: llama threadpool init, n_threads = 4
0.00.614.139 I 
0.00.614.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.152 I 
0.00.614.287 I sampler seed: 1234
0.00.614.292 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.333 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.336 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.336 I 
I believe the meaning of life is that I am currently generated by the model of the










The input to the last week;


    if (!recipe, if $x03. The court held a large numbers were also a local law firmness and the present invention describes a few years in

0.01.363.779 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49134.95 tokens per second)
0.01.363.779 I llama_perf_context_print:        load time =     604.30 ms
0.01.363.781 I llama_perf_context_print: prompt eval time =      57.80 ms /     7 tokens (    8.26 ms per token,   121.11 tokens per second)
0.01.363.782 I llama_perf_context_print:        eval time =     689.26 ms /    63 runs   (   10.94 ms per token,    91.40 tokens per second)
0.01.363.782 I llama_perf_context_print:       total time =     750.36 ms /    70 tokens
0.01.364.053 I ggml_metal_free: deallocating

real	0m1.377s
user	0m0.093s
sys	0m0.144s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.582 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.876 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.888 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.892 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.996 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.029.999 I llama_model_loader: - type  f32:  194 tensors
0.00.029.999 I llama_model_loader: - type q5_K:   61 tensors
0.00.029.999 I llama_model_loader: - type q6_K:   37 tensors
0.00.030.000 I print_info: file format = GGUF V3 (latest)
0.00.030.000 I print_info: file type   = Q5_K - Medium
0.00.030.001 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.037.898 I load: special tokens cache size = 25
0.00.044.119 I load: token to piece cache size = 0.2984 MB
0.00.044.133 I print_info: arch             = gptneox
0.00.044.134 I print_info: vocab_only       = 0
0.00.044.134 I print_info: n_ctx_train      = 2048
0.00.044.135 I print_info: n_embd           = 2048
0.00.044.135 I print_info: n_layer          = 24
0.00.044.137 I print_info: n_head           = 16
0.00.044.138 I print_info: n_head_kv        = 16
0.00.044.138 I print_info: n_rot            = 32
0.00.044.138 I print_info: n_swa            = 0
0.00.044.142 I print_info: n_embd_head_k    = 128
0.00.044.142 I print_info: n_embd_head_v    = 128
0.00.044.142 I print_info: n_gqa            = 1
0.00.044.143 I print_info: n_embd_k_gqa     = 2048
0.00.044.144 I print_info: n_embd_v_gqa     = 2048
0.00.044.144 I print_info: f_norm_eps       = 1.0e-05
0.00.044.145 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.145 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.147 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.147 I print_info: f_logit_scale    = 0.0e+00
0.00.044.148 I print_info: n_ff             = 8192
0.00.044.148 I print_info: n_expert         = 0
0.00.044.148 I print_info: n_expert_used    = 0
0.00.044.148 I print_info: causal attn      = 1
0.00.044.149 I print_info: pooling type     = 0
0.00.044.149 I print_info: rope type        = 2
0.00.044.149 I print_info: rope scaling     = linear
0.00.044.149 I print_info: freq_base_train  = 10000.0
0.00.044.150 I print_info: freq_scale_train = 1
0.00.044.150 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.150 I print_info: rope_finetuned   = unknown
0.00.044.150 I print_info: ssm_d_conv       = 0
0.00.044.150 I print_info: ssm_d_inner      = 0
0.00.044.151 I print_info: ssm_d_state      = 0
0.00.044.151 I print_info: ssm_dt_rank      = 0
0.00.044.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.153 I print_info: model type       = 1.4B
0.00.044.153 I print_info: model params     = 1.41 B
0.00.044.153 I print_info: general.name     = 1.4B
0.00.044.154 I print_info: vocab type       = BPE
0.00.044.154 I print_info: n_vocab          = 50304
0.00.044.154 I print_info: n_merges         = 50009
0.00.044.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.155 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.155 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.155 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.155 I print_info: LF token         = 187 'Ċ'
0.00.044.155 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.156 I print_info: max token length = 1024
0.00.044.156 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.348 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.353 I load_tensors: offloading output layer to GPU
0.00.598.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.370 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.598.372 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.313 I llama_context: constructing llama_context
0.00.599.320 I llama_context: n_seq_max     = 1
0.00.599.320 I llama_context: n_ctx         = 2048
0.00.599.320 I llama_context: n_ctx_per_seq = 2048
0.00.599.321 I llama_context: n_batch       = 2048
0.00.599.321 I llama_context: n_ubatch      = 512
0.00.599.321 I llama_context: flash_attn    = 0
0.00.599.322 I llama_context: freq_base     = 10000.0
0.00.599.323 I llama_context: freq_scale    = 1
0.00.599.324 I ggml_metal_init: allocating
0.00.599.351 I ggml_metal_init: found device: Apple M4
0.00.599.367 I ggml_metal_init: picking default device: Apple M4
0.00.600.462 I ggml_metal_init: using embedded metal library
0.00.608.002 I ggml_metal_init: GPU name:   Apple M4
0.00.608.009 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.009 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.010 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.011 I ggml_metal_init: simdgroup reduction   = true
0.00.608.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.011 I ggml_metal_init: has residency sets    = true
0.00.608.011 I ggml_metal_init: has bfloat            = true
0.00.608.012 I ggml_metal_init: use bfloat            = true
0.00.608.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.936 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.624.477 I init:      Metal compute buffer size =   102.25 MiB
0.00.624.479 I init:        CPU compute buffer size =     5.01 MiB
0.00.624.479 I init: graph nodes  = 943
0.00.624.479 I init: graph splits = 2
0.00.624.481 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.624.485 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.624.598 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.624.599 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.978 I main: llama threadpool init, n_threads = 4
0.00.658.018 I 
0.00.658.032 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.033 I 
0.00.658.207 I sampler seed: 1234
0.00.658.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.658.230 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.658.230 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.658.230 I 
I believe the meaning of life is to a way of the same.













The case, and a long, the most likely be a few months, and is not be at the presence, so that the most popular in the point, and the case, and the old and the
The

0.01.486.522 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.486.522 I llama_perf_context_print:        load time =     647.67 ms
0.01.486.523 I llama_perf_context_print: prompt eval time =      52.56 ms /     7 tokens (    7.51 ms per token,   133.18 tokens per second)
0.01.486.524 I llama_perf_context_print:        eval time =     773.19 ms /    63 runs   (   12.27 ms per token,    81.48 tokens per second)
0.01.486.524 I llama_perf_context_print:       total time =     829.27 ms /    70 tokens
0.01.486.747 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.088s
sys	0m0.113s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.499 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.505 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.506 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.491 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.492 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.493 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.493 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.493 I llama_model_loader: - type  f32:  194 tensors
0.00.024.494 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.494 I print_info: file format = GGUF V3 (latest)
0.00.024.495 I print_info: file type   = Q6_K
0.00.024.497 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.637 I load: special tokens cache size = 25
0.00.038.844 I load: token to piece cache size = 0.2984 MB
0.00.038.858 I print_info: arch             = gptneox
0.00.038.859 I print_info: vocab_only       = 0
0.00.038.859 I print_info: n_ctx_train      = 2048
0.00.038.860 I print_info: n_embd           = 2048
0.00.038.860 I print_info: n_layer          = 24
0.00.038.863 I print_info: n_head           = 16
0.00.038.863 I print_info: n_head_kv        = 16
0.00.038.863 I print_info: n_rot            = 32
0.00.038.864 I print_info: n_swa            = 0
0.00.038.864 I print_info: n_embd_head_k    = 128
0.00.038.864 I print_info: n_embd_head_v    = 128
0.00.038.865 I print_info: n_gqa            = 1
0.00.038.865 I print_info: n_embd_k_gqa     = 2048
0.00.038.868 I print_info: n_embd_v_gqa     = 2048
0.00.038.869 I print_info: f_norm_eps       = 1.0e-05
0.00.038.869 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.869 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.869 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.870 I print_info: f_logit_scale    = 0.0e+00
0.00.038.870 I print_info: n_ff             = 8192
0.00.038.870 I print_info: n_expert         = 0
0.00.038.871 I print_info: n_expert_used    = 0
0.00.038.871 I print_info: causal attn      = 1
0.00.038.871 I print_info: pooling type     = 0
0.00.038.871 I print_info: rope type        = 2
0.00.038.871 I print_info: rope scaling     = linear
0.00.038.871 I print_info: freq_base_train  = 10000.0
0.00.038.872 I print_info: freq_scale_train = 1
0.00.038.872 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.873 I print_info: rope_finetuned   = unknown
0.00.038.873 I print_info: ssm_d_conv       = 0
0.00.038.873 I print_info: ssm_d_inner      = 0
0.00.038.873 I print_info: ssm_d_state      = 0
0.00.038.873 I print_info: ssm_dt_rank      = 0
0.00.038.873 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.874 I print_info: model type       = 1.4B
0.00.038.874 I print_info: model params     = 1.41 B
0.00.038.874 I print_info: general.name     = 1.4B
0.00.038.875 I print_info: vocab type       = BPE
0.00.038.875 I print_info: n_vocab          = 50304
0.00.038.875 I print_info: n_merges         = 50009
0.00.038.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: LF token         = 187 'Ċ'
0.00.038.876 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: max token length = 1024
0.00.038.877 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.679.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.164 I load_tensors: offloading output layer to GPU
0.00.679.165 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.188 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.679.190 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.680.705 I llama_context: constructing llama_context
0.00.680.708 I llama_context: n_seq_max     = 1
0.00.680.708 I llama_context: n_ctx         = 2048
0.00.680.709 I llama_context: n_ctx_per_seq = 2048
0.00.680.709 I llama_context: n_batch       = 2048
0.00.680.710 I llama_context: n_ubatch      = 512
0.00.680.710 I llama_context: flash_attn    = 0
0.00.680.711 I llama_context: freq_base     = 10000.0
0.00.680.712 I llama_context: freq_scale    = 1
0.00.680.713 I ggml_metal_init: allocating
0.00.680.760 I ggml_metal_init: found device: Apple M4
0.00.680.784 I ggml_metal_init: picking default device: Apple M4
0.00.682.345 I ggml_metal_init: using embedded metal library
0.00.691.646 I ggml_metal_init: GPU name:   Apple M4
0.00.691.649 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.650 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.651 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.651 I ggml_metal_init: simdgroup reduction   = true
0.00.691.651 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.652 I ggml_metal_init: has residency sets    = true
0.00.691.652 I ggml_metal_init: has bfloat            = true
0.00.691.652 I ggml_metal_init: use bfloat            = true
0.00.691.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.939 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.719.434 I init:      Metal compute buffer size =   102.25 MiB
0.00.719.437 I init:        CPU compute buffer size =     5.01 MiB
0.00.719.438 I init: graph nodes  = 943
0.00.719.438 I init: graph splits = 2
0.00.719.441 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.719.451 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.579 I main: llama threadpool init, n_threads = 4
0.00.759.618 I 
0.00.759.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.632 I 
0.00.759.813 I sampler seed: 1234
0.00.759.818 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.857 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.858 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.860 I 
I believe the meaning of life is to get it will also a little in the first, the other things.





The two-2)




<div>





  0
" class="fig"}). The fact that are the





" (P. The

0.01.617.894 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.617.895 I llama_perf_context_print:        load time =     749.98 ms
0.01.617.896 I llama_perf_context_print: prompt eval time =      57.80 ms /     7 tokens (    8.26 ms per token,   121.10 tokens per second)
0.01.617.897 I llama_perf_context_print:        eval time =     797.55 ms /    63 runs   (   12.66 ms per token,    78.99 tokens per second)
0.01.617.898 I llama_perf_context_print:       total time =     859.13 ms /    70 tokens
0.01.618.181 I ggml_metal_free: deallocating

real	0m1.632s
user	0m0.098s
sys	0m0.171s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.663 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.069 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.175 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.194 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.198 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.203 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.203 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.321 I llama_model_loader: - type  f32:  194 tensors
0.00.057.321 I llama_model_loader: - type  f16:   98 tensors
0.00.057.322 I print_info: file format = GGUF V3 (latest)
0.00.057.323 I print_info: file type   = all F32 (guessed)
0.00.057.324 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.148 I load: special tokens cache size = 25
0.00.078.066 I load: token to piece cache size = 0.2984 MB
0.00.078.080 I print_info: arch             = gptneox
0.00.078.082 I print_info: vocab_only       = 0
0.00.078.082 I print_info: n_ctx_train      = 2048
0.00.078.082 I print_info: n_embd           = 2048
0.00.078.082 I print_info: n_layer          = 24
0.00.078.085 I print_info: n_head           = 16
0.00.078.086 I print_info: n_head_kv        = 16
0.00.078.087 I print_info: n_rot            = 32
0.00.078.087 I print_info: n_swa            = 0
0.00.078.087 I print_info: n_embd_head_k    = 128
0.00.078.087 I print_info: n_embd_head_v    = 128
0.00.078.088 I print_info: n_gqa            = 1
0.00.078.089 I print_info: n_embd_k_gqa     = 2048
0.00.078.090 I print_info: n_embd_v_gqa     = 2048
0.00.078.092 I print_info: f_norm_eps       = 1.0e-05
0.00.078.092 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.092 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.093 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.093 I print_info: f_logit_scale    = 0.0e+00
0.00.078.096 I print_info: n_ff             = 8192
0.00.078.096 I print_info: n_expert         = 0
0.00.078.096 I print_info: n_expert_used    = 0
0.00.078.096 I print_info: causal attn      = 1
0.00.078.096 I print_info: pooling type     = 0
0.00.078.096 I print_info: rope type        = 2
0.00.078.097 I print_info: rope scaling     = linear
0.00.078.097 I print_info: freq_base_train  = 10000.0
0.00.078.097 I print_info: freq_scale_train = 1
0.00.078.098 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.098 I print_info: rope_finetuned   = unknown
0.00.078.098 I print_info: ssm_d_conv       = 0
0.00.078.099 I print_info: ssm_d_inner      = 0
0.00.078.099 I print_info: ssm_d_state      = 0
0.00.078.100 I print_info: ssm_dt_rank      = 0
0.00.078.100 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.100 I print_info: model type       = 1.4B
0.00.078.104 I print_info: model params     = 1.41 B
0.00.078.105 I print_info: general.name     = 1.4B
0.00.078.105 I print_info: vocab type       = BPE
0.00.078.105 I print_info: n_vocab          = 50304
0.00.078.107 I print_info: n_merges         = 50009
0.00.078.107 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.107 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.107 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.108 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.108 I print_info: LF token         = 187 'Ċ'
0.00.078.108 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.109 I print_info: max token length = 1024
0.00.078.109 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.060.080 I load_tensors: offloading 24 repeating layers to GPU
0.01.060.084 I load_tensors: offloading output layer to GPU
0.01.060.084 I load_tensors: offloaded 25/25 layers to GPU
0.01.060.111 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.060.114 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.061.370 I llama_context: constructing llama_context
0.01.061.372 I llama_context: n_seq_max     = 1
0.01.061.372 I llama_context: n_ctx         = 128
0.01.061.372 I llama_context: n_ctx_per_seq = 128
0.01.061.372 I llama_context: n_batch       = 128
0.01.061.373 I llama_context: n_ubatch      = 128
0.01.061.373 I llama_context: flash_attn    = 0
0.01.061.374 I llama_context: freq_base     = 10000.0
0.01.061.374 I llama_context: freq_scale    = 1
0.01.061.374 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.061.375 I ggml_metal_init: allocating
0.01.061.458 I ggml_metal_init: found device: Apple M4
0.01.061.480 I ggml_metal_init: picking default device: Apple M4
0.01.062.690 I ggml_metal_init: using embedded metal library
0.01.066.421 I ggml_metal_init: GPU name:   Apple M4
0.01.066.424 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.066.425 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.066.425 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.066.426 I ggml_metal_init: simdgroup reduction   = true
0.01.066.426 I ggml_metal_init: simdgroup matrix mul. = true
0.01.066.426 I ggml_metal_init: has residency sets    = true
0.01.066.426 I ggml_metal_init: has bfloat            = true
0.01.066.426 I ggml_metal_init: use bfloat            = true
0.01.066.427 I ggml_metal_init: hasUnifiedMemory      = true
0.01.066.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.076.782 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.078.466 I init:      Metal compute buffer size =    25.56 MiB
0.01.078.468 I init:        CPU compute buffer size =     1.06 MiB
0.01.078.468 I init: graph nodes  = 943
0.01.078.468 I init: graph splits = 2
0.01.078.470 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.01.078.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.078.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.111.542 I 
0.01.111.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.111.564 I perplexity: tokenizing the input ..
0.01.116.635 I perplexity: tokenization took 5.069 ms
0.01.116.642 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.236.425 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.239.174 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.239.223 I llama_perf_context_print:        load time =    1087.46 ms
0.01.239.225 I llama_perf_context_print: prompt eval time =     119.77 ms /   128 tokens (    0.94 ms per token,  1068.69 tokens per second)
0.01.239.226 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.239.230 I llama_perf_context_print:       total time =     127.68 ms /   129 tokens
0.01.239.961 I ggml_metal_free: deallocating

real	0m1.439s
user	0m0.105s
sys	0m0.230s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.715 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.909 I llama_model_loader: - type  f32:  194 tensors
0.00.025.910 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.910 I print_info: file format = GGUF V3 (latest)
0.00.025.911 I print_info: file type   = Q8_0
0.00.025.912 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.979 I load: special tokens cache size = 25
0.00.040.430 I load: token to piece cache size = 0.2984 MB
0.00.040.447 I print_info: arch             = gptneox
0.00.040.448 I print_info: vocab_only       = 0
0.00.040.448 I print_info: n_ctx_train      = 2048
0.00.040.448 I print_info: n_embd           = 2048
0.00.040.448 I print_info: n_layer          = 24
0.00.040.452 I print_info: n_head           = 16
0.00.040.453 I print_info: n_head_kv        = 16
0.00.040.453 I print_info: n_rot            = 32
0.00.040.453 I print_info: n_swa            = 0
0.00.040.454 I print_info: n_embd_head_k    = 128
0.00.040.454 I print_info: n_embd_head_v    = 128
0.00.040.454 I print_info: n_gqa            = 1
0.00.040.455 I print_info: n_embd_k_gqa     = 2048
0.00.040.456 I print_info: n_embd_v_gqa     = 2048
0.00.040.456 I print_info: f_norm_eps       = 1.0e-05
0.00.040.460 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.460 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.460 I print_info: f_logit_scale    = 0.0e+00
0.00.040.461 I print_info: n_ff             = 8192
0.00.040.461 I print_info: n_expert         = 0
0.00.040.461 I print_info: n_expert_used    = 0
0.00.040.461 I print_info: causal attn      = 1
0.00.040.462 I print_info: pooling type     = 0
0.00.040.462 I print_info: rope type        = 2
0.00.040.462 I print_info: rope scaling     = linear
0.00.040.462 I print_info: freq_base_train  = 10000.0
0.00.040.462 I print_info: freq_scale_train = 1
0.00.040.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.464 I print_info: rope_finetuned   = unknown
0.00.040.464 I print_info: ssm_d_conv       = 0
0.00.040.464 I print_info: ssm_d_inner      = 0
0.00.040.464 I print_info: ssm_d_state      = 0
0.00.040.464 I print_info: ssm_dt_rank      = 0
0.00.040.465 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.465 I print_info: model type       = 1.4B
0.00.040.465 I print_info: model params     = 1.41 B
0.00.040.465 I print_info: general.name     = 1.4B
0.00.040.466 I print_info: vocab type       = BPE
0.00.040.466 I print_info: n_vocab          = 50304
0.00.040.466 I print_info: n_merges         = 50009
0.00.040.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: LF token         = 187 'Ċ'
0.00.040.467 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: max token length = 1024
0.00.040.468 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.896.853 I load_tensors: offloading 24 repeating layers to GPU
0.00.896.860 I load_tensors: offloading output layer to GPU
0.00.896.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.896.892 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.896.895 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.898.307 I llama_context: constructing llama_context
0.00.898.309 I llama_context: n_seq_max     = 1
0.00.898.310 I llama_context: n_ctx         = 128
0.00.898.310 I llama_context: n_ctx_per_seq = 128
0.00.898.311 I llama_context: n_batch       = 128
0.00.898.311 I llama_context: n_ubatch      = 128
0.00.898.311 I llama_context: flash_attn    = 0
0.00.898.312 I llama_context: freq_base     = 10000.0
0.00.898.312 I llama_context: freq_scale    = 1
0.00.898.313 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.898.314 I ggml_metal_init: allocating
0.00.898.389 I ggml_metal_init: found device: Apple M4
0.00.898.399 I ggml_metal_init: picking default device: Apple M4
0.00.899.858 I ggml_metal_init: using embedded metal library
0.00.904.895 I ggml_metal_init: GPU name:   Apple M4
0.00.904.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.904.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.904.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.904.900 I ggml_metal_init: simdgroup reduction   = true
0.00.904.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.904.901 I ggml_metal_init: has residency sets    = true
0.00.904.901 I ggml_metal_init: has bfloat            = true
0.00.904.901 I ggml_metal_init: use bfloat            = true
0.00.904.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.904.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.919.409 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.922.096 I init:      Metal compute buffer size =    25.56 MiB
0.00.922.098 I init:        CPU compute buffer size =     1.06 MiB
0.00.922.099 I init: graph nodes  = 943
0.00.922.099 I init: graph splits = 2
0.00.922.101 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.922.102 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.922.102 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.944.784 I 
0.00.944.805 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.944.808 I perplexity: tokenizing the input ..
0.00.951.015 I perplexity: tokenization took 6.204 ms
0.00.951.026 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.089.824 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.091.159 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.091.184 I llama_perf_context_print:        load time =     935.59 ms
0.01.091.185 I llama_perf_context_print: prompt eval time =     138.79 ms /   128 tokens (    1.08 ms per token,   922.28 tokens per second)
0.01.091.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.091.186 I llama_perf_context_print:       total time =     146.40 ms /   129 tokens
0.01.091.594 I ggml_metal_free: deallocating

real	0m1.105s
user	0m0.073s
sys	0m0.150s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.059 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.389 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.397 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.397 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.398 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.398 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.398 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.399 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.400 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.400 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.405 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.407 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.407 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.725 I llama_model_loader: - type  f32:  194 tensors
0.00.026.726 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.726 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.727 I print_info: file format = GGUF V3 (latest)
0.00.026.727 I print_info: file type   = Q4_0
0.00.026.728 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.367 I load: special tokens cache size = 25
0.00.041.474 I load: token to piece cache size = 0.2984 MB
0.00.041.491 I print_info: arch             = gptneox
0.00.041.492 I print_info: vocab_only       = 0
0.00.041.492 I print_info: n_ctx_train      = 2048
0.00.041.492 I print_info: n_embd           = 2048
0.00.041.493 I print_info: n_layer          = 24
0.00.041.497 I print_info: n_head           = 16
0.00.041.500 I print_info: n_head_kv        = 16
0.00.041.500 I print_info: n_rot            = 32
0.00.041.500 I print_info: n_swa            = 0
0.00.041.500 I print_info: n_embd_head_k    = 128
0.00.041.500 I print_info: n_embd_head_v    = 128
0.00.041.501 I print_info: n_gqa            = 1
0.00.041.502 I print_info: n_embd_k_gqa     = 2048
0.00.041.502 I print_info: n_embd_v_gqa     = 2048
0.00.041.503 I print_info: f_norm_eps       = 1.0e-05
0.00.041.503 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.503 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.504 I print_info: f_logit_scale    = 0.0e+00
0.00.041.504 I print_info: n_ff             = 8192
0.00.041.504 I print_info: n_expert         = 0
0.00.041.504 I print_info: n_expert_used    = 0
0.00.041.506 I print_info: causal attn      = 1
0.00.041.506 I print_info: pooling type     = 0
0.00.041.506 I print_info: rope type        = 2
0.00.041.506 I print_info: rope scaling     = linear
0.00.041.506 I print_info: freq_base_train  = 10000.0
0.00.041.507 I print_info: freq_scale_train = 1
0.00.041.507 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.507 I print_info: rope_finetuned   = unknown
0.00.041.507 I print_info: ssm_d_conv       = 0
0.00.041.507 I print_info: ssm_d_inner      = 0
0.00.041.507 I print_info: ssm_d_state      = 0
0.00.041.508 I print_info: ssm_dt_rank      = 0
0.00.041.508 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.508 I print_info: model type       = 1.4B
0.00.041.508 I print_info: model params     = 1.41 B
0.00.041.508 I print_info: general.name     = 1.4B
0.00.041.509 I print_info: vocab type       = BPE
0.00.041.509 I print_info: n_vocab          = 50304
0.00.041.509 I print_info: n_merges         = 50009
0.00.041.509 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.510 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.510 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.510 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.510 I print_info: LF token         = 187 'Ċ'
0.00.041.511 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.511 I print_info: max token length = 1024
0.00.041.511 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.635.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.957 I load_tensors: offloading output layer to GPU
0.00.635.958 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.992 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.635.993 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.637.668 I llama_context: constructing llama_context
0.00.637.675 I llama_context: n_seq_max     = 1
0.00.637.675 I llama_context: n_ctx         = 128
0.00.637.676 I llama_context: n_ctx_per_seq = 128
0.00.637.676 I llama_context: n_batch       = 128
0.00.637.676 I llama_context: n_ubatch      = 128
0.00.637.677 I llama_context: flash_attn    = 0
0.00.637.678 I llama_context: freq_base     = 10000.0
0.00.637.678 I llama_context: freq_scale    = 1
0.00.637.679 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.637.681 I ggml_metal_init: allocating
0.00.637.789 I ggml_metal_init: found device: Apple M4
0.00.637.814 I ggml_metal_init: picking default device: Apple M4
0.00.639.639 I ggml_metal_init: using embedded metal library
0.00.645.273 I ggml_metal_init: GPU name:   Apple M4
0.00.645.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.286 I ggml_metal_init: simdgroup reduction   = true
0.00.645.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.287 I ggml_metal_init: has residency sets    = true
0.00.645.287 I ggml_metal_init: has bfloat            = true
0.00.645.287 I ggml_metal_init: use bfloat            = true
0.00.645.289 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.910 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.668.379 I init:      Metal compute buffer size =    25.56 MiB
0.00.668.381 I init:        CPU compute buffer size =     1.06 MiB
0.00.668.381 I init: graph nodes  = 943
0.00.668.382 I init: graph splits = 2
0.00.668.385 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.668.386 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.525 I 
0.00.691.551 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.555 I perplexity: tokenizing the input ..
0.00.699.437 I perplexity: tokenization took 7.879 ms
0.00.699.444 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.207 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.824.633 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.824.659 I llama_perf_context_print:        load time =     681.46 ms
0.00.824.660 I llama_perf_context_print: prompt eval time =     123.75 ms /   128 tokens (    0.97 ms per token,  1034.32 tokens per second)
0.00.824.661 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.662 I llama_perf_context_print:       total time =     133.13 ms /   129 tokens
0.00.825.039 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.080s
sys	0m0.130s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.769 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.989 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.995 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.997 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.998 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.998 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.999 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.000 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.000 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.001 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.001 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.002 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.004 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.004 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.004 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.006 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.131 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.131 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.132 I llama_model_loader: - type  f32:  194 tensors
0.00.025.132 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.132 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.133 I print_info: file format = GGUF V3 (latest)
0.00.025.133 I print_info: file type   = Q4_1
0.00.025.135 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.423 I load: special tokens cache size = 25
0.00.039.448 I load: token to piece cache size = 0.2984 MB
0.00.039.466 I print_info: arch             = gptneox
0.00.039.467 I print_info: vocab_only       = 0
0.00.039.467 I print_info: n_ctx_train      = 2048
0.00.039.467 I print_info: n_embd           = 2048
0.00.039.468 I print_info: n_layer          = 24
0.00.039.471 I print_info: n_head           = 16
0.00.039.475 I print_info: n_head_kv        = 16
0.00.039.475 I print_info: n_rot            = 32
0.00.039.476 I print_info: n_swa            = 0
0.00.039.476 I print_info: n_embd_head_k    = 128
0.00.039.476 I print_info: n_embd_head_v    = 128
0.00.039.476 I print_info: n_gqa            = 1
0.00.039.477 I print_info: n_embd_k_gqa     = 2048
0.00.039.477 I print_info: n_embd_v_gqa     = 2048
0.00.039.478 I print_info: f_norm_eps       = 1.0e-05
0.00.039.478 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.478 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.479 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.479 I print_info: f_logit_scale    = 0.0e+00
0.00.039.479 I print_info: n_ff             = 8192
0.00.039.480 I print_info: n_expert         = 0
0.00.039.480 I print_info: n_expert_used    = 0
0.00.039.480 I print_info: causal attn      = 1
0.00.039.480 I print_info: pooling type     = 0
0.00.039.480 I print_info: rope type        = 2
0.00.039.481 I print_info: rope scaling     = linear
0.00.039.481 I print_info: freq_base_train  = 10000.0
0.00.039.481 I print_info: freq_scale_train = 1
0.00.039.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.482 I print_info: rope_finetuned   = unknown
0.00.039.482 I print_info: ssm_d_conv       = 0
0.00.039.482 I print_info: ssm_d_inner      = 0
0.00.039.482 I print_info: ssm_d_state      = 0
0.00.039.482 I print_info: ssm_dt_rank      = 0
0.00.039.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.483 I print_info: model params     = 1.41 B
0.00.039.485 I print_info: general.name     = 1.4B
0.00.039.486 I print_info: vocab type       = BPE
0.00.039.486 I print_info: n_vocab          = 50304
0.00.039.486 I print_info: n_merges         = 50009
0.00.039.486 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: LF token         = 187 'Ċ'
0.00.039.487 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: max token length = 1024
0.00.039.488 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.684 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.702 I load_tensors: offloading output layer to GPU
0.00.648.703 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.740 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.648.741 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.650.360 I llama_context: constructing llama_context
0.00.650.363 I llama_context: n_seq_max     = 1
0.00.650.364 I llama_context: n_ctx         = 128
0.00.650.364 I llama_context: n_ctx_per_seq = 128
0.00.650.365 I llama_context: n_batch       = 128
0.00.650.365 I llama_context: n_ubatch      = 128
0.00.650.365 I llama_context: flash_attn    = 0
0.00.650.367 I llama_context: freq_base     = 10000.0
0.00.650.368 I llama_context: freq_scale    = 1
0.00.650.368 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.370 I ggml_metal_init: allocating
0.00.650.453 I ggml_metal_init: found device: Apple M4
0.00.650.469 I ggml_metal_init: picking default device: Apple M4
0.00.652.363 I ggml_metal_init: using embedded metal library
0.00.659.175 I ggml_metal_init: GPU name:   Apple M4
0.00.659.183 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.185 I ggml_metal_init: simdgroup reduction   = true
0.00.659.186 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.186 I ggml_metal_init: has residency sets    = true
0.00.659.186 I ggml_metal_init: has bfloat            = true
0.00.659.186 I ggml_metal_init: use bfloat            = true
0.00.659.187 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.323 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.680.569 I init:      Metal compute buffer size =    25.56 MiB
0.00.680.571 I init:        CPU compute buffer size =     1.06 MiB
0.00.680.572 I init: graph nodes  = 943
0.00.680.572 I init: graph splits = 2
0.00.680.574 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.680.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.680.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.696 I 
0.00.701.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.723 I perplexity: tokenizing the input ..
0.00.709.390 I perplexity: tokenization took 7.662 ms
0.00.709.397 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.714 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.847.041 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.847.072 I llama_perf_context_print:        load time =     692.92 ms
0.00.847.073 I llama_perf_context_print: prompt eval time =     136.31 ms /   128 tokens (    1.06 ms per token,   939.06 tokens per second)
0.00.847.074 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.075 I llama_perf_context_print:       total time =     145.38 ms /   129 tokens
0.00.847.500 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.079s
sys	0m0.122s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.871 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.016 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.018 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.018 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.021 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.954 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.138 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.138 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.139 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.139 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.139 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.140 I llama_model_loader: - type  f32:  194 tensors
0.00.026.140 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.140 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.141 I print_info: file format = GGUF V3 (latest)
0.00.026.142 I print_info: file type   = Q5_0
0.00.026.143 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.281 I load: special tokens cache size = 25
0.00.040.421 I load: token to piece cache size = 0.2984 MB
0.00.040.438 I print_info: arch             = gptneox
0.00.040.438 I print_info: vocab_only       = 0
0.00.040.439 I print_info: n_ctx_train      = 2048
0.00.040.439 I print_info: n_embd           = 2048
0.00.040.439 I print_info: n_layer          = 24
0.00.040.443 I print_info: n_head           = 16
0.00.040.444 I print_info: n_head_kv        = 16
0.00.040.444 I print_info: n_rot            = 32
0.00.040.445 I print_info: n_swa            = 0
0.00.040.445 I print_info: n_embd_head_k    = 128
0.00.040.447 I print_info: n_embd_head_v    = 128
0.00.040.448 I print_info: n_gqa            = 1
0.00.040.448 I print_info: n_embd_k_gqa     = 2048
0.00.040.450 I print_info: n_embd_v_gqa     = 2048
0.00.040.451 I print_info: f_norm_eps       = 1.0e-05
0.00.040.451 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.452 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.453 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.453 I print_info: f_logit_scale    = 0.0e+00
0.00.040.454 I print_info: n_ff             = 8192
0.00.040.454 I print_info: n_expert         = 0
0.00.040.454 I print_info: n_expert_used    = 0
0.00.040.454 I print_info: causal attn      = 1
0.00.040.454 I print_info: pooling type     = 0
0.00.040.454 I print_info: rope type        = 2
0.00.040.455 I print_info: rope scaling     = linear
0.00.040.455 I print_info: freq_base_train  = 10000.0
0.00.040.455 I print_info: freq_scale_train = 1
0.00.040.455 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.456 I print_info: rope_finetuned   = unknown
0.00.040.457 I print_info: ssm_d_conv       = 0
0.00.040.457 I print_info: ssm_d_inner      = 0
0.00.040.457 I print_info: ssm_d_state      = 0
0.00.040.457 I print_info: ssm_dt_rank      = 0
0.00.040.457 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.457 I print_info: model type       = 1.4B
0.00.040.458 I print_info: model params     = 1.41 B
0.00.040.458 I print_info: general.name     = 1.4B
0.00.040.458 I print_info: vocab type       = BPE
0.00.040.458 I print_info: n_vocab          = 50304
0.00.040.459 I print_info: n_merges         = 50009
0.00.040.459 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: LF token         = 187 'Ċ'
0.00.040.460 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.460 I print_info: max token length = 1024
0.00.040.460 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.700.841 I load_tensors: offloading 24 repeating layers to GPU
0.00.700.857 I load_tensors: offloading output layer to GPU
0.00.700.858 I load_tensors: offloaded 25/25 layers to GPU
0.00.700.890 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.700.891 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.702.503 I llama_context: constructing llama_context
0.00.702.507 I llama_context: n_seq_max     = 1
0.00.702.508 I llama_context: n_ctx         = 128
0.00.702.508 I llama_context: n_ctx_per_seq = 128
0.00.702.509 I llama_context: n_batch       = 128
0.00.702.509 I llama_context: n_ubatch      = 128
0.00.702.509 I llama_context: flash_attn    = 0
0.00.702.511 I llama_context: freq_base     = 10000.0
0.00.702.512 I llama_context: freq_scale    = 1
0.00.702.513 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.702.515 I ggml_metal_init: allocating
0.00.702.613 I ggml_metal_init: found device: Apple M4
0.00.702.636 I ggml_metal_init: picking default device: Apple M4
0.00.704.575 I ggml_metal_init: using embedded metal library
0.00.711.340 I ggml_metal_init: GPU name:   Apple M4
0.00.711.347 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.711.348 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.711.348 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.711.349 I ggml_metal_init: simdgroup reduction   = true
0.00.711.349 I ggml_metal_init: simdgroup matrix mul. = true
0.00.711.349 I ggml_metal_init: has residency sets    = true
0.00.711.350 I ggml_metal_init: has bfloat            = true
0.00.711.350 I ggml_metal_init: use bfloat            = true
0.00.711.351 I ggml_metal_init: hasUnifiedMemory      = true
0.00.711.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.728.438 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.731.880 I init:      Metal compute buffer size =    25.56 MiB
0.00.731.881 I init:        CPU compute buffer size =     1.06 MiB
0.00.731.882 I init: graph nodes  = 943
0.00.731.883 I init: graph splits = 2
0.00.731.885 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.731.886 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.731.887 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.601 I 
0.00.756.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.627 I perplexity: tokenizing the input ..
0.00.763.373 I perplexity: tokenization took 6.743 ms
0.00.763.380 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.375 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.900.714 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.900.736 I llama_perf_context_print:        load time =     746.73 ms
0.00.900.737 I llama_perf_context_print: prompt eval time =     135.98 ms /   128 tokens (    1.06 ms per token,   941.29 tokens per second)
0.00.900.738 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.900.738 I llama_perf_context_print:       total time =     144.13 ms /   129 tokens
0.00.901.154 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.078s
sys	0m0.138s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.797 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.811 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.776 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.037 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.038 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.038 I llama_model_loader: - type  f32:  194 tensors
0.00.025.039 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.039 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.040 I print_info: file format = GGUF V3 (latest)
0.00.025.040 I print_info: file type   = Q5_1
0.00.025.042 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.349 I load: special tokens cache size = 25
0.00.039.554 I load: token to piece cache size = 0.2984 MB
0.00.039.573 I print_info: arch             = gptneox
0.00.039.574 I print_info: vocab_only       = 0
0.00.039.574 I print_info: n_ctx_train      = 2048
0.00.039.574 I print_info: n_embd           = 2048
0.00.039.574 I print_info: n_layer          = 24
0.00.039.578 I print_info: n_head           = 16
0.00.039.579 I print_info: n_head_kv        = 16
0.00.039.579 I print_info: n_rot            = 32
0.00.039.579 I print_info: n_swa            = 0
0.00.039.579 I print_info: n_embd_head_k    = 128
0.00.039.580 I print_info: n_embd_head_v    = 128
0.00.039.580 I print_info: n_gqa            = 1
0.00.039.581 I print_info: n_embd_k_gqa     = 2048
0.00.039.584 I print_info: n_embd_v_gqa     = 2048
0.00.039.585 I print_info: f_norm_eps       = 1.0e-05
0.00.039.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.585 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.586 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.586 I print_info: f_logit_scale    = 0.0e+00
0.00.039.586 I print_info: n_ff             = 8192
0.00.039.587 I print_info: n_expert         = 0
0.00.039.587 I print_info: n_expert_used    = 0
0.00.039.587 I print_info: causal attn      = 1
0.00.039.587 I print_info: pooling type     = 0
0.00.039.587 I print_info: rope type        = 2
0.00.039.587 I print_info: rope scaling     = linear
0.00.039.587 I print_info: freq_base_train  = 10000.0
0.00.039.588 I print_info: freq_scale_train = 1
0.00.039.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.588 I print_info: rope_finetuned   = unknown
0.00.039.588 I print_info: ssm_d_conv       = 0
0.00.039.588 I print_info: ssm_d_inner      = 0
0.00.039.588 I print_info: ssm_d_state      = 0
0.00.039.589 I print_info: ssm_dt_rank      = 0
0.00.039.590 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.591 I print_info: model type       = 1.4B
0.00.039.591 I print_info: model params     = 1.41 B
0.00.039.591 I print_info: general.name     = 1.4B
0.00.039.592 I print_info: vocab type       = BPE
0.00.039.592 I print_info: n_vocab          = 50304
0.00.039.592 I print_info: n_merges         = 50009
0.00.039.592 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.592 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.593 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.594 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.594 I print_info: LF token         = 187 'Ċ'
0.00.039.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.595 I print_info: max token length = 1024
0.00.039.595 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.623.354 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.372 I load_tensors: offloading output layer to GPU
0.00.623.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.407 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.623.409 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.624.608 I llama_context: constructing llama_context
0.00.624.612 I llama_context: n_seq_max     = 1
0.00.624.613 I llama_context: n_ctx         = 128
0.00.624.613 I llama_context: n_ctx_per_seq = 128
0.00.624.614 I llama_context: n_batch       = 128
0.00.624.614 I llama_context: n_ubatch      = 128
0.00.624.614 I llama_context: flash_attn    = 0
0.00.624.617 I llama_context: freq_base     = 10000.0
0.00.624.617 I llama_context: freq_scale    = 1
0.00.624.618 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.624.620 I ggml_metal_init: allocating
0.00.624.667 I ggml_metal_init: found device: Apple M4
0.00.624.679 I ggml_metal_init: picking default device: Apple M4
0.00.626.249 I ggml_metal_init: using embedded metal library
0.00.632.536 I ggml_metal_init: GPU name:   Apple M4
0.00.632.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.543 I ggml_metal_init: simdgroup reduction   = true
0.00.632.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.543 I ggml_metal_init: has residency sets    = true
0.00.632.543 I ggml_metal_init: has bfloat            = true
0.00.632.544 I ggml_metal_init: use bfloat            = true
0.00.632.545 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.761 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.653.235 I init:      Metal compute buffer size =    25.56 MiB
0.00.653.236 I init:        CPU compute buffer size =     1.06 MiB
0.00.653.237 I init: graph nodes  = 943
0.00.653.237 I init: graph splits = 2
0.00.653.239 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.653.241 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.653.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.157 I 
0.00.684.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.186 I perplexity: tokenizing the input ..
0.00.691.524 I perplexity: tokenization took 7.335 ms
0.00.691.532 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.702 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.843.033 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.843.063 I llama_perf_context_print:        load time =     675.39 ms
0.00.843.064 I llama_perf_context_print: prompt eval time =     150.16 ms /   128 tokens (    1.17 ms per token,   852.44 tokens per second)
0.00.843.065 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.065 I llama_perf_context_print:       total time =     158.91 ms /   129 tokens
0.00.843.470 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.078s
sys	0m0.145s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.592 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.601 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.912 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.914 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.916 I llama_model_loader: - type  f32:  194 tensors
0.00.025.917 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.917 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.917 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.918 I print_info: file format = GGUF V3 (latest)
0.00.025.919 I print_info: file type   = Q2_K - Medium
0.00.025.920 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.373 I load: special tokens cache size = 25
0.00.040.321 I load: token to piece cache size = 0.2984 MB
0.00.040.336 I print_info: arch             = gptneox
0.00.040.338 I print_info: vocab_only       = 0
0.00.040.338 I print_info: n_ctx_train      = 2048
0.00.040.338 I print_info: n_embd           = 2048
0.00.040.338 I print_info: n_layer          = 24
0.00.040.343 I print_info: n_head           = 16
0.00.040.344 I print_info: n_head_kv        = 16
0.00.040.344 I print_info: n_rot            = 32
0.00.040.344 I print_info: n_swa            = 0
0.00.040.344 I print_info: n_embd_head_k    = 128
0.00.040.346 I print_info: n_embd_head_v    = 128
0.00.040.346 I print_info: n_gqa            = 1
0.00.040.347 I print_info: n_embd_k_gqa     = 2048
0.00.040.350 I print_info: n_embd_v_gqa     = 2048
0.00.040.350 I print_info: f_norm_eps       = 1.0e-05
0.00.040.351 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.351 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.351 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.357 I print_info: f_logit_scale    = 0.0e+00
0.00.040.366 I print_info: n_ff             = 8192
0.00.040.367 I print_info: n_expert         = 0
0.00.040.367 I print_info: n_expert_used    = 0
0.00.040.368 I print_info: causal attn      = 1
0.00.040.368 I print_info: pooling type     = 0
0.00.040.368 I print_info: rope type        = 2
0.00.040.368 I print_info: rope scaling     = linear
0.00.040.369 I print_info: freq_base_train  = 10000.0
0.00.040.369 I print_info: freq_scale_train = 1
0.00.040.369 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.369 I print_info: rope_finetuned   = unknown
0.00.040.369 I print_info: ssm_d_conv       = 0
0.00.040.369 I print_info: ssm_d_inner      = 0
0.00.040.370 I print_info: ssm_d_state      = 0
0.00.040.370 I print_info: ssm_dt_rank      = 0
0.00.040.370 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.370 I print_info: model type       = 1.4B
0.00.040.370 I print_info: model params     = 1.41 B
0.00.040.371 I print_info: general.name     = 1.4B
0.00.040.371 I print_info: vocab type       = BPE
0.00.040.371 I print_info: n_vocab          = 50304
0.00.040.371 I print_info: n_merges         = 50009
0.00.040.372 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.373 I print_info: LF token         = 187 'Ċ'
0.00.040.374 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.375 I print_info: max token length = 1024
0.00.040.375 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.347.401 I load_tensors: offloading 24 repeating layers to GPU
0.00.347.413 I load_tensors: offloading output layer to GPU
0.00.347.414 I load_tensors: offloaded 25/25 layers to GPU
0.00.347.444 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.347.450 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.349.105 I llama_context: constructing llama_context
0.00.349.112 I llama_context: n_seq_max     = 1
0.00.349.113 I llama_context: n_ctx         = 128
0.00.349.113 I llama_context: n_ctx_per_seq = 128
0.00.349.113 I llama_context: n_batch       = 128
0.00.349.114 I llama_context: n_ubatch      = 128
0.00.349.114 I llama_context: flash_attn    = 0
0.00.349.116 I llama_context: freq_base     = 10000.0
0.00.349.116 I llama_context: freq_scale    = 1
0.00.349.117 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.349.119 I ggml_metal_init: allocating
0.00.349.192 I ggml_metal_init: found device: Apple M4
0.00.349.205 I ggml_metal_init: picking default device: Apple M4
0.00.351.005 I ggml_metal_init: using embedded metal library
0.00.356.302 I ggml_metal_init: GPU name:   Apple M4
0.00.356.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.356.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.356.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.356.324 I ggml_metal_init: simdgroup reduction   = true
0.00.356.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.356.325 I ggml_metal_init: has residency sets    = true
0.00.356.325 I ggml_metal_init: has bfloat            = true
0.00.356.325 I ggml_metal_init: use bfloat            = true
0.00.356.328 I ggml_metal_init: hasUnifiedMemory      = true
0.00.356.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.814 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.381.471 I init:      Metal compute buffer size =    25.56 MiB
0.00.381.473 I init:        CPU compute buffer size =     1.06 MiB
0.00.381.474 I init: graph nodes  = 943
0.00.381.474 I init: graph splits = 2
0.00.381.477 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.381.478 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.381.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.410.447 I 
0.00.410.471 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.410.476 I perplexity: tokenizing the input ..
0.00.417.368 I perplexity: tokenization took 6.888 ms
0.00.417.376 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.559.810 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.151 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.173 I llama_perf_context_print:        load time =     400.51 ms
0.00.561.174 I llama_perf_context_print: prompt eval time =     142.42 ms /   128 tokens (    1.11 ms per token,   898.74 tokens per second)
0.00.561.174 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.175 I llama_perf_context_print:       total time =     150.73 ms /   129 tokens
0.00.561.533 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.080s
sys	0m0.090s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.981 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.053 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.064 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.065 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.065 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.066 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.066 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.251 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.314 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.317 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.318 I llama_model_loader: - type  f32:  194 tensors
0.00.025.318 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.318 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.319 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.319 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.320 I print_info: file format = GGUF V3 (latest)
0.00.025.320 I print_info: file type   = Q3_K - Medium
0.00.025.321 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.265 I load: special tokens cache size = 25
0.00.039.369 I load: token to piece cache size = 0.2984 MB
0.00.039.387 I print_info: arch             = gptneox
0.00.039.388 I print_info: vocab_only       = 0
0.00.039.388 I print_info: n_ctx_train      = 2048
0.00.039.388 I print_info: n_embd           = 2048
0.00.039.389 I print_info: n_layer          = 24
0.00.039.393 I print_info: n_head           = 16
0.00.039.393 I print_info: n_head_kv        = 16
0.00.039.393 I print_info: n_rot            = 32
0.00.039.394 I print_info: n_swa            = 0
0.00.039.394 I print_info: n_embd_head_k    = 128
0.00.039.394 I print_info: n_embd_head_v    = 128
0.00.039.394 I print_info: n_gqa            = 1
0.00.039.395 I print_info: n_embd_k_gqa     = 2048
0.00.039.397 I print_info: n_embd_v_gqa     = 2048
0.00.039.398 I print_info: f_norm_eps       = 1.0e-05
0.00.039.398 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.398 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.398 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.399 I print_info: f_logit_scale    = 0.0e+00
0.00.039.399 I print_info: n_ff             = 8192
0.00.039.399 I print_info: n_expert         = 0
0.00.039.400 I print_info: n_expert_used    = 0
0.00.039.400 I print_info: causal attn      = 1
0.00.039.400 I print_info: pooling type     = 0
0.00.039.400 I print_info: rope type        = 2
0.00.039.400 I print_info: rope scaling     = linear
0.00.039.401 I print_info: freq_base_train  = 10000.0
0.00.039.401 I print_info: freq_scale_train = 1
0.00.039.401 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.401 I print_info: rope_finetuned   = unknown
0.00.039.402 I print_info: ssm_d_conv       = 0
0.00.039.402 I print_info: ssm_d_inner      = 0
0.00.039.402 I print_info: ssm_d_state      = 0
0.00.039.402 I print_info: ssm_dt_rank      = 0
0.00.039.402 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.402 I print_info: model type       = 1.4B
0.00.039.403 I print_info: model params     = 1.41 B
0.00.039.403 I print_info: general.name     = 1.4B
0.00.039.403 I print_info: vocab type       = BPE
0.00.039.403 I print_info: n_vocab          = 50304
0.00.039.404 I print_info: n_merges         = 50009
0.00.039.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.406 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.406 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.406 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.406 I print_info: LF token         = 187 'Ċ'
0.00.039.407 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.407 I print_info: max token length = 1024
0.00.039.407 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.923 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.941 I load_tensors: offloading output layer to GPU
0.00.435.942 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.974 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.976 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.647 I llama_context: constructing llama_context
0.00.437.650 I llama_context: n_seq_max     = 1
0.00.437.651 I llama_context: n_ctx         = 128
0.00.437.651 I llama_context: n_ctx_per_seq = 128
0.00.437.652 I llama_context: n_batch       = 128
0.00.437.652 I llama_context: n_ubatch      = 128
0.00.437.652 I llama_context: flash_attn    = 0
0.00.437.655 I llama_context: freq_base     = 10000.0
0.00.437.655 I llama_context: freq_scale    = 1
0.00.437.656 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.658 I ggml_metal_init: allocating
0.00.437.748 I ggml_metal_init: found device: Apple M4
0.00.437.763 I ggml_metal_init: picking default device: Apple M4
0.00.439.668 I ggml_metal_init: using embedded metal library
0.00.445.417 I ggml_metal_init: GPU name:   Apple M4
0.00.445.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.428 I ggml_metal_init: simdgroup reduction   = true
0.00.445.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.428 I ggml_metal_init: has residency sets    = true
0.00.445.428 I ggml_metal_init: has bfloat            = true
0.00.445.429 I ggml_metal_init: use bfloat            = true
0.00.445.441 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.720 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.467.941 I init:      Metal compute buffer size =    25.56 MiB
0.00.467.943 I init:        CPU compute buffer size =     1.06 MiB
0.00.467.943 I init: graph nodes  = 943
0.00.467.944 I init: graph splits = 2
0.00.467.946 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.467.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.467.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.250 I 
0.00.496.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.277 I perplexity: tokenizing the input ..
0.00.503.898 I perplexity: tokenization took 7.617 ms
0.00.503.905 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.587 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.930 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.960 I llama_perf_context_print:        load time =     487.27 ms
0.00.647.960 I llama_perf_context_print: prompt eval time =     142.66 ms /   128 tokens (    1.11 ms per token,   897.21 tokens per second)
0.00.647.961 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.961 I llama_perf_context_print:       total time =     151.71 ms /   129 tokens
0.00.648.321 I ggml_metal_free: deallocating

real	0m0.662s
user	0m0.078s
sys	0m0.100s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.909 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.915 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.921 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.922 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.925 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.925 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.926 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.928 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.928 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.928 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.926 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.123 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.120 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.121 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.122 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.122 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.122 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.123 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.123 I llama_model_loader: - type  f32:  194 tensors
0.00.025.124 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.124 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.124 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.125 I print_info: file format = GGUF V3 (latest)
0.00.025.125 I print_info: file type   = Q4_K - Medium
0.00.025.127 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.449 I load: special tokens cache size = 25
0.00.039.692 I load: token to piece cache size = 0.2984 MB
0.00.039.708 I print_info: arch             = gptneox
0.00.039.709 I print_info: vocab_only       = 0
0.00.039.709 I print_info: n_ctx_train      = 2048
0.00.039.710 I print_info: n_embd           = 2048
0.00.039.710 I print_info: n_layer          = 24
0.00.039.714 I print_info: n_head           = 16
0.00.039.714 I print_info: n_head_kv        = 16
0.00.039.715 I print_info: n_rot            = 32
0.00.039.717 I print_info: n_swa            = 0
0.00.039.717 I print_info: n_embd_head_k    = 128
0.00.039.717 I print_info: n_embd_head_v    = 128
0.00.039.718 I print_info: n_gqa            = 1
0.00.039.719 I print_info: n_embd_k_gqa     = 2048
0.00.039.719 I print_info: n_embd_v_gqa     = 2048
0.00.039.720 I print_info: f_norm_eps       = 1.0e-05
0.00.039.720 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.720 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.720 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.721 I print_info: f_logit_scale    = 0.0e+00
0.00.039.721 I print_info: n_ff             = 8192
0.00.039.722 I print_info: n_expert         = 0
0.00.039.722 I print_info: n_expert_used    = 0
0.00.039.722 I print_info: causal attn      = 1
0.00.039.722 I print_info: pooling type     = 0
0.00.039.722 I print_info: rope type        = 2
0.00.039.722 I print_info: rope scaling     = linear
0.00.039.723 I print_info: freq_base_train  = 10000.0
0.00.039.723 I print_info: freq_scale_train = 1
0.00.039.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.723 I print_info: rope_finetuned   = unknown
0.00.039.723 I print_info: ssm_d_conv       = 0
0.00.039.723 I print_info: ssm_d_inner      = 0
0.00.039.724 I print_info: ssm_d_state      = 0
0.00.039.724 I print_info: ssm_dt_rank      = 0
0.00.039.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.724 I print_info: model type       = 1.4B
0.00.039.724 I print_info: model params     = 1.41 B
0.00.039.724 I print_info: general.name     = 1.4B
0.00.039.725 I print_info: vocab type       = BPE
0.00.039.725 I print_info: n_vocab          = 50304
0.00.039.725 I print_info: n_merges         = 50009
0.00.039.726 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: LF token         = 187 'Ċ'
0.00.039.726 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: max token length = 1024
0.00.039.727 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.546.940 I load_tensors: offloading 24 repeating layers to GPU
0.00.546.948 I load_tensors: offloading output layer to GPU
0.00.546.949 I load_tensors: offloaded 25/25 layers to GPU
0.00.546.984 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.546.985 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.548.638 I llama_context: constructing llama_context
0.00.548.643 I llama_context: n_seq_max     = 1
0.00.548.644 I llama_context: n_ctx         = 128
0.00.548.644 I llama_context: n_ctx_per_seq = 128
0.00.548.645 I llama_context: n_batch       = 128
0.00.548.645 I llama_context: n_ubatch      = 128
0.00.548.646 I llama_context: flash_attn    = 0
0.00.548.648 I llama_context: freq_base     = 10000.0
0.00.548.648 I llama_context: freq_scale    = 1
0.00.548.649 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.548.664 I ggml_metal_init: allocating
0.00.548.781 I ggml_metal_init: found device: Apple M4
0.00.548.830 I ggml_metal_init: picking default device: Apple M4
0.00.551.023 I ggml_metal_init: using embedded metal library
0.00.558.065 I ggml_metal_init: GPU name:   Apple M4
0.00.558.070 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.071 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.072 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.072 I ggml_metal_init: simdgroup reduction   = true
0.00.558.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.073 I ggml_metal_init: has residency sets    = true
0.00.558.073 I ggml_metal_init: has bfloat            = true
0.00.558.073 I ggml_metal_init: use bfloat            = true
0.00.558.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.011 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.579.361 I init:      Metal compute buffer size =    25.56 MiB
0.00.579.363 I init:        CPU compute buffer size =     1.06 MiB
0.00.579.364 I init: graph nodes  = 943
0.00.579.364 I init: graph splits = 2
0.00.579.367 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.579.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.579.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.605.573 I 
0.00.605.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.605.597 I perplexity: tokenizing the input ..
0.00.612.522 I perplexity: tokenization took 6.922 ms
0.00.612.532 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.755.826 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.757.179 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.757.199 I llama_perf_context_print:        load time =     596.70 ms
0.00.757.200 I llama_perf_context_print: prompt eval time =     143.28 ms /   128 tokens (    1.12 ms per token,   893.34 tokens per second)
0.00.757.201 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.757.201 I llama_perf_context_print:       total time =     151.63 ms /   129 tokens
0.00.757.600 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.079s
sys	0m0.139s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.068 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.091 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.229 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.230 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.231 I llama_model_loader: - type  f32:  194 tensors
0.00.026.231 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.231 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.232 I print_info: file format = GGUF V3 (latest)
0.00.026.233 I print_info: file type   = Q5_K - Medium
0.00.026.234 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.240 I load: special tokens cache size = 25
0.00.040.231 I load: token to piece cache size = 0.2984 MB
0.00.040.247 I print_info: arch             = gptneox
0.00.040.248 I print_info: vocab_only       = 0
0.00.040.248 I print_info: n_ctx_train      = 2048
0.00.040.248 I print_info: n_embd           = 2048
0.00.040.249 I print_info: n_layer          = 24
0.00.040.252 I print_info: n_head           = 16
0.00.040.253 I print_info: n_head_kv        = 16
0.00.040.253 I print_info: n_rot            = 32
0.00.040.253 I print_info: n_swa            = 0
0.00.040.253 I print_info: n_embd_head_k    = 128
0.00.040.254 I print_info: n_embd_head_v    = 128
0.00.040.254 I print_info: n_gqa            = 1
0.00.040.255 I print_info: n_embd_k_gqa     = 2048
0.00.040.257 I print_info: n_embd_v_gqa     = 2048
0.00.040.258 I print_info: f_norm_eps       = 1.0e-05
0.00.040.258 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.258 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.258 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.259 I print_info: f_logit_scale    = 0.0e+00
0.00.040.259 I print_info: n_ff             = 8192
0.00.040.259 I print_info: n_expert         = 0
0.00.040.260 I print_info: n_expert_used    = 0
0.00.040.260 I print_info: causal attn      = 1
0.00.040.260 I print_info: pooling type     = 0
0.00.040.260 I print_info: rope type        = 2
0.00.040.260 I print_info: rope scaling     = linear
0.00.040.260 I print_info: freq_base_train  = 10000.0
0.00.040.261 I print_info: freq_scale_train = 1
0.00.040.261 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.261 I print_info: rope_finetuned   = unknown
0.00.040.261 I print_info: ssm_d_conv       = 0
0.00.040.262 I print_info: ssm_d_inner      = 0
0.00.040.262 I print_info: ssm_d_state      = 0
0.00.040.262 I print_info: ssm_dt_rank      = 0
0.00.040.262 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.262 I print_info: model type       = 1.4B
0.00.040.262 I print_info: model params     = 1.41 B
0.00.040.263 I print_info: general.name     = 1.4B
0.00.040.263 I print_info: vocab type       = BPE
0.00.040.263 I print_info: n_vocab          = 50304
0.00.040.264 I print_info: n_merges         = 50009
0.00.040.264 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: LF token         = 187 'Ċ'
0.00.040.265 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.265 I print_info: max token length = 1024
0.00.040.265 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.733 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.752 I load_tensors: offloading output layer to GPU
0.00.587.753 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.790 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.791 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.242 I llama_context: constructing llama_context
0.00.589.247 I llama_context: n_seq_max     = 1
0.00.589.248 I llama_context: n_ctx         = 128
0.00.589.248 I llama_context: n_ctx_per_seq = 128
0.00.589.248 I llama_context: n_batch       = 128
0.00.589.249 I llama_context: n_ubatch      = 128
0.00.589.249 I llama_context: flash_attn    = 0
0.00.589.251 I llama_context: freq_base     = 10000.0
0.00.589.252 I llama_context: freq_scale    = 1
0.00.589.252 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.255 I ggml_metal_init: allocating
0.00.589.355 I ggml_metal_init: found device: Apple M4
0.00.589.370 I ggml_metal_init: picking default device: Apple M4
0.00.591.289 I ggml_metal_init: using embedded metal library
0.00.598.040 I ggml_metal_init: GPU name:   Apple M4
0.00.598.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.048 I ggml_metal_init: simdgroup reduction   = true
0.00.598.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.049 I ggml_metal_init: has residency sets    = true
0.00.598.049 I ggml_metal_init: has bfloat            = true
0.00.598.049 I ggml_metal_init: use bfloat            = true
0.00.598.050 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.064 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.530 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.620.017 I init:      Metal compute buffer size =    25.56 MiB
0.00.620.019 I init:        CPU compute buffer size =     1.06 MiB
0.00.620.020 I init: graph nodes  = 943
0.00.620.020 I init: graph splits = 2
0.00.620.023 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.620.024 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.024 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.660 I 
0.00.652.687 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.699 I perplexity: tokenizing the input ..
0.00.659.587 I perplexity: tokenization took 6.885 ms
0.00.659.593 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.013 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.798.362 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.798.385 I llama_perf_context_print:        load time =     642.71 ms
0.00.798.385 I llama_perf_context_print: prompt eval time =     137.41 ms /   128 tokens (    1.07 ms per token,   931.52 tokens per second)
0.00.798.386 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.386 I llama_perf_context_print:       total time =     145.73 ms /   129 tokens
0.00.798.737 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.078s
sys	0m0.127s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.548 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.734 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.741 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.748 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.749 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.750 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.750 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.753 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.680 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.680 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.680 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.681 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.681 I llama_model_loader: - type  f32:  194 tensors
0.00.027.682 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.683 I print_info: file format = GGUF V3 (latest)
0.00.027.683 I print_info: file type   = Q6_K
0.00.027.684 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.036.007 I load: special tokens cache size = 25
0.00.041.874 I load: token to piece cache size = 0.2984 MB
0.00.041.892 I print_info: arch             = gptneox
0.00.041.892 I print_info: vocab_only       = 0
0.00.041.893 I print_info: n_ctx_train      = 2048
0.00.041.893 I print_info: n_embd           = 2048
0.00.041.893 I print_info: n_layer          = 24
0.00.041.897 I print_info: n_head           = 16
0.00.041.898 I print_info: n_head_kv        = 16
0.00.041.898 I print_info: n_rot            = 32
0.00.041.898 I print_info: n_swa            = 0
0.00.041.898 I print_info: n_embd_head_k    = 128
0.00.041.898 I print_info: n_embd_head_v    = 128
0.00.041.899 I print_info: n_gqa            = 1
0.00.041.906 I print_info: n_embd_k_gqa     = 2048
0.00.041.906 I print_info: n_embd_v_gqa     = 2048
0.00.041.907 I print_info: f_norm_eps       = 1.0e-05
0.00.041.907 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.907 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.912 I print_info: f_logit_scale    = 0.0e+00
0.00.041.913 I print_info: n_ff             = 8192
0.00.041.913 I print_info: n_expert         = 0
0.00.041.913 I print_info: n_expert_used    = 0
0.00.041.913 I print_info: causal attn      = 1
0.00.041.913 I print_info: pooling type     = 0
0.00.041.913 I print_info: rope type        = 2
0.00.041.914 I print_info: rope scaling     = linear
0.00.041.914 I print_info: freq_base_train  = 10000.0
0.00.041.914 I print_info: freq_scale_train = 1
0.00.041.914 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.914 I print_info: rope_finetuned   = unknown
0.00.041.915 I print_info: ssm_d_conv       = 0
0.00.041.915 I print_info: ssm_d_inner      = 0
0.00.041.915 I print_info: ssm_d_state      = 0
0.00.041.915 I print_info: ssm_dt_rank      = 0
0.00.041.916 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.917 I print_info: model type       = 1.4B
0.00.041.917 I print_info: model params     = 1.41 B
0.00.041.917 I print_info: general.name     = 1.4B
0.00.041.918 I print_info: vocab type       = BPE
0.00.041.918 I print_info: n_vocab          = 50304
0.00.041.918 I print_info: n_merges         = 50009
0.00.041.918 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.918 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.919 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.919 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.919 I print_info: LF token         = 187 'Ċ'
0.00.041.919 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.919 I print_info: max token length = 1024
0.00.041.920 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.392.659 I load_tensors: offloading 24 repeating layers to GPU
0.00.392.663 I load_tensors: offloading output layer to GPU
0.00.392.664 I load_tensors: offloaded 25/25 layers to GPU
0.00.392.689 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.392.691 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.394.268 I llama_context: constructing llama_context
0.00.394.270 I llama_context: n_seq_max     = 1
0.00.394.271 I llama_context: n_ctx         = 128
0.00.394.271 I llama_context: n_ctx_per_seq = 128
0.00.394.271 I llama_context: n_batch       = 128
0.00.394.272 I llama_context: n_ubatch      = 128
0.00.394.272 I llama_context: flash_attn    = 0
0.00.394.273 I llama_context: freq_base     = 10000.0
0.00.394.274 I llama_context: freq_scale    = 1
0.00.394.274 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.394.275 I ggml_metal_init: allocating
0.00.394.335 I ggml_metal_init: found device: Apple M4
0.00.394.355 I ggml_metal_init: picking default device: Apple M4
0.00.395.788 I ggml_metal_init: using embedded metal library
0.00.401.596 I ggml_metal_init: GPU name:   Apple M4
0.00.401.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.401.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.401.602 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.401.602 I ggml_metal_init: simdgroup reduction   = true
0.00.401.602 I ggml_metal_init: simdgroup matrix mul. = true
0.00.401.602 I ggml_metal_init: has residency sets    = true
0.00.401.603 I ggml_metal_init: has bfloat            = true
0.00.401.603 I ggml_metal_init: use bfloat            = true
0.00.401.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.401.606 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.417.740 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.420.770 I init:      Metal compute buffer size =    25.56 MiB
0.00.420.772 I init:        CPU compute buffer size =     1.06 MiB
0.00.420.772 I init: graph nodes  = 943
0.00.420.773 I init: graph splits = 2
0.00.420.775 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.420.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.420.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.455.694 I 
0.00.455.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.455.717 I perplexity: tokenizing the input ..
0.00.462.834 I perplexity: tokenization took 7.113 ms
0.00.462.846 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.595.383 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.596.746 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.596.768 I llama_perf_context_print:        load time =     446.14 ms
0.00.596.769 I llama_perf_context_print: prompt eval time =     132.53 ms /   128 tokens (    1.04 ms per token,   965.86 tokens per second)
0.00.596.770 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.596.770 I llama_perf_context_print:       total time =     141.07 ms /   129 tokens
0.00.597.176 I ggml_metal_free: deallocating

real	0m0.611s
user	0m0.075s
sys	0m0.107s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.273 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.067 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.401 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.413 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.414 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.417 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.418 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.418 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.419 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.435 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.199 I llama_model_loader: - type  f32:  194 tensors
0.00.055.199 I llama_model_loader: - type  f16:   98 tensors
0.00.055.200 I print_info: file format = GGUF V3 (latest)
0.00.055.201 I print_info: file type   = all F32 (guessed)
0.00.055.202 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.426 I load: special tokens cache size = 25
0.00.073.966 I load: token to piece cache size = 0.2984 MB
0.00.073.982 I print_info: arch             = gptneox
0.00.073.983 I print_info: vocab_only       = 0
0.00.073.983 I print_info: n_ctx_train      = 2048
0.00.073.984 I print_info: n_embd           = 2048
0.00.073.984 I print_info: n_layer          = 24
0.00.073.986 I print_info: n_head           = 16
0.00.073.987 I print_info: n_head_kv        = 16
0.00.073.987 I print_info: n_rot            = 32
0.00.073.987 I print_info: n_swa            = 0
0.00.073.987 I print_info: n_embd_head_k    = 128
0.00.073.987 I print_info: n_embd_head_v    = 128
0.00.073.988 I print_info: n_gqa            = 1
0.00.073.989 I print_info: n_embd_k_gqa     = 2048
0.00.073.989 I print_info: n_embd_v_gqa     = 2048
0.00.073.990 I print_info: f_norm_eps       = 1.0e-05
0.00.073.990 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.993 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.993 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.993 I print_info: f_logit_scale    = 0.0e+00
0.00.073.994 I print_info: n_ff             = 8192
0.00.073.994 I print_info: n_expert         = 0
0.00.073.994 I print_info: n_expert_used    = 0
0.00.073.994 I print_info: causal attn      = 1
0.00.073.995 I print_info: pooling type     = 0
0.00.073.995 I print_info: rope type        = 2
0.00.073.995 I print_info: rope scaling     = linear
0.00.073.995 I print_info: freq_base_train  = 10000.0
0.00.073.996 I print_info: freq_scale_train = 1
0.00.073.996 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.997 I print_info: rope_finetuned   = unknown
0.00.073.997 I print_info: ssm_d_conv       = 0
0.00.074.000 I print_info: ssm_d_inner      = 0
0.00.074.000 I print_info: ssm_d_state      = 0
0.00.074.000 I print_info: ssm_dt_rank      = 0
0.00.074.001 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.002 I print_info: model type       = 1.4B
0.00.074.002 I print_info: model params     = 1.41 B
0.00.074.003 I print_info: general.name     = 1.4B
0.00.074.003 I print_info: vocab type       = BPE
0.00.074.003 I print_info: n_vocab          = 50304
0.00.074.003 I print_info: n_merges         = 50009
0.00.074.004 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.004 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.004 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.004 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.004 I print_info: LF token         = 187 'Ċ'
0.00.074.005 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.005 I print_info: max token length = 1024
0.00.074.006 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.338.447 I load_tensors: offloading 24 repeating layers to GPU
0.01.338.450 I load_tensors: offloading output layer to GPU
0.01.338.451 I load_tensors: offloaded 25/25 layers to GPU
0.01.338.467 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.338.469 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.338.920 I llama_context: constructing llama_context
0.01.338.921 I llama_context: n_seq_max     = 1
0.01.338.921 I llama_context: n_ctx         = 128
0.01.338.922 I llama_context: n_ctx_per_seq = 128
0.01.338.922 I llama_context: n_batch       = 128
0.01.338.922 I llama_context: n_ubatch      = 128
0.01.338.922 I llama_context: flash_attn    = 0
0.01.338.922 I llama_context: freq_base     = 10000.0
0.01.338.923 I llama_context: freq_scale    = 1
0.01.338.923 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.338.925 I ggml_metal_init: allocating
0.01.338.968 I ggml_metal_init: found device: Apple M4
0.01.338.973 I ggml_metal_init: picking default device: Apple M4
0.01.339.661 I ggml_metal_init: using embedded metal library
0.01.342.129 I ggml_metal_init: GPU name:   Apple M4
0.01.342.130 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.342.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.342.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.342.132 I ggml_metal_init: simdgroup reduction   = true
0.01.342.132 I ggml_metal_init: simdgroup matrix mul. = true
0.01.342.132 I ggml_metal_init: has residency sets    = true
0.01.342.132 I ggml_metal_init: has bfloat            = true
0.01.342.132 I ggml_metal_init: use bfloat            = true
0.01.342.133 I ggml_metal_init: hasUnifiedMemory      = true
0.01.342.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.352.614 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.354.362 I init:      Metal compute buffer size =    25.56 MiB
0.01.354.364 I init:        CPU compute buffer size =     1.06 MiB
0.01.354.364 I init: graph nodes  = 943
0.01.354.364 I init: graph splits = 2
0.01.354.365 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.01.354.366 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.354.366 I 
0.01.354.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.354.397 I compute_imatrix: tokenizing the input ..
0.01.358.418 I compute_imatrix: tokenization took 4.02 ms
0.01.358.421 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.641.916 I compute_imatrix: 0.28 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.644.627 I llama_perf_context_print:        load time =    1616.85 ms
0.01.644.628 I llama_perf_context_print: prompt eval time =     283.49 ms /   128 tokens (    2.21 ms per token,   451.52 tokens per second)
0.01.644.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.644.629 I llama_perf_context_print:       total time =    1619.55 ms /   129 tokens
0.01.645.213 I ggml_metal_free: deallocating

real	0m1.828s
user	0m0.137s
sys	0m0.219s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159308a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159309180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159309730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159309ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15930a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15930a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15930adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15930b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15930b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15930be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15930c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15930c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15930d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15930db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15930e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15930ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15930f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15930f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15930ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159310780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159310ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1593115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159311ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159312580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159312ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159312f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159313570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1593141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159314720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1593149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159314e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159315140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1593159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159315f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1593161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159316670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159316b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159316fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159317450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1593178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159317d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159318230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1593186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159318b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159318e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159319440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159319a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15931a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15931a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15931af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15931b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15931bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15931c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15931c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15931cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15931d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15931d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15931dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15931e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15931e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15931ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15931f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15931f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15931fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15931ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1593203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159320840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159320ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159321180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159321620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159321ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159321f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159322400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159322950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159322ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1593233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159323940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159323e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1593243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159324930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159324e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1593253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159325920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159325e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1593263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159326910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159326e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1593273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159327900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159327e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1593283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1593288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159328e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159329390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1593298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159329e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15932a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15931a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15932a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15932afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15932b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15932ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15932bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15932c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15932ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15932cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15932d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15932da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15932df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15932e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15932ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15932ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15932f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15932f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15932fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159330290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159330730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159330bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159331070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159331510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1593319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159331e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1593322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159332790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159332c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1593330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159333570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159333a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159333eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159334350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1593347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159334c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159335130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1593355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159335a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159335f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1593363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159336850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159336cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159337190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159337630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159337ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159337f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159338410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1593388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159338d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1593391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159339690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159339b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159339fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15933a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15933a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15933adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15933b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15933b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15933bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15933c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15933c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15933c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15933ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15933d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15933d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15933dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15933e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15933e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15933e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15933ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15933f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15933f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15933fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1593400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159340590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159340a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159340ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159341370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159341810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159341cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159342150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1593425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159342a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159342f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1593433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159343870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159343d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1593441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159344650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159344af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159344f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159345430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1593458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159345d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159346210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1593466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159346c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159347150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1593476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159347bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159347eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1593484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159348ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1593490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1593498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159349d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15934a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15934a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15934ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15934b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15934b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15934bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15934c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15934c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15934cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15934d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15934d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15934df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15934e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15934e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15934ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15934f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15934f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15934fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159350440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159350990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159350ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159351430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159351980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159351ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159352420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159352970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159352ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159353410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159353960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159353eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159354400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159354950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159354ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1593553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159355940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159355e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1593563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159356930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159356e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1593573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159357920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159357e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1593583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159358910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159358e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1593593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159359900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159359e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15935a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15935a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15935ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15935b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15935b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15935be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15935c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15935c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15935ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15935d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15935d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15935de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15935e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15935e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15935ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15935f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15935f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15935fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159360130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1593605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159360a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159360f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1593613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159361850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159361cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159362190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159362630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159362ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159362f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159363410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1593638b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159363e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159364520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159364c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159365360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159365a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159365d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159366530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1593667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159366e00 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 943
init: graph splits = 2
get_kv_self: llama_context does not have a KV cache
0.00.715.926 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.715.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
get_kv_self: llama_context does not have a KV cache
main : serialized state into 201251 out of a maximum of 201251 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159206040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1592064b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159206920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159206d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159207200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159207670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159207ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159207f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1592083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159208930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159208da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159209420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x159209f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15920a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15920af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15920b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15920bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15920c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15920cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15920d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15920da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15920e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15920e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15920efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15920f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15920f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15920fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1592100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159210550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1592109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159210e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159211360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1592117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159211a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159211f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159212370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1592127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159212c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1592130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159213530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1592139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159213e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159214280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1592146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159214b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159214fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159215440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1592158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159215d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159216190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159216600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159216a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159216ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159217350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1592177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159217c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1592181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1592186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159218b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159218f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1592193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159219860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159219cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15921a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15921a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15921aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15921ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15921b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15921b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15921bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15921c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15921c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15921c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15921cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15921d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15921d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15921daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15921df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15921e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15921e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15921ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15921f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15921f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15921fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15921fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1592202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159220750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159220bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159221030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1592214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159221910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159221d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1592221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159222660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159222ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159222f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1592233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159223820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159223c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159224100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159224570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1592249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159224e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1592252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159225730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159225ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159226010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159226480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1592268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159226d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1592271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159227640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159227ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159227f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159228390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159228800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159228c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1592290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159229550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1592299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159229e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15922a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15922a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15922ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15922aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15922b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15922b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15922bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15922c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15922c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15922ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15922cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15922d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15922d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15922dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15922e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15922e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15922e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15922ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15922f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15922f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15922fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15922ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159230440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1592308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159230d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159231190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159231600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159231a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159231ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159232350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1592327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159232c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1592330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159233510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159233980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159233df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159234260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1592346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159234b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159234fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159235420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159235890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159235d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159236170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1592365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159237210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1592374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159237790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159237c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159238070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1592384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159238950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159238dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159239230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1592396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159239b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159239f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15923a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15923a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15923acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15923b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15923b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15923ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15923be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15923c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15923c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15923cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15923d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15923d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15923d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15923dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15923e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15923e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15923eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15923ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15923f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15923f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15923fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159240120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159240590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159240a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159240f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159241470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1592418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159241d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1592421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159242630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159242b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159243060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159243bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159243e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159244450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159244a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159244fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159245590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159245b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159246110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1592466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159246c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159247250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159247810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159247dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159248390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159248950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159248f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1592494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159249a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15924a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15924a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15924abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15924b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15924b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15924bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15924c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15924c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15924ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15924d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15924d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15924df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15924e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15924eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15924f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15924f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15924fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159250210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1592507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159250d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159251350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159251910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159251ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159252490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159252a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159253010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1592535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159253b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159254150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159254710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159254cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159255290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159255850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159255e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1592563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159256990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159256f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159257510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159257ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159258090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159258590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159258a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159258f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159259490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159259990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159259e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15925a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15925a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15925ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15925b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15925b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15925bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15925c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15925c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15925cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15925d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15925dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15925e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15925eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15925edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15925f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15925f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15925fe80 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 943
init: graph splits = 2
main : deserialized state from 201251 out of a maximum of 201251 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x172504820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x172504c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x172505100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x172505570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1725059e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x172505e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1725062c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x172506730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x172506ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x172507110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x172507580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x172507c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x172508720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x172508ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1725096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x172509e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x17250a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x17250ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x17250b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x17250bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x17250c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x17250c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x17250d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x17250d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x17250ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x17250e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x17250e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x17250e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x17250ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x17250f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x17250f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x17250fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x17250ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x172510270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1725106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x172510b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x172510fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x172511430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1725118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x172511d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x172512180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1725125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x172512a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x172512ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x172513340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1725137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x172513c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x172514090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x172514500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x172514970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x172514de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x172515250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1725156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x172515b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x172515fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x172516410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x172516980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x172516e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1725172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x172517760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x172517bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x172518040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1725184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x172518920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x172518d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x172519200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x172519670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x172519ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x172519f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x17251a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x17251a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x17251aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x17251b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x17251b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x17251b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x17251be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x17251c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x17251c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x17251cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x17251d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x17251d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x17251d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x17251dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x17251e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x17251e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x17251eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x17251ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x17251f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x17251f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x17251fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1725200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x172520560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1725209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x172520e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1725212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x172521720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x172521b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x172522000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x172522470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1725228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x172522d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1725231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x172523630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x172523aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x172524410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1725246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x172524b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x172524fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x172525420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x172525890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x172525d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x172526170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1725265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x172526a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x172526ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x172527330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1725277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x172527c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x172528080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1725284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x172528960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x172528dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x172529240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1725296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x172529b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x172529f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x17252a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x17252a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x17252ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x17252b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x17252b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x17252ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x17252bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x17252c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x17252c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x17252cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x17252d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x17252d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x17252d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x17252ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x17252e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x17252e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x17252eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x17252ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x17252f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x17252f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x17252fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x172530130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1725305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x172530a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x172530e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1725312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x172531760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x172531bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x172532040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1725324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x172532920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x172532d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x172533200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x172533670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x172533ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x172533f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1725343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x172534830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x172534ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x172535110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x172535580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1725359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x172535e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1725362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x172536740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x172536bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x172537020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x172537490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x172537900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x172537d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1725381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x172538650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x172538ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x172538f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1725393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x172539810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x172539c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x17253a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x17253a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x17253a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x17253ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x17253b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x17253b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x17253bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x17253c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x17253c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x17253c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x17253cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x17253d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x17253d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x17253daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x17253df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x17253e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x17253e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x17253ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x17253f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x17253f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x17253f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x17253fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x172540290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x172540700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x172540c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x172541100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x172541570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1725420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x172542380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x172542640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x172542ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x172542f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x172543390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x172543800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x172543c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1725440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x172544550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1725449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x172544e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1725452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x172545710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x172545b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x172545ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x172546460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1725468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x172546d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1725471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x172547620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x172547a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x172547f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x172548370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1725487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x172548c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1725490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x172549530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1725499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x172549e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x17254a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x17254a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x17254ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x17254afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x17254b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x17254b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x17254bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x17254c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x17254c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x17254ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x17254cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x17254d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x17254d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x17254dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x17254e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x17254e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x17254e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x17254edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x17254f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x17254f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x17254fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x17254ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x172550420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x172550890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x172550d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x172551170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1725515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x172551a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x172551ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x172552330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1725527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x172552c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x172553080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1725534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x172553960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x172553dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x172554240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1725546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x172554b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x172554f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x172555400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x172555870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x172555ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x172556750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x172556e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x172557590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x172557cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x172557f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1725583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1725589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x172558ff0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 943
init: graph splits = 2
main : deserialized state from 201251 out of a maximum of 201251 bytes
main : seq 0 copied, 0 bytes
get_kv_self: llama_context does not have a KV cache
main : kv cache cleared
main : seq 1 restored, 0 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L


second run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L


single seq run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L

real	0m1.588s
user	0m0.243s
sys	0m0.174s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125f10c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125f11380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125f11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125f12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125f12a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125f12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125f135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125f13b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125f14050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125f14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125f14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125f15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125f15d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125f16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125f16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125f17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125f17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125f181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125f18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125f190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125f197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125f19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125f1a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125f1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125f1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125f1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125f1c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125f1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125f1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125f1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125f1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125f1e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125f1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125f1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125f1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125f1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125f1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125f1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125f1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125f20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125f208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125f20d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125f21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125f21640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125f21c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125f22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125f22b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125f23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125f237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125f23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125f243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125f249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125f251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125f25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125f25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125f263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125f26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125f26e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125f27320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125f277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125f27c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125f28100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125f285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125f28a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125f28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125f29380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125f29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125f29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125f2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125f2a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125f2ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125f2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125f2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125f2bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125f2c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125f2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125f2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125f2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125f2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125f2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125f2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125f2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125f2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125f2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125f2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125f2fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125f30050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125f305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125f30af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125f31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125f31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125f31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125f32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125f32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125f22260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125f329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125f331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125f336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125f33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125f34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125f346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125f34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125f35180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125f356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125f35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125f36170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125f366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125f36c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125f37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125f376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125f37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125f37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125f38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125f38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125f39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125f39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125f39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125f3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125f3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125f3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125f3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125f3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125f3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125f3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125f3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125f3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125f3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125f3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125f3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125f3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125f3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125f3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125f3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125f3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125f3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125f3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125f3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125f3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125f40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125f40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125f40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125f40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125f413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125f41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125f41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125f421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125f42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125f42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125f42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125f43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125f438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125f43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125f44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125f446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125f44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125f45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125f454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125f45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125f45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125f46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125f46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125f46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125f47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125f47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125f479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125f47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125f482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125f48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125f48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125f490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125f49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125f49a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125f49eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125f4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125f4a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125f4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125f4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125f4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125f4ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125f4bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125f4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125f4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125f4ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125f4d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125f4d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125f4dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125f4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125f4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125f4ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125f4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125f4f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125f4fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125f500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125f506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125f50cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125f512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125f51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125f51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125f52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125f52840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125f52e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125f53640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125f53ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125f53f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125f54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125f54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125f55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125f55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125f55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125f56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125f56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125f56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125f57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125f57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125f57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125f580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125f58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125f58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125f590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125f59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125f59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125f5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125f5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125f5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125f5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125f5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125f5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125f5c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125f5c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125f5cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125f5d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125f5d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125f5db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125f5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125f5e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125f5eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125f5f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125f5f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125f5fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125f60070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125f605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125f60b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125f61060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125f615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125f61b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125f62050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125f625a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125f62af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125f63040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125f63590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125f63ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125f64030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125f64580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125f64ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125f65020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125f65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125f65ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125f66010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125f66560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125f66ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125f67000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125f67550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125f679f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125f67e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125f68330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125f687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125f68c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125f69110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125f695b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125f69a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125f69ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125f6a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125f6a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125f6acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125f6b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125f6b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125f6bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125f6c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125f6c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125f6ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125f6d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125f6dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125f6df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125f6e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125f6e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125f6f000 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   106.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 944
init: graph splits = 2
get_kv_self: llama_context does not have a KV cache
0.00.068.348 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.068.352 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.068.352 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
get_kv_self: llama_context does not have a KV cache
main : serialized state into 201251 out of a maximum of 201251 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127004fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127005420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1270059e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127005f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127006540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127006af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1270070a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127007650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127007c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127008100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127008600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127008b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127009620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127009dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12700a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12700ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12700b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12700bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12700c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12700ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12700d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12700d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12700df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12700e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12700edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12700f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12700f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12700fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1270102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127010ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127010f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127011210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127011aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127011fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1270122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127012740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127012be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127013080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127013520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1270139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127013e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127014300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1270147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127014c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127014f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127015510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127015b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127016130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127016740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127016d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127017970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127017f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127018590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127018d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127019220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1270196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127019980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127019f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12701a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12701ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12701b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12701b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12701ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12701bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12701c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12701c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12701cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12701d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12701d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12701da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12701df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12701e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12701e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12701ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12701f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12701f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12701fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127020380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1270208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127020e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127021370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1270218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127021e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127022360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1270228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127022e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127023350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1270238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127023df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127024340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127024890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127024de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127025330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127025880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127026320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127026870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127026dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127027310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127027860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127027db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127028300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127028850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127028da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1270292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127029840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127029d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12702a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12702a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12702ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12702b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12702b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12702bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12702c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12702c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12702caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12702cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12702d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12702d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12702dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12702e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12702e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12702eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12702efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12702f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12702f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12702fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127030220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1270306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127030b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127031000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1270314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127031940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127031de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127032280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127032720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127032bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127033060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127033500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1270339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127033e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1270342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127034780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127034c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1270350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127035560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127035a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127035ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127036340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1270367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127036c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127037120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1270375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127037a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127037f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1270383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127038840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127038ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127039180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127039620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127039ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127039f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12703a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12703a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12703ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12703b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12703b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12703bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12703bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12703c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12703c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12703cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12703d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12703d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12703db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12703e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12703e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12703e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12703ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12703f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12703f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12703fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127040080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127040520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1270409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127040e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127041300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1270417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127041c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1270420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127042580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127042a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127042f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1270434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127043a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127043f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127044220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127044830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127044e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127045450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127045c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1270460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1270463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1270469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127046fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1270477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127047c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1270480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127048590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127048d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127049290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1270497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127049d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12704a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12704a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12704ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12704b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12704b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12704bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12704c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12704c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12704cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12704d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12704d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12704dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12704e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12704e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12704ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12704f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12704f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12704fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127050220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127050770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127050cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127051210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127051760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127051cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127052200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127052750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127052ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1270531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127053740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1270541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127054730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127054c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1270551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127055c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1270561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127056710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127056c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1270571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127057700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127057c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1270581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1270586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127058c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127059190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1270596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127059c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12705a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12705a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12705ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12705b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12705b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12705bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12705c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12705c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12705c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12705cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12705d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12705d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12705dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12705e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12705e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12705e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12705ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12705f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12705f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12705fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127060170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127060890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127060fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1270616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127061df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1270620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1270628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127062b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127063170 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   106.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 944
init: graph splits = 2
main : deserialized state from 201251 out of a maximum of 201251 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e056f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e05b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e05fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e06440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e068b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e06d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e07190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e07600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e07a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e07fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e08430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e08ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e09d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e0a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e0acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e0b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e0c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e0c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e0d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e0d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e0df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e0f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e0fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e11590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e11a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e11e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e12750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e13030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e14660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e14ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e17830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e18a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e19c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e1bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125e1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125e1c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125e1cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125e1d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125e1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125e1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125e1ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125e1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125e1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125e1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125e1f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125e1f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125e1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125e1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125e20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125e206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125e20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125e20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125e21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125e21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125e21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125e22160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125e225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125e22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125e22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125e23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125e23790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125e23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125e24070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125e244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125e24950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125e25190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125e25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125e258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125e25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125e261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125e26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125e26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125e27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125e3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125e402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e42e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e43100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e43ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e44580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e44e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e46020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e46d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e47650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e47ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e48c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e49560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e49e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e4ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e4caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e4cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e4e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e4e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e4ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e4ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e50450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e50d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e51610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e52360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125e527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125e52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e53990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e54270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e54fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e55430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e58a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e58cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125e59160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e59760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e59d70 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   106.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 944
init: graph splits = 2
main : deserialized state from 201251 out of a maximum of 201251 bytes
main : seq 0 copied, 0 bytes
get_kv_self: llama_context does not have a KV cache
main : kv cache cleared
main : seq 1 restored, 0 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L


second run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L


single seq run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L

real	0m0.756s
user	0m0.191s
sys	0m0.044s
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]10.1498,'
+ qnt=f16
++ echo '[1]10.1498,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1498
++ echo '10.1498 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 10.1498
+ return 0
  - f16 @ 10.1498 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]10.1362,'
+ qnt=q8_0
++ echo '[1]10.1362,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1362
++ echo '10.1362 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 10.1362
+ return 0
  - q8_0 @ 10.1362 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]11.1740,'
+ qnt=q4_0
++ echo '[1]11.1740,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=11.1740
++ echo '11.1740 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 11.1740
+ return 0
  - q4_0 @ 11.1740 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]10.5507,'
+ qnt=q4_1
++ echo '[1]10.5507,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.5507
++ echo '10.5507 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 10.5507
+ return 0
  - q4_1 @ 10.5507 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]10.0972,'
+ qnt=q5_0
++ echo '[1]10.0972,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.0972
++ echo '10.0972 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 10.0972
+ return 0
  - q5_0 @ 10.0972 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]10.1971,'
+ qnt=q5_1
++ echo '[1]10.1971,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1971
++ echo '10.1971 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 10.1971
+ return 0
  - q5_1 @ 10.1971 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]12.0517,'
+ qnt=q3_k
++ echo '[1]12.0517,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=12.0517
++ echo '12.0517 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 12.0517
+ return 0
  - q3_k @ 12.0517 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]10.1031,'
+ qnt=q4_k
++ echo '[1]10.1031,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1031
++ echo '10.1031 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 10.1031
+ return 0
  - q4_k @ 10.1031 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]10.2433,'
+ qnt=q5_k
++ echo '[1]10.2433,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.2433
++ echo '10.2433 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 10.2433
+ return 0
  - q5_k @ 10.2433 OK
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]10.3179,'
+ qnt=q6_k
++ echo '[1]10.3179,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.3179
++ echo '10.3179 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 10.3179
+ return 0
  - q6_k @ 10.3179 OK
+ cat /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ grep Final
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_with_model_debug.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.42 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.62 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.04 sec*proc (2 tests)

Total Test time (real) =   1.06 sec
        1.08 real         0.50 user         0.09 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_with_model_release.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/8b/c4a9b2a3b2b2febf2883aeed0c27da8c0f7a9c/ggml-100-mac-m4/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.29 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.12 user         0.08 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
