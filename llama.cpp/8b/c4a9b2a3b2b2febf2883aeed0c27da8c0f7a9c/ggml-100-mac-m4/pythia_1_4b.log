Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.613s
user	0m0.842s
sys	0m1.305s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target xxhash
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Linking CXX executable ../../bin/llama-gguf
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Built target llava
[ 33%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-chat
[ 49%] Linking CXX executable ../bin/test-log
[ 50%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-sampling
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-chat
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched-bench
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-infill
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-perplexity
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookahead
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-cli
[ 81%] Generating loading.html.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Built target llama-parallel
[ 83%] Built target llama-perplexity
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-speculative-simple
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.203s
user	0m6.636s
sys	0m10.548s

main: quantize time =  9346.41 ms
main:    total time =  9346.41 ms

main: quantize time =  2452.12 ms
main:    total time =  2452.12 ms

main: quantize time =  2189.17 ms
main:    total time =  2189.17 ms

main: quantize time =  2682.34 ms
main:    total time =  2682.34 ms

main: quantize time =  2142.41 ms
main:    total time =  2142.41 ms

main: quantize time =  5169.47 ms
main:    total time =  5169.47 ms

main: quantize time =  5776.53 ms
main:    total time =  5776.53 ms

main: quantize time =  6918.37 ms
main:    total time =  6918.37 ms

main: quantize time =  5784.48 ms
main:    total time =  5784.48 ms

main: quantize time =  4633.19 ms
main:    total time =  4633.19 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.232 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.409 I main: llama backend init
0.00.000.417 I main: load the model and apply lora adapter, if any
0.00.096.480 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.109.977 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.110.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.110.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.110.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.110.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.110.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.110.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.110.018 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.110.018 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.110.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.110.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.110.021 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.110.022 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.110.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.110.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.110.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.110.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.117.175 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.119.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.126.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.126.710 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.126.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.126.711 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.126.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.126.714 I llama_model_loader: - type  f32:  194 tensors
0.00.126.714 I llama_model_loader: - type  f16:   98 tensors
0.00.126.717 I print_info: file format = GGUF V3 (latest)
0.00.126.719 I print_info: file type   = all F32 (guessed)
0.00.126.741 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.137.870 I load: special tokens cache size = 25
0.00.144.947 I load: token to piece cache size = 0.2984 MB
0.00.144.973 I print_info: arch             = gptneox
0.00.144.974 I print_info: vocab_only       = 0
0.00.144.974 I print_info: n_ctx_train      = 2048
0.00.144.974 I print_info: n_embd           = 2048
0.00.144.975 I print_info: n_layer          = 24
0.00.144.978 I print_info: n_head           = 16
0.00.144.979 I print_info: n_head_kv        = 16
0.00.144.979 I print_info: n_rot            = 32
0.00.144.979 I print_info: n_swa            = 0
0.00.144.979 I print_info: n_embd_head_k    = 128
0.00.144.979 I print_info: n_embd_head_v    = 128
0.00.144.980 I print_info: n_gqa            = 1
0.00.144.981 I print_info: n_embd_k_gqa     = 2048
0.00.144.981 I print_info: n_embd_v_gqa     = 2048
0.00.144.982 I print_info: f_norm_eps       = 1.0e-05
0.00.144.983 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.144.983 I print_info: f_clamp_kqv      = 0.0e+00
0.00.144.983 I print_info: f_max_alibi_bias = 0.0e+00
0.00.144.983 I print_info: f_logit_scale    = 0.0e+00
0.00.144.984 I print_info: n_ff             = 8192
0.00.144.984 I print_info: n_expert         = 0
0.00.144.984 I print_info: n_expert_used    = 0
0.00.144.984 I print_info: causal attn      = 1
0.00.144.984 I print_info: pooling type     = 0
0.00.144.984 I print_info: rope type        = 2
0.00.144.985 I print_info: rope scaling     = linear
0.00.144.985 I print_info: freq_base_train  = 10000.0
0.00.144.985 I print_info: freq_scale_train = 1
0.00.144.985 I print_info: n_ctx_orig_yarn  = 2048
0.00.144.986 I print_info: rope_finetuned   = unknown
0.00.144.986 I print_info: ssm_d_conv       = 0
0.00.144.986 I print_info: ssm_d_inner      = 0
0.00.144.986 I print_info: ssm_d_state      = 0
0.00.144.986 I print_info: ssm_dt_rank      = 0
0.00.144.986 I print_info: ssm_dt_b_c_rms   = 0
0.00.144.987 I print_info: model type       = 1.4B
0.00.144.987 I print_info: model params     = 1.41 B
0.00.144.987 I print_info: general.name     = 1.4B
0.00.144.987 I print_info: vocab type       = BPE
0.00.144.988 I print_info: n_vocab          = 50304
0.00.144.988 I print_info: n_merges         = 50009
0.00.144.988 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.144.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.144.988 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.144.989 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.144.989 I print_info: LF token         = 187 'Ċ'
0.00.144.989 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.144.989 I print_info: max token length = 1024
0.00.144.990 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.180.017 I load_tensors: offloading 24 repeating layers to GPU
0.00.180.019 I load_tensors: offloading output layer to GPU
0.00.180.020 I load_tensors: offloaded 25/25 layers to GPU
0.00.180.039 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.180.041 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.180.384 I llama_context: constructing llama_context
0.00.180.385 I llama_context: n_seq_max     = 1
0.00.180.385 I llama_context: n_ctx         = 2048
0.00.180.386 I llama_context: n_ctx_per_seq = 2048
0.00.180.386 I llama_context: n_batch       = 2048
0.00.180.386 I llama_context: n_ubatch      = 512
0.00.180.386 I llama_context: flash_attn    = 0
0.00.180.387 I llama_context: freq_base     = 10000.0
0.00.180.387 I llama_context: freq_scale    = 1
0.00.180.388 I ggml_metal_init: allocating
0.00.180.405 I ggml_metal_init: found device: Apple M4
0.00.180.411 I ggml_metal_init: picking default device: Apple M4
0.00.181.010 I ggml_metal_init: using embedded metal library
0.00.230.614 I ggml_metal_init: GPU name:   Apple M4
0.00.230.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.230.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.230.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.230.620 I ggml_metal_init: simdgroup reduction   = true
0.00.230.620 I ggml_metal_init: simdgroup matrix mul. = true
0.00.230.621 I ggml_metal_init: has residency sets    = true
0.00.230.621 I ggml_metal_init: has bfloat            = true
0.00.230.621 I ggml_metal_init: use bfloat            = true
0.00.230.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.230.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.278.793 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.283.447 I init:      Metal compute buffer size =   102.25 MiB
0.00.283.449 I init:        CPU compute buffer size =     5.01 MiB
0.00.283.449 I init: graph nodes  = 943
0.00.283.450 I init: graph splits = 2
0.00.283.451 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.283.453 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.283.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.283.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.317.088 I main: llama threadpool init, n_threads = 4
0.00.317.123 I 
0.00.317.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.317.138 I 
0.00.317.181 I sampler seed: 1234
0.00.317.187 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.317.210 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.317.211 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.317.211 I 
I believe the meaning of life is to do.
The following the court, and the second, I'm not have a large, we see and the world, the new and the case where the time, the following day of the presence of a “t-1) = new, a little bit of the first-type="table-type="

0.02.184.387 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47364.91 tokens per second)
0.02.184.388 I llama_perf_context_print:        load time =     219.73 ms
0.02.184.392 I llama_perf_context_print: prompt eval time =      43.93 ms /     7 tokens (    6.28 ms per token,   159.33 tokens per second)
0.02.184.393 I llama_perf_context_print:        eval time =    1820.63 ms /    63 runs   (   28.90 ms per token,    34.60 tokens per second)
0.02.184.394 I llama_perf_context_print:       total time =    1868.16 ms /    70 tokens
0.02.184.650 I ggml_metal_free: deallocating

real	0m2.516s
user	0m0.119s
sys	0m0.081s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.236 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.414 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.415 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.415 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.417 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.418 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.418 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.421 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.404 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.597 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.598 I llama_model_loader: - type  f32:  194 tensors
0.00.034.598 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.599 I print_info: file format = GGUF V3 (latest)
0.00.034.600 I print_info: file type   = Q8_0
0.00.034.601 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.561 I load: special tokens cache size = 25
0.00.048.380 I load: token to piece cache size = 0.2984 MB
0.00.048.394 I print_info: arch             = gptneox
0.00.048.395 I print_info: vocab_only       = 0
0.00.048.395 I print_info: n_ctx_train      = 2048
0.00.048.396 I print_info: n_embd           = 2048
0.00.048.396 I print_info: n_layer          = 24
0.00.048.400 I print_info: n_head           = 16
0.00.048.401 I print_info: n_head_kv        = 16
0.00.048.401 I print_info: n_rot            = 32
0.00.048.401 I print_info: n_swa            = 0
0.00.048.401 I print_info: n_embd_head_k    = 128
0.00.048.402 I print_info: n_embd_head_v    = 128
0.00.048.402 I print_info: n_gqa            = 1
0.00.048.403 I print_info: n_embd_k_gqa     = 2048
0.00.048.404 I print_info: n_embd_v_gqa     = 2048
0.00.048.405 I print_info: f_norm_eps       = 1.0e-05
0.00.048.405 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.405 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.405 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.405 I print_info: f_logit_scale    = 0.0e+00
0.00.048.406 I print_info: n_ff             = 8192
0.00.048.406 I print_info: n_expert         = 0
0.00.048.406 I print_info: n_expert_used    = 0
0.00.048.407 I print_info: causal attn      = 1
0.00.048.407 I print_info: pooling type     = 0
0.00.048.407 I print_info: rope type        = 2
0.00.048.408 I print_info: rope scaling     = linear
0.00.048.409 I print_info: freq_base_train  = 10000.0
0.00.048.409 I print_info: freq_scale_train = 1
0.00.048.409 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.410 I print_info: rope_finetuned   = unknown
0.00.048.410 I print_info: ssm_d_conv       = 0
0.00.048.410 I print_info: ssm_d_inner      = 0
0.00.048.410 I print_info: ssm_d_state      = 0
0.00.048.410 I print_info: ssm_dt_rank      = 0
0.00.048.410 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.411 I print_info: model type       = 1.4B
0.00.048.411 I print_info: model params     = 1.41 B
0.00.048.411 I print_info: general.name     = 1.4B
0.00.048.412 I print_info: vocab type       = BPE
0.00.048.412 I print_info: n_vocab          = 50304
0.00.048.412 I print_info: n_merges         = 50009
0.00.048.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.413 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.413 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.413 I print_info: LF token         = 187 'Ċ'
0.00.048.413 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.413 I print_info: max token length = 1024
0.00.048.415 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.231.269 I load_tensors: offloading 24 repeating layers to GPU
0.01.231.274 I load_tensors: offloading output layer to GPU
0.01.231.276 I load_tensors: offloaded 25/25 layers to GPU
0.01.231.295 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.231.298 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.232.330 I llama_context: constructing llama_context
0.01.232.332 I llama_context: n_seq_max     = 1
0.01.232.333 I llama_context: n_ctx         = 2048
0.01.232.333 I llama_context: n_ctx_per_seq = 2048
0.01.232.333 I llama_context: n_batch       = 2048
0.01.232.333 I llama_context: n_ubatch      = 512
0.01.232.334 I llama_context: flash_attn    = 0
0.01.232.334 I llama_context: freq_base     = 10000.0
0.01.232.335 I llama_context: freq_scale    = 1
0.01.232.335 I ggml_metal_init: allocating
0.01.232.345 I ggml_metal_init: found device: Apple M4
0.01.232.351 I ggml_metal_init: picking default device: Apple M4
0.01.233.636 I ggml_metal_init: using embedded metal library
0.01.239.210 I ggml_metal_init: GPU name:   Apple M4
0.01.239.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.239.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.239.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.239.215 I ggml_metal_init: simdgroup reduction   = true
0.01.239.215 I ggml_metal_init: simdgroup matrix mul. = true
0.01.239.215 I ggml_metal_init: has residency sets    = true
0.01.239.216 I ggml_metal_init: has bfloat            = true
0.01.239.216 I ggml_metal_init: use bfloat            = true
0.01.239.217 I ggml_metal_init: hasUnifiedMemory      = true
0.01.239.218 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.254.681 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.265.996 I init:      Metal compute buffer size =   102.25 MiB
0.01.265.997 I init:        CPU compute buffer size =     5.01 MiB
0.01.265.998 I init: graph nodes  = 943
0.01.265.998 I init: graph splits = 2
0.01.266.000 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.01.266.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.266.176 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.266.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.291.436 I main: llama threadpool init, n_threads = 4
0.01.291.473 I 
0.01.291.488 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.291.488 I 
0.01.291.642 I sampler seed: 1234
0.01.291.647 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.291.657 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.291.657 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.291.657 I 
I believe the meaning of life is to do the way the same time-1.










" to the following:




    }



The results in this year to get him, and the way.








" (3. The

0.02.373.845 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63111.11 tokens per second)
0.02.373.845 I llama_perf_context_print:        load time =    1281.26 ms
0.02.373.846 I llama_perf_context_print: prompt eval time =      48.77 ms /     7 tokens (    6.97 ms per token,   143.53 tokens per second)
0.02.373.847 I llama_perf_context_print:        eval time =    1030.83 ms /    63 runs   (   16.36 ms per token,    61.12 tokens per second)
0.02.373.848 I llama_perf_context_print:       total time =    1083.34 ms /    70 tokens
0.02.374.117 I ggml_metal_free: deallocating

real	0m2.390s
user	0m0.099s
sys	0m0.186s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.920 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.002 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.015 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.017 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.017 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.017 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.018 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.020 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.022 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.023 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.023 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.724 I llama_model_loader: - type  f32:  194 tensors
0.00.038.725 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.726 I print_info: file format = GGUF V3 (latest)
0.00.038.726 I print_info: file type   = Q4_0
0.00.038.727 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.220 I load: special tokens cache size = 25
0.00.055.593 I load: token to piece cache size = 0.2984 MB
0.00.055.609 I print_info: arch             = gptneox
0.00.055.611 I print_info: vocab_only       = 0
0.00.055.611 I print_info: n_ctx_train      = 2048
0.00.055.611 I print_info: n_embd           = 2048
0.00.055.612 I print_info: n_layer          = 24
0.00.055.616 I print_info: n_head           = 16
0.00.055.617 I print_info: n_head_kv        = 16
0.00.055.617 I print_info: n_rot            = 32
0.00.055.617 I print_info: n_swa            = 0
0.00.055.617 I print_info: n_embd_head_k    = 128
0.00.055.618 I print_info: n_embd_head_v    = 128
0.00.055.620 I print_info: n_gqa            = 1
0.00.055.621 I print_info: n_embd_k_gqa     = 2048
0.00.055.622 I print_info: n_embd_v_gqa     = 2048
0.00.055.623 I print_info: f_norm_eps       = 1.0e-05
0.00.055.623 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.623 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.623 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.624 I print_info: f_logit_scale    = 0.0e+00
0.00.055.625 I print_info: n_ff             = 8192
0.00.055.630 I print_info: n_expert         = 0
0.00.055.630 I print_info: n_expert_used    = 0
0.00.055.630 I print_info: causal attn      = 1
0.00.055.630 I print_info: pooling type     = 0
0.00.055.631 I print_info: rope type        = 2
0.00.055.631 I print_info: rope scaling     = linear
0.00.055.631 I print_info: freq_base_train  = 10000.0
0.00.055.636 I print_info: freq_scale_train = 1
0.00.055.637 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.637 I print_info: rope_finetuned   = unknown
0.00.055.637 I print_info: ssm_d_conv       = 0
0.00.055.638 I print_info: ssm_d_inner      = 0
0.00.055.638 I print_info: ssm_d_state      = 0
0.00.055.638 I print_info: ssm_dt_rank      = 0
0.00.055.638 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.638 I print_info: model type       = 1.4B
0.00.055.639 I print_info: model params     = 1.41 B
0.00.055.641 I print_info: general.name     = 1.4B
0.00.055.642 I print_info: vocab type       = BPE
0.00.055.642 I print_info: n_vocab          = 50304
0.00.055.644 I print_info: n_merges         = 50009
0.00.055.644 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.644 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.645 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.655 I print_info: LF token         = 187 'Ċ'
0.00.055.656 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.656 I print_info: max token length = 1024
0.00.055.657 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.708.639 I load_tensors: offloading 24 repeating layers to GPU
0.00.708.655 I load_tensors: offloading output layer to GPU
0.00.708.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.708.696 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.708.697 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.710.090 I llama_context: constructing llama_context
0.00.710.094 I llama_context: n_seq_max     = 1
0.00.710.094 I llama_context: n_ctx         = 2048
0.00.710.095 I llama_context: n_ctx_per_seq = 2048
0.00.710.095 I llama_context: n_batch       = 2048
0.00.710.096 I llama_context: n_ubatch      = 512
0.00.710.096 I llama_context: flash_attn    = 0
0.00.710.099 I llama_context: freq_base     = 10000.0
0.00.710.099 I llama_context: freq_scale    = 1
0.00.710.101 I ggml_metal_init: allocating
0.00.710.227 I ggml_metal_init: found device: Apple M4
0.00.710.242 I ggml_metal_init: picking default device: Apple M4
0.00.712.216 I ggml_metal_init: using embedded metal library
0.00.717.875 I ggml_metal_init: GPU name:   Apple M4
0.00.717.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.882 I ggml_metal_init: simdgroup reduction   = true
0.00.717.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.883 I ggml_metal_init: has residency sets    = true
0.00.717.883 I ggml_metal_init: has bfloat            = true
0.00.717.883 I ggml_metal_init: use bfloat            = true
0.00.717.884 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.737.180 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.748.427 I init:      Metal compute buffer size =   102.25 MiB
0.00.748.429 I init:        CPU compute buffer size =     5.01 MiB
0.00.748.429 I init: graph nodes  = 943
0.00.748.430 I init: graph splits = 2
0.00.748.433 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.748.440 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.975 I main: llama threadpool init, n_threads = 4
0.00.774.016 I 
0.00.774.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.031 I 
0.00.774.205 I sampler seed: 1234
0.00.774.211 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.221 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.222 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.222 I 
I believe the meaning of life is to sell them to


“It’s a large masses.
The first time. Influence, the other people don’s, as possible.com. Aspectate (d
' (Table 6. A A A A A A A A A A A A A.html">
It

0.01.451.754 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.451.754 I llama_perf_context_print:        load time =     763.06 ms
0.01.451.755 I llama_perf_context_print: prompt eval time =      48.99 ms /     7 tokens (    7.00 ms per token,   142.88 tokens per second)
0.01.451.756 I llama_perf_context_print:        eval time =     625.89 ms /    63 runs   (    9.93 ms per token,   100.66 tokens per second)
0.01.451.756 I llama_perf_context_print:       total time =     678.77 ms /    70 tokens
0.01.451.983 I ggml_metal_free: deallocating

real	0m1.477s
user	0m0.107s
sys	0m0.135s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.017.018 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.037.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.554 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.047.433 I llama_model_loader: - type  f32:  194 tensors
0.00.047.433 I llama_model_loader: - type q4_1:   97 tensors
0.00.047.433 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.434 I print_info: file format = GGUF V3 (latest)
0.00.047.434 I print_info: file type   = Q4_1
0.00.047.438 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.056.620 I load: special tokens cache size = 25
0.00.064.959 I load: token to piece cache size = 0.2984 MB
0.00.064.974 I print_info: arch             = gptneox
0.00.064.975 I print_info: vocab_only       = 0
0.00.064.975 I print_info: n_ctx_train      = 2048
0.00.064.976 I print_info: n_embd           = 2048
0.00.064.976 I print_info: n_layer          = 24
0.00.064.979 I print_info: n_head           = 16
0.00.064.980 I print_info: n_head_kv        = 16
0.00.064.980 I print_info: n_rot            = 32
0.00.064.980 I print_info: n_swa            = 0
0.00.064.981 I print_info: n_embd_head_k    = 128
0.00.064.981 I print_info: n_embd_head_v    = 128
0.00.064.982 I print_info: n_gqa            = 1
0.00.064.983 I print_info: n_embd_k_gqa     = 2048
0.00.064.983 I print_info: n_embd_v_gqa     = 2048
0.00.064.984 I print_info: f_norm_eps       = 1.0e-05
0.00.064.984 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.984 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.987 I print_info: f_logit_scale    = 0.0e+00
0.00.064.988 I print_info: n_ff             = 8192
0.00.064.988 I print_info: n_expert         = 0
0.00.064.989 I print_info: n_expert_used    = 0
0.00.064.989 I print_info: causal attn      = 1
0.00.064.989 I print_info: pooling type     = 0
0.00.064.989 I print_info: rope type        = 2
0.00.064.989 I print_info: rope scaling     = linear
0.00.064.990 I print_info: freq_base_train  = 10000.0
0.00.064.990 I print_info: freq_scale_train = 1
0.00.064.990 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.991 I print_info: rope_finetuned   = unknown
0.00.064.991 I print_info: ssm_d_conv       = 0
0.00.064.991 I print_info: ssm_d_inner      = 0
0.00.064.991 I print_info: ssm_d_state      = 0
0.00.064.991 I print_info: ssm_dt_rank      = 0
0.00.064.991 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.992 I print_info: model type       = 1.4B
0.00.064.996 I print_info: model params     = 1.41 B
0.00.064.996 I print_info: general.name     = 1.4B
0.00.064.997 I print_info: vocab type       = BPE
0.00.064.997 I print_info: n_vocab          = 50304
0.00.064.998 I print_info: n_merges         = 50009
0.00.064.998 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.998 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.998 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.999 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.999 I print_info: LF token         = 187 'Ċ'
0.00.064.999 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.001 I print_info: max token length = 1024
0.00.065.001 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.683.250 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.268 I load_tensors: offloading output layer to GPU
0.00.683.269 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.302 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.683.303 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.684.888 I llama_context: constructing llama_context
0.00.684.891 I llama_context: n_seq_max     = 1
0.00.684.892 I llama_context: n_ctx         = 2048
0.00.684.892 I llama_context: n_ctx_per_seq = 2048
0.00.684.893 I llama_context: n_batch       = 2048
0.00.684.893 I llama_context: n_ubatch      = 512
0.00.684.893 I llama_context: flash_attn    = 0
0.00.684.896 I llama_context: freq_base     = 10000.0
0.00.684.903 I llama_context: freq_scale    = 1
0.00.684.905 I ggml_metal_init: allocating
0.00.684.979 I ggml_metal_init: found device: Apple M4
0.00.685.002 I ggml_metal_init: picking default device: Apple M4
0.00.686.932 I ggml_metal_init: using embedded metal library
0.00.692.393 I ggml_metal_init: GPU name:   Apple M4
0.00.692.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.405 I ggml_metal_init: simdgroup reduction   = true
0.00.692.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.405 I ggml_metal_init: has residency sets    = true
0.00.692.405 I ggml_metal_init: has bfloat            = true
0.00.692.406 I ggml_metal_init: use bfloat            = true
0.00.692.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.433 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.724.588 I init:      Metal compute buffer size =   102.25 MiB
0.00.724.590 I init:        CPU compute buffer size =     5.01 MiB
0.00.724.591 I init: graph nodes  = 943
0.00.724.591 I init: graph splits = 2
0.00.724.594 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.724.603 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.811 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.063 I main: llama threadpool init, n_threads = 4
0.00.752.101 I 
0.00.752.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.117 I 
0.00.752.307 I sampler seed: 1234
0.00.752.313 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.323 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.323 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.323 I 
I believe the meaning of life is that.



















"







"







" class='stylesheet.




















0.01.465.846 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.01.465.847 I llama_perf_context_print:        load time =     734.07 ms
0.01.465.848 I llama_perf_context_print: prompt eval time =      49.14 ms /     7 tokens (    7.02 ms per token,   142.46 tokens per second)
0.01.465.851 I llama_perf_context_print:        eval time =     661.94 ms /    63 runs   (   10.51 ms per token,    95.17 tokens per second)
0.01.465.852 I llama_perf_context_print:       total time =     714.76 ms /    70 tokens
0.01.466.056 I ggml_metal_free: deallocating

real	0m1.482s
user	0m0.107s
sys	0m0.136s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.023 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.723 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.724 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.726 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.720 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.516 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.518 I llama_model_loader: - type  f32:  194 tensors
0.00.025.519 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.519 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.519 I print_info: file format = GGUF V3 (latest)
0.00.025.520 I print_info: file type   = Q5_0
0.00.025.524 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.547 I load: special tokens cache size = 25
0.00.039.580 I load: token to piece cache size = 0.2984 MB
0.00.039.595 I print_info: arch             = gptneox
0.00.039.596 I print_info: vocab_only       = 0
0.00.039.596 I print_info: n_ctx_train      = 2048
0.00.039.596 I print_info: n_embd           = 2048
0.00.039.596 I print_info: n_layer          = 24
0.00.039.599 I print_info: n_head           = 16
0.00.039.600 I print_info: n_head_kv        = 16
0.00.039.600 I print_info: n_rot            = 32
0.00.039.600 I print_info: n_swa            = 0
0.00.039.601 I print_info: n_embd_head_k    = 128
0.00.039.601 I print_info: n_embd_head_v    = 128
0.00.039.602 I print_info: n_gqa            = 1
0.00.039.602 I print_info: n_embd_k_gqa     = 2048
0.00.039.603 I print_info: n_embd_v_gqa     = 2048
0.00.039.604 I print_info: f_norm_eps       = 1.0e-05
0.00.039.604 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.606 I print_info: f_logit_scale    = 0.0e+00
0.00.039.607 I print_info: n_ff             = 8192
0.00.039.607 I print_info: n_expert         = 0
0.00.039.607 I print_info: n_expert_used    = 0
0.00.039.607 I print_info: causal attn      = 1
0.00.039.607 I print_info: pooling type     = 0
0.00.039.609 I print_info: rope type        = 2
0.00.039.609 I print_info: rope scaling     = linear
0.00.039.609 I print_info: freq_base_train  = 10000.0
0.00.039.612 I print_info: freq_scale_train = 1
0.00.039.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.613 I print_info: rope_finetuned   = unknown
0.00.039.613 I print_info: ssm_d_conv       = 0
0.00.039.613 I print_info: ssm_d_inner      = 0
0.00.039.615 I print_info: ssm_d_state      = 0
0.00.039.615 I print_info: ssm_dt_rank      = 0
0.00.039.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.615 I print_info: model type       = 1.4B
0.00.039.616 I print_info: model params     = 1.41 B
0.00.039.616 I print_info: general.name     = 1.4B
0.00.039.616 I print_info: vocab type       = BPE
0.00.039.616 I print_info: n_vocab          = 50304
0.00.039.616 I print_info: n_merges         = 50009
0.00.039.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.617 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: LF token         = 187 'Ċ'
0.00.039.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: max token length = 1024
0.00.039.623 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.711.096 I load_tensors: offloading 24 repeating layers to GPU
0.00.711.112 I load_tensors: offloading output layer to GPU
0.00.711.113 I load_tensors: offloaded 25/25 layers to GPU
0.00.711.146 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.711.147 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.712.600 I llama_context: constructing llama_context
0.00.712.604 I llama_context: n_seq_max     = 1
0.00.712.605 I llama_context: n_ctx         = 2048
0.00.712.606 I llama_context: n_ctx_per_seq = 2048
0.00.712.606 I llama_context: n_batch       = 2048
0.00.712.606 I llama_context: n_ubatch      = 512
0.00.712.607 I llama_context: flash_attn    = 0
0.00.712.609 I llama_context: freq_base     = 10000.0
0.00.712.610 I llama_context: freq_scale    = 1
0.00.712.612 I ggml_metal_init: allocating
0.00.712.692 I ggml_metal_init: found device: Apple M4
0.00.712.716 I ggml_metal_init: picking default device: Apple M4
0.00.714.286 I ggml_metal_init: using embedded metal library
0.00.720.625 I ggml_metal_init: GPU name:   Apple M4
0.00.720.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.720.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.720.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.720.631 I ggml_metal_init: simdgroup reduction   = true
0.00.720.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.720.632 I ggml_metal_init: has residency sets    = true
0.00.720.632 I ggml_metal_init: has bfloat            = true
0.00.720.632 I ggml_metal_init: use bfloat            = true
0.00.720.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.720.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.737.994 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.750.564 I init:      Metal compute buffer size =   102.25 MiB
0.00.750.566 I init:        CPU compute buffer size =     5.01 MiB
0.00.750.567 I init: graph nodes  = 943
0.00.750.567 I init: graph splits = 2
0.00.750.570 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.750.581 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.799 I main: llama threadpool init, n_threads = 4
0.00.777.844 I 
0.00.777.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.858 I 
0.00.778.015 I sampler seed: 1234
0.00.778.021 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.031 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.031 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.032 I 
I believe the meaning of life is to have a result is a little on the



<div>



<c^2d-on and a new (E_2


<tr> <http://www.


<T_t_i= = 0. He also has been introduced

0.01.541.521 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.541.522 I llama_perf_context_print:        load time =     767.86 ms
0.01.541.524 I llama_perf_context_print: prompt eval time =      42.92 ms /     7 tokens (    6.13 ms per token,   163.11 tokens per second)
0.01.541.524 I llama_perf_context_print:        eval time =     718.03 ms /    63 runs   (   11.40 ms per token,    87.74 tokens per second)
0.01.541.525 I llama_perf_context_print:       total time =     764.64 ms /    70 tokens
0.01.541.798 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.100s
sys	0m0.159s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.637 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.334 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.335 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.335 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.337 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.337 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.338 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.338 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.339 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.392 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.198 I llama_model_loader: - type  f32:  194 tensors
0.00.025.198 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.199 I print_info: file format = GGUF V3 (latest)
0.00.025.199 I print_info: file type   = Q5_1
0.00.025.200 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.309 I load: special tokens cache size = 25
0.00.039.458 I load: token to piece cache size = 0.2984 MB
0.00.039.472 I print_info: arch             = gptneox
0.00.039.473 I print_info: vocab_only       = 0
0.00.039.474 I print_info: n_ctx_train      = 2048
0.00.039.474 I print_info: n_embd           = 2048
0.00.039.474 I print_info: n_layer          = 24
0.00.039.477 I print_info: n_head           = 16
0.00.039.478 I print_info: n_head_kv        = 16
0.00.039.478 I print_info: n_rot            = 32
0.00.039.480 I print_info: n_swa            = 0
0.00.039.481 I print_info: n_embd_head_k    = 128
0.00.039.481 I print_info: n_embd_head_v    = 128
0.00.039.482 I print_info: n_gqa            = 1
0.00.039.483 I print_info: n_embd_k_gqa     = 2048
0.00.039.483 I print_info: n_embd_v_gqa     = 2048
0.00.039.484 I print_info: f_norm_eps       = 1.0e-05
0.00.039.484 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.484 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.484 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.484 I print_info: f_logit_scale    = 0.0e+00
0.00.039.485 I print_info: n_ff             = 8192
0.00.039.485 I print_info: n_expert         = 0
0.00.039.485 I print_info: n_expert_used    = 0
0.00.039.486 I print_info: causal attn      = 1
0.00.039.486 I print_info: pooling type     = 0
0.00.039.486 I print_info: rope type        = 2
0.00.039.486 I print_info: rope scaling     = linear
0.00.039.487 I print_info: freq_base_train  = 10000.0
0.00.039.487 I print_info: freq_scale_train = 1
0.00.039.487 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.487 I print_info: rope_finetuned   = unknown
0.00.039.487 I print_info: ssm_d_conv       = 0
0.00.039.487 I print_info: ssm_d_inner      = 0
0.00.039.488 I print_info: ssm_d_state      = 0
0.00.039.488 I print_info: ssm_dt_rank      = 0
0.00.039.488 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.488 I print_info: model type       = 1.4B
0.00.039.489 I print_info: model params     = 1.41 B
0.00.039.489 I print_info: general.name     = 1.4B
0.00.039.489 I print_info: vocab type       = BPE
0.00.039.489 I print_info: n_vocab          = 50304
0.00.039.490 I print_info: n_merges         = 50009
0.00.039.490 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.491 I print_info: LF token         = 187 'Ċ'
0.00.039.491 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.491 I print_info: max token length = 1024
0.00.039.491 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.145 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.158 I load_tensors: offloading output layer to GPU
0.00.620.159 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.187 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.620.188 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.621.689 I llama_context: constructing llama_context
0.00.621.696 I llama_context: n_seq_max     = 1
0.00.621.697 I llama_context: n_ctx         = 2048
0.00.621.697 I llama_context: n_ctx_per_seq = 2048
0.00.621.698 I llama_context: n_batch       = 2048
0.00.621.698 I llama_context: n_ubatch      = 512
0.00.621.699 I llama_context: flash_attn    = 0
0.00.621.700 I llama_context: freq_base     = 10000.0
0.00.621.700 I llama_context: freq_scale    = 1
0.00.621.703 I ggml_metal_init: allocating
0.00.621.752 I ggml_metal_init: found device: Apple M4
0.00.621.774 I ggml_metal_init: picking default device: Apple M4
0.00.623.551 I ggml_metal_init: using embedded metal library
0.00.630.185 I ggml_metal_init: GPU name:   Apple M4
0.00.630.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.192 I ggml_metal_init: simdgroup reduction   = true
0.00.630.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.193 I ggml_metal_init: has residency sets    = true
0.00.630.193 I ggml_metal_init: has bfloat            = true
0.00.630.193 I ggml_metal_init: use bfloat            = true
0.00.630.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.792 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.659.655 I init:      Metal compute buffer size =   102.25 MiB
0.00.659.657 I init:        CPU compute buffer size =     5.01 MiB
0.00.659.657 I init: graph nodes  = 943
0.00.659.657 I init: graph splits = 2
0.00.659.660 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.659.672 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.659.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.659.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.532 I main: llama threadpool init, n_threads = 4
0.00.686.574 I 
0.00.686.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.592 I 
0.00.686.798 I sampler seed: 1234
0.00.686.805 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.815 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.815 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.816 I 
I believe the meaning of life is to do is an end of the next day, and then the only a lot of the following the point for it is the first.



"







A) in a result in the first in the most likely to the following a lot of the case is the most recently

0.01.502.563 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.502.564 I llama_perf_context_print:        load time =     676.98 ms
0.01.502.565 I llama_perf_context_print: prompt eval time =      42.08 ms /     7 tokens (    6.01 ms per token,   166.37 tokens per second)
0.01.502.565 I llama_perf_context_print:        eval time =     771.03 ms /    63 runs   (   12.24 ms per token,    81.71 tokens per second)
0.01.502.566 I llama_perf_context_print:       total time =     816.95 ms /    70 tokens
0.01.502.785 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.102s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.521 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.074 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.081 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.085 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.086 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.087 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.089 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.026 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.007 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.007 I llama_model_loader: - type  f32:  194 tensors
0.00.025.008 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.008 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.008 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.009 I print_info: file format = GGUF V3 (latest)
0.00.025.009 I print_info: file type   = Q2_K - Medium
0.00.025.010 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.856 I load: special tokens cache size = 25
0.00.038.901 I load: token to piece cache size = 0.2984 MB
0.00.038.915 I print_info: arch             = gptneox
0.00.038.916 I print_info: vocab_only       = 0
0.00.038.916 I print_info: n_ctx_train      = 2048
0.00.038.917 I print_info: n_embd           = 2048
0.00.038.917 I print_info: n_layer          = 24
0.00.038.924 I print_info: n_head           = 16
0.00.038.925 I print_info: n_head_kv        = 16
0.00.038.925 I print_info: n_rot            = 32
0.00.038.925 I print_info: n_swa            = 0
0.00.038.927 I print_info: n_embd_head_k    = 128
0.00.038.927 I print_info: n_embd_head_v    = 128
0.00.038.927 I print_info: n_gqa            = 1
0.00.038.928 I print_info: n_embd_k_gqa     = 2048
0.00.038.929 I print_info: n_embd_v_gqa     = 2048
0.00.038.929 I print_info: f_norm_eps       = 1.0e-05
0.00.038.929 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.930 I print_info: f_logit_scale    = 0.0e+00
0.00.038.930 I print_info: n_ff             = 8192
0.00.038.931 I print_info: n_expert         = 0
0.00.038.931 I print_info: n_expert_used    = 0
0.00.038.931 I print_info: causal attn      = 1
0.00.038.931 I print_info: pooling type     = 0
0.00.038.931 I print_info: rope type        = 2
0.00.038.933 I print_info: rope scaling     = linear
0.00.038.933 I print_info: freq_base_train  = 10000.0
0.00.038.933 I print_info: freq_scale_train = 1
0.00.038.933 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.933 I print_info: rope_finetuned   = unknown
0.00.038.933 I print_info: ssm_d_conv       = 0
0.00.038.934 I print_info: ssm_d_inner      = 0
0.00.038.935 I print_info: ssm_d_state      = 0
0.00.038.935 I print_info: ssm_dt_rank      = 0
0.00.038.935 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.935 I print_info: model type       = 1.4B
0.00.038.935 I print_info: model params     = 1.41 B
0.00.038.936 I print_info: general.name     = 1.4B
0.00.038.936 I print_info: vocab type       = BPE
0.00.038.936 I print_info: n_vocab          = 50304
0.00.038.937 I print_info: n_merges         = 50009
0.00.038.937 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: LF token         = 187 'Ċ'
0.00.038.938 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.939 I print_info: max token length = 1024
0.00.038.939 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.349.217 I load_tensors: offloading 24 repeating layers to GPU
0.00.349.235 I load_tensors: offloading output layer to GPU
0.00.349.236 I load_tensors: offloaded 25/25 layers to GPU
0.00.349.270 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.349.271 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.350.991 I llama_context: constructing llama_context
0.00.350.997 I llama_context: n_seq_max     = 1
0.00.350.998 I llama_context: n_ctx         = 2048
0.00.350.999 I llama_context: n_ctx_per_seq = 2048
0.00.350.999 I llama_context: n_batch       = 2048
0.00.350.999 I llama_context: n_ubatch      = 512
0.00.350.999 I llama_context: flash_attn    = 0
0.00.351.001 I llama_context: freq_base     = 10000.0
0.00.351.002 I llama_context: freq_scale    = 1
0.00.351.004 I ggml_metal_init: allocating
0.00.351.099 I ggml_metal_init: found device: Apple M4
0.00.351.128 I ggml_metal_init: picking default device: Apple M4
0.00.353.064 I ggml_metal_init: using embedded metal library
0.00.358.655 I ggml_metal_init: GPU name:   Apple M4
0.00.358.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.358.677 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.358.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.358.679 I ggml_metal_init: simdgroup reduction   = true
0.00.358.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.358.680 I ggml_metal_init: has residency sets    = true
0.00.358.680 I ggml_metal_init: has bfloat            = true
0.00.358.680 I ggml_metal_init: use bfloat            = true
0.00.358.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.358.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.379.592 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.390.472 I init:      Metal compute buffer size =   102.25 MiB
0.00.390.474 I init:        CPU compute buffer size =     5.01 MiB
0.00.390.475 I init: graph nodes  = 943
0.00.390.476 I init: graph splits = 2
0.00.390.478 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.390.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.390.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.390.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.569 I main: llama threadpool init, n_threads = 4
0.00.416.610 I 
0.00.416.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.627 I 
0.00.416.815 I sampler seed: 1234
0.00.416.820 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.416.830 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.416.831 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.416.831 I 
I believe the meaning of life is the stal?!</__?                                                                                                                                                        }
1}

?</}\|\|\?
-??
_1????                                } \__yy|2???!
                                }
\:???</:1}.

0.01.079.774 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.079.775 I llama_perf_context_print:        load time =     406.14 ms
0.01.079.776 I llama_perf_context_print: prompt eval time =      35.61 ms /     7 tokens (    5.09 ms per token,   196.58 tokens per second)
0.01.079.776 I llama_perf_context_print:        eval time =     624.74 ms /    63 runs   (    9.92 ms per token,   100.84 tokens per second)
0.01.079.777 I llama_perf_context_print:       total time =     664.12 ms /    70 tokens
0.01.079.967 I ggml_metal_free: deallocating

real	0m1.096s
user	0m0.103s
sys	0m0.101s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.162 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.166 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.173 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.173 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.174 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.119 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.119 I llama_model_loader: - type  f32:  194 tensors
0.00.025.119 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.120 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.120 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.120 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.121 I print_info: file format = GGUF V3 (latest)
0.00.025.121 I print_info: file type   = Q3_K - Medium
0.00.025.122 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.999 I load: special tokens cache size = 25
0.00.038.795 I load: token to piece cache size = 0.2984 MB
0.00.038.809 I print_info: arch             = gptneox
0.00.038.810 I print_info: vocab_only       = 0
0.00.038.810 I print_info: n_ctx_train      = 2048
0.00.038.810 I print_info: n_embd           = 2048
0.00.038.811 I print_info: n_layer          = 24
0.00.038.813 I print_info: n_head           = 16
0.00.038.814 I print_info: n_head_kv        = 16
0.00.038.814 I print_info: n_rot            = 32
0.00.038.814 I print_info: n_swa            = 0
0.00.038.815 I print_info: n_embd_head_k    = 128
0.00.038.815 I print_info: n_embd_head_v    = 128
0.00.038.816 I print_info: n_gqa            = 1
0.00.038.816 I print_info: n_embd_k_gqa     = 2048
0.00.038.817 I print_info: n_embd_v_gqa     = 2048
0.00.038.818 I print_info: f_norm_eps       = 1.0e-05
0.00.038.818 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.818 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.820 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.820 I print_info: f_logit_scale    = 0.0e+00
0.00.038.821 I print_info: n_ff             = 8192
0.00.038.821 I print_info: n_expert         = 0
0.00.038.821 I print_info: n_expert_used    = 0
0.00.038.823 I print_info: causal attn      = 1
0.00.038.824 I print_info: pooling type     = 0
0.00.038.824 I print_info: rope type        = 2
0.00.038.824 I print_info: rope scaling     = linear
0.00.038.824 I print_info: freq_base_train  = 10000.0
0.00.038.828 I print_info: freq_scale_train = 1
0.00.038.828 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.828 I print_info: rope_finetuned   = unknown
0.00.038.828 I print_info: ssm_d_conv       = 0
0.00.038.829 I print_info: ssm_d_inner      = 0
0.00.038.829 I print_info: ssm_d_state      = 0
0.00.038.829 I print_info: ssm_dt_rank      = 0
0.00.038.829 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.829 I print_info: model type       = 1.4B
0.00.038.830 I print_info: model params     = 1.41 B
0.00.038.830 I print_info: general.name     = 1.4B
0.00.038.831 I print_info: vocab type       = BPE
0.00.038.831 I print_info: n_vocab          = 50304
0.00.038.831 I print_info: n_merges         = 50009
0.00.038.832 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.832 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.832 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.832 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.833 I print_info: LF token         = 187 'Ċ'
0.00.038.834 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.834 I print_info: max token length = 1024
0.00.038.834 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.434.673 I load_tensors: offloading 24 repeating layers to GPU
0.00.434.689 I load_tensors: offloading output layer to GPU
0.00.434.690 I load_tensors: offloaded 25/25 layers to GPU
0.00.434.724 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.434.725 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.436.427 I llama_context: constructing llama_context
0.00.436.430 I llama_context: n_seq_max     = 1
0.00.436.431 I llama_context: n_ctx         = 2048
0.00.436.431 I llama_context: n_ctx_per_seq = 2048
0.00.436.432 I llama_context: n_batch       = 2048
0.00.436.432 I llama_context: n_ubatch      = 512
0.00.436.432 I llama_context: flash_attn    = 0
0.00.436.435 I llama_context: freq_base     = 10000.0
0.00.436.436 I llama_context: freq_scale    = 1
0.00.436.438 I ggml_metal_init: allocating
0.00.436.512 I ggml_metal_init: found device: Apple M4
0.00.436.538 I ggml_metal_init: picking default device: Apple M4
0.00.438.475 I ggml_metal_init: using embedded metal library
0.00.444.952 I ggml_metal_init: GPU name:   Apple M4
0.00.444.957 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.960 I ggml_metal_init: simdgroup reduction   = true
0.00.444.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.960 I ggml_metal_init: has residency sets    = true
0.00.444.961 I ggml_metal_init: has bfloat            = true
0.00.444.961 I ggml_metal_init: use bfloat            = true
0.00.444.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.831 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.474.800 I init:      Metal compute buffer size =   102.25 MiB
0.00.474.802 I init:        CPU compute buffer size =     5.01 MiB
0.00.474.802 I init: graph nodes  = 943
0.00.474.803 I init: graph splits = 2
0.00.474.805 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.474.812 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.474.992 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.474.993 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.087 I main: llama threadpool init, n_threads = 4
0.00.501.132 I 
0.00.501.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.501.152 I 
0.00.501.331 I sampler seed: 1234
0.00.501.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.351 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.351 I 
I believe the meaning of life is that are


















The U.






	if (or




3 -i}\k - - 2x2.







The variable.
The

0.01.232.760 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.232.761 I llama_perf_context_print:        load time =     491.14 ms
0.01.232.762 I llama_perf_context_print: prompt eval time =      49.40 ms /     7 tokens (    7.06 ms per token,   141.69 tokens per second)
0.01.232.764 I llama_perf_context_print:        eval time =     679.51 ms /    63 runs   (   10.79 ms per token,    92.71 tokens per second)
0.01.232.764 I llama_perf_context_print:       total time =     732.75 ms /    70 tokens
0.01.233.031 I ggml_metal_free: deallocating

real	0m1.247s
user	0m0.104s
sys	0m0.108s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.115 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.939 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.940 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.909 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.046 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.905 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.905 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.906 I llama_model_loader: - type  f32:  194 tensors
0.00.025.906 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.906 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.906 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.907 I print_info: file format = GGUF V3 (latest)
0.00.025.907 I print_info: file type   = Q4_K - Medium
0.00.025.908 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.832 I load: special tokens cache size = 25
0.00.039.651 I load: token to piece cache size = 0.2984 MB
0.00.039.665 I print_info: arch             = gptneox
0.00.039.666 I print_info: vocab_only       = 0
0.00.039.666 I print_info: n_ctx_train      = 2048
0.00.039.666 I print_info: n_embd           = 2048
0.00.039.666 I print_info: n_layer          = 24
0.00.039.669 I print_info: n_head           = 16
0.00.039.670 I print_info: n_head_kv        = 16
0.00.039.670 I print_info: n_rot            = 32
0.00.039.670 I print_info: n_swa            = 0
0.00.039.671 I print_info: n_embd_head_k    = 128
0.00.039.671 I print_info: n_embd_head_v    = 128
0.00.039.671 I print_info: n_gqa            = 1
0.00.039.672 I print_info: n_embd_k_gqa     = 2048
0.00.039.674 I print_info: n_embd_v_gqa     = 2048
0.00.039.675 I print_info: f_norm_eps       = 1.0e-05
0.00.039.675 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.675 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.676 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.676 I print_info: f_logit_scale    = 0.0e+00
0.00.039.676 I print_info: n_ff             = 8192
0.00.039.676 I print_info: n_expert         = 0
0.00.039.677 I print_info: n_expert_used    = 0
0.00.039.677 I print_info: causal attn      = 1
0.00.039.678 I print_info: pooling type     = 0
0.00.039.679 I print_info: rope type        = 2
0.00.039.679 I print_info: rope scaling     = linear
0.00.039.679 I print_info: freq_base_train  = 10000.0
0.00.039.680 I print_info: freq_scale_train = 1
0.00.039.680 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.680 I print_info: rope_finetuned   = unknown
0.00.039.680 I print_info: ssm_d_conv       = 0
0.00.039.680 I print_info: ssm_d_inner      = 0
0.00.039.681 I print_info: ssm_d_state      = 0
0.00.039.681 I print_info: ssm_dt_rank      = 0
0.00.039.681 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.681 I print_info: model type       = 1.4B
0.00.039.681 I print_info: model params     = 1.41 B
0.00.039.682 I print_info: general.name     = 1.4B
0.00.039.682 I print_info: vocab type       = BPE
0.00.039.682 I print_info: n_vocab          = 50304
0.00.039.682 I print_info: n_merges         = 50009
0.00.039.682 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: LF token         = 187 'Ċ'
0.00.039.684 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.684 I print_info: max token length = 1024
0.00.039.685 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.548.781 I load_tensors: offloading 24 repeating layers to GPU
0.00.548.798 I load_tensors: offloading output layer to GPU
0.00.548.798 I load_tensors: offloaded 25/25 layers to GPU
0.00.548.830 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.548.831 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.550.165 I llama_context: constructing llama_context
0.00.550.170 I llama_context: n_seq_max     = 1
0.00.550.171 I llama_context: n_ctx         = 2048
0.00.550.171 I llama_context: n_ctx_per_seq = 2048
0.00.550.171 I llama_context: n_batch       = 2048
0.00.550.172 I llama_context: n_ubatch      = 512
0.00.550.172 I llama_context: flash_attn    = 0
0.00.550.175 I llama_context: freq_base     = 10000.0
0.00.550.175 I llama_context: freq_scale    = 1
0.00.550.177 I ggml_metal_init: allocating
0.00.550.246 I ggml_metal_init: found device: Apple M4
0.00.550.292 I ggml_metal_init: picking default device: Apple M4
0.00.552.138 I ggml_metal_init: using embedded metal library
0.00.558.011 I ggml_metal_init: GPU name:   Apple M4
0.00.558.017 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.019 I ggml_metal_init: simdgroup reduction   = true
0.00.558.019 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.019 I ggml_metal_init: has residency sets    = true
0.00.558.019 I ggml_metal_init: has bfloat            = true
0.00.558.020 I ggml_metal_init: use bfloat            = true
0.00.558.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.025 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.573.864 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.584.453 I init:      Metal compute buffer size =   102.25 MiB
0.00.584.456 I init:        CPU compute buffer size =     5.01 MiB
0.00.584.456 I init: graph nodes  = 943
0.00.584.456 I init: graph splits = 2
0.00.584.459 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.584.468 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.584.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.584.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.102 I main: llama threadpool init, n_threads = 4
0.00.614.139 I 
0.00.614.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.152 I 
0.00.614.287 I sampler seed: 1234
0.00.614.292 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.333 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.336 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.336 I 
I believe the meaning of life is that I am currently generated by the model of the










The input to the last week;


    if (!recipe, if $x03. The court held a large numbers were also a local law firmness and the present invention describes a few years in

0.01.363.779 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49134.95 tokens per second)
0.01.363.779 I llama_perf_context_print:        load time =     604.30 ms
0.01.363.781 I llama_perf_context_print: prompt eval time =      57.80 ms /     7 tokens (    8.26 ms per token,   121.11 tokens per second)
0.01.363.782 I llama_perf_context_print:        eval time =     689.26 ms /    63 runs   (   10.94 ms per token,    91.40 tokens per second)
0.01.363.782 I llama_perf_context_print:       total time =     750.36 ms /    70 tokens
0.01.364.053 I ggml_metal_free: deallocating

real	0m1.377s
user	0m0.093s
sys	0m0.144s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.582 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.876 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.888 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.892 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.996 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.029.999 I llama_model_loader: - type  f32:  194 tensors
0.00.029.999 I llama_model_loader: - type q5_K:   61 tensors
0.00.029.999 I llama_model_loader: - type q6_K:   37 tensors
0.00.030.000 I print_info: file format = GGUF V3 (latest)
0.00.030.000 I print_info: file type   = Q5_K - Medium
0.00.030.001 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.037.898 I load: special tokens cache size = 25
0.00.044.119 I load: token to piece cache size = 0.2984 MB
0.00.044.133 I print_info: arch             = gptneox
0.00.044.134 I print_info: vocab_only       = 0
0.00.044.134 I print_info: n_ctx_train      = 2048
0.00.044.135 I print_info: n_embd           = 2048
0.00.044.135 I print_info: n_layer          = 24
0.00.044.137 I print_info: n_head           = 16
0.00.044.138 I print_info: n_head_kv        = 16
0.00.044.138 I print_info: n_rot            = 32
0.00.044.138 I print_info: n_swa            = 0
0.00.044.142 I print_info: n_embd_head_k    = 128
0.00.044.142 I print_info: n_embd_head_v    = 128
0.00.044.142 I print_info: n_gqa            = 1
0.00.044.143 I print_info: n_embd_k_gqa     = 2048
0.00.044.144 I print_info: n_embd_v_gqa     = 2048
0.00.044.144 I print_info: f_norm_eps       = 1.0e-05
0.00.044.145 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.145 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.147 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.147 I print_info: f_logit_scale    = 0.0e+00
0.00.044.148 I print_info: n_ff             = 8192
0.00.044.148 I print_info: n_expert         = 0
0.00.044.148 I print_info: n_expert_used    = 0
0.00.044.148 I print_info: causal attn      = 1
0.00.044.149 I print_info: pooling type     = 0
0.00.044.149 I print_info: rope type        = 2
0.00.044.149 I print_info: rope scaling     = linear
0.00.044.149 I print_info: freq_base_train  = 10000.0
0.00.044.150 I print_info: freq_scale_train = 1
0.00.044.150 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.150 I print_info: rope_finetuned   = unknown
0.00.044.150 I print_info: ssm_d_conv       = 0
0.00.044.150 I print_info: ssm_d_inner      = 0
0.00.044.151 I print_info: ssm_d_state      = 0
0.00.044.151 I print_info: ssm_dt_rank      = 0
0.00.044.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.153 I print_info: model type       = 1.4B
0.00.044.153 I print_info: model params     = 1.41 B
0.00.044.153 I print_info: general.name     = 1.4B
0.00.044.154 I print_info: vocab type       = BPE
0.00.044.154 I print_info: n_vocab          = 50304
0.00.044.154 I print_info: n_merges         = 50009
0.00.044.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.155 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.155 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.155 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.155 I print_info: LF token         = 187 'Ċ'
0.00.044.155 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.156 I print_info: max token length = 1024
0.00.044.156 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.348 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.353 I load_tensors: offloading output layer to GPU
0.00.598.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.370 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.598.372 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.313 I llama_context: constructing llama_context
0.00.599.320 I llama_context: n_seq_max     = 1
0.00.599.320 I llama_context: n_ctx         = 2048
0.00.599.320 I llama_context: n_ctx_per_seq = 2048
0.00.599.321 I llama_context: n_batch       = 2048
0.00.599.321 I llama_context: n_ubatch      = 512
0.00.599.321 I llama_context: flash_attn    = 0
0.00.599.322 I llama_context: freq_base     = 10000.0
0.00.599.323 I llama_context: freq_scale    = 1
0.00.599.324 I ggml_metal_init: allocating
0.00.599.351 I ggml_metal_init: found device: Apple M4
0.00.599.367 I ggml_metal_init: picking default device: Apple M4
0.00.600.462 I ggml_metal_init: using embedded metal library
0.00.608.002 I ggml_metal_init: GPU name:   Apple M4
0.00.608.009 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.009 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.010 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.011 I ggml_metal_init: simdgroup reduction   = true
0.00.608.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.011 I ggml_metal_init: has residency sets    = true
0.00.608.011 I ggml_metal_init: has bfloat            = true
0.00.608.012 I ggml_metal_init: use bfloat            = true
0.00.608.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.936 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.624.477 I init:      Metal compute buffer size =   102.25 MiB
0.00.624.479 I init:        CPU compute buffer size =     5.01 MiB
0.00.624.479 I init: graph nodes  = 943
0.00.624.479 I init: graph splits = 2
0.00.624.481 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.624.485 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.624.598 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.624.599 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.978 I main: llama threadpool init, n_threads = 4
0.00.658.018 I 
0.00.658.032 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.033 I 
0.00.658.207 I sampler seed: 1234
0.00.658.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.658.230 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.658.230 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.658.230 I 
I believe the meaning of life is to a way of the same.













The case, and a long, the most likely be a few months, and is not be at the presence, so that the most popular in the point, and the case, and the old and the
The

0.01.486.522 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.486.522 I llama_perf_context_print:        load time =     647.67 ms
0.01.486.523 I llama_perf_context_print: prompt eval time =      52.56 ms /     7 tokens (    7.51 ms per token,   133.18 tokens per second)
0.01.486.524 I llama_perf_context_print:        eval time =     773.19 ms /    63 runs   (   12.27 ms per token,    81.48 tokens per second)
0.01.486.524 I llama_perf_context_print:       total time =     829.27 ms /    70 tokens
0.01.486.747 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.088s
sys	0m0.113s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.499 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.505 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.506 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.491 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.492 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.493 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.493 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.493 I llama_model_loader: - type  f32:  194 tensors
0.00.024.494 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.494 I print_info: file format = GGUF V3 (latest)
0.00.024.495 I print_info: file type   = Q6_K
0.00.024.497 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.637 I load: special tokens cache size = 25
0.00.038.844 I load: token to piece cache size = 0.2984 MB
0.00.038.858 I print_info: arch             = gptneox
0.00.038.859 I print_info: vocab_only       = 0
0.00.038.859 I print_info: n_ctx_train      = 2048
0.00.038.860 I print_info: n_embd           = 2048
0.00.038.860 I print_info: n_layer          = 24
0.00.038.863 I print_info: n_head           = 16
0.00.038.863 I print_info: n_head_kv        = 16
0.00.038.863 I print_info: n_rot            = 32
0.00.038.864 I print_info: n_swa            = 0
0.00.038.864 I print_info: n_embd_head_k    = 128
0.00.038.864 I print_info: n_embd_head_v    = 128
0.00.038.865 I print_info: n_gqa            = 1
0.00.038.865 I print_info: n_embd_k_gqa     = 2048
0.00.038.868 I print_info: n_embd_v_gqa     = 2048
0.00.038.869 I print_info: f_norm_eps       = 1.0e-05
0.00.038.869 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.869 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.869 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.870 I print_info: f_logit_scale    = 0.0e+00
0.00.038.870 I print_info: n_ff             = 8192
0.00.038.870 I print_info: n_expert         = 0
0.00.038.871 I print_info: n_expert_used    = 0
0.00.038.871 I print_info: causal attn      = 1
0.00.038.871 I print_info: pooling type     = 0
0.00.038.871 I print_info: rope type        = 2
0.00.038.871 I print_info: rope scaling     = linear
0.00.038.871 I print_info: freq_base_train  = 10000.0
0.00.038.872 I print_info: freq_scale_train = 1
0.00.038.872 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.873 I print_info: rope_finetuned   = unknown
0.00.038.873 I print_info: ssm_d_conv       = 0
0.00.038.873 I print_info: ssm_d_inner      = 0
0.00.038.873 I print_info: ssm_d_state      = 0
0.00.038.873 I print_info: ssm_dt_rank      = 0
0.00.038.873 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.874 I print_info: model type       = 1.4B
0.00.038.874 I print_info: model params     = 1.41 B
0.00.038.874 I print_info: general.name     = 1.4B
0.00.038.875 I print_info: vocab type       = BPE
0.00.038.875 I print_info: n_vocab          = 50304
0.00.038.875 I print_info: n_merges         = 50009
0.00.038.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: LF token         = 187 'Ċ'
0.00.038.876 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: max token length = 1024
0.00.038.877 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.679.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.164 I load_tensors: offloading output layer to GPU
0.00.679.165 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.188 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.679.190 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.680.705 I llama_context: constructing llama_context
0.00.680.708 I llama_context: n_seq_max     = 1
0.00.680.708 I llama_context: n_ctx         = 2048
0.00.680.709 I llama_context: n_ctx_per_seq = 2048
0.00.680.709 I llama_context: n_batch       = 2048
0.00.680.710 I llama_context: n_ubatch      = 512
0.00.680.710 I llama_context: flash_attn    = 0
0.00.680.711 I llama_context: freq_base     = 10000.0
0.00.680.712 I llama_context: freq_scale    = 1
0.00.680.713 I ggml_metal_init: allocating
0.00.680.760 I ggml_metal_init: found device: Apple M4
0.00.680.784 I ggml_metal_init: picking default device: Apple M4
0.00.682.345 I ggml_metal_init: using embedded metal library
0.00.691.646 I ggml_metal_init: GPU name:   Apple M4
0.00.691.649 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.650 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.651 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.651 I ggml_metal_init: simdgroup reduction   = true
0.00.691.651 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.652 I ggml_metal_init: has residency sets    = true
0.00.691.652 I ggml_metal_init: has bfloat            = true
0.00.691.652 I ggml_metal_init: use bfloat            = true
0.00.691.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.939 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.719.434 I init:      Metal compute buffer size =   102.25 MiB
0.00.719.437 I init:        CPU compute buffer size =     5.01 MiB
0.00.719.438 I init: graph nodes  = 943
0.00.719.438 I init: graph splits = 2
0.00.719.441 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.719.451 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.579 I main: llama threadpool init, n_threads = 4
0.00.759.618 I 
0.00.759.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.632 I 
0.00.759.813 I sampler seed: 1234
0.00.759.818 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.857 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.858 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.860 I 
I believe the meaning of life is to get it will also a little in the first, the other things.





The two-2)




<div>





  0
" class="fig"}). The fact that are the





" (P. The

0.01.617.894 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.617.895 I llama_perf_context_print:        load time =     749.98 ms
0.01.617.896 I llama_perf_context_print: prompt eval time =      57.80 ms /     7 tokens (    8.26 ms per token,   121.10 tokens per second)
0.01.617.897 I llama_perf_context_print:        eval time =     797.55 ms /    63 runs   (   12.66 ms per token,    78.99 tokens per second)
0.01.617.898 I llama_perf_context_print:       total time =     859.13 ms /    70 tokens
0.01.618.181 I ggml_metal_free: deallocating

real	0m1.632s
user	0m0.098s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.663 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.069 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.175 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.194 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.198 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.203 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.203 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.321 I llama_model_loader: - type  f32:  194 tensors
0.00.057.321 I llama_model_loader: - type  f16:   98 tensors
0.00.057.322 I print_info: file format = GGUF V3 (latest)
0.00.057.323 I print_info: file type   = all F32 (guessed)
0.00.057.324 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.148 I load: special tokens cache size = 25
0.00.078.066 I load: token to piece cache size = 0.2984 MB
0.00.078.080 I print_info: arch             = gptneox
0.00.078.082 I print_info: vocab_only       = 0
0.00.078.082 I print_info: n_ctx_train      = 2048
0.00.078.082 I print_info: n_embd           = 2048
0.00.078.082 I print_info: n_layer          = 24
0.00.078.085 I print_info: n_head           = 16
0.00.078.086 I print_info: n_head_kv        = 16
0.00.078.087 I print_info: n_rot            = 32
0.00.078.087 I print_info: n_swa            = 0
0.00.078.087 I print_info: n_embd_head_k    = 128
0.00.078.087 I print_info: n_embd_head_v    = 128
0.00.078.088 I print_info: n_gqa            = 1
0.00.078.089 I print_info: n_embd_k_gqa     = 2048
0.00.078.090 I print_info: n_embd_v_gqa     = 2048
0.00.078.092 I print_info: f_norm_eps       = 1.0e-05
0.00.078.092 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.092 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.093 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.093 I print_info: f_logit_scale    = 0.0e+00
0.00.078.096 I print_info: n_ff             = 8192
0.00.078.096 I print_info: n_expert         = 0
0.00.078.096 I print_info: n_expert_used    = 0
0.00.078.096 I print_info: causal attn      = 1
0.00.078.096 I print_info: pooling type     = 0
0.00.078.096 I print_info: rope type        = 2
0.00.078.097 I print_info: rope scaling     = linear
0.00.078.097 I print_info: freq_base_train  = 10000.0
0.00.078.097 I print_info: freq_scale_train = 1
0.00.078.098 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.098 I print_info: rope_finetuned   = unknown
0.00.078.098 I print_info: ssm_d_conv       = 0
0.00.078.099 I print_info: ssm_d_inner      = 0
0.00.078.099 I print_info: ssm_d_state      = 0
0.00.078.100 I print_info: ssm_dt_rank      = 0
0.00.078.100 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.100 I print_info: model type       = 1.4B
0.00.078.104 I print_info: model params     = 1.41 B
0.00.078.105 I print_info: general.name     = 1.4B
0.00.078.105 I print_info: vocab type       = BPE
0.00.078.105 I print_info: n_vocab          = 50304
0.00.078.107 I print_info: n_merges         = 50009
0.00.078.107 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.107 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.107 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.108 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.108 I print_info: LF token         = 187 'Ċ'
0.00.078.108 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.109 I print_info: max token length = 1024
0.00.078.109 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.060.080 I load_tensors: offloading 24 repeating layers to GPU
0.01.060.084 I load_tensors: offloading output layer to GPU
0.01.060.084 I load_tensors: offloaded 25/25 layers to GPU
0.01.060.111 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.060.114 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.061.370 I llama_context: constructing llama_context
0.01.061.372 I llama_context: n_seq_max     = 1
0.01.061.372 I llama_context: n_ctx         = 128
0.01.061.372 I llama_context: n_ctx_per_seq = 128
0.01.061.372 I llama_context: n_batch       = 128
0.01.061.373 I llama_context: n_ubatch      = 128
0.01.061.373 I llama_context: flash_attn    = 0
0.01.061.374 I llama_context: freq_base     = 10000.0
0.01.061.374 I llama_context: freq_scale    = 1
0.01.061.374 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.061.375 I ggml_metal_init: allocating
0.01.061.458 I ggml_metal_init: found device: Apple M4
0.01.061.480 I ggml_metal_init: picking default device: Apple M4
0.01.062.690 I ggml_metal_init: using embedded metal library
0.01.066.421 I ggml_metal_init: GPU name:   Apple M4
0.01.066.424 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.066.425 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.066.425 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.066.426 I ggml_metal_init: simdgroup reduction   = true
0.01.066.426 I ggml_metal_init: simdgroup matrix mul. = true
0.01.066.426 I ggml_metal_init: has residency sets    = true
0.01.066.426 I ggml_metal_init: has bfloat            = true
0.01.066.426 I ggml_metal_init: use bfloat            = true
0.01.066.427 I ggml_metal_init: hasUnifiedMemory      = true
0.01.066.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.076.782 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.078.466 I init:      Metal compute buffer size =    25.56 MiB
0.01.078.468 I init:        CPU compute buffer size =     1.06 MiB
0.01.078.468 I init: graph nodes  = 943
0.01.078.468 I init: graph splits = 2
0.01.078.470 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.01.078.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.078.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.111.542 I 
0.01.111.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.111.564 I perplexity: tokenizing the input ..
0.01.116.635 I perplexity: tokenization took 5.069 ms
0.01.116.642 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.236.425 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.239.174 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.239.223 I llama_perf_context_print:        load time =    1087.46 ms
0.01.239.225 I llama_perf_context_print: prompt eval time =     119.77 ms /   128 tokens (    0.94 ms per token,  1068.69 tokens per second)
0.01.239.226 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.239.230 I llama_perf_context_print:       total time =     127.68 ms /   129 tokens
0.01.239.961 I ggml_metal_free: deallocating

real	0m1.439s
user	0m0.105s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.715 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.909 I llama_model_loader: - type  f32:  194 tensors
0.00.025.910 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.910 I print_info: file format = GGUF V3 (latest)
0.00.025.911 I print_info: file type   = Q8_0
0.00.025.912 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.979 I load: special tokens cache size = 25
0.00.040.430 I load: token to piece cache size = 0.2984 MB
0.00.040.447 I print_info: arch             = gptneox
0.00.040.448 I print_info: vocab_only       = 0
0.00.040.448 I print_info: n_ctx_train      = 2048
0.00.040.448 I print_info: n_embd           = 2048
0.00.040.448 I print_info: n_layer          = 24
0.00.040.452 I print_info: n_head           = 16
0.00.040.453 I print_info: n_head_kv        = 16
0.00.040.453 I print_info: n_rot            = 32
0.00.040.453 I print_info: n_swa            = 0
0.00.040.454 I print_info: n_embd_head_k    = 128
0.00.040.454 I print_info: n_embd_head_v    = 128
0.00.040.454 I print_info: n_gqa            = 1
0.00.040.455 I print_info: n_embd_k_gqa     = 2048
0.00.040.456 I print_info: n_embd_v_gqa     = 2048
0.00.040.456 I print_info: f_norm_eps       = 1.0e-05
0.00.040.460 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.460 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.460 I print_info: f_logit_scale    = 0.0e+00
0.00.040.461 I print_info: n_ff             = 8192
0.00.040.461 I print_info: n_expert         = 0
0.00.040.461 I print_info: n_expert_used    = 0
0.00.040.461 I print_info: causal attn      = 1
0.00.040.462 I print_info: pooling type     = 0
0.00.040.462 I print_info: rope type        = 2
0.00.040.462 I print_info: rope scaling     = linear
0.00.040.462 I print_info: freq_base_train  = 10000.0
0.00.040.462 I print_info: freq_scale_train = 1
0.00.040.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.464 I print_info: rope_finetuned   = unknown
0.00.040.464 I print_info: ssm_d_conv       = 0
0.00.040.464 I print_info: ssm_d_inner      = 0
0.00.040.464 I print_info: ssm_d_state      = 0
0.00.040.464 I print_info: ssm_dt_rank      = 0
0.00.040.465 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.465 I print_info: model type       = 1.4B
0.00.040.465 I print_info: model params     = 1.41 B
0.00.040.465 I print_info: general.name     = 1.4B
0.00.040.466 I print_info: vocab type       = BPE
0.00.040.466 I print_info: n_vocab          = 50304
0.00.040.466 I print_info: n_merges         = 50009
0.00.040.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: LF token         = 187 'Ċ'
0.00.040.467 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: max token length = 1024
0.00.040.468 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.896.853 I load_tensors: offloading 24 repeating layers to GPU
0.00.896.860 I load_tensors: offloading output layer to GPU
0.00.896.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.896.892 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.896.895 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.898.307 I llama_context: constructing llama_context
0.00.898.309 I llama_context: n_seq_max     = 1
0.00.898.310 I llama_context: n_ctx         = 128
0.00.898.310 I llama_context: n_ctx_per_seq = 128
0.00.898.311 I llama_context: n_batch       = 128
0.00.898.311 I llama_context: n_ubatch      = 128
0.00.898.311 I llama_context: flash_attn    = 0
0.00.898.312 I llama_context: freq_base     = 10000.0
0.00.898.312 I llama_context: freq_scale    = 1
0.00.898.313 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.898.314 I ggml_metal_init: allocating
0.00.898.389 I ggml_metal_init: found device: Apple M4
0.00.898.399 I ggml_metal_init: picking default device: Apple M4
0.00.899.858 I ggml_metal_init: using embedded metal library
0.00.904.895 I ggml_metal_init: GPU name:   Apple M4
0.00.904.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.904.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.904.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.904.900 I ggml_metal_init: simdgroup reduction   = true
0.00.904.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.904.901 I ggml_metal_init: has residency sets    = true
0.00.904.901 I ggml_metal_init: has bfloat            = true
0.00.904.901 I ggml_metal_init: use bfloat            = true
0.00.904.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.904.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.919.409 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.922.096 I init:      Metal compute buffer size =    25.56 MiB
0.00.922.098 I init:        CPU compute buffer size =     1.06 MiB
0.00.922.099 I init: graph nodes  = 943
0.00.922.099 I init: graph splits = 2
0.00.922.101 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.922.102 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.922.102 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.944.784 I 
0.00.944.805 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.944.808 I perplexity: tokenizing the input ..
0.00.951.015 I perplexity: tokenization took 6.204 ms
0.00.951.026 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.089.824 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.091.159 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.091.184 I llama_perf_context_print:        load time =     935.59 ms
0.01.091.185 I llama_perf_context_print: prompt eval time =     138.79 ms /   128 tokens (    1.08 ms per token,   922.28 tokens per second)
0.01.091.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.091.186 I llama_perf_context_print:       total time =     146.40 ms /   129 tokens
0.01.091.594 I ggml_metal_free: deallocating

real	0m1.105s
user	0m0.073s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.059 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.389 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.397 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.397 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.398 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.398 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.398 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.399 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.400 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.400 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.405 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.407 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.407 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.725 I llama_model_loader: - type  f32:  194 tensors
0.00.026.726 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.726 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.727 I print_info: file format = GGUF V3 (latest)
0.00.026.727 I print_info: file type   = Q4_0
0.00.026.728 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.367 I load: special tokens cache size = 25
0.00.041.474 I load: token to piece cache size = 0.2984 MB
0.00.041.491 I print_info: arch             = gptneox
0.00.041.492 I print_info: vocab_only       = 0
0.00.041.492 I print_info: n_ctx_train      = 2048
0.00.041.492 I print_info: n_embd           = 2048
0.00.041.493 I print_info: n_layer          = 24
0.00.041.497 I print_info: n_head           = 16
0.00.041.500 I print_info: n_head_kv        = 16
0.00.041.500 I print_info: n_rot            = 32
0.00.041.500 I print_info: n_swa            = 0
0.00.041.500 I print_info: n_embd_head_k    = 128
0.00.041.500 I print_info: n_embd_head_v    = 128
0.00.041.501 I print_info: n_gqa            = 1
0.00.041.502 I print_info: n_embd_k_gqa     = 2048
0.00.041.502 I print_info: n_embd_v_gqa     = 2048
0.00.041.503 I print_info: f_norm_eps       = 1.0e-05
0.00.041.503 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.503 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.504 I print_info: f_logit_scale    = 0.0e+00
0.00.041.504 I print_info: n_ff             = 8192
0.00.041.504 I print_info: n_expert         = 0
0.00.041.504 I print_info: n_expert_used    = 0
0.00.041.506 I print_info: causal attn      = 1
0.00.041.506 I print_info: pooling type     = 0
0.00.041.506 I print_info: rope type        = 2
0.00.041.506 I print_info: rope scaling     = linear
0.00.041.506 I print_info: freq_base_train  = 10000.0
0.00.041.507 I print_info: freq_scale_train = 1
0.00.041.507 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.507 I print_info: rope_finetuned   = unknown
0.00.041.507 I print_info: ssm_d_conv       = 0
0.00.041.507 I print_info: ssm_d_inner      = 0
0.00.041.507 I print_info: ssm_d_state      = 0
0.00.041.508 I print_info: ssm_dt_rank      = 0
0.00.041.508 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.508 I print_info: model type       = 1.4B
0.00.041.508 I print_info: model params     = 1.41 B
0.00.041.508 I print_info: general.name     = 1.4B
0.00.041.509 I print_info: vocab type       = BPE
0.00.041.509 I print_info: n_vocab          = 50304
0.00.041.509 I print_info: n_merges         = 50009
0.00.041.509 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.510 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.510 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.510 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.510 I print_info: LF token         = 187 'Ċ'
0.00.041.511 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.511 I print_info: max token length = 1024
0.00.041.511 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.635.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.957 I load_tensors: offloading output layer to GPU
0.00.635.958 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.992 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.635.993 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.637.668 I llama_context: constructing llama_context
0.00.637.675 I llama_context: n_seq_max     = 1
0.00.637.675 I llama_context: n_ctx         = 128
0.00.637.676 I llama_context: n_ctx_per_seq = 128
0.00.637.676 I llama_context: n_batch       = 128
0.00.637.676 I llama_context: n_ubatch      = 128
0.00.637.677 I llama_context: flash_attn    = 0
0.00.637.678 I llama_context: freq_base     = 10000.0
0.00.637.678 I llama_context: freq_scale    = 1
0.00.637.679 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.637.681 I ggml_metal_init: allocating
0.00.637.789 I ggml_metal_init: found device: Apple M4
0.00.637.814 I ggml_metal_init: picking default device: Apple M4
0.00.639.639 I ggml_metal_init: using embedded metal library
0.00.645.273 I ggml_metal_init: GPU name:   Apple M4
0.00.645.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.286 I ggml_metal_init: simdgroup reduction   = true
0.00.645.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.287 I ggml_metal_init: has residency sets    = true
0.00.645.287 I ggml_metal_init: has bfloat            = true
0.00.645.287 I ggml_metal_init: use bfloat            = true
0.00.645.289 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.910 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.668.379 I init:      Metal compute buffer size =    25.56 MiB
0.00.668.381 I init:        CPU compute buffer size =     1.06 MiB
0.00.668.381 I init: graph nodes  = 943
0.00.668.382 I init: graph splits = 2
0.00.668.385 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.668.386 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.525 I 
0.00.691.551 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.555 I perplexity: tokenizing the input ..
0.00.699.437 I perplexity: tokenization took 7.879 ms
0.00.699.444 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.207 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.824.633 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.824.659 I llama_perf_context_print:        load time =     681.46 ms
0.00.824.660 I llama_perf_context_print: prompt eval time =     123.75 ms /   128 tokens (    0.97 ms per token,  1034.32 tokens per second)
0.00.824.661 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.662 I llama_perf_context_print:       total time =     133.13 ms /   129 tokens
0.00.825.039 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.080s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.769 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.989 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.995 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.997 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.998 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.998 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.999 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.000 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.000 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.001 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.001 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.002 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.004 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.004 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.004 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.006 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.131 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.131 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.132 I llama_model_loader: - type  f32:  194 tensors
0.00.025.132 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.132 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.133 I print_info: file format = GGUF V3 (latest)
0.00.025.133 I print_info: file type   = Q4_1
0.00.025.135 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.423 I load: special tokens cache size = 25
0.00.039.448 I load: token to piece cache size = 0.2984 MB
0.00.039.466 I print_info: arch             = gptneox
0.00.039.467 I print_info: vocab_only       = 0
0.00.039.467 I print_info: n_ctx_train      = 2048
0.00.039.467 I print_info: n_embd           = 2048
0.00.039.468 I print_info: n_layer          = 24
0.00.039.471 I print_info: n_head           = 16
0.00.039.475 I print_info: n_head_kv        = 16
0.00.039.475 I print_info: n_rot            = 32
0.00.039.476 I print_info: n_swa            = 0
0.00.039.476 I print_info: n_embd_head_k    = 128
0.00.039.476 I print_info: n_embd_head_v    = 128
0.00.039.476 I print_info: n_gqa            = 1
0.00.039.477 I print_info: n_embd_k_gqa     = 2048
0.00.039.477 I print_info: n_embd_v_gqa     = 2048
0.00.039.478 I print_info: f_norm_eps       = 1.0e-05
0.00.039.478 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.478 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.479 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.479 I print_info: f_logit_scale    = 0.0e+00
0.00.039.479 I print_info: n_ff             = 8192
0.00.039.480 I print_info: n_expert         = 0
0.00.039.480 I print_info: n_expert_used    = 0
0.00.039.480 I print_info: causal attn      = 1
0.00.039.480 I print_info: pooling type     = 0
0.00.039.480 I print_info: rope type        = 2
0.00.039.481 I print_info: rope scaling     = linear
0.00.039.481 I print_info: freq_base_train  = 10000.0
0.00.039.481 I print_info: freq_scale_train = 1
0.00.039.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.482 I print_info: rope_finetuned   = unknown
0.00.039.482 I print_info: ssm_d_conv       = 0
0.00.039.482 I print_info: ssm_d_inner      = 0
0.00.039.482 I print_info: ssm_d_state      = 0
0.00.039.482 I print_info: ssm_dt_rank      = 0
0.00.039.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.483 I print_info: model params     = 1.41 B
0.00.039.485 I print_info: general.name     = 1.4B
0.00.039.486 I print_info: vocab type       = BPE
0.00.039.486 I print_info: n_vocab          = 50304
0.00.039.486 I print_info: n_merges         = 50009
0.00.039.486 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: LF token         = 187 'Ċ'
0.00.039.487 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.487 I print_info: max token length = 1024
0.00.039.488 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.684 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.702 I load_tensors: offloading output layer to GPU
0.00.648.703 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.740 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.648.741 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.650.360 I llama_context: constructing llama_context
0.00.650.363 I llama_context: n_seq_max     = 1
0.00.650.364 I llama_context: n_ctx         = 128
0.00.650.364 I llama_context: n_ctx_per_seq = 128
0.00.650.365 I llama_context: n_batch       = 128
0.00.650.365 I llama_context: n_ubatch      = 128
0.00.650.365 I llama_context: flash_attn    = 0
0.00.650.367 I llama_context: freq_base     = 10000.0
0.00.650.368 I llama_context: freq_scale    = 1
0.00.650.368 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.370 I ggml_metal_init: allocating
0.00.650.453 I ggml_metal_init: found device: Apple M4
0.00.650.469 I ggml_metal_init: picking default device: Apple M4
0.00.652.363 I ggml_metal_init: using embedded metal library
0.00.659.175 I ggml_metal_init: GPU name:   Apple M4
0.00.659.183 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.185 I ggml_metal_init: simdgroup reduction   = true
0.00.659.186 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.186 I ggml_metal_init: has residency sets    = true
0.00.659.186 I ggml_metal_init: has bfloat            = true
0.00.659.186 I ggml_metal_init: use bfloat            = true
0.00.659.187 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.323 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.680.569 I init:      Metal compute buffer size =    25.56 MiB
0.00.680.571 I init:        CPU compute buffer size =     1.06 MiB
0.00.680.572 I init: graph nodes  = 943
0.00.680.572 I init: graph splits = 2
0.00.680.574 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.680.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.680.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.696 I 
0.00.701.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.723 I perplexity: tokenizing the input ..
0.00.709.390 I perplexity: tokenization took 7.662 ms
0.00.709.397 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.714 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.847.041 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.847.072 I llama_perf_context_print:        load time =     692.92 ms
0.00.847.073 I llama_perf_context_print: prompt eval time =     136.31 ms /   128 tokens (    1.06 ms per token,   939.06 tokens per second)
0.00.847.074 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.075 I llama_perf_context_print:       total time =     145.38 ms /   129 tokens
0.00.847.500 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.079s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.871 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.016 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.018 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.018 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.021 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.954 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.138 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.138 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.139 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.139 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.139 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.140 I llama_model_loader: - type  f32:  194 tensors
0.00.026.140 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.140 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.141 I print_info: file format = GGUF V3 (latest)
0.00.026.142 I print_info: file type   = Q5_0
0.00.026.143 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.281 I load: special tokens cache size = 25
0.00.040.421 I load: token to piece cache size = 0.2984 MB
0.00.040.438 I print_info: arch             = gptneox
0.00.040.438 I print_info: vocab_only       = 0
0.00.040.439 I print_info: n_ctx_train      = 2048
0.00.040.439 I print_info: n_embd           = 2048
0.00.040.439 I print_info: n_layer          = 24
0.00.040.443 I print_info: n_head           = 16
0.00.040.444 I print_info: n_head_kv        = 16
0.00.040.444 I print_info: n_rot            = 32
0.00.040.445 I print_info: n_swa            = 0
0.00.040.445 I print_info: n_embd_head_k    = 128
0.00.040.447 I print_info: n_embd_head_v    = 128
0.00.040.448 I print_info: n_gqa            = 1
0.00.040.448 I print_info: n_embd_k_gqa     = 2048
0.00.040.450 I print_info: n_embd_v_gqa     = 2048
0.00.040.451 I print_info: f_norm_eps       = 1.0e-05
0.00.040.451 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.452 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.453 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.453 I print_info: f_logit_scale    = 0.0e+00
0.00.040.454 I print_info: n_ff             = 8192
0.00.040.454 I print_info: n_expert         = 0
0.00.040.454 I print_info: n_expert_used    = 0
0.00.040.454 I print_info: causal attn      = 1
0.00.040.454 I print_info: pooling type     = 0
0.00.040.454 I print_info: rope type        = 2
0.00.040.455 I print_info: rope scaling     = linear
0.00.040.455 I print_info: freq_base_train  = 10000.0
0.00.040.455 I print_info: freq_scale_train = 1
0.00.040.455 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.456 I print_info: rope_finetuned   = unknown
0.00.040.457 I print_info: ssm_d_conv       = 0
0.00.040.457 I print_info: ssm_d_inner      = 0
0.00.040.457 I print_info: ssm_d_state      = 0
0.00.040.457 I print_info: ssm_dt_rank      = 0
0.00.040.457 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.457 I print_info: model type       = 1.4B
0.00.040.458 I print_info: model params     = 1.41 B
0.00.040.458 I print_info: general.name     = 1.4B
0.00.040.458 I print_info: vocab type       = BPE
0.00.040.458 I print_info: n_vocab          = 50304
0.00.040.459 I print_info: n_merges         = 50009
0.00.040.459 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: LF token         = 187 'Ċ'
0.00.040.460 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.460 I print_info: max token length = 1024
0.00.040.460 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.700.841 I load_tensors: offloading 24 repeating layers to GPU
0.00.700.857 I load_tensors: offloading output layer to GPU
0.00.700.858 I load_tensors: offloaded 25/25 layers to GPU
0.00.700.890 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.700.891 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.702.503 I llama_context: constructing llama_context
0.00.702.507 I llama_context: n_seq_max     = 1
0.00.702.508 I llama_context: n_ctx         = 128
0.00.702.508 I llama_context: n_ctx_per_seq = 128
0.00.702.509 I llama_context: n_batch       = 128
0.00.702.509 I llama_context: n_ubatch      = 128
0.00.702.509 I llama_context: flash_attn    = 0
0.00.702.511 I llama_context: freq_base     = 10000.0
0.00.702.512 I llama_context: freq_scale    = 1
0.00.702.513 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.702.515 I ggml_metal_init: allocating
0.00.702.613 I ggml_metal_init: found device: Apple M4
0.00.702.636 I ggml_metal_init: picking default device: Apple M4
0.00.704.575 I ggml_metal_init: using embedded metal library
0.00.711.340 I ggml_metal_init: GPU name:   Apple M4
0.00.711.347 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.711.348 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.711.348 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.711.349 I ggml_metal_init: simdgroup reduction   = true
0.00.711.349 I ggml_metal_init: simdgroup matrix mul. = true
0.00.711.349 I ggml_metal_init: has residency sets    = true
0.00.711.350 I ggml_metal_init: has bfloat            = true
0.00.711.350 I ggml_metal_init: use bfloat            = true
0.00.711.351 I ggml_metal_init: hasUnifiedMemory      = true
0.00.711.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.728.438 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.731.880 I init:      Metal compute buffer size =    25.56 MiB
0.00.731.881 I init:        CPU compute buffer size =     1.06 MiB
0.00.731.882 I init: graph nodes  = 943
0.00.731.883 I init: graph splits = 2
0.00.731.885 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.731.886 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.731.887 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.601 I 
0.00.756.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.627 I perplexity: tokenizing the input ..
0.00.763.373 I perplexity: tokenization took 6.743 ms
0.00.763.380 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.375 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.900.714 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.900.736 I llama_perf_context_print:        load time =     746.73 ms
0.00.900.737 I llama_perf_context_print: prompt eval time =     135.98 ms /   128 tokens (    1.06 ms per token,   941.29 tokens per second)
0.00.900.738 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.900.738 I llama_perf_context_print:       total time =     144.13 ms /   129 tokens
0.00.901.154 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.078s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.797 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.811 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.776 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.037 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.038 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.038 I llama_model_loader: - type  f32:  194 tensors
0.00.025.039 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.039 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.040 I print_info: file format = GGUF V3 (latest)
0.00.025.040 I print_info: file type   = Q5_1
0.00.025.042 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.349 I load: special tokens cache size = 25
0.00.039.554 I load: token to piece cache size = 0.2984 MB
0.00.039.573 I print_info: arch             = gptneox
0.00.039.574 I print_info: vocab_only       = 0
0.00.039.574 I print_info: n_ctx_train      = 2048
0.00.039.574 I print_info: n_embd           = 2048
0.00.039.574 I print_info: n_layer          = 24
0.00.039.578 I print_info: n_head           = 16
0.00.039.579 I print_info: n_head_kv        = 16
0.00.039.579 I print_info: n_rot            = 32
0.00.039.579 I print_info: n_swa            = 0
0.00.039.579 I print_info: n_embd_head_k    = 128
0.00.039.580 I print_info: n_embd_head_v    = 128
0.00.039.580 I print_info: n_gqa            = 1
0.00.039.581 I print_info: n_embd_k_gqa     = 2048
0.00.039.584 I print_info: n_embd_v_gqa     = 2048
0.00.039.585 I print_info: f_norm_eps       = 1.0e-05
0.00.039.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.585 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.586 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.586 I print_info: f_logit_scale    = 0.0e+00
0.00.039.586 I print_info: n_ff             = 8192
0.00.039.587 I print_info: n_expert         = 0
0.00.039.587 I print_info: n_expert_used    = 0
0.00.039.587 I print_info: causal attn      = 1
0.00.039.587 I print_info: pooling type     = 0
0.00.039.587 I print_info: rope type        = 2
0.00.039.587 I print_info: rope scaling     = linear
0.00.039.587 I print_info: freq_base_train  = 10000.0
0.00.039.588 I print_info: freq_scale_train = 1
0.00.039.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.588 I print_info: rope_finetuned   = unknown
0.00.039.588 I print_info: ssm_d_conv       = 0
0.00.039.588 I print_info: ssm_d_inner      = 0
0.00.039.588 I print_info: ssm_d_state      = 0
0.00.039.589 I print_info: ssm_dt_rank      = 0
0.00.039.590 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.591 I print_info: model type       = 1.4B
0.00.039.591 I print_info: model params     = 1.41 B
0.00.039.591 I print_info: general.name     = 1.4B
0.00.039.592 I print_info: vocab type       = BPE
0.00.039.592 I print_info: n_vocab          = 50304
0.00.039.592 I print_info: n_merges         = 50009
0.00.039.592 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.592 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.593 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.594 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.594 I print_info: LF token         = 187 'Ċ'
0.00.039.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.595 I print_info: max token length = 1024
0.00.039.595 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.623.354 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.372 I load_tensors: offloading output layer to GPU
0.00.623.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.407 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.623.409 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.624.608 I llama_context: constructing llama_context
0.00.624.612 I llama_context: n_seq_max     = 1
0.00.624.613 I llama_context: n_ctx         = 128
0.00.624.613 I llama_context: n_ctx_per_seq = 128
0.00.624.614 I llama_context: n_batch       = 128
0.00.624.614 I llama_context: n_ubatch      = 128
0.00.624.614 I llama_context: flash_attn    = 0
0.00.624.617 I llama_context: freq_base     = 10000.0
0.00.624.617 I llama_context: freq_scale    = 1
0.00.624.618 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.624.620 I ggml_metal_init: allocating
0.00.624.667 I ggml_metal_init: found device: Apple M4
0.00.624.679 I ggml_metal_init: picking default device: Apple M4
0.00.626.249 I ggml_metal_init: using embedded metal library
0.00.632.536 I ggml_metal_init: GPU name:   Apple M4
0.00.632.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.543 I ggml_metal_init: simdgroup reduction   = true
0.00.632.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.543 I ggml_metal_init: has residency sets    = true
0.00.632.543 I ggml_metal_init: has bfloat            = true
0.00.632.544 I ggml_metal_init: use bfloat            = true
0.00.632.545 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.761 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.653.235 I init:      Metal compute buffer size =    25.56 MiB
0.00.653.236 I init:        CPU compute buffer size =     1.06 MiB
0.00.653.237 I init: graph nodes  = 943
0.00.653.237 I init: graph splits = 2
0.00.653.239 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.653.241 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.653.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.157 I 
0.00.684.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.186 I perplexity: tokenizing the input ..
0.00.691.524 I perplexity: tokenization took 7.335 ms
0.00.691.532 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.702 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.843.033 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.843.063 I llama_perf_context_print:        load time =     675.39 ms
0.00.843.064 I llama_perf_context_print: prompt eval time =     150.16 ms /   128 tokens (    1.17 ms per token,   852.44 tokens per second)
0.00.843.065 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.065 I llama_perf_context_print:       total time =     158.91 ms /   129 tokens
0.00.843.470 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.078s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.592 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.601 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.912 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.914 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.916 I llama_model_loader: - type  f32:  194 tensors
0.00.025.917 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.917 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.917 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.918 I print_info: file format = GGUF V3 (latest)
0.00.025.919 I print_info: file type   = Q2_K - Medium
0.00.025.920 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.373 I load: special tokens cache size = 25
0.00.040.321 I load: token to piece cache size = 0.2984 MB
0.00.040.336 I print_info: arch             = gptneox
0.00.040.338 I print_info: vocab_only       = 0
0.00.040.338 I print_info: n_ctx_train      = 2048
0.00.040.338 I print_info: n_embd           = 2048
0.00.040.338 I print_info: n_layer          = 24
0.00.040.343 I print_info: n_head           = 16
0.00.040.344 I print_info: n_head_kv        = 16
0.00.040.344 I print_info: n_rot            = 32
0.00.040.344 I print_info: n_swa            = 0
0.00.040.344 I print_info: n_embd_head_k    = 128
0.00.040.346 I print_info: n_embd_head_v    = 128
0.00.040.346 I print_info: n_gqa            = 1
0.00.040.347 I print_info: n_embd_k_gqa     = 2048
0.00.040.350 I print_info: n_embd_v_gqa     = 2048
0.00.040.350 I print_info: f_norm_eps       = 1.0e-05
0.00.040.351 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.351 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.351 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.357 I print_info: f_logit_scale    = 0.0e+00
0.00.040.366 I print_info: n_ff             = 8192
0.00.040.367 I print_info: n_expert         = 0
0.00.040.367 I print_info: n_expert_used    = 0
0.00.040.368 I print_info: causal attn      = 1
0.00.040.368 I print_info: pooling type     = 0
0.00.040.368 I print_info: rope type        = 2
0.00.040.368 I print_info: rope scaling     = linear
0.00.040.369 I print_info: freq_base_train  = 10000.0
0.00.040.369 I print_info: freq_scale_train = 1
0.00.040.369 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.369 I print_info: rope_finetuned   = unknown
0.00.040.369 I print_info: ssm_d_conv       = 0
0.00.040.369 I print_info: ssm_d_inner      = 0
0.00.040.370 I print_info: ssm_d_state      = 0
0.00.040.370 I print_info: ssm_dt_rank      = 0
0.00.040.370 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.370 I print_info: model type       = 1.4B
0.00.040.370 I print_info: model params     = 1.41 B
0.00.040.371 I print_info: general.name     = 1.4B
0.00.040.371 I print_info: vocab type       = BPE
0.00.040.371 I print_info: n_vocab          = 50304
0.00.040.371 I print_info: n_merges         = 50009
0.00.040.372 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.373 I print_info: LF token         = 187 'Ċ'
0.00.040.374 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.375 I print_info: max token length = 1024
0.00.040.375 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.347.401 I load_tensors: offloading 24 repeating layers to GPU
0.00.347.413 I load_tensors: offloading output layer to GPU
0.00.347.414 I load_tensors: offloaded 25/25 layers to GPU
0.00.347.444 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.347.450 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.349.105 I llama_context: constructing llama_context
0.00.349.112 I llama_context: n_seq_max     = 1
0.00.349.113 I llama_context: n_ctx         = 128
0.00.349.113 I llama_context: n_ctx_per_seq = 128
0.00.349.113 I llama_context: n_batch       = 128
0.00.349.114 I llama_context: n_ubatch      = 128
0.00.349.114 I llama_context: flash_attn    = 0
0.00.349.116 I llama_context: freq_base     = 10000.0
0.00.349.116 I llama_context: freq_scale    = 1
0.00.349.117 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.349.119 I ggml_metal_init: allocating
0.00.349.192 I ggml_metal_init: found device: Apple M4
0.00.349.205 I ggml_metal_init: picking default device: Apple M4
0.00.351.005 I ggml_metal_init: using embedded metal library
0.00.356.302 I ggml_metal_init: GPU name:   Apple M4
0.00.356.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.356.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.356.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.356.324 I ggml_metal_init: simdgroup reduction   = true
0.00.356.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.356.325 I ggml_metal_init: has residency sets    = true
0.00.356.325 I ggml_metal_init: has bfloat            = true
0.00.356.325 I ggml_metal_init: use bfloat            = true
0.00.356.328 I ggml_metal_init: hasUnifiedMemory      = true
0.00.356.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.814 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.381.471 I init:      Metal compute buffer size =    25.56 MiB
0.00.381.473 I init:        CPU compute buffer size =     1.06 MiB
0.00.381.474 I init: graph nodes  = 943
0.00.381.474 I init: graph splits = 2
0.00.381.477 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.381.478 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.381.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.410.447 I 
0.00.410.471 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.410.476 I perplexity: tokenizing the input ..
0.00.417.368 I perplexity: tokenization took 6.888 ms
0.00.417.376 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.559.810 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.151 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.173 I llama_perf_context_print:        load time =     400.51 ms
0.00.561.174 I llama_perf_context_print: prompt eval time =     142.42 ms /   128 tokens (    1.11 ms per token,   898.74 tokens per second)
0.00.561.174 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.175 I llama_perf_context_print:       total time =     150.73 ms /   129 tokens
0.00.561.533 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.080s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.981 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.053 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.064 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.065 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.065 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.066 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.066 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.251 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.314 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.317 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.318 I llama_model_loader: - type  f32:  194 tensors
0.00.025.318 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.318 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.319 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.319 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.320 I print_info: file format = GGUF V3 (latest)
0.00.025.320 I print_info: file type   = Q3_K - Medium
0.00.025.321 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.265 I load: special tokens cache size = 25
0.00.039.369 I load: token to piece cache size = 0.2984 MB
0.00.039.387 I print_info: arch             = gptneox
0.00.039.388 I print_info: vocab_only       = 0
0.00.039.388 I print_info: n_ctx_train      = 2048
0.00.039.388 I print_info: n_embd           = 2048
0.00.039.389 I print_info: n_layer          = 24
0.00.039.393 I print_info: n_head           = 16
0.00.039.393 I print_info: n_head_kv        = 16
0.00.039.393 I print_info: n_rot            = 32
0.00.039.394 I print_info: n_swa            = 0
0.00.039.394 I print_info: n_embd_head_k    = 128
0.00.039.394 I print_info: n_embd_head_v    = 128
0.00.039.394 I print_info: n_gqa            = 1
0.00.039.395 I print_info: n_embd_k_gqa     = 2048
0.00.039.397 I print_info: n_embd_v_gqa     = 2048
0.00.039.398 I print_info: f_norm_eps       = 1.0e-05
0.00.039.398 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.398 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.398 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.399 I print_info: f_logit_scale    = 0.0e+00
0.00.039.399 I print_info: n_ff             = 8192
0.00.039.399 I print_info: n_expert         = 0
0.00.039.400 I print_info: n_expert_used    = 0
0.00.039.400 I print_info: causal attn      = 1
0.00.039.400 I print_info: pooling type     = 0
0.00.039.400 I print_info: rope type        = 2
0.00.039.400 I print_info: rope scaling     = linear
0.00.039.401 I print_info: freq_base_train  = 10000.0
0.00.039.401 I print_info: freq_scale_train = 1
0.00.039.401 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.401 I print_info: rope_finetuned   = unknown
0.00.039.402 I print_info: ssm_d_conv       = 0
0.00.039.402 I print_info: ssm_d_inner      = 0
0.00.039.402 I print_info: ssm_d_state      = 0
0.00.039.402 I print_info: ssm_dt_rank      = 0
0.00.039.402 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.402 I print_info: model type       = 1.4B
0.00.039.403 I print_info: model params     = 1.41 B
0.00.039.403 I print_info: general.name     = 1.4B
0.00.039.403 I print_info: vocab type       = BPE
0.00.039.403 I print_info: n_vocab          = 50304
0.00.039.404 I print_info: n_merges         = 50009
0.00.039.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.406 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.406 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.406 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.406 I print_info: LF token         = 187 'Ċ'
0.00.039.407 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.407 I print_info: max token length = 1024
0.00.039.407 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.923 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.941 I load_tensors: offloading output layer to GPU
0.00.435.942 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.974 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.976 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.647 I llama_context: constructing llama_context
0.00.437.650 I llama_context: n_seq_max     = 1
0.00.437.651 I llama_context: n_ctx         = 128
0.00.437.651 I llama_context: n_ctx_per_seq = 128
0.00.437.652 I llama_context: n_batch       = 128
0.00.437.652 I llama_context: n_ubatch      = 128
0.00.437.652 I llama_context: flash_attn    = 0
0.00.437.655 I llama_context: freq_base     = 10000.0
0.00.437.655 I llama_context: freq_scale    = 1
0.00.437.656 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.658 I ggml_metal_init: allocating
0.00.437.748 I ggml_metal_init: found device: Apple M4
0.00.437.763 I ggml_metal_init: picking default device: Apple M4
0.00.439.668 I ggml_metal_init: using embedded metal library
0.00.445.417 I ggml_metal_init: GPU name:   Apple M4
0.00.445.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.428 I ggml_metal_init: simdgroup reduction   = true
0.00.445.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.428 I ggml_metal_init: has residency sets    = true
0.00.445.428 I ggml_metal_init: has bfloat            = true
0.00.445.429 I ggml_metal_init: use bfloat            = true
0.00.445.441 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.720 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.467.941 I init:      Metal compute buffer size =    25.56 MiB
0.00.467.943 I init:        CPU compute buffer size =     1.06 MiB
0.00.467.943 I init: graph nodes  = 943
0.00.467.944 I init: graph splits = 2
0.00.467.946 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.467.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.467.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.250 I 
0.00.496.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.277 I perplexity: tokenizing the input ..
0.00.503.898 I perplexity: tokenization took 7.617 ms
0.00.503.905 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.587 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.930 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.960 I llama_perf_context_print:        load time =     487.27 ms
0.00.647.960 I llama_perf_context_print: prompt eval time =     142.66 ms /   128 tokens (    1.11 ms per token,   897.21 tokens per second)
0.00.647.961 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.961 I llama_perf_context_print:       total time =     151.71 ms /   129 tokens
0.00.648.321 I ggml_metal_free: deallocating

real	0m0.662s
user	0m0.078s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.909 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.915 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.921 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.922 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.925 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.925 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.926 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.928 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.928 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.928 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.926 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.123 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.120 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.121 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.122 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.122 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.122 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.123 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.123 I llama_model_loader: - type  f32:  194 tensors
0.00.025.124 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.124 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.124 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.125 I print_info: file format = GGUF V3 (latest)
0.00.025.125 I print_info: file type   = Q4_K - Medium
0.00.025.127 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.449 I load: special tokens cache size = 25
0.00.039.692 I load: token to piece cache size = 0.2984 MB
0.00.039.708 I print_info: arch             = gptneox
0.00.039.709 I print_info: vocab_only       = 0
0.00.039.709 I print_info: n_ctx_train      = 2048
0.00.039.710 I print_info: n_embd           = 2048
0.00.039.710 I print_info: n_layer          = 24
0.00.039.714 I print_info: n_head           = 16
0.00.039.714 I print_info: n_head_kv        = 16
0.00.039.715 I print_info: n_rot            = 32
0.00.039.717 I print_info: n_swa            = 0
0.00.039.717 I print_info: n_embd_head_k    = 128
0.00.039.717 I print_info: n_embd_head_v    = 128
0.00.039.718 I print_info: n_gqa            = 1
0.00.039.719 I print_info: n_embd_k_gqa     = 2048
0.00.039.719 I print_info: n_embd_v_gqa     = 2048
0.00.039.720 I print_info: f_norm_eps       = 1.0e-05
0.00.039.720 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.720 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.720 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.721 I print_info: f_logit_scale    = 0.0e+00
0.00.039.721 I print_info: n_ff             = 8192
0.00.039.722 I print_info: n_expert         = 0
0.00.039.722 I print_info: n_expert_used    = 0
0.00.039.722 I print_info: causal attn      = 1
0.00.039.722 I print_info: pooling type     = 0
0.00.039.722 I print_info: rope type        = 2
0.00.039.722 I print_info: rope scaling     = linear
0.00.039.723 I print_info: freq_base_train  = 10000.0
0.00.039.723 I print_info: freq_scale_train = 1
0.00.039.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.723 I print_info: rope_finetuned   = unknown
0.00.039.723 I print_info: ssm_d_conv       = 0
0.00.039.723 I print_info: ssm_d_inner      = 0
0.00.039.724 I print_info: ssm_d_state      = 0
0.00.039.724 I print_info: ssm_dt_rank      = 0
0.00.039.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.724 I print_info: model type       = 1.4B
0.00.039.724 I print_info: model params     = 1.41 B
0.00.039.724 I print_info: general.name     = 1.4B
0.00.039.725 I print_info: vocab type       = BPE
0.00.039.725 I print_info: n_vocab          = 50304
0.00.039.725 I print_info: n_merges         = 50009
0.00.039.726 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: LF token         = 187 'Ċ'
0.00.039.726 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: max token length = 1024
0.00.039.727 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.546.940 I load_tensors: offloading 24 repeating layers to GPU
0.00.546.948 I load_tensors: offloading output layer to GPU
0.00.546.949 I load_tensors: offloaded 25/25 layers to GPU
0.00.546.984 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.546.985 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.548.638 I llama_context: constructing llama_context
0.00.548.643 I llama_context: n_seq_max     = 1
0.00.548.644 I llama_context: n_ctx         = 128
0.00.548.644 I llama_context: n_ctx_per_seq = 128
0.00.548.645 I llama_context: n_batch       = 128
0.00.548.645 I llama_context: n_ubatch      = 128
0.00.548.646 I llama_context: flash_attn    = 0
0.00.548.648 I llama_context: freq_base     = 10000.0
0.00.548.648 I llama_context: freq_scale    = 1
0.00.548.649 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.548.664 I ggml_metal_init: allocating
0.00.548.781 I ggml_metal_init: found device: Apple M4
0.00.548.830 I ggml_metal_init: picking default device: Apple M4
0.00.551.023 I ggml_metal_init: using embedded metal library
0.00.558.065 I ggml_metal_init: GPU name:   Apple M4
0.00.558.070 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.071 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.072 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.072 I ggml_metal_init: simdgroup reduction   = true
0.00.558.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.073 I ggml_metal_init: has residency sets    = true
0.00.558.073 I ggml_metal_init: has bfloat            = true
0.00.558.073 I ggml_metal_init: use bfloat            = true
0.00.558.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.011 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.579.361 I init:      Metal compute buffer size =    25.56 MiB
0.00.579.363 I init:        CPU compute buffer size =     1.06 MiB
0.00.579.364 I init: graph nodes  = 943
0.00.579.364 I init: graph splits = 2
0.00.579.367 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.579.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.579.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.605.573 I 
0.00.605.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.605.597 I perplexity: tokenizing the input ..
0.00.612.522 I perplexity: tokenization took 6.922 ms
0.00.612.532 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.755.826 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.757.179 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.757.199 I llama_perf_context_print:        load time =     596.70 ms
0.00.757.200 I llama_perf_context_print: prompt eval time =     143.28 ms /   128 tokens (    1.12 ms per token,   893.34 tokens per second)
0.00.757.201 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.757.201 I llama_perf_context_print:       total time =     151.63 ms /   129 tokens
0.00.757.600 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.079s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.068 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.091 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.229 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.230 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.231 I llama_model_loader: - type  f32:  194 tensors
0.00.026.231 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.231 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.232 I print_info: file format = GGUF V3 (latest)
0.00.026.233 I print_info: file type   = Q5_K - Medium
0.00.026.234 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.240 I load: special tokens cache size = 25
0.00.040.231 I load: token to piece cache size = 0.2984 MB
0.00.040.247 I print_info: arch             = gptneox
0.00.040.248 I print_info: vocab_only       = 0
0.00.040.248 I print_info: n_ctx_train      = 2048
0.00.040.248 I print_info: n_embd           = 2048
0.00.040.249 I print_info: n_layer          = 24
0.00.040.252 I print_info: n_head           = 16
0.00.040.253 I print_info: n_head_kv        = 16
0.00.040.253 I print_info: n_rot            = 32
0.00.040.253 I print_info: n_swa            = 0
0.00.040.253 I print_info: n_embd_head_k    = 128
0.00.040.254 I print_info: n_embd_head_v    = 128
0.00.040.254 I print_info: n_gqa            = 1
0.00.040.255 I print_info: n_embd_k_gqa     = 2048
0.00.040.257 I print_info: n_embd_v_gqa     = 2048
0.00.040.258 I print_info: f_norm_eps       = 1.0e-05
0.00.040.258 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.258 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.258 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.259 I print_info: f_logit_scale    = 0.0e+00
0.00.040.259 I print_info: n_ff             = 8192
0.00.040.259 I print_info: n_expert         = 0
0.00.040.260 I print_info: n_expert_used    = 0
0.00.040.260 I print_info: causal attn      = 1
0.00.040.260 I print_info: pooling type     = 0
0.00.040.260 I print_info: rope type        = 2
0.00.040.260 I print_info: rope scaling     = linear
0.00.040.260 I print_info: freq_base_train  = 10000.0
0.00.040.261 I print_info: freq_scale_train = 1
0.00.040.261 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.261 I print_info: rope_finetuned   = unknown
0.00.040.261 I print_info: ssm_d_conv       = 0
0.00.040.262 I print_info: ssm_d_inner      = 0
0.00.040.262 I print_info: ssm_d_state      = 0
0.00.040.262 I print_info: ssm_dt_rank      = 0
0.00.040.262 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.262 I print_info: model type       = 1.4B
0.00.040.262 I print_info: model params     = 1.41 B
0.00.040.263 I print_info: general.name     = 1.4B
0.00.040.263 I print_info: vocab type       = BPE
0.00.040.263 I print_info: n_vocab          = 50304
0.00.040.264 I print_info: n_merges         = 50009
0.00.040.264 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: LF token         = 187 'Ċ'
0.00.040.265 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.265 I print_info: max token length = 1024
0.00.040.265 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.733 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.752 I load_tensors: offloading output layer to GPU
0.00.587.753 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.790 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.791 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.242 I llama_context: constructing llama_context
0.00.589.247 I llama_context: n_seq_max     = 1
0.00.589.248 I llama_context: n_ctx         = 128
0.00.589.248 I llama_context: n_ctx_per_seq = 128
0.00.589.248 I llama_context: n_batch       = 128
0.00.589.249 I llama_context: n_ubatch      = 128
0.00.589.249 I llama_context: flash_attn    = 0
0.00.589.251 I llama_context: freq_base     = 10000.0
0.00.589.252 I llama_context: freq_scale    = 1
0.00.589.252 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.255 I ggml_metal_init: allocating
0.00.589.355 I ggml_metal_init: found device: Apple M4
0.00.589.370 I ggml_metal_init: picking default device: Apple M4
0.00.591.289 I ggml_metal_init: using embedded metal library
0.00.598.040 I ggml_metal_init: GPU name:   Apple M4
0.00.598.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.048 I ggml_metal_init: simdgroup reduction   = true
0.00.598.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.049 I ggml_metal_init: has residency sets    = true
0.00.598.049 I ggml_metal_init: has bfloat            = true
0.00.598.049 I ggml_metal_init: use bfloat            = true
0.00.598.050 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.064 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.530 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.620.017 I init:      Metal compute buffer size =    25.56 MiB
0.00.620.019 I init:        CPU compute buffer size =     1.06 MiB
0.00.620.020 I init: graph nodes  = 943
0.00.620.020 I init: graph splits = 2
0.00.620.023 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.620.024 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.024 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.660 I 
0.00.652.687 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.699 I perplexity: tokenizing the input ..
0.00.659.587 I perplexity: tokenization took 6.885 ms
0.00.659.593 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.013 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.798.362 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.798.385 I llama_perf_context_print:        load time =     642.71 ms
0.00.798.385 I llama_perf_context_print: prompt eval time =     137.41 ms /   128 tokens (    1.07 ms per token,   931.52 tokens per second)
0.00.798.386 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.386 I llama_perf_context_print:       total time =     145.73 ms /   129 tokens
0.00.798.737 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.078s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.548 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.734 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.741 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.748 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.749 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.750 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.750 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.753 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.680 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.680 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.680 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.681 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.681 I llama_model_loader: - type  f32:  194 tensors
0.00.027.682 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.683 I print_info: file format = GGUF V3 (latest)
0.00.027.683 I print_info: file type   = Q6_K
0.00.027.684 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.036.007 I load: special tokens cache size = 25
0.00.041.874 I load: token to piece cache size = 0.2984 MB
0.00.041.892 I print_info: arch             = gptneox
0.00.041.892 I print_info: vocab_only       = 0
0.00.041.893 I print_info: n_ctx_train      = 2048
0.00.041.893 I print_info: n_embd           = 2048
0.00.041.893 I print_info: n_layer          = 24
0.00.041.897 I print_info: n_head           = 16
0.00.041.898 I print_info: n_head_kv        = 16
0.00.041.898 I print_info: n_rot            = 32
0.00.041.898 I print_info: n_swa            = 0
0.00.041.898 I print_info: n_embd_head_k    = 128
0.00.041.898 I print_info: n_embd_head_v    = 128
0.00.041.899 I print_info: n_gqa            = 1
0.00.041.906 I print_info: n_embd_k_gqa     = 2048
0.00.041.906 I print_info: n_embd_v_gqa     = 2048
0.00.041.907 I print_info: f_norm_eps       = 1.0e-05
0.00.041.907 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.907 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.912 I print_info: f_logit_scale    = 0.0e+00
0.00.041.913 I print_info: n_ff             = 8192
0.00.041.913 I print_info: n_expert         = 0
0.00.041.913 I print_info: n_expert_used    = 0
0.00.041.913 I print_info: causal attn      = 1
0.00.041.913 I print_info: pooling type     = 0
0.00.041.913 I print_info: rope type        = 2
0.00.041.914 I print_info: rope scaling     = linear
0.00.041.914 I print_info: freq_base_train  = 10000.0
0.00.041.914 I print_info: freq_scale_train = 1
0.00.041.914 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.914 I print_info: rope_finetuned   = unknown
0.00.041.915 I print_info: ssm_d_conv       = 0
0.00.041.915 I print_info: ssm_d_inner      = 0
0.00.041.915 I print_info: ssm_d_state      = 0
0.00.041.915 I print_info: ssm_dt_rank      = 0
0.00.041.916 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.917 I print_info: model type       = 1.4B
0.00.041.917 I print_info: model params     = 1.41 B
0.00.041.917 I print_info: general.name     = 1.4B
0.00.041.918 I print_info: vocab type       = BPE
0.00.041.918 I print_info: n_vocab          = 50304
0.00.041.918 I print_info: n_merges         = 50009
0.00.041.918 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.918 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.919 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.919 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.919 I print_info: LF token         = 187 'Ċ'
0.00.041.919 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.919 I print_info: max token length = 1024
0.00.041.920 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.392.659 I load_tensors: offloading 24 repeating layers to GPU
0.00.392.663 I load_tensors: offloading output layer to GPU
0.00.392.664 I load_tensors: offloaded 25/25 layers to GPU
0.00.392.689 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.392.691 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.394.268 I llama_context: constructing llama_context
0.00.394.270 I llama_context: n_seq_max     = 1
0.00.394.271 I llama_context: n_ctx         = 128
0.00.394.271 I llama_context: n_ctx_per_seq = 128
0.00.394.271 I llama_context: n_batch       = 128
0.00.394.272 I llama_context: n_ubatch      = 128
0.00.394.272 I llama_context: flash_attn    = 0
0.00.394.273 I llama_context: freq_base     = 10000.0
0.00.394.274 I llama_context: freq_scale    = 1
0.00.394.274 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.394.275 I ggml_metal_init: allocating
0.00.394.335 I ggml_metal_init: found device: Apple M4
0.00.394.355 I ggml_metal_init: picking default device: Apple M4
0.00.395.788 I ggml_metal_init: using embedded metal library
0.00.401.596 I ggml_metal_init: GPU name:   Apple M4
0.00.401.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.401.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.401.602 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.401.602 I ggml_metal_init: simdgroup reduction   = true
0.00.401.602 I ggml_metal_init: simdgroup matrix mul. = true
0.00.401.602 I ggml_metal_init: has residency sets    = true
0.00.401.603 I ggml_metal_init: has bfloat            = true
0.00.401.603 I ggml_metal_init: use bfloat            = true
0.00.401.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.401.606 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.417.740 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.420.770 I init:      Metal compute buffer size =    25.56 MiB
0.00.420.772 I init:        CPU compute buffer size =     1.06 MiB
0.00.420.772 I init: graph nodes  = 943
0.00.420.773 I init: graph splits = 2
0.00.420.775 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.420.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.420.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.455.694 I 
0.00.455.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.455.717 I perplexity: tokenizing the input ..
0.00.462.834 I perplexity: tokenization took 7.113 ms
0.00.462.846 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.595.383 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.596.746 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.596.768 I llama_perf_context_print:        load time =     446.14 ms
0.00.596.769 I llama_perf_context_print: prompt eval time =     132.53 ms /   128 tokens (    1.04 ms per token,   965.86 tokens per second)
0.00.596.770 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.596.770 I llama_perf_context_print:       total time =     141.07 ms /   129 tokens
0.00.597.176 I ggml_metal_free: deallocating

real	0m0.611s
user	0m0.075s
sys	0m0.107s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.273 I build: 4815 (8bc4a9b2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.067 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.401 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.413 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.414 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.417 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.418 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.418 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.419 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.435 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.199 I llama_model_loader: - type  f32:  194 tensors
0.00.055.199 I llama_model_loader: - type  f16:   98 tensors
0.00.055.200 I print_info: file format = GGUF V3 (latest)
0.00.055.201 I print_info: file type   = all F32 (guessed)
0.00.055.202 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.426 I load: special tokens cache size = 25
0.00.073.966 I load: token to piece cache size = 0.2984 MB
0.00.073.982 I print_info: arch             = gptneox
0.00.073.983 I print_info: vocab_only       = 0
0.00.073.983 I print_info: n_ctx_train      = 2048
0.00.073.984 I print_info: n_embd           = 2048
0.00.073.984 I print_info: n_layer          = 24
0.00.073.986 I print_info: n_head           = 16
0.00.073.987 I print_info: n_head_kv        = 16
0.00.073.987 I print_info: n_rot            = 32
0.00.073.987 I print_info: n_swa            = 0
0.00.073.987 I print_info: n_embd_head_k    = 128
0.00.073.987 I print_info: n_embd_head_v    = 128
0.00.073.988 I print_info: n_gqa            = 1
0.00.073.989 I print_info: n_embd_k_gqa     = 2048
0.00.073.989 I print_info: n_embd_v_gqa     = 2048
0.00.073.990 I print_info: f_norm_eps       = 1.0e-05
0.00.073.990 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.993 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.993 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.993 I print_info: f_logit_scale    = 0.0e+00
0.00.073.994 I print_info: n_ff             = 8192
0.00.073.994 I print_info: n_expert         = 0
0.00.073.994 I print_info: n_expert_used    = 0
0.00.073.994 I print_info: causal attn      = 1
0.00.073.995 I print_info: pooling type     = 0
0.00.073.995 I print_info: rope type        = 2
0.00.073.995 I print_info: rope scaling     = linear
0.00.073.995 I print_info: freq_base_train  = 10000.0
0.00.073.996 I print_info: freq_scale_train = 1
0.00.073.996 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.997 I print_info: rope_finetuned   = unknown
0.00.073.997 I print_info: ssm_d_conv       = 0
0.00.074.000 I print_info: ssm_d_inner      = 0
0.00.074.000 I print_info: ssm_d_state      = 0
0.00.074.000 I print_info: ssm_dt_rank      = 0
0.00.074.001 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.002 I print_info: model type       = 1.4B
0.00.074.002 I print_info: model params     = 1.41 B
0.00.074.003 I print_info: general.name     = 1.4B
0.00.074.003 I print_info: vocab type       = BPE
0.00.074.003 I print_info: n_vocab          = 50304
0.00.074.003 I print_info: n_merges         = 50009
0.00.074.004 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.004 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.004 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.004 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.004 I print_info: LF token         = 187 'Ċ'
0.00.074.005 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.005 I print_info: max token length = 1024
0.00.074.006 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.338.447 I load_tensors: offloading 24 repeating layers to GPU
0.01.338.450 I load_tensors: offloading output layer to GPU
0.01.338.451 I load_tensors: offloaded 25/25 layers to GPU
0.01.338.467 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.338.469 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.338.920 I llama_context: constructing llama_context
0.01.338.921 I llama_context: n_seq_max     = 1
0.01.338.921 I llama_context: n_ctx         = 128
0.01.338.922 I llama_context: n_ctx_per_seq = 128
0.01.338.922 I llama_context: n_batch       = 128
0.01.338.922 I llama_context: n_ubatch      = 128
0.01.338.922 I llama_context: flash_attn    = 0
0.01.338.922 I llama_context: freq_base     = 10000.0
0.01.338.923 I llama_context: freq_scale    = 1
0.01.338.923 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.338.925 I ggml_metal_init: allocating
0.01.338.968 I ggml_metal_init: found device: Apple M4
0.01.338.973 I ggml_metal_init: picking default device: Apple M4
0.01.339.661 I ggml_metal_init: using embedded metal library
0.01.342.129 I ggml_metal_init: GPU name:   Apple M4
0.01.342.130 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.342.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.342.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.342.132 I ggml_metal_init: simdgroup reduction   = true
0.01.342.132 I ggml_metal_init: simdgroup matrix mul. = true
0.01.342.132 I ggml_metal_init: has residency sets    = true
0.01.342.132 I ggml_metal_init: has bfloat            = true
0.01.342.132 I ggml_metal_init: use bfloat            = true
0.01.342.133 I ggml_metal_init: hasUnifiedMemory      = true
0.01.342.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.352.614 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.354.362 I init:      Metal compute buffer size =    25.56 MiB
0.01.354.364 I init:        CPU compute buffer size =     1.06 MiB
0.01.354.364 I init: graph nodes  = 943
0.01.354.364 I init: graph splits = 2
0.01.354.365 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.01.354.366 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.354.366 I 
0.01.354.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.354.397 I compute_imatrix: tokenizing the input ..
0.01.358.418 I compute_imatrix: tokenization took 4.02 ms
0.01.358.421 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.641.916 I compute_imatrix: 0.28 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.644.627 I llama_perf_context_print:        load time =    1616.85 ms
0.01.644.628 I llama_perf_context_print: prompt eval time =     283.49 ms /   128 tokens (    2.21 ms per token,   451.52 tokens per second)
0.01.644.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.644.629 I llama_perf_context_print:       total time =    1619.55 ms /   129 tokens
0.01.645.213 I ggml_metal_free: deallocating

real	0m1.828s
user	0m0.137s
sys	0m0.219s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159308a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159309180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159309730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159309ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15930a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15930a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15930adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15930b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15930b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15930be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15930c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15930c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15930d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15930db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15930e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15930ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15930f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15930f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15930ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159310780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159310ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1593115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159311ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159312580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159312ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159312f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159313570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1593141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159314720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1593149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159314e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159315140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1593159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159315f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1593161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159316670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159316b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159316fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159317450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1593178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159317d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159318230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1593186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159318b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159318e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159319440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159319a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15931a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15931a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15931af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15931b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15931bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15931c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15931c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15931cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15931d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15931d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15931dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15931e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15931e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15931ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15931f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15931f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15931fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15931ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1593203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159320840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159320ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159321180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159321620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159321ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159321f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159322400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159322950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159322ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1593233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159323940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159323e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1593243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159324930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159324e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1593253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159325920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159325e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1593263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159326910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159326e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1593273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159327900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159327e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1593283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1593288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159328e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159329390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1593298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159329e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15932a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15931a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15932a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15932afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15932b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15932ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15932bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15932c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15932ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15932cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15932d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15932da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15932df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15932e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15932ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15932ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15932f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15932f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15932fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159330290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159330730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159330bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159331070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159331510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1593319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159331e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1593322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159332790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159332c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1593330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159333570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159333a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159333eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159334350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1593347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159334c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159335130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1593355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159335a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159335f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1593363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159336850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159336cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159337190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159337630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159337ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159337f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159338410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1593388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159338d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1593391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159339690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159339b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159339fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15933a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15933a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15933adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15933b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15933b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15933bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15933c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15933c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15933c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15933ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15933d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15933d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15933dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15933e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15933e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15933e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15933ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15933f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15933f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15933fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1593400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159340590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159340a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159340ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159341370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159341810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159341cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159342150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1593425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159342a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159342f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1593433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159343870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159343d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1593441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159344650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159344af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159344f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159345430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1593458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159345d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159346210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1593466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159346c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159347150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1593476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159347bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159347eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1593484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159348ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1593490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1593498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159349d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15934a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15934a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15934ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15934b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15934b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15934bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15934c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15934c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15934cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15934d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15934d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15934df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15934e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15934e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15934ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15934f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15934f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15934fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159350440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159350990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159350ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159351430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159351980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159351ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159352420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159352970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159352ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159353410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159353960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159353eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159354400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159354950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159354ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1593553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159355940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159355e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1593563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159356930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159356e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1593573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159357920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159357e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1593583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159358910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159358e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1593593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159359900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159359e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15935a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15935a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15935ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15935b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15935b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15935be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15935c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15935c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15935ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15935d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15935d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15935de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15935e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15935e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15935ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15935f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15935f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15935fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159360130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1593605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159360a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159360f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1593613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159361850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159361cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159362190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159362630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159362ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159362f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159363410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1593638b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159363e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159364520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159364c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159365360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159365a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159365d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159366530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1593667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159366e00 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 943
init: graph splits = 2
get_kv_self: llama_context does not have a KV cache
0.00.715.926 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.715.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
get_kv_self: llama_context does not have a KV cache
main : serialized state into 201251 out of a maximum of 201251 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159206040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1592064b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159206920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159206d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159207200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159207670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159207ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159207f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1592083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159208930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159208da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159209420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x159209f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15920a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15920af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15920b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15920bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15920c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15920cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15920d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15920da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15920e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15920e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15920efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15920f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15920f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15920fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1592100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159210550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1592109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159210e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159211360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1592117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159211a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159211f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159212370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1592127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159212c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1592130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159213530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1592139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159213e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159214280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1592146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159214b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159214fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159215440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1592158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159215d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159216190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159216600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159216a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159216ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159217350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1592177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159217c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1592181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1592186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159218b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159218f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1592193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159219860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159219cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15921a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15921a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15921aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15921ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15921b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15921b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15921bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15921c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15921c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15921c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15921cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15921d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15921d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15921daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15921df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15921e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15921e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15921ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15921f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15921f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15921fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15921fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1592202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159220750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159220bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159221030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1592214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159221910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159221d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1592221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159222660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159222ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159222f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1592233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159223820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159223c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159224100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159224570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1592249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159224e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1592252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159225730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159225ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159226010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159226480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1592268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159226d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1592271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159227640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159227ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159227f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159228390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159228800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159228c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1592290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159229550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1592299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159229e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15922a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15922a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15922ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15922aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15922b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15922b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15922bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15922c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15922c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15922ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15922cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15922d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15922d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15922dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15922e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15922e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15922e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15922ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15922f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15922f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15922fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15922ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159230440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1592308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159230d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159231190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159231600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159231a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159231ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159232350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1592327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159232c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1592330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159233510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159233980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159233df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159234260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1592346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159234b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159234fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159235420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159235890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159235d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159236170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1592365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159237210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1592374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159237790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159237c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159238070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1592384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159238950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159238dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159239230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1592396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159239b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159239f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15923a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15923a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15923acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15923b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15923b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15923ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15923be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15923c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15923c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15923cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15923d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15923d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15923d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15923dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15923e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15923e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15923eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15923ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15923f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15923f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15923fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159240120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159240590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159240a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159240f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159241470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1592418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159241d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1592421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159242630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159242b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159243060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159243bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159243e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159244450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159244a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159244fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159245590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159245b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159246110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1592466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159246c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159247250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159247810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159247dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159248390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159248950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159248f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1592494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159249a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15924a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15924a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15924abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15924b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15924b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15924bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15924c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15924c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15924ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15924d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15924d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15924df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15924e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15924eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15924f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15924f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15924fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159250210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1592507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159250d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159251350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159251910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159251ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159252490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159252a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159253010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1592535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159253b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159254150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159254710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159254cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159255290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159255850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159255e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1592563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159256990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159256f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159257510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159257ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159258090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159258590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159258a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159258f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159259490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159259990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159259e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15925a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15925a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15925ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15925b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15925b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15925bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15925c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15925c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15925cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15925d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15925dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15925e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15925eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15925edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15925f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15925f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15925fe80 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 943
init: graph splits = 2
main : deserialized state from 201251 out of a maximum of 201251 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x172504820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x172504c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x172505100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x172505570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1725059e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x172505e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1725062c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x172506730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x172506ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x172507110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x172507580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x172507c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x172508720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x172508ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1725096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x172509e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x17250a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x17250ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x17250b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x17250bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x17250c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x17250c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x17250d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x17250d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x17250ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x17250e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x17250e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x17250e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x17250ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x17250f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x17250f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x17250fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x17250ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x172510270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1725106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x172510b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x172510fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x172511430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1725118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x172511d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x172512180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1725125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x172512a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x172512ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x172513340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1725137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x172513c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x172514090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x172514500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x172514970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x172514de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x172515250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1725156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x172515b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x172515fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x172516410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x172516980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x172516e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1725172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x172517760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x172517bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x172518040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1725184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x172518920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x172518d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x172519200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x172519670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x172519ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x172519f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x17251a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x17251a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x17251aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x17251b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x17251b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x17251b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x17251be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x17251c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x17251c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x17251cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x17251d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x17251d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x17251d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x17251dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x17251e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x17251e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x17251eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x17251ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x17251f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x17251f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x17251fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1725200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x172520560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1725209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x172520e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1725212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x172521720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x172521b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x172522000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x172522470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1725228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x172522d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1725231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x172523630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x172523aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x172524410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1725246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x172524b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x172524fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x172525420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x172525890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x172525d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x172526170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1725265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x172526a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x172526ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x172527330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1725277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x172527c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x172528080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1725284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x172528960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x172528dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x172529240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1725296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x172529b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x172529f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x17252a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x17252a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x17252ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x17252b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x17252b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x17252ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x17252bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x17252c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x17252c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x17252cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x17252d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x17252d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x17252d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x17252ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x17252e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x17252e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x17252eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x17252ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x17252f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x17252f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x17252fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x172530130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1725305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x172530a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x172530e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1725312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x172531760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x172531bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x172532040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1725324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x172532920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x172532d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x172533200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x172533670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x172533ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x172533f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1725343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x172534830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x172534ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x172535110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x172535580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1725359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x172535e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1725362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x172536740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x172536bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x172537020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x172537490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x172537900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x172537d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1725381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x172538650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x172538ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x172538f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1725393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x172539810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x172539c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x17253a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x17253a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x17253a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x17253ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x17253b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x17253b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x17253bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x17253c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x17253c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x17253c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x17253cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x17253d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x17253d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x17253daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x17253df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x17253e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x17253e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x17253ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x17253f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x17253f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x17253f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x17253fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x172540290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x172540700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x172540c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x172541100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x172541570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1725420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x172542380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x172542640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x172542ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x172542f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x172543390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x172543800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x172543c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1725440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x172544550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1725449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x172544e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1725452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x172545710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x172545b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x172545ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x172546460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1725468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x172546d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1725471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x172547620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x172547a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x172547f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x172548370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1725487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x172548c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1725490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x172549530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1725499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x172549e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x17254a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x17254a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x17254ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x17254afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x17254b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x17254b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x17254bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x17254c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x17254c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x17254ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x17254cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x17254d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x17254d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x17254dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x17254e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x17254e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x17254e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x17254edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x17254f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x17254f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x17254fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x17254ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x172550420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x172550890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x172550d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x172551170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1725515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x172551a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x172551ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x172552330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1725527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x172552c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x172553080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1725534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x172553960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x172553dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x172554240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1725546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x172554b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x172554f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x172555400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x172555870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x172555ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x172556750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x172556e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x172557590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x172557cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x172557f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1725583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1725589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x172558ff0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 943
init: graph splits = 2
main : deserialized state from 201251 out of a maximum of 201251 bytes
main : seq 0 copied, 0 bytes
get_kv_self: llama_context does not have a KV cache
main : kv cache cleared
main : seq 1 restored, 0 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L


second run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L


single seq run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L

real	0m1.588s
user	0m0.243s
sys	0m0.174s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4815 (8bc4a9b2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125f10c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125f11380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125f11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125f12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125f12a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125f12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125f135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125f13b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125f14050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125f14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125f14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125f15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125f15d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125f16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125f16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125f17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125f17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125f181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125f18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125f190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125f197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125f19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125f1a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125f1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125f1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125f1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125f1c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125f1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125f1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125f1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125f1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125f1e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125f1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125f1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125f1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125f1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125f1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125f1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125f1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125f20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125f208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125f20d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125f21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125f21640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125f21c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125f22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125f22b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125f23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125f237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125f23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125f243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125f249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125f251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125f25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125f25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125f263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125f26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125f26e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125f27320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125f277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125f27c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125f28100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125f285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125f28a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125f28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125f29380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125f29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125f29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125f2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125f2a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125f2ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125f2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125f2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125f2bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125f2c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125f2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125f2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125f2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125f2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125f2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125f2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125f2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125f2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125f2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125f2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125f2fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125f30050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125f305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125f30af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125f31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125f31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125f31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125f32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125f32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125f22260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125f329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125f331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125f336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125f33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125f34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125f346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125f34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125f35180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125f356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125f35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125f36170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125f366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125f36c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125f37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125f376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125f37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125f37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125f38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125f38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125f39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125f39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125f39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125f3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125f3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125f3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125f3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125f3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125f3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125f3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125f3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125f3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125f3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125f3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125f3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125f3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125f3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125f3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125f3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125f3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125f3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125f3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125f3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125f3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125f40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125f40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125f40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125f40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125f413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125f41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125f41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125f421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125f42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125f42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125f42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125f43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125f438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125f43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125f44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125f446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125f44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125f45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125f454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125f45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125f45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125f46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125f46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125f46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125f47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125f47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125f479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125f47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125f482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125f48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125f48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125f490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125f49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125f49a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125f49eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125f4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125f4a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125f4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125f4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125f4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125f4ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125f4bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125f4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125f4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125f4ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125f4d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125f4d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125f4dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125f4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125f4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125f4ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125f4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125f4f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125f4fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125f500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125f506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125f50cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125f512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125f51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125f51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125f52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125f52840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125f52e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125f53640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125f53ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125f53f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125f54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125f54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125f55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125f55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125f55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125f56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125f56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125f56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125f57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125f57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125f57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125f580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125f58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125f58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125f590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125f59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125f59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125f5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125f5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125f5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125f5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125f5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125f5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125f5c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125f5c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125f5cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125f5d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125f5d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125f5db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125f5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125f5e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125f5eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125f5f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125f5f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125f5fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125f60070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125f605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125f60b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125f61060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125f615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125f61b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125f62050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125f625a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125f62af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125f63040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125f63590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125f63ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125f64030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125f64580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125f64ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125f65020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125f65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125f65ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125f66010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125f66560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125f66ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125f67000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125f67550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125f679f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125f67e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125f68330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125f687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125f68c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125f69110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125f695b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125f69a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125f69ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125f6a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125f6a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125f6acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125f6b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125f6b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125f6bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125f6c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125f6c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125f6ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125f6d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125f6dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125f6df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125f6e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125f6e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125f6f000 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   106.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 944
init: graph splits = 2
get_kv_self: llama_context does not have a KV cache
0.00.068.348 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.068.352 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.068.352 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
get_kv_self: llama_context does not have a KV cache
main : serialized state into 201251 out of a maximum of 201251 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127004fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127005420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1270059e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127005f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127006540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127006af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1270070a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127007650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127007c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127008100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127008600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127008b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127009620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127009dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12700a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12700ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12700b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12700bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12700c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12700ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12700d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12700d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12700df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12700e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12700edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12700f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12700f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12700fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1270102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127010ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127010f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127011210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127011aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127011fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1270122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127012740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127012be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127013080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127013520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1270139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127013e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127014300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1270147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127014c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127014f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127015510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127015b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127016130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127016740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127016d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127017970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127017f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127018590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127018d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127019220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1270196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127019980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127019f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12701a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12701ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12701b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12701b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12701ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12701bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12701c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12701c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12701cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12701d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12701d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12701da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12701df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12701e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12701e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12701ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12701f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12701f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12701fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127020380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1270208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127020e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127021370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1270218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127021e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127022360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1270228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127022e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127023350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1270238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127023df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127024340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127024890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127024de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127025330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127025880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127026320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127026870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127026dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127027310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127027860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127027db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127028300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127028850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127028da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1270292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127029840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127029d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12702a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12702a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12702ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12702b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12702b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12702bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12702c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12702c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12702caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12702cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12702d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12702d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12702dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12702e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12702e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12702eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12702efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12702f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12702f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12702fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127030220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1270306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127030b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127031000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1270314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127031940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127031de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127032280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127032720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127032bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127033060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127033500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1270339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127033e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1270342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127034780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127034c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1270350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127035560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127035a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127035ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127036340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1270367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127036c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127037120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1270375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127037a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127037f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1270383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127038840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127038ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127039180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127039620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127039ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127039f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12703a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12703a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12703ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12703b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12703b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12703bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12703bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12703c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12703c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12703cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12703d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12703d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12703db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12703e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12703e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12703e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12703ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12703f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12703f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12703fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127040080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127040520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1270409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127040e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127041300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1270417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127041c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1270420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127042580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127042a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127042f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1270434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127043a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127043f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127044220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127044830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127044e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127045450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127045c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1270460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1270463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1270469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127046fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1270477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127047c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1270480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127048590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127048d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127049290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1270497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127049d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12704a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12704a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12704ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12704b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12704b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12704bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12704c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12704c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12704cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12704d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12704d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12704dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12704e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12704e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12704ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12704f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12704f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12704fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127050220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127050770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127050cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127051210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127051760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127051cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127052200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127052750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127052ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1270531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127053740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1270541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127054730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127054c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1270551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127055c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1270561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127056710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127056c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1270571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127057700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127057c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1270581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1270586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127058c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127059190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1270596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127059c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12705a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12705a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12705ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12705b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12705b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12705bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12705c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12705c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12705c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12705cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12705d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12705d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12705dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12705e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12705e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12705e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12705ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12705f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12705f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12705fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127060170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127060890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127060fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1270616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127061df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1270620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1270628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127062b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127063170 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   106.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 944
init: graph splits = 2
main : deserialized state from 201251 out of a maximum of 201251 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e056f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e05b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e05fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e06440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e068b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e06d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e07190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e07600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e07a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e07fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e08430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e08ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e09d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e0a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e0acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e0b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e0c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e0c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e0d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e0d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e0df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e0f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e0fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e11590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e11a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e11e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e12750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e13030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e14660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e14ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e17830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e18a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e19c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e1bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125e1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125e1c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125e1cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125e1d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125e1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125e1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125e1ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125e1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125e1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125e1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125e1f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125e1f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125e1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125e1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125e20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125e206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125e20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125e20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125e21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125e21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125e21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125e22160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125e225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125e22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125e22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125e23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125e23790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125e23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125e24070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125e244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125e24950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125e25190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125e25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125e258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125e25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125e261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125e26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125e26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125e27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125e3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125e402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e42e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e43100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e43ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e44580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e44e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e46020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e46d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e47650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e47ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e48c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e49560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e49e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e4ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e4caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e4cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e4e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e4e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e4ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e4ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e50450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e50d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e51610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e52360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125e527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125e52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e53990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e54270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e54fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e55430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e58a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e58cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125e59160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e59760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e59d70 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
init:      Metal compute buffer size =   106.25 MiB
init:        CPU compute buffer size =     5.01 MiB
init: graph nodes  = 944
init: graph splits = 2
main : deserialized state from 201251 out of a maximum of 201251 bytes
main : seq 0 copied, 0 bytes
get_kv_self: llama_context does not have a KV cache
main : kv cache cleared
main : seq 1 restored, 0 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L


second run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L


single seq run: The quick brown fox jumps wrong differentntscript__y'" "" + 0. You compute the L

real	0m0.756s
user	0m0.191s
sys	0m0.044s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
