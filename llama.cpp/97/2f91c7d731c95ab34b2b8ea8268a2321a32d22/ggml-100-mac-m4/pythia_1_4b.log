Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.670s
user	0m0.883s
sys	0m1.315s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Built target build_info
[  4%] Built target sha1
[  4%] Built target xxhash
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-simple
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 53%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Built target test-chat-template
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 63%] Built target test-backend-ops
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Built target test-rope
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-bench
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-cli
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-parallel
[ 81%] Built target llama-passkey
[ 81%] Built target llama-lookahead
[ 81%] Generating loading.html.hpp
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Built target llama-quantize
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-tts
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-tts
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.177s
user	0m6.587s
sys	0m9.603s

main: quantize time =  3198.82 ms
main:    total time =  3198.82 ms

main: quantize time =  2597.62 ms
main:    total time =  2597.62 ms

main: quantize time =  2056.82 ms
main:    total time =  2056.82 ms

main: quantize time =  2283.73 ms
main:    total time =  2283.73 ms

main: quantize time =  1612.56 ms
main:    total time =  1612.56 ms

main: quantize time =  5345.39 ms
main:    total time =  5345.39 ms

main: quantize time =  5923.64 ms
main:    total time =  5923.64 ms

main: quantize time =  7052.45 ms
main:    total time =  7052.45 ms

main: quantize time =  6870.55 ms
main:    total time =  6870.55 ms

main: quantize time =  4672.09 ms
main:    total time =  4672.09 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.218 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.373 I main: llama backend init
0.00.000.379 I main: load the model and apply lora adapter, if any
0.00.048.286 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.061.065 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.061.085 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.061.089 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.061.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.061.090 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.061.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.061.091 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.061.093 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.061.094 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.061.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.061.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.061.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.061.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.061.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.061.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.061.104 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.061.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.070.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.072.380 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.692 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.695 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.696 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.696 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.697 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.698 I llama_model_loader: - type  f32:  194 tensors
0.00.079.698 I llama_model_loader: - type  f16:   98 tensors
0.00.079.699 I print_info: file format = GGUF V3 (latest)
0.00.079.701 I print_info: file type   = all F32 (guessed)
0.00.079.702 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.093.564 I load: special tokens cache size = 25
0.00.101.892 I load: token to piece cache size = 0.2984 MB
0.00.101.895 I print_info: arch             = gptneox
0.00.101.895 I print_info: vocab_only       = 0
0.00.101.896 I print_info: n_ctx_train      = 2048
0.00.101.896 I print_info: n_embd           = 2048
0.00.101.896 I print_info: n_layer          = 24
0.00.101.899 I print_info: n_head           = 16
0.00.101.900 I print_info: n_head_kv        = 16
0.00.101.901 I print_info: n_rot            = 32
0.00.101.901 I print_info: n_swa            = 0
0.00.101.901 I print_info: n_embd_head_k    = 128
0.00.101.901 I print_info: n_embd_head_v    = 128
0.00.101.902 I print_info: n_gqa            = 1
0.00.101.903 I print_info: n_embd_k_gqa     = 2048
0.00.101.904 I print_info: n_embd_v_gqa     = 2048
0.00.101.904 I print_info: f_norm_eps       = 1.0e-05
0.00.101.905 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.101.905 I print_info: f_clamp_kqv      = 0.0e+00
0.00.101.905 I print_info: f_max_alibi_bias = 0.0e+00
0.00.101.905 I print_info: f_logit_scale    = 0.0e+00
0.00.101.906 I print_info: n_ff             = 8192
0.00.101.906 I print_info: n_expert         = 0
0.00.101.906 I print_info: n_expert_used    = 0
0.00.101.906 I print_info: causal attn      = 1
0.00.101.906 I print_info: pooling type     = 0
0.00.101.908 I print_info: rope type        = 2
0.00.101.908 I print_info: rope scaling     = linear
0.00.101.909 I print_info: freq_base_train  = 10000.0
0.00.101.909 I print_info: freq_scale_train = 1
0.00.101.909 I print_info: n_ctx_orig_yarn  = 2048
0.00.101.910 I print_info: rope_finetuned   = unknown
0.00.101.910 I print_info: ssm_d_conv       = 0
0.00.101.910 I print_info: ssm_d_inner      = 0
0.00.101.910 I print_info: ssm_d_state      = 0
0.00.101.910 I print_info: ssm_dt_rank      = 0
0.00.101.910 I print_info: ssm_dt_b_c_rms   = 0
0.00.101.911 I print_info: model type       = 1.4B
0.00.101.911 I print_info: model params     = 1.41 B
0.00.101.911 I print_info: general.name     = 1.4B
0.00.101.912 I print_info: vocab type       = BPE
0.00.101.912 I print_info: n_vocab          = 50304
0.00.101.912 I print_info: n_merges         = 50009
0.00.101.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.101.913 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.101.913 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.101.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.101.913 I print_info: LF token         = 187 'Ċ'
0.00.101.914 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.101.914 I print_info: max token length = 1024
0.00.101.914 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.149.658 I load_tensors: offloading 24 repeating layers to GPU
0.00.149.663 I load_tensors: offloading output layer to GPU
0.00.149.663 I load_tensors: offloaded 25/25 layers to GPU
0.00.149.687 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.149.688 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.150.250 I llama_context: n_seq_max     = 1
0.00.150.251 I llama_context: n_ctx         = 2048
0.00.150.251 I llama_context: n_ctx_per_seq = 2048
0.00.150.251 I llama_context: n_batch       = 2048
0.00.150.251 I llama_context: n_ubatch      = 512
0.00.150.252 I llama_context: flash_attn    = 0
0.00.150.252 I llama_context: freq_base     = 10000.0
0.00.150.252 I llama_context: freq_scale    = 1
0.00.150.253 I ggml_metal_init: allocating
0.00.150.319 I ggml_metal_init: found device: Apple M4
0.00.150.324 I ggml_metal_init: picking default device: Apple M4
0.00.151.035 I ggml_metal_init: using embedded metal library
0.00.160.812 I ggml_metal_init: GPU name:   Apple M4
0.00.160.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.160.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.160.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.160.814 I ggml_metal_init: simdgroup reduction   = true
0.00.160.814 I ggml_metal_init: simdgroup matrix mul. = true
0.00.160.815 I ggml_metal_init: has residency sets    = true
0.00.160.815 I ggml_metal_init: has bfloat            = true
0.00.160.815 I ggml_metal_init: use bfloat            = true
0.00.160.815 I ggml_metal_init: hasUnifiedMemory      = true
0.00.160.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.202.623 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.231.425 I init:      Metal KV buffer size =   384.00 MiB
0.00.231.432 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.231.458 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.235.692 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.235.694 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.235.694 I llama_context: graph nodes  = 967
0.00.235.695 I llama_context: graph splits = 2
0.00.235.700 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.235.828 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.235.829 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.294.293 I main: llama threadpool init, n_threads = 4
0.00.294.330 I 
0.00.294.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.294.360 I 
0.00.294.499 I sampler seed: 1234
0.00.294.503 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.294.526 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.294.527 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.294.528 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.139.037 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.02.139.037 I llama_perf_context_print:        load time =     245.17 ms
0.02.139.038 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.02.139.039 I llama_perf_context_print:        eval time =    1798.01 ms /    63 runs   (   28.54 ms per token,    35.04 tokens per second)
0.02.139.040 I llama_perf_context_print:       total time =    1845.57 ms /    70 tokens
0.02.142.620 I ggml_metal_free: deallocating

real	0m2.463s
user	0m0.130s
sys	0m0.137s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.075 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.114 I main: llama backend init
0.00.000.116 I main: load the model and apply lora adapter, if any
0.00.009.966 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.261 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.269 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.271 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.271 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.272 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.272 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.273 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.273 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.273 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.275 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.275 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.276 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.196 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.017 I llama_model_loader: - type  f32:  194 tensors
0.00.029.018 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.019 I print_info: file format = GGUF V3 (latest)
0.00.029.019 I print_info: file type   = Q8_0
0.00.029.021 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.037.551 I load: special tokens cache size = 25
0.00.043.493 I load: token to piece cache size = 0.2984 MB
0.00.043.497 I print_info: arch             = gptneox
0.00.043.497 I print_info: vocab_only       = 0
0.00.043.497 I print_info: n_ctx_train      = 2048
0.00.043.497 I print_info: n_embd           = 2048
0.00.043.498 I print_info: n_layer          = 24
0.00.043.504 I print_info: n_head           = 16
0.00.043.505 I print_info: n_head_kv        = 16
0.00.043.505 I print_info: n_rot            = 32
0.00.043.505 I print_info: n_swa            = 0
0.00.043.505 I print_info: n_embd_head_k    = 128
0.00.043.506 I print_info: n_embd_head_v    = 128
0.00.043.506 I print_info: n_gqa            = 1
0.00.043.507 I print_info: n_embd_k_gqa     = 2048
0.00.043.508 I print_info: n_embd_v_gqa     = 2048
0.00.043.508 I print_info: f_norm_eps       = 1.0e-05
0.00.043.513 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.513 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.513 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.513 I print_info: f_logit_scale    = 0.0e+00
0.00.043.514 I print_info: n_ff             = 8192
0.00.043.514 I print_info: n_expert         = 0
0.00.043.514 I print_info: n_expert_used    = 0
0.00.043.515 I print_info: causal attn      = 1
0.00.043.516 I print_info: pooling type     = 0
0.00.043.516 I print_info: rope type        = 2
0.00.043.517 I print_info: rope scaling     = linear
0.00.043.517 I print_info: freq_base_train  = 10000.0
0.00.043.517 I print_info: freq_scale_train = 1
0.00.043.518 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.518 I print_info: rope_finetuned   = unknown
0.00.043.518 I print_info: ssm_d_conv       = 0
0.00.043.518 I print_info: ssm_d_inner      = 0
0.00.043.518 I print_info: ssm_d_state      = 0
0.00.043.518 I print_info: ssm_dt_rank      = 0
0.00.043.518 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.519 I print_info: model type       = 1.4B
0.00.043.519 I print_info: model params     = 1.41 B
0.00.043.519 I print_info: general.name     = 1.4B
0.00.043.520 I print_info: vocab type       = BPE
0.00.043.520 I print_info: n_vocab          = 50304
0.00.043.520 I print_info: n_merges         = 50009
0.00.043.520 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.520 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.521 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.521 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.521 I print_info: LF token         = 187 'Ċ'
0.00.043.521 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.521 I print_info: max token length = 1024
0.00.043.522 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.189.405 I load_tensors: offloading 24 repeating layers to GPU
0.01.189.410 I load_tensors: offloading output layer to GPU
0.01.189.411 I load_tensors: offloaded 25/25 layers to GPU
0.01.189.435 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.189.438 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.190.274 I llama_context: n_seq_max     = 1
0.01.190.276 I llama_context: n_ctx         = 2048
0.01.190.277 I llama_context: n_ctx_per_seq = 2048
0.01.190.277 I llama_context: n_batch       = 2048
0.01.190.277 I llama_context: n_ubatch      = 512
0.01.190.278 I llama_context: flash_attn    = 0
0.01.190.278 I llama_context: freq_base     = 10000.0
0.01.190.279 I llama_context: freq_scale    = 1
0.01.190.280 I ggml_metal_init: allocating
0.01.190.294 I ggml_metal_init: found device: Apple M4
0.01.190.305 I ggml_metal_init: picking default device: Apple M4
0.01.191.510 I ggml_metal_init: using embedded metal library
0.01.196.802 I ggml_metal_init: GPU name:   Apple M4
0.01.196.805 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.196.806 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.196.807 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.196.807 I ggml_metal_init: simdgroup reduction   = true
0.01.196.808 I ggml_metal_init: simdgroup matrix mul. = true
0.01.196.808 I ggml_metal_init: has residency sets    = true
0.01.196.808 I ggml_metal_init: has bfloat            = true
0.01.196.808 I ggml_metal_init: use bfloat            = true
0.01.196.809 I ggml_metal_init: hasUnifiedMemory      = true
0.01.196.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.214.367 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.273.803 I init:      Metal KV buffer size =   384.00 MiB
0.01.273.813 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.273.840 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.278.935 I llama_context:      Metal compute buffer size =   102.25 MiB
0.01.278.937 I llama_context:        CPU compute buffer size =     8.01 MiB
0.01.278.937 I llama_context: graph nodes  = 967
0.01.278.938 I llama_context: graph splits = 2
0.01.278.943 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.279.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.279.073 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.334.599 I main: llama threadpool init, n_threads = 4
0.01.334.643 I 
0.01.334.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.334.669 I 
0.01.334.820 I sampler seed: 1234
0.01.334.825 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.334.863 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.334.867 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.334.867 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.439.145 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50318.92 tokens per second)
0.02.439.146 I llama_perf_context_print:        load time =    1323.90 ms
0.02.439.147 I llama_perf_context_print: prompt eval time =      49.23 ms /     7 tokens (    7.03 ms per token,   142.20 tokens per second)
0.02.439.148 I llama_perf_context_print:        eval time =    1052.34 ms /    63 runs   (   16.70 ms per token,    59.87 tokens per second)
0.02.439.148 I llama_perf_context_print:       total time =    1105.28 ms /    70 tokens
0.02.442.984 I ggml_metal_free: deallocating

real	0m2.462s
user	0m0.109s
sys	0m0.271s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.022.554 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.042.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.814 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.816 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.819 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.822 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.822 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.492 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.506 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.506 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.507 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.054.507 I llama_model_loader: - type  f32:  194 tensors
0.00.054.507 I llama_model_loader: - type q4_0:   97 tensors
0.00.054.508 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.508 I print_info: file format = GGUF V3 (latest)
0.00.054.509 I print_info: file type   = Q4_0
0.00.054.510 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.066.897 I load: special tokens cache size = 25
0.00.077.944 I load: token to piece cache size = 0.2984 MB
0.00.077.949 I print_info: arch             = gptneox
0.00.077.949 I print_info: vocab_only       = 0
0.00.077.950 I print_info: n_ctx_train      = 2048
0.00.077.950 I print_info: n_embd           = 2048
0.00.077.951 I print_info: n_layer          = 24
0.00.077.955 I print_info: n_head           = 16
0.00.077.959 I print_info: n_head_kv        = 16
0.00.077.959 I print_info: n_rot            = 32
0.00.077.959 I print_info: n_swa            = 0
0.00.077.960 I print_info: n_embd_head_k    = 128
0.00.077.960 I print_info: n_embd_head_v    = 128
0.00.077.961 I print_info: n_gqa            = 1
0.00.077.969 I print_info: n_embd_k_gqa     = 2048
0.00.077.970 I print_info: n_embd_v_gqa     = 2048
0.00.077.971 I print_info: f_norm_eps       = 1.0e-05
0.00.077.972 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.973 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.973 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.973 I print_info: f_logit_scale    = 0.0e+00
0.00.077.974 I print_info: n_ff             = 8192
0.00.077.974 I print_info: n_expert         = 0
0.00.077.977 I print_info: n_expert_used    = 0
0.00.077.977 I print_info: causal attn      = 1
0.00.077.977 I print_info: pooling type     = 0
0.00.077.977 I print_info: rope type        = 2
0.00.077.978 I print_info: rope scaling     = linear
0.00.077.978 I print_info: freq_base_train  = 10000.0
0.00.077.979 I print_info: freq_scale_train = 1
0.00.077.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.979 I print_info: rope_finetuned   = unknown
0.00.077.980 I print_info: ssm_d_conv       = 0
0.00.077.980 I print_info: ssm_d_inner      = 0
0.00.077.980 I print_info: ssm_d_state      = 0
0.00.077.980 I print_info: ssm_dt_rank      = 0
0.00.077.981 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.981 I print_info: model type       = 1.4B
0.00.077.982 I print_info: model params     = 1.41 B
0.00.077.982 I print_info: general.name     = 1.4B
0.00.077.983 I print_info: vocab type       = BPE
0.00.077.984 I print_info: n_vocab          = 50304
0.00.077.984 I print_info: n_merges         = 50009
0.00.077.985 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.987 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.988 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.988 I print_info: LF token         = 187 'Ċ'
0.00.077.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.989 I print_info: max token length = 1024
0.00.077.989 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.654.068 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.083 I load_tensors: offloading output layer to GPU
0.00.654.085 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.132 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.654.135 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.655.529 I llama_context: n_seq_max     = 1
0.00.655.532 I llama_context: n_ctx         = 2048
0.00.655.532 I llama_context: n_ctx_per_seq = 2048
0.00.655.533 I llama_context: n_batch       = 2048
0.00.655.533 I llama_context: n_ubatch      = 512
0.00.655.534 I llama_context: flash_attn    = 0
0.00.655.536 I llama_context: freq_base     = 10000.0
0.00.655.537 I llama_context: freq_scale    = 1
0.00.655.546 I ggml_metal_init: allocating
0.00.655.609 I ggml_metal_init: found device: Apple M4
0.00.655.624 I ggml_metal_init: picking default device: Apple M4
0.00.657.406 I ggml_metal_init: using embedded metal library
0.00.663.574 I ggml_metal_init: GPU name:   Apple M4
0.00.663.580 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.581 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.583 I ggml_metal_init: simdgroup reduction   = true
0.00.663.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.583 I ggml_metal_init: has residency sets    = true
0.00.663.584 I ggml_metal_init: has bfloat            = true
0.00.663.584 I ggml_metal_init: use bfloat            = true
0.00.663.585 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.176 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.884 I init:      Metal KV buffer size =   384.00 MiB
0.00.740.890 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.740.909 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.745.202 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.745.204 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.745.204 I llama_context: graph nodes  = 967
0.00.745.205 I llama_context: graph splits = 2
0.00.745.210 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.745.340 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.745.341 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.771 I main: llama threadpool init, n_threads = 4
0.00.798.818 I 
0.00.798.839 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.841 I 
0.00.798.994 I sampler seed: 1234
0.00.798.998 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.018 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.018 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.018 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.490.190 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.490.191 I llama_perf_context_print:        load time =     775.53 ms
0.01.490.191 I llama_perf_context_print: prompt eval time =      49.06 ms /     7 tokens (    7.01 ms per token,   142.69 tokens per second)
0.01.490.192 I llama_perf_context_print:        eval time =     639.23 ms /    63 runs   (   10.15 ms per token,    98.56 tokens per second)
0.01.490.192 I llama_perf_context_print:       total time =     692.10 ms /    70 tokens
0.01.494.234 I ggml_metal_free: deallocating

real	0m1.517s
user	0m0.124s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.691 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.813 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.813 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.817 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.817 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.818 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.819 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.819 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.822 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.822 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.838 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.744 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.745 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.745 I llama_model_loader: - type  f32:  194 tensors
0.00.036.746 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.746 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.746 I print_info: file format = GGUF V3 (latest)
0.00.036.747 I print_info: file type   = Q4_1
0.00.036.748 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.516 I load: special tokens cache size = 25
0.00.052.594 I load: token to piece cache size = 0.2984 MB
0.00.052.597 I print_info: arch             = gptneox
0.00.052.597 I print_info: vocab_only       = 0
0.00.052.597 I print_info: n_ctx_train      = 2048
0.00.052.597 I print_info: n_embd           = 2048
0.00.052.598 I print_info: n_layer          = 24
0.00.052.600 I print_info: n_head           = 16
0.00.052.601 I print_info: n_head_kv        = 16
0.00.052.601 I print_info: n_rot            = 32
0.00.052.601 I print_info: n_swa            = 0
0.00.052.602 I print_info: n_embd_head_k    = 128
0.00.052.602 I print_info: n_embd_head_v    = 128
0.00.052.603 I print_info: n_gqa            = 1
0.00.052.603 I print_info: n_embd_k_gqa     = 2048
0.00.052.604 I print_info: n_embd_v_gqa     = 2048
0.00.052.605 I print_info: f_norm_eps       = 1.0e-05
0.00.052.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.605 I print_info: f_logit_scale    = 0.0e+00
0.00.052.606 I print_info: n_ff             = 8192
0.00.052.606 I print_info: n_expert         = 0
0.00.052.606 I print_info: n_expert_used    = 0
0.00.052.606 I print_info: causal attn      = 1
0.00.052.606 I print_info: pooling type     = 0
0.00.052.608 I print_info: rope type        = 2
0.00.052.610 I print_info: rope scaling     = linear
0.00.052.610 I print_info: freq_base_train  = 10000.0
0.00.052.610 I print_info: freq_scale_train = 1
0.00.052.611 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.611 I print_info: rope_finetuned   = unknown
0.00.052.611 I print_info: ssm_d_conv       = 0
0.00.052.611 I print_info: ssm_d_inner      = 0
0.00.052.611 I print_info: ssm_d_state      = 0
0.00.052.611 I print_info: ssm_dt_rank      = 0
0.00.052.611 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.612 I print_info: model type       = 1.4B
0.00.052.612 I print_info: model params     = 1.41 B
0.00.052.612 I print_info: general.name     = 1.4B
0.00.052.613 I print_info: vocab type       = BPE
0.00.052.613 I print_info: n_vocab          = 50304
0.00.052.613 I print_info: n_merges         = 50009
0.00.052.613 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.613 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.614 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.614 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.614 I print_info: LF token         = 187 'Ċ'
0.00.052.614 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.615 I print_info: max token length = 1024
0.00.052.615 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.708.277 I load_tensors: offloading 24 repeating layers to GPU
0.00.708.294 I load_tensors: offloading output layer to GPU
0.00.708.296 I load_tensors: offloaded 25/25 layers to GPU
0.00.708.328 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.708.330 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.709.906 I llama_context: n_seq_max     = 1
0.00.709.909 I llama_context: n_ctx         = 2048
0.00.709.910 I llama_context: n_ctx_per_seq = 2048
0.00.709.910 I llama_context: n_batch       = 2048
0.00.709.911 I llama_context: n_ubatch      = 512
0.00.709.911 I llama_context: flash_attn    = 0
0.00.709.914 I llama_context: freq_base     = 10000.0
0.00.709.914 I llama_context: freq_scale    = 1
0.00.709.917 I ggml_metal_init: allocating
0.00.709.992 I ggml_metal_init: found device: Apple M4
0.00.710.006 I ggml_metal_init: picking default device: Apple M4
0.00.711.817 I ggml_metal_init: using embedded metal library
0.00.718.550 I ggml_metal_init: GPU name:   Apple M4
0.00.718.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.718.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.718.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.718.557 I ggml_metal_init: simdgroup reduction   = true
0.00.718.558 I ggml_metal_init: simdgroup matrix mul. = true
0.00.718.558 I ggml_metal_init: has residency sets    = true
0.00.718.558 I ggml_metal_init: has bfloat            = true
0.00.718.558 I ggml_metal_init: use bfloat            = true
0.00.718.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.718.561 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.736.476 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.787.926 I init:      Metal KV buffer size =   384.00 MiB
0.00.787.932 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.787.955 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.792.327 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.792.329 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.792.329 I llama_context: graph nodes  = 967
0.00.792.329 I llama_context: graph splits = 2
0.00.792.334 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.792.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.792.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.849.097 I main: llama threadpool init, n_threads = 4
0.00.849.137 I 
0.00.849.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.849.160 I 
0.00.849.311 I sampler seed: 1234
0.00.849.315 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.849.333 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.849.334 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.849.334 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.579.723 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.579.723 I llama_perf_context_print:        load time =     839.65 ms
0.01.579.725 I llama_perf_context_print: prompt eval time =      44.69 ms /     7 tokens (    6.38 ms per token,   156.65 tokens per second)
0.01.579.725 I llama_perf_context_print:        eval time =     683.16 ms /    63 runs   (   10.84 ms per token,    92.22 tokens per second)
0.01.579.726 I llama_perf_context_print:       total time =     731.38 ms /    70 tokens
0.01.583.835 I ggml_metal_free: deallocating

real	0m1.601s
user	0m0.111s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.011.391 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.975 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.980 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.981 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.982 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.987 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.989 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.989 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.990 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.991 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.994 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.564 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.565 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.565 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.566 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.566 I llama_model_loader: - type  f32:  194 tensors
0.00.029.566 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.567 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.567 I print_info: file format = GGUF V3 (latest)
0.00.029.567 I print_info: file type   = Q5_0
0.00.029.568 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.037.484 I load: special tokens cache size = 25
0.00.043.535 I load: token to piece cache size = 0.2984 MB
0.00.043.538 I print_info: arch             = gptneox
0.00.043.538 I print_info: vocab_only       = 0
0.00.043.538 I print_info: n_ctx_train      = 2048
0.00.043.539 I print_info: n_embd           = 2048
0.00.043.539 I print_info: n_layer          = 24
0.00.043.542 I print_info: n_head           = 16
0.00.043.542 I print_info: n_head_kv        = 16
0.00.043.542 I print_info: n_rot            = 32
0.00.043.543 I print_info: n_swa            = 0
0.00.043.544 I print_info: n_embd_head_k    = 128
0.00.043.544 I print_info: n_embd_head_v    = 128
0.00.043.545 I print_info: n_gqa            = 1
0.00.043.546 I print_info: n_embd_k_gqa     = 2048
0.00.043.547 I print_info: n_embd_v_gqa     = 2048
0.00.043.547 I print_info: f_norm_eps       = 1.0e-05
0.00.043.548 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.549 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.550 I print_info: f_logit_scale    = 0.0e+00
0.00.043.550 I print_info: n_ff             = 8192
0.00.043.550 I print_info: n_expert         = 0
0.00.043.551 I print_info: n_expert_used    = 0
0.00.043.551 I print_info: causal attn      = 1
0.00.043.551 I print_info: pooling type     = 0
0.00.043.552 I print_info: rope type        = 2
0.00.043.553 I print_info: rope scaling     = linear
0.00.043.553 I print_info: freq_base_train  = 10000.0
0.00.043.553 I print_info: freq_scale_train = 1
0.00.043.554 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.554 I print_info: rope_finetuned   = unknown
0.00.043.554 I print_info: ssm_d_conv       = 0
0.00.043.554 I print_info: ssm_d_inner      = 0
0.00.043.554 I print_info: ssm_d_state      = 0
0.00.043.554 I print_info: ssm_dt_rank      = 0
0.00.043.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.555 I print_info: model type       = 1.4B
0.00.043.555 I print_info: model params     = 1.41 B
0.00.043.555 I print_info: general.name     = 1.4B
0.00.043.556 I print_info: vocab type       = BPE
0.00.043.556 I print_info: n_vocab          = 50304
0.00.043.556 I print_info: n_merges         = 50009
0.00.043.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.557 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.557 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.557 I print_info: LF token         = 187 'Ċ'
0.00.043.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.558 I print_info: max token length = 1024
0.00.043.558 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.754.885 I load_tensors: offloading 24 repeating layers to GPU
0.00.754.898 I load_tensors: offloading output layer to GPU
0.00.754.899 I load_tensors: offloaded 25/25 layers to GPU
0.00.754.934 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.754.936 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.756.370 I llama_context: n_seq_max     = 1
0.00.756.373 I llama_context: n_ctx         = 2048
0.00.756.374 I llama_context: n_ctx_per_seq = 2048
0.00.756.374 I llama_context: n_batch       = 2048
0.00.756.374 I llama_context: n_ubatch      = 512
0.00.756.375 I llama_context: flash_attn    = 0
0.00.756.376 I llama_context: freq_base     = 10000.0
0.00.756.377 I llama_context: freq_scale    = 1
0.00.756.386 I ggml_metal_init: allocating
0.00.756.460 I ggml_metal_init: found device: Apple M4
0.00.756.474 I ggml_metal_init: picking default device: Apple M4
0.00.758.168 I ggml_metal_init: using embedded metal library
0.00.764.518 I ggml_metal_init: GPU name:   Apple M4
0.00.764.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.764.522 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.764.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.764.523 I ggml_metal_init: simdgroup reduction   = true
0.00.764.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.764.524 I ggml_metal_init: has residency sets    = true
0.00.764.524 I ggml_metal_init: has bfloat            = true
0.00.764.524 I ggml_metal_init: use bfloat            = true
0.00.764.525 I ggml_metal_init: hasUnifiedMemory      = true
0.00.764.527 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.782.257 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.839.048 I init:      Metal KV buffer size =   384.00 MiB
0.00.839.055 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.839.079 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.843.054 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.843.057 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.843.057 I llama_context: graph nodes  = 967
0.00.843.057 I llama_context: graph splits = 2
0.00.843.062 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.843.185 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.843.185 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.901.257 I main: llama threadpool init, n_threads = 4
0.00.901.304 I 
0.00.901.328 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.901.329 I 
0.00.901.483 I sampler seed: 1234
0.00.901.488 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.901.499 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.901.501 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.901.501 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.689.194 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.689.194 I llama_perf_context_print:        load time =     889.16 ms
0.01.689.195 I llama_perf_context_print: prompt eval time =      50.58 ms /     7 tokens (    7.23 ms per token,   138.39 tokens per second)
0.01.689.196 I llama_perf_context_print:        eval time =     734.09 ms /    63 runs   (   11.65 ms per token,    85.82 tokens per second)
0.01.689.196 I llama_perf_context_print:       total time =     788.64 ms /    70 tokens
0.01.693.158 I ggml_metal_free: deallocating

real	0m1.712s
user	0m0.108s
sys	0m0.226s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.239 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.738 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.423 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.109 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.112 I llama_model_loader: - type  f32:  194 tensors
0.00.025.112 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.113 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.113 I print_info: file format = GGUF V3 (latest)
0.00.025.114 I print_info: file type   = Q5_1
0.00.025.115 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.109 I load: special tokens cache size = 25
0.00.039.102 I load: token to piece cache size = 0.2984 MB
0.00.039.105 I print_info: arch             = gptneox
0.00.039.106 I print_info: vocab_only       = 0
0.00.039.106 I print_info: n_ctx_train      = 2048
0.00.039.106 I print_info: n_embd           = 2048
0.00.039.106 I print_info: n_layer          = 24
0.00.039.109 I print_info: n_head           = 16
0.00.039.110 I print_info: n_head_kv        = 16
0.00.039.110 I print_info: n_rot            = 32
0.00.039.110 I print_info: n_swa            = 0
0.00.039.112 I print_info: n_embd_head_k    = 128
0.00.039.112 I print_info: n_embd_head_v    = 128
0.00.039.113 I print_info: n_gqa            = 1
0.00.039.114 I print_info: n_embd_k_gqa     = 2048
0.00.039.114 I print_info: n_embd_v_gqa     = 2048
0.00.039.115 I print_info: f_norm_eps       = 1.0e-05
0.00.039.115 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.115 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.115 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.116 I print_info: f_logit_scale    = 0.0e+00
0.00.039.116 I print_info: n_ff             = 8192
0.00.039.117 I print_info: n_expert         = 0
0.00.039.117 I print_info: n_expert_used    = 0
0.00.039.117 I print_info: causal attn      = 1
0.00.039.117 I print_info: pooling type     = 0
0.00.039.117 I print_info: rope type        = 2
0.00.039.118 I print_info: rope scaling     = linear
0.00.039.118 I print_info: freq_base_train  = 10000.0
0.00.039.118 I print_info: freq_scale_train = 1
0.00.039.118 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.119 I print_info: rope_finetuned   = unknown
0.00.039.120 I print_info: ssm_d_conv       = 0
0.00.039.121 I print_info: ssm_d_inner      = 0
0.00.039.121 I print_info: ssm_d_state      = 0
0.00.039.121 I print_info: ssm_dt_rank      = 0
0.00.039.122 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.123 I print_info: model type       = 1.4B
0.00.039.124 I print_info: model params     = 1.41 B
0.00.039.124 I print_info: general.name     = 1.4B
0.00.039.124 I print_info: vocab type       = BPE
0.00.039.124 I print_info: n_vocab          = 50304
0.00.039.125 I print_info: n_merges         = 50009
0.00.039.125 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.125 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.125 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.127 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.127 I print_info: LF token         = 187 'Ċ'
0.00.039.127 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.127 I print_info: max token length = 1024
0.00.039.128 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.623.576 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.592 I load_tensors: offloading output layer to GPU
0.00.623.594 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.629 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.623.631 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.625.076 I llama_context: n_seq_max     = 1
0.00.625.080 I llama_context: n_ctx         = 2048
0.00.625.080 I llama_context: n_ctx_per_seq = 2048
0.00.625.080 I llama_context: n_batch       = 2048
0.00.625.081 I llama_context: n_ubatch      = 512
0.00.625.081 I llama_context: flash_attn    = 0
0.00.625.083 I llama_context: freq_base     = 10000.0
0.00.625.083 I llama_context: freq_scale    = 1
0.00.625.086 I ggml_metal_init: allocating
0.00.625.167 I ggml_metal_init: found device: Apple M4
0.00.625.180 I ggml_metal_init: picking default device: Apple M4
0.00.627.027 I ggml_metal_init: using embedded metal library
0.00.633.548 I ggml_metal_init: GPU name:   Apple M4
0.00.633.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.633.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.633.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.633.553 I ggml_metal_init: simdgroup reduction   = true
0.00.633.553 I ggml_metal_init: simdgroup matrix mul. = true
0.00.633.553 I ggml_metal_init: has residency sets    = true
0.00.633.554 I ggml_metal_init: has bfloat            = true
0.00.633.554 I ggml_metal_init: use bfloat            = true
0.00.633.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.633.556 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.921 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.552 I init:      Metal KV buffer size =   384.00 MiB
0.00.703.560 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.703.584 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.707.907 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.707.909 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.707.910 I llama_context: graph nodes  = 967
0.00.707.910 I llama_context: graph splits = 2
0.00.707.916 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.708.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.708.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.177 I main: llama threadpool init, n_threads = 4
0.00.769.221 I 
0.00.769.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.243 I 
0.00.769.398 I sampler seed: 1234
0.00.769.403 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.413 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.414 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.414 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.608.590 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.608.591 I llama_perf_context_print:        load time =     759.24 ms
0.01.608.591 I llama_perf_context_print: prompt eval time =      52.09 ms /     7 tokens (    7.44 ms per token,   134.38 tokens per second)
0.01.608.592 I llama_perf_context_print:        eval time =     784.16 ms /    63 runs   (   12.45 ms per token,    80.34 tokens per second)
0.01.608.596 I llama_perf_context_print:       total time =     840.11 ms /    70 tokens
0.01.612.555 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.107s
sys	0m0.223s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.429 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.055 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.056 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.063 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.063 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.851 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.718 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.719 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.720 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.720 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.720 I llama_model_loader: - type  f32:  194 tensors
0.00.024.721 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.721 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.721 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.722 I print_info: file format = GGUF V3 (latest)
0.00.024.722 I print_info: file type   = Q2_K - Medium
0.00.024.723 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.924 I load: special tokens cache size = 25
0.00.039.041 I load: token to piece cache size = 0.2984 MB
0.00.039.044 I print_info: arch             = gptneox
0.00.039.044 I print_info: vocab_only       = 0
0.00.039.044 I print_info: n_ctx_train      = 2048
0.00.039.045 I print_info: n_embd           = 2048
0.00.039.045 I print_info: n_layer          = 24
0.00.039.048 I print_info: n_head           = 16
0.00.039.055 I print_info: n_head_kv        = 16
0.00.039.055 I print_info: n_rot            = 32
0.00.039.055 I print_info: n_swa            = 0
0.00.039.055 I print_info: n_embd_head_k    = 128
0.00.039.056 I print_info: n_embd_head_v    = 128
0.00.039.057 I print_info: n_gqa            = 1
0.00.039.057 I print_info: n_embd_k_gqa     = 2048
0.00.039.058 I print_info: n_embd_v_gqa     = 2048
0.00.039.059 I print_info: f_norm_eps       = 1.0e-05
0.00.039.059 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.060 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.060 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.060 I print_info: f_logit_scale    = 0.0e+00
0.00.039.061 I print_info: n_ff             = 8192
0.00.039.061 I print_info: n_expert         = 0
0.00.039.061 I print_info: n_expert_used    = 0
0.00.039.065 I print_info: causal attn      = 1
0.00.039.068 I print_info: pooling type     = 0
0.00.039.069 I print_info: rope type        = 2
0.00.039.070 I print_info: rope scaling     = linear
0.00.039.071 I print_info: freq_base_train  = 10000.0
0.00.039.072 I print_info: freq_scale_train = 1
0.00.039.072 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.072 I print_info: rope_finetuned   = unknown
0.00.039.072 I print_info: ssm_d_conv       = 0
0.00.039.072 I print_info: ssm_d_inner      = 0
0.00.039.073 I print_info: ssm_d_state      = 0
0.00.039.073 I print_info: ssm_dt_rank      = 0
0.00.039.073 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.073 I print_info: model type       = 1.4B
0.00.039.075 I print_info: model params     = 1.41 B
0.00.039.075 I print_info: general.name     = 1.4B
0.00.039.076 I print_info: vocab type       = BPE
0.00.039.076 I print_info: n_vocab          = 50304
0.00.039.076 I print_info: n_merges         = 50009
0.00.039.078 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.078 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.078 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.078 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.079 I print_info: LF token         = 187 'Ċ'
0.00.039.080 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.081 I print_info: max token length = 1024
0.00.039.081 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.359.130 I load_tensors: offloading 24 repeating layers to GPU
0.00.359.145 I load_tensors: offloading output layer to GPU
0.00.359.146 I load_tensors: offloaded 25/25 layers to GPU
0.00.359.179 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.359.180 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.360.890 I llama_context: n_seq_max     = 1
0.00.360.893 I llama_context: n_ctx         = 2048
0.00.360.893 I llama_context: n_ctx_per_seq = 2048
0.00.360.894 I llama_context: n_batch       = 2048
0.00.360.894 I llama_context: n_ubatch      = 512
0.00.360.895 I llama_context: flash_attn    = 0
0.00.360.897 I llama_context: freq_base     = 10000.0
0.00.360.897 I llama_context: freq_scale    = 1
0.00.360.900 I ggml_metal_init: allocating
0.00.360.960 I ggml_metal_init: found device: Apple M4
0.00.360.974 I ggml_metal_init: picking default device: Apple M4
0.00.362.848 I ggml_metal_init: using embedded metal library
0.00.368.344 I ggml_metal_init: GPU name:   Apple M4
0.00.368.357 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.368.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.368.359 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.368.359 I ggml_metal_init: simdgroup reduction   = true
0.00.368.360 I ggml_metal_init: simdgroup matrix mul. = true
0.00.368.360 I ggml_metal_init: has residency sets    = true
0.00.368.360 I ggml_metal_init: has bfloat            = true
0.00.368.360 I ggml_metal_init: use bfloat            = true
0.00.368.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.368.367 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.389.522 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.450.922 I init:      Metal KV buffer size =   384.00 MiB
0.00.450.931 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.450.962 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.455.660 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.455.662 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.455.662 I llama_context: graph nodes  = 967
0.00.455.662 I llama_context: graph splits = 2
0.00.455.666 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.455.792 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.455.792 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.161 I main: llama threadpool init, n_threads = 4
0.00.513.204 I 
0.00.513.225 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.226 I 
0.00.513.396 I sampler seed: 1234
0.00.513.400 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.421 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.421 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.421 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.184.021 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.184.022 I llama_perf_context_print:        load time =     503.04 ms
0.01.184.022 I llama_perf_context_print: prompt eval time =      35.43 ms /     7 tokens (    5.06 ms per token,   197.55 tokens per second)
0.01.184.023 I llama_perf_context_print:        eval time =     632.33 ms /    63 runs   (   10.04 ms per token,    99.63 tokens per second)
0.01.184.024 I llama_perf_context_print:       total time =     671.55 ms /    70 tokens
0.01.187.915 I ggml_metal_free: deallocating

real	0m1.204s
user	0m0.112s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.652 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.274 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.280 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.286 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.286 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.287 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.287 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.093 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.851 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.851 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.852 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.852 I llama_model_loader: - type  f32:  194 tensors
0.00.024.853 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.853 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.853 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.853 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.854 I print_info: file format = GGUF V3 (latest)
0.00.024.854 I print_info: file type   = Q3_K - Medium
0.00.024.855 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.683 I load: special tokens cache size = 25
0.00.038.890 I load: token to piece cache size = 0.2984 MB
0.00.038.892 I print_info: arch             = gptneox
0.00.038.893 I print_info: vocab_only       = 0
0.00.038.893 I print_info: n_ctx_train      = 2048
0.00.038.893 I print_info: n_embd           = 2048
0.00.038.893 I print_info: n_layer          = 24
0.00.038.897 I print_info: n_head           = 16
0.00.038.898 I print_info: n_head_kv        = 16
0.00.038.898 I print_info: n_rot            = 32
0.00.038.898 I print_info: n_swa            = 0
0.00.038.898 I print_info: n_embd_head_k    = 128
0.00.038.901 I print_info: n_embd_head_v    = 128
0.00.038.901 I print_info: n_gqa            = 1
0.00.038.902 I print_info: n_embd_k_gqa     = 2048
0.00.038.912 I print_info: n_embd_v_gqa     = 2048
0.00.038.913 I print_info: f_norm_eps       = 1.0e-05
0.00.038.913 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.914 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.914 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.914 I print_info: f_logit_scale    = 0.0e+00
0.00.038.916 I print_info: n_ff             = 8192
0.00.038.916 I print_info: n_expert         = 0
0.00.038.916 I print_info: n_expert_used    = 0
0.00.038.918 I print_info: causal attn      = 1
0.00.038.919 I print_info: pooling type     = 0
0.00.038.919 I print_info: rope type        = 2
0.00.038.919 I print_info: rope scaling     = linear
0.00.038.920 I print_info: freq_base_train  = 10000.0
0.00.038.920 I print_info: freq_scale_train = 1
0.00.038.920 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.921 I print_info: rope_finetuned   = unknown
0.00.038.921 I print_info: ssm_d_conv       = 0
0.00.038.921 I print_info: ssm_d_inner      = 0
0.00.038.921 I print_info: ssm_d_state      = 0
0.00.038.921 I print_info: ssm_dt_rank      = 0
0.00.038.921 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.921 I print_info: model type       = 1.4B
0.00.038.922 I print_info: model params     = 1.41 B
0.00.038.923 I print_info: general.name     = 1.4B
0.00.038.923 I print_info: vocab type       = BPE
0.00.038.923 I print_info: n_vocab          = 50304
0.00.038.924 I print_info: n_merges         = 50009
0.00.038.924 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.924 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.924 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.924 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.925 I print_info: LF token         = 187 'Ċ'
0.00.038.925 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.925 I print_info: max token length = 1024
0.00.038.925 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.938 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.954 I load_tensors: offloading output layer to GPU
0.00.440.954 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.985 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.986 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.349 I llama_context: n_seq_max     = 1
0.00.442.355 I llama_context: n_ctx         = 2048
0.00.442.356 I llama_context: n_ctx_per_seq = 2048
0.00.442.356 I llama_context: n_batch       = 2048
0.00.442.357 I llama_context: n_ubatch      = 512
0.00.442.357 I llama_context: flash_attn    = 0
0.00.442.359 I llama_context: freq_base     = 10000.0
0.00.442.359 I llama_context: freq_scale    = 1
0.00.442.361 I ggml_metal_init: allocating
0.00.442.401 I ggml_metal_init: found device: Apple M4
0.00.442.420 I ggml_metal_init: picking default device: Apple M4
0.00.444.351 I ggml_metal_init: using embedded metal library
0.00.451.114 I ggml_metal_init: GPU name:   Apple M4
0.00.451.128 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.129 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.130 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.130 I ggml_metal_init: simdgroup reduction   = true
0.00.451.131 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.131 I ggml_metal_init: has residency sets    = true
0.00.451.131 I ggml_metal_init: has bfloat            = true
0.00.451.131 I ggml_metal_init: use bfloat            = true
0.00.451.137 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.142 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.510 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.528.585 I init:      Metal KV buffer size =   384.00 MiB
0.00.528.592 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.528.625 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.532.708 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.532.710 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.532.710 I llama_context: graph nodes  = 967
0.00.532.711 I llama_context: graph splits = 2
0.00.532.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.532.856 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.532.856 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.598 I main: llama threadpool init, n_threads = 4
0.00.590.638 I 
0.00.590.659 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.659 I 
0.00.590.827 I sampler seed: 1234
0.00.590.831 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.590.842 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.590.843 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.590.843 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.329.037 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48696.84 tokens per second)
0.01.329.038 I llama_perf_context_print:        load time =     581.26 ms
0.01.329.039 I llama_perf_context_print: prompt eval time =      44.63 ms /     7 tokens (    6.38 ms per token,   156.83 tokens per second)
0.01.329.039 I llama_perf_context_print:        eval time =     691.12 ms /    63 runs   (   10.97 ms per token,    91.16 tokens per second)
0.01.329.040 I llama_perf_context_print:       total time =     739.13 ms /    70 tokens
0.01.331.985 I ggml_metal_free: deallocating

real	0m1.346s
user	0m0.111s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.029 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.618 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.625 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.626 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.628 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.629 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.630 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.634 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.585 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.499 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.501 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.502 I llama_model_loader: - type  f32:  194 tensors
0.00.026.502 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.502 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.502 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.508 I print_info: file format = GGUF V3 (latest)
0.00.026.508 I print_info: file type   = Q4_K - Medium
0.00.026.509 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.972 I load: special tokens cache size = 25
0.00.041.101 I load: token to piece cache size = 0.2984 MB
0.00.041.105 I print_info: arch             = gptneox
0.00.041.106 I print_info: vocab_only       = 0
0.00.041.106 I print_info: n_ctx_train      = 2048
0.00.041.106 I print_info: n_embd           = 2048
0.00.041.106 I print_info: n_layer          = 24
0.00.041.110 I print_info: n_head           = 16
0.00.041.111 I print_info: n_head_kv        = 16
0.00.041.111 I print_info: n_rot            = 32
0.00.041.111 I print_info: n_swa            = 0
0.00.041.111 I print_info: n_embd_head_k    = 128
0.00.041.112 I print_info: n_embd_head_v    = 128
0.00.041.112 I print_info: n_gqa            = 1
0.00.041.113 I print_info: n_embd_k_gqa     = 2048
0.00.041.114 I print_info: n_embd_v_gqa     = 2048
0.00.041.114 I print_info: f_norm_eps       = 1.0e-05
0.00.041.115 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.115 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.115 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.115 I print_info: f_logit_scale    = 0.0e+00
0.00.041.118 I print_info: n_ff             = 8192
0.00.041.118 I print_info: n_expert         = 0
0.00.041.119 I print_info: n_expert_used    = 0
0.00.041.119 I print_info: causal attn      = 1
0.00.041.120 I print_info: pooling type     = 0
0.00.041.121 I print_info: rope type        = 2
0.00.041.122 I print_info: rope scaling     = linear
0.00.041.122 I print_info: freq_base_train  = 10000.0
0.00.041.122 I print_info: freq_scale_train = 1
0.00.041.122 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.123 I print_info: rope_finetuned   = unknown
0.00.041.123 I print_info: ssm_d_conv       = 0
0.00.041.123 I print_info: ssm_d_inner      = 0
0.00.041.123 I print_info: ssm_d_state      = 0
0.00.041.123 I print_info: ssm_dt_rank      = 0
0.00.041.123 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.123 I print_info: model type       = 1.4B
0.00.041.124 I print_info: model params     = 1.41 B
0.00.041.124 I print_info: general.name     = 1.4B
0.00.041.124 I print_info: vocab type       = BPE
0.00.041.124 I print_info: n_vocab          = 50304
0.00.041.125 I print_info: n_merges         = 50009
0.00.041.125 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.125 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.125 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.125 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.125 I print_info: LF token         = 187 'Ċ'
0.00.041.126 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.126 I print_info: max token length = 1024
0.00.041.126 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.737 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.745 I load_tensors: offloading output layer to GPU
0.00.538.745 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.764 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.765 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.539.518 I llama_context: n_seq_max     = 1
0.00.539.521 I llama_context: n_ctx         = 2048
0.00.539.521 I llama_context: n_ctx_per_seq = 2048
0.00.539.522 I llama_context: n_batch       = 2048
0.00.539.522 I llama_context: n_ubatch      = 512
0.00.539.522 I llama_context: flash_attn    = 0
0.00.539.524 I llama_context: freq_base     = 10000.0
0.00.539.524 I llama_context: freq_scale    = 1
0.00.539.525 I ggml_metal_init: allocating
0.00.539.571 I ggml_metal_init: found device: Apple M4
0.00.539.582 I ggml_metal_init: picking default device: Apple M4
0.00.540.719 I ggml_metal_init: using embedded metal library
0.00.545.042 I ggml_metal_init: GPU name:   Apple M4
0.00.545.048 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.545.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.545.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.545.050 I ggml_metal_init: simdgroup reduction   = true
0.00.545.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.545.051 I ggml_metal_init: has residency sets    = true
0.00.545.051 I ggml_metal_init: has bfloat            = true
0.00.545.051 I ggml_metal_init: use bfloat            = true
0.00.545.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.545.056 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.560.945 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.592.037 I init:      Metal KV buffer size =   384.00 MiB
0.00.592.044 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.592.075 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.597.210 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.597.211 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.597.211 I llama_context: graph nodes  = 967
0.00.597.212 I llama_context: graph splits = 2
0.00.597.217 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.597.349 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.597.349 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.525 I main: llama threadpool init, n_threads = 4
0.00.655.564 I 
0.00.655.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.582 I 
0.00.655.755 I sampler seed: 1234
0.00.655.760 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.655.771 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.655.771 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.655.775 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.426.185 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47301.80 tokens per second)
0.01.426.186 I llama_perf_context_print:        load time =     645.81 ms
0.01.426.187 I llama_perf_context_print: prompt eval time =      57.43 ms /     7 tokens (    8.20 ms per token,   121.89 tokens per second)
0.01.426.187 I llama_perf_context_print:        eval time =     710.34 ms /    63 runs   (   11.28 ms per token,    88.69 tokens per second)
0.01.426.188 I llama_perf_context_print:       total time =     771.35 ms /    70 tokens
0.01.430.033 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.105s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.199 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.204 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.206 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.206 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.207 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.212 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.710 I llama_model_loader: - type  f32:  194 tensors
0.00.025.710 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.710 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.711 I print_info: file format = GGUF V3 (latest)
0.00.025.711 I print_info: file type   = Q5_K - Medium
0.00.025.714 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.673 I load: special tokens cache size = 25
0.00.039.654 I load: token to piece cache size = 0.2984 MB
0.00.039.656 I print_info: arch             = gptneox
0.00.039.656 I print_info: vocab_only       = 0
0.00.039.657 I print_info: n_ctx_train      = 2048
0.00.039.657 I print_info: n_embd           = 2048
0.00.039.657 I print_info: n_layer          = 24
0.00.039.660 I print_info: n_head           = 16
0.00.039.660 I print_info: n_head_kv        = 16
0.00.039.660 I print_info: n_rot            = 32
0.00.039.661 I print_info: n_swa            = 0
0.00.039.661 I print_info: n_embd_head_k    = 128
0.00.039.663 I print_info: n_embd_head_v    = 128
0.00.039.664 I print_info: n_gqa            = 1
0.00.039.664 I print_info: n_embd_k_gqa     = 2048
0.00.039.665 I print_info: n_embd_v_gqa     = 2048
0.00.039.666 I print_info: f_norm_eps       = 1.0e-05
0.00.039.667 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.667 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.667 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.667 I print_info: f_logit_scale    = 0.0e+00
0.00.039.668 I print_info: n_ff             = 8192
0.00.039.668 I print_info: n_expert         = 0
0.00.039.668 I print_info: n_expert_used    = 0
0.00.039.669 I print_info: causal attn      = 1
0.00.039.669 I print_info: pooling type     = 0
0.00.039.670 I print_info: rope type        = 2
0.00.039.672 I print_info: rope scaling     = linear
0.00.039.672 I print_info: freq_base_train  = 10000.0
0.00.039.673 I print_info: freq_scale_train = 1
0.00.039.673 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.673 I print_info: rope_finetuned   = unknown
0.00.039.673 I print_info: ssm_d_conv       = 0
0.00.039.673 I print_info: ssm_d_inner      = 0
0.00.039.673 I print_info: ssm_d_state      = 0
0.00.039.674 I print_info: ssm_dt_rank      = 0
0.00.039.674 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.674 I print_info: model type       = 1.4B
0.00.039.675 I print_info: model params     = 1.41 B
0.00.039.675 I print_info: general.name     = 1.4B
0.00.039.675 I print_info: vocab type       = BPE
0.00.039.676 I print_info: n_vocab          = 50304
0.00.039.677 I print_info: n_merges         = 50009
0.00.039.677 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.677 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.678 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.678 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.678 I print_info: LF token         = 187 'Ċ'
0.00.039.678 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.679 I print_info: max token length = 1024
0.00.039.679 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.595.700 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.714 I load_tensors: offloading output layer to GPU
0.00.595.715 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.748 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.595.753 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.597.197 I llama_context: n_seq_max     = 1
0.00.597.200 I llama_context: n_ctx         = 2048
0.00.597.201 I llama_context: n_ctx_per_seq = 2048
0.00.597.201 I llama_context: n_batch       = 2048
0.00.597.202 I llama_context: n_ubatch      = 512
0.00.597.202 I llama_context: flash_attn    = 0
0.00.597.203 I llama_context: freq_base     = 10000.0
0.00.597.204 I llama_context: freq_scale    = 1
0.00.597.205 I ggml_metal_init: allocating
0.00.597.216 I ggml_metal_init: found device: Apple M4
0.00.597.229 I ggml_metal_init: picking default device: Apple M4
0.00.598.713 I ggml_metal_init: using embedded metal library
0.00.604.980 I ggml_metal_init: GPU name:   Apple M4
0.00.604.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.984 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.985 I ggml_metal_init: simdgroup reduction   = true
0.00.604.986 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.986 I ggml_metal_init: has residency sets    = true
0.00.604.986 I ggml_metal_init: has bfloat            = true
0.00.604.986 I ggml_metal_init: use bfloat            = true
0.00.604.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.989 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.712 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.698 I init:      Metal KV buffer size =   384.00 MiB
0.00.678.709 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.678.733 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.683.384 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.683.387 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.683.387 I llama_context: graph nodes  = 967
0.00.683.387 I llama_context: graph splits = 2
0.00.683.393 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.683.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.683.515 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.729 I main: llama threadpool init, n_threads = 4
0.00.747.773 I 
0.00.747.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.797 I 
0.00.747.942 I sampler seed: 1234
0.00.747.947 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.957 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.961 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.961 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.598.326 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.598.327 I llama_perf_context_print:        load time =     737.23 ms
0.01.598.329 I llama_perf_context_print: prompt eval time =      51.31 ms /     7 tokens (    7.33 ms per token,   136.42 tokens per second)
0.01.598.329 I llama_perf_context_print:        eval time =     796.14 ms /    63 runs   (   12.64 ms per token,    79.13 tokens per second)
0.01.598.330 I llama_perf_context_print:       total time =     851.29 ms /    70 tokens
0.01.602.222 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.106s
sys	0m0.218s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.716 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.214 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.220 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.227 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.227 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.228 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.229 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.232 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.093 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.825 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.825 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.826 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.826 I llama_model_loader: - type  f32:  194 tensors
0.00.024.826 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.827 I print_info: file format = GGUF V3 (latest)
0.00.024.827 I print_info: file type   = Q6_K
0.00.024.828 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.638 I load: special tokens cache size = 25
0.00.038.669 I load: token to piece cache size = 0.2984 MB
0.00.038.672 I print_info: arch             = gptneox
0.00.038.672 I print_info: vocab_only       = 0
0.00.038.673 I print_info: n_ctx_train      = 2048
0.00.038.673 I print_info: n_embd           = 2048
0.00.038.673 I print_info: n_layer          = 24
0.00.038.676 I print_info: n_head           = 16
0.00.038.677 I print_info: n_head_kv        = 16
0.00.038.677 I print_info: n_rot            = 32
0.00.038.677 I print_info: n_swa            = 0
0.00.038.677 I print_info: n_embd_head_k    = 128
0.00.038.680 I print_info: n_embd_head_v    = 128
0.00.038.680 I print_info: n_gqa            = 1
0.00.038.681 I print_info: n_embd_k_gqa     = 2048
0.00.038.682 I print_info: n_embd_v_gqa     = 2048
0.00.038.682 I print_info: f_norm_eps       = 1.0e-05
0.00.038.682 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.683 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.683 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.683 I print_info: f_logit_scale    = 0.0e+00
0.00.038.684 I print_info: n_ff             = 8192
0.00.038.684 I print_info: n_expert         = 0
0.00.038.684 I print_info: n_expert_used    = 0
0.00.038.684 I print_info: causal attn      = 1
0.00.038.684 I print_info: pooling type     = 0
0.00.038.684 I print_info: rope type        = 2
0.00.038.686 I print_info: rope scaling     = linear
0.00.038.687 I print_info: freq_base_train  = 10000.0
0.00.038.688 I print_info: freq_scale_train = 1
0.00.038.688 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.688 I print_info: rope_finetuned   = unknown
0.00.038.688 I print_info: ssm_d_conv       = 0
0.00.038.688 I print_info: ssm_d_inner      = 0
0.00.038.689 I print_info: ssm_d_state      = 0
0.00.038.689 I print_info: ssm_dt_rank      = 0
0.00.038.689 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.693 I print_info: model type       = 1.4B
0.00.038.694 I print_info: model params     = 1.41 B
0.00.038.694 I print_info: general.name     = 1.4B
0.00.038.694 I print_info: vocab type       = BPE
0.00.038.695 I print_info: n_vocab          = 50304
0.00.038.695 I print_info: n_merges         = 50009
0.00.038.697 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.698 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.698 I print_info: LF token         = 187 'Ċ'
0.00.038.698 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.698 I print_info: max token length = 1024
0.00.038.699 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.250 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.255 I load_tensors: offloading output layer to GPU
0.00.653.257 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.281 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.653.283 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.654.771 I llama_context: n_seq_max     = 1
0.00.654.773 I llama_context: n_ctx         = 2048
0.00.654.774 I llama_context: n_ctx_per_seq = 2048
0.00.654.775 I llama_context: n_batch       = 2048
0.00.654.775 I llama_context: n_ubatch      = 512
0.00.654.775 I llama_context: flash_attn    = 0
0.00.654.776 I llama_context: freq_base     = 10000.0
0.00.654.777 I llama_context: freq_scale    = 1
0.00.654.778 I ggml_metal_init: allocating
0.00.654.801 I ggml_metal_init: found device: Apple M4
0.00.654.812 I ggml_metal_init: picking default device: Apple M4
0.00.656.320 I ggml_metal_init: using embedded metal library
0.00.662.064 I ggml_metal_init: GPU name:   Apple M4
0.00.662.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.662.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.662.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.662.069 I ggml_metal_init: simdgroup reduction   = true
0.00.662.069 I ggml_metal_init: simdgroup matrix mul. = true
0.00.662.069 I ggml_metal_init: has residency sets    = true
0.00.662.070 I ggml_metal_init: has bfloat            = true
0.00.662.070 I ggml_metal_init: use bfloat            = true
0.00.662.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.662.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.561 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.184 I init:      Metal KV buffer size =   384.00 MiB
0.00.727.190 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.727.256 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.731.308 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.731.310 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.731.310 I llama_context: graph nodes  = 967
0.00.731.311 I llama_context: graph splits = 2
0.00.731.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.731.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.655 I main: llama threadpool init, n_threads = 4
0.00.798.700 I 
0.00.798.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.722 I 
0.00.798.875 I sampler seed: 1234
0.00.798.879 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.924 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.928 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.928 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.681.014 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.681.014 I llama_perf_context_print:        load time =     789.24 ms
0.01.681.016 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.78 tokens per second)
0.01.681.016 I llama_perf_context_print:        eval time =     824.85 ms /    63 runs   (   13.09 ms per token,    76.38 tokens per second)
0.01.681.017 I llama_perf_context_print:       total time =     883.06 ms /    70 tokens
0.01.684.914 I ggml_metal_free: deallocating

real	0m1.701s
user	0m0.108s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.648 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.161 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.166 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.172 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.172 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.175 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.177 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.803 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.807 I llama_model_loader: - type  f32:  194 tensors
0.00.050.807 I llama_model_loader: - type  f16:   98 tensors
0.00.050.808 I print_info: file format = GGUF V3 (latest)
0.00.050.809 I print_info: file type   = all F32 (guessed)
0.00.050.816 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.902 I load: special tokens cache size = 25
0.00.070.568 I load: token to piece cache size = 0.2984 MB
0.00.070.571 I print_info: arch             = gptneox
0.00.070.571 I print_info: vocab_only       = 0
0.00.070.572 I print_info: n_ctx_train      = 2048
0.00.070.572 I print_info: n_embd           = 2048
0.00.070.572 I print_info: n_layer          = 24
0.00.070.575 I print_info: n_head           = 16
0.00.070.576 I print_info: n_head_kv        = 16
0.00.070.576 I print_info: n_rot            = 32
0.00.070.577 I print_info: n_swa            = 0
0.00.070.577 I print_info: n_embd_head_k    = 128
0.00.070.577 I print_info: n_embd_head_v    = 128
0.00.070.578 I print_info: n_gqa            = 1
0.00.070.578 I print_info: n_embd_k_gqa     = 2048
0.00.070.579 I print_info: n_embd_v_gqa     = 2048
0.00.070.580 I print_info: f_norm_eps       = 1.0e-05
0.00.070.580 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.580 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.581 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.581 I print_info: f_logit_scale    = 0.0e+00
0.00.070.582 I print_info: n_ff             = 8192
0.00.070.582 I print_info: n_expert         = 0
0.00.070.582 I print_info: n_expert_used    = 0
0.00.070.582 I print_info: causal attn      = 1
0.00.070.582 I print_info: pooling type     = 0
0.00.070.582 I print_info: rope type        = 2
0.00.070.585 I print_info: rope scaling     = linear
0.00.070.585 I print_info: freq_base_train  = 10000.0
0.00.070.585 I print_info: freq_scale_train = 1
0.00.070.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.586 I print_info: rope_finetuned   = unknown
0.00.070.586 I print_info: ssm_d_conv       = 0
0.00.070.586 I print_info: ssm_d_inner      = 0
0.00.070.586 I print_info: ssm_d_state      = 0
0.00.070.586 I print_info: ssm_dt_rank      = 0
0.00.070.586 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.586 I print_info: model type       = 1.4B
0.00.070.587 I print_info: model params     = 1.41 B
0.00.070.587 I print_info: general.name     = 1.4B
0.00.070.588 I print_info: vocab type       = BPE
0.00.070.588 I print_info: n_vocab          = 50304
0.00.070.588 I print_info: n_merges         = 50009
0.00.070.588 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.588 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.588 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.589 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.589 I print_info: LF token         = 187 'Ċ'
0.00.070.589 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.589 I print_info: max token length = 1024
0.00.070.592 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.443.088 I load_tensors: offloading 24 repeating layers to GPU
0.01.443.091 I load_tensors: offloading output layer to GPU
0.01.443.092 I load_tensors: offloaded 25/25 layers to GPU
0.01.443.112 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.443.114 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.444.156 I llama_context: n_seq_max     = 1
0.01.444.157 I llama_context: n_ctx         = 128
0.01.444.157 I llama_context: n_ctx_per_seq = 128
0.01.444.157 I llama_context: n_batch       = 128
0.01.444.158 I llama_context: n_ubatch      = 128
0.01.444.158 I llama_context: flash_attn    = 0
0.01.444.158 I llama_context: freq_base     = 10000.0
0.01.444.159 I llama_context: freq_scale    = 1
0.01.444.159 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.444.160 I ggml_metal_init: allocating
0.01.444.220 I ggml_metal_init: found device: Apple M4
0.01.444.227 I ggml_metal_init: picking default device: Apple M4
0.01.445.275 I ggml_metal_init: using embedded metal library
0.01.449.178 I ggml_metal_init: GPU name:   Apple M4
0.01.449.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.449.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.449.182 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.449.182 I ggml_metal_init: simdgroup reduction   = true
0.01.449.182 I ggml_metal_init: simdgroup matrix mul. = true
0.01.449.182 I ggml_metal_init: has residency sets    = true
0.01.449.183 I ggml_metal_init: has bfloat            = true
0.01.449.183 I ggml_metal_init: use bfloat            = true
0.01.449.183 I ggml_metal_init: hasUnifiedMemory      = true
0.01.449.184 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.460.123 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.461.905 I init:      Metal KV buffer size =    24.00 MiB
0.01.461.907 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.461.936 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.463.577 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.463.578 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.463.579 I llama_context: graph nodes  = 967
0.01.463.579 I llama_context: graph splits = 2
0.01.463.581 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.463.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.498.791 I 
0.01.498.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.498.845 I perplexity: tokenizing the input ..
0.01.504.084 I perplexity: tokenization took 5.236 ms
0.01.504.106 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.623.165 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.624.507 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.624.523 I llama_perf_context_print:        load time =    1478.15 ms
0.01.624.524 I llama_perf_context_print: prompt eval time =     118.76 ms /   128 tokens (    0.93 ms per token,  1077.84 tokens per second)
0.01.624.524 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.624.525 I llama_perf_context_print:       total time =     125.73 ms /   129 tokens
0.01.625.114 I ggml_metal_free: deallocating

real	0m1.809s
user	0m0.095s
sys	0m0.263s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.251 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.267 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.273 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.277 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.278 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.278 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.278 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.279 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.280 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.280 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.280 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.281 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.281 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.281 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.282 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.284 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.284 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.029 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.030 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.030 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.031 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.031 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.032 I llama_model_loader: - type  f32:  194 tensors
0.00.026.032 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.033 I print_info: file format = GGUF V3 (latest)
0.00.026.033 I print_info: file type   = Q8_0
0.00.026.034 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.210 I load: special tokens cache size = 25
0.00.040.305 I load: token to piece cache size = 0.2984 MB
0.00.040.310 I print_info: arch             = gptneox
0.00.040.311 I print_info: vocab_only       = 0
0.00.040.311 I print_info: n_ctx_train      = 2048
0.00.040.311 I print_info: n_embd           = 2048
0.00.040.311 I print_info: n_layer          = 24
0.00.040.316 I print_info: n_head           = 16
0.00.040.317 I print_info: n_head_kv        = 16
0.00.040.317 I print_info: n_rot            = 32
0.00.040.317 I print_info: n_swa            = 0
0.00.040.317 I print_info: n_embd_head_k    = 128
0.00.040.317 I print_info: n_embd_head_v    = 128
0.00.040.318 I print_info: n_gqa            = 1
0.00.040.319 I print_info: n_embd_k_gqa     = 2048
0.00.040.319 I print_info: n_embd_v_gqa     = 2048
0.00.040.320 I print_info: f_norm_eps       = 1.0e-05
0.00.040.320 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.321 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.321 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.321 I print_info: f_logit_scale    = 0.0e+00
0.00.040.321 I print_info: n_ff             = 8192
0.00.040.322 I print_info: n_expert         = 0
0.00.040.322 I print_info: n_expert_used    = 0
0.00.040.322 I print_info: causal attn      = 1
0.00.040.322 I print_info: pooling type     = 0
0.00.040.322 I print_info: rope type        = 2
0.00.040.322 I print_info: rope scaling     = linear
0.00.040.323 I print_info: freq_base_train  = 10000.0
0.00.040.323 I print_info: freq_scale_train = 1
0.00.040.323 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.323 I print_info: rope_finetuned   = unknown
0.00.040.323 I print_info: ssm_d_conv       = 0
0.00.040.323 I print_info: ssm_d_inner      = 0
0.00.040.324 I print_info: ssm_d_state      = 0
0.00.040.324 I print_info: ssm_dt_rank      = 0
0.00.040.324 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.324 I print_info: model type       = 1.4B
0.00.040.324 I print_info: model params     = 1.41 B
0.00.040.324 I print_info: general.name     = 1.4B
0.00.040.325 I print_info: vocab type       = BPE
0.00.040.328 I print_info: n_vocab          = 50304
0.00.040.328 I print_info: n_merges         = 50009
0.00.040.328 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.328 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.329 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.330 I print_info: LF token         = 187 'Ċ'
0.00.040.330 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.330 I print_info: max token length = 1024
0.00.040.331 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.919.236 I load_tensors: offloading 24 repeating layers to GPU
0.00.919.242 I load_tensors: offloading output layer to GPU
0.00.919.243 I load_tensors: offloaded 25/25 layers to GPU
0.00.919.272 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.919.275 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.920.882 I llama_context: n_seq_max     = 1
0.00.920.884 I llama_context: n_ctx         = 128
0.00.920.884 I llama_context: n_ctx_per_seq = 128
0.00.920.884 I llama_context: n_batch       = 128
0.00.920.885 I llama_context: n_ubatch      = 128
0.00.920.886 I llama_context: flash_attn    = 0
0.00.920.887 I llama_context: freq_base     = 10000.0
0.00.920.887 I llama_context: freq_scale    = 1
0.00.920.888 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.920.889 I ggml_metal_init: allocating
0.00.920.958 I ggml_metal_init: found device: Apple M4
0.00.920.973 I ggml_metal_init: picking default device: Apple M4
0.00.922.258 I ggml_metal_init: using embedded metal library
0.00.927.316 I ggml_metal_init: GPU name:   Apple M4
0.00.927.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.927.319 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.927.320 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.927.320 I ggml_metal_init: simdgroup reduction   = true
0.00.927.321 I ggml_metal_init: simdgroup matrix mul. = true
0.00.927.321 I ggml_metal_init: has residency sets    = true
0.00.927.322 I ggml_metal_init: has bfloat            = true
0.00.927.322 I ggml_metal_init: use bfloat            = true
0.00.927.323 I ggml_metal_init: hasUnifiedMemory      = true
0.00.927.325 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.943.134 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.946.536 I init:      Metal KV buffer size =    24.00 MiB
0.00.946.539 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.946.666 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.949.736 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.949.738 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.949.739 I llama_context: graph nodes  = 967
0.00.949.739 I llama_context: graph splits = 2
0.00.949.742 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.949.742 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.976.969 I 
0.00.977.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.977.056 I perplexity: tokenizing the input ..
0.00.984.521 I perplexity: tokenization took 7.462 ms
0.00.984.541 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.123.117 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.124.456 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.124.470 I llama_perf_context_print:        load time =     967.05 ms
0.01.124.471 I llama_perf_context_print: prompt eval time =     137.66 ms /   128 tokens (    1.08 ms per token,   929.85 tokens per second)
0.01.124.472 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.124.472 I llama_perf_context_print:       total time =     147.51 ms /   129 tokens
0.01.125.028 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.077s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.251 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.102 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.348 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.355 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.107 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.093 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.894 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.895 I llama_model_loader: - type  f32:  194 tensors
0.00.025.895 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.896 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.896 I print_info: file format = GGUF V3 (latest)
0.00.025.897 I print_info: file type   = Q4_0
0.00.025.898 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.989 I load: special tokens cache size = 25
0.00.040.006 I load: token to piece cache size = 0.2984 MB
0.00.040.010 I print_info: arch             = gptneox
0.00.040.010 I print_info: vocab_only       = 0
0.00.040.010 I print_info: n_ctx_train      = 2048
0.00.040.010 I print_info: n_embd           = 2048
0.00.040.011 I print_info: n_layer          = 24
0.00.040.015 I print_info: n_head           = 16
0.00.040.018 I print_info: n_head_kv        = 16
0.00.040.018 I print_info: n_rot            = 32
0.00.040.018 I print_info: n_swa            = 0
0.00.040.018 I print_info: n_embd_head_k    = 128
0.00.040.018 I print_info: n_embd_head_v    = 128
0.00.040.019 I print_info: n_gqa            = 1
0.00.040.020 I print_info: n_embd_k_gqa     = 2048
0.00.040.024 I print_info: n_embd_v_gqa     = 2048
0.00.040.025 I print_info: f_norm_eps       = 1.0e-05
0.00.040.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.026 I print_info: f_logit_scale    = 0.0e+00
0.00.040.027 I print_info: n_ff             = 8192
0.00.040.027 I print_info: n_expert         = 0
0.00.040.027 I print_info: n_expert_used    = 0
0.00.040.027 I print_info: causal attn      = 1
0.00.040.028 I print_info: pooling type     = 0
0.00.040.032 I print_info: rope type        = 2
0.00.040.033 I print_info: rope scaling     = linear
0.00.040.033 I print_info: freq_base_train  = 10000.0
0.00.040.034 I print_info: freq_scale_train = 1
0.00.040.034 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.035 I print_info: rope_finetuned   = unknown
0.00.040.035 I print_info: ssm_d_conv       = 0
0.00.040.035 I print_info: ssm_d_inner      = 0
0.00.040.036 I print_info: ssm_d_state      = 0
0.00.040.036 I print_info: ssm_dt_rank      = 0
0.00.040.036 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.036 I print_info: model type       = 1.4B
0.00.040.036 I print_info: model params     = 1.41 B
0.00.040.037 I print_info: general.name     = 1.4B
0.00.040.037 I print_info: vocab type       = BPE
0.00.040.037 I print_info: n_vocab          = 50304
0.00.040.037 I print_info: n_merges         = 50009
0.00.040.037 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: LF token         = 187 'Ċ'
0.00.040.039 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.039 I print_info: max token length = 1024
0.00.040.039 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.188 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.202 I load_tensors: offloading output layer to GPU
0.00.599.203 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.233 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.599.235 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.600.920 I llama_context: n_seq_max     = 1
0.00.600.923 I llama_context: n_ctx         = 128
0.00.600.924 I llama_context: n_ctx_per_seq = 128
0.00.600.925 I llama_context: n_batch       = 128
0.00.600.925 I llama_context: n_ubatch      = 128
0.00.600.925 I llama_context: flash_attn    = 0
0.00.600.927 I llama_context: freq_base     = 10000.0
0.00.600.928 I llama_context: freq_scale    = 1
0.00.600.928 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.600.931 I ggml_metal_init: allocating
0.00.601.007 I ggml_metal_init: found device: Apple M4
0.00.601.020 I ggml_metal_init: picking default device: Apple M4
0.00.602.834 I ggml_metal_init: using embedded metal library
0.00.608.686 I ggml_metal_init: GPU name:   Apple M4
0.00.608.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.692 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.693 I ggml_metal_init: simdgroup reduction   = true
0.00.608.694 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.694 I ggml_metal_init: has residency sets    = true
0.00.608.695 I ggml_metal_init: has bfloat            = true
0.00.608.695 I ggml_metal_init: use bfloat            = true
0.00.608.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.557 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.126 I init:      Metal KV buffer size =    24.00 MiB
0.00.631.130 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.157 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.634.510 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.634.512 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.634.512 I llama_context: graph nodes  = 967
0.00.634.513 I llama_context: graph splits = 2
0.00.634.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.010 I 
0.00.658.083 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.104 I perplexity: tokenizing the input ..
0.00.664.613 I perplexity: tokenization took 6.508 ms
0.00.664.629 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.610 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.788.902 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.788.916 I llama_perf_context_print:        load time =     647.90 ms
0.00.788.917 I llama_perf_context_print: prompt eval time =     122.43 ms /   128 tokens (    0.96 ms per token,  1045.46 tokens per second)
0.00.788.918 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.919 I llama_perf_context_print:       total time =     130.91 ms /   129 tokens
0.00.789.471 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.078s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.003 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.395 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.404 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.413 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.935 I llama_model_loader: - type  f32:  194 tensors
0.00.024.935 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.936 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.936 I print_info: file format = GGUF V3 (latest)
0.00.024.937 I print_info: file type   = Q4_1
0.00.024.943 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.042 I load: special tokens cache size = 25
0.00.039.129 I load: token to piece cache size = 0.2984 MB
0.00.039.134 I print_info: arch             = gptneox
0.00.039.134 I print_info: vocab_only       = 0
0.00.039.134 I print_info: n_ctx_train      = 2048
0.00.039.134 I print_info: n_embd           = 2048
0.00.039.135 I print_info: n_layer          = 24
0.00.039.139 I print_info: n_head           = 16
0.00.039.140 I print_info: n_head_kv        = 16
0.00.039.140 I print_info: n_rot            = 32
0.00.039.140 I print_info: n_swa            = 0
0.00.039.140 I print_info: n_embd_head_k    = 128
0.00.039.141 I print_info: n_embd_head_v    = 128
0.00.039.141 I print_info: n_gqa            = 1
0.00.039.142 I print_info: n_embd_k_gqa     = 2048
0.00.039.144 I print_info: n_embd_v_gqa     = 2048
0.00.039.144 I print_info: f_norm_eps       = 1.0e-05
0.00.039.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.145 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.145 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.145 I print_info: f_logit_scale    = 0.0e+00
0.00.039.146 I print_info: n_ff             = 8192
0.00.039.146 I print_info: n_expert         = 0
0.00.039.148 I print_info: n_expert_used    = 0
0.00.039.148 I print_info: causal attn      = 1
0.00.039.149 I print_info: pooling type     = 0
0.00.039.149 I print_info: rope type        = 2
0.00.039.149 I print_info: rope scaling     = linear
0.00.039.149 I print_info: freq_base_train  = 10000.0
0.00.039.151 I print_info: freq_scale_train = 1
0.00.039.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.151 I print_info: rope_finetuned   = unknown
0.00.039.151 I print_info: ssm_d_conv       = 0
0.00.039.152 I print_info: ssm_d_inner      = 0
0.00.039.152 I print_info: ssm_d_state      = 0
0.00.039.152 I print_info: ssm_dt_rank      = 0
0.00.039.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.152 I print_info: model type       = 1.4B
0.00.039.152 I print_info: model params     = 1.41 B
0.00.039.153 I print_info: general.name     = 1.4B
0.00.039.153 I print_info: vocab type       = BPE
0.00.039.153 I print_info: n_vocab          = 50304
0.00.039.153 I print_info: n_merges         = 50009
0.00.039.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.157 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.157 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.157 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.157 I print_info: LF token         = 187 'Ċ'
0.00.039.158 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.158 I print_info: max token length = 1024
0.00.039.159 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.682.653 I load_tensors: offloading 24 repeating layers to GPU
0.00.682.668 I load_tensors: offloading output layer to GPU
0.00.682.669 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.704 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.682.706 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.684.350 I llama_context: n_seq_max     = 1
0.00.684.353 I llama_context: n_ctx         = 128
0.00.684.354 I llama_context: n_ctx_per_seq = 128
0.00.684.355 I llama_context: n_batch       = 128
0.00.684.355 I llama_context: n_ubatch      = 128
0.00.684.355 I llama_context: flash_attn    = 0
0.00.684.357 I llama_context: freq_base     = 10000.0
0.00.684.358 I llama_context: freq_scale    = 1
0.00.684.359 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.684.361 I ggml_metal_init: allocating
0.00.684.447 I ggml_metal_init: found device: Apple M4
0.00.684.462 I ggml_metal_init: picking default device: Apple M4
0.00.686.262 I ggml_metal_init: using embedded metal library
0.00.693.064 I ggml_metal_init: GPU name:   Apple M4
0.00.693.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.075 I ggml_metal_init: simdgroup reduction   = true
0.00.693.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.076 I ggml_metal_init: has residency sets    = true
0.00.693.076 I ggml_metal_init: has bfloat            = true
0.00.693.077 I ggml_metal_init: use bfloat            = true
0.00.693.078 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.710.651 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.293 I init:      Metal KV buffer size =    24.00 MiB
0.00.714.297 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.714.323 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.717.462 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.717.464 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.717.464 I llama_context: graph nodes  = 967
0.00.717.465 I llama_context: graph splits = 2
0.00.717.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.717.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.114 I 
0.00.747.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.215 I perplexity: tokenizing the input ..
0.00.754.353 I perplexity: tokenization took 7.136 ms
0.00.754.376 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.885.139 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.886.481 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.886.494 I llama_perf_context_print:        load time =     738.10 ms
0.00.886.495 I llama_perf_context_print: prompt eval time =     129.78 ms /   128 tokens (    1.01 ms per token,   986.30 tokens per second)
0.00.886.495 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.886.496 I llama_perf_context_print:       total time =     139.38 ms /   129 tokens
0.00.887.073 I ggml_metal_free: deallocating

real	0m0.902s
user	0m0.080s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.036 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.374 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.376 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.380 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.380 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.380 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.382 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.384 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.384 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.384 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.182 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.111 I llama_model_loader: - type  f32:  194 tensors
0.00.026.111 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.111 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.112 I print_info: file format = GGUF V3 (latest)
0.00.026.117 I print_info: file type   = Q5_0
0.00.026.118 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.056 I load: special tokens cache size = 25
0.00.040.146 I load: token to piece cache size = 0.2984 MB
0.00.040.151 I print_info: arch             = gptneox
0.00.040.151 I print_info: vocab_only       = 0
0.00.040.151 I print_info: n_ctx_train      = 2048
0.00.040.151 I print_info: n_embd           = 2048
0.00.040.152 I print_info: n_layer          = 24
0.00.040.156 I print_info: n_head           = 16
0.00.040.157 I print_info: n_head_kv        = 16
0.00.040.157 I print_info: n_rot            = 32
0.00.040.157 I print_info: n_swa            = 0
0.00.040.160 I print_info: n_embd_head_k    = 128
0.00.040.160 I print_info: n_embd_head_v    = 128
0.00.040.161 I print_info: n_gqa            = 1
0.00.040.161 I print_info: n_embd_k_gqa     = 2048
0.00.040.162 I print_info: n_embd_v_gqa     = 2048
0.00.040.162 I print_info: f_norm_eps       = 1.0e-05
0.00.040.166 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.167 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.167 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.167 I print_info: f_logit_scale    = 0.0e+00
0.00.040.168 I print_info: n_ff             = 8192
0.00.040.169 I print_info: n_expert         = 0
0.00.040.169 I print_info: n_expert_used    = 0
0.00.040.170 I print_info: causal attn      = 1
0.00.040.170 I print_info: pooling type     = 0
0.00.040.170 I print_info: rope type        = 2
0.00.040.170 I print_info: rope scaling     = linear
0.00.040.170 I print_info: freq_base_train  = 10000.0
0.00.040.171 I print_info: freq_scale_train = 1
0.00.040.171 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.171 I print_info: rope_finetuned   = unknown
0.00.040.171 I print_info: ssm_d_conv       = 0
0.00.040.171 I print_info: ssm_d_inner      = 0
0.00.040.171 I print_info: ssm_d_state      = 0
0.00.040.171 I print_info: ssm_dt_rank      = 0
0.00.040.171 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.172 I print_info: model type       = 1.4B
0.00.040.174 I print_info: model params     = 1.41 B
0.00.040.174 I print_info: general.name     = 1.4B
0.00.040.175 I print_info: vocab type       = BPE
0.00.040.175 I print_info: n_vocab          = 50304
0.00.040.175 I print_info: n_merges         = 50009
0.00.040.176 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.176 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.176 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.176 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.176 I print_info: LF token         = 187 'Ċ'
0.00.040.177 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.177 I print_info: max token length = 1024
0.00.040.177 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.720.388 I load_tensors: offloading 24 repeating layers to GPU
0.00.720.401 I load_tensors: offloading output layer to GPU
0.00.720.402 I load_tensors: offloaded 25/25 layers to GPU
0.00.720.438 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.720.440 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.721.973 I llama_context: n_seq_max     = 1
0.00.721.976 I llama_context: n_ctx         = 128
0.00.721.976 I llama_context: n_ctx_per_seq = 128
0.00.721.977 I llama_context: n_batch       = 128
0.00.721.977 I llama_context: n_ubatch      = 128
0.00.721.977 I llama_context: flash_attn    = 0
0.00.721.979 I llama_context: freq_base     = 10000.0
0.00.721.980 I llama_context: freq_scale    = 1
0.00.721.981 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.721.983 I ggml_metal_init: allocating
0.00.722.084 I ggml_metal_init: found device: Apple M4
0.00.722.097 I ggml_metal_init: picking default device: Apple M4
0.00.724.043 I ggml_metal_init: using embedded metal library
0.00.730.777 I ggml_metal_init: GPU name:   Apple M4
0.00.730.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.730.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.730.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.730.784 I ggml_metal_init: simdgroup reduction   = true
0.00.730.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.730.785 I ggml_metal_init: has residency sets    = true
0.00.730.785 I ggml_metal_init: has bfloat            = true
0.00.730.785 I ggml_metal_init: use bfloat            = true
0.00.730.786 I ggml_metal_init: hasUnifiedMemory      = true
0.00.730.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.748.174 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.751.649 I init:      Metal KV buffer size =    24.00 MiB
0.00.751.653 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.751.678 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.754.977 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.754.979 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.754.979 I llama_context: graph nodes  = 967
0.00.754.979 I llama_context: graph splits = 2
0.00.754.983 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.754.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.494 I 
0.00.783.566 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.583 I perplexity: tokenizing the input ..
0.00.790.798 I perplexity: tokenization took 7.212 ms
0.00.790.816 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.926.834 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.928.177 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.928.189 I llama_perf_context_print:        load time =     773.45 ms
0.00.928.189 I llama_perf_context_print: prompt eval time =     135.14 ms /   128 tokens (    1.06 ms per token,   947.14 tokens per second)
0.00.928.190 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.928.191 I llama_perf_context_print:       total time =     144.70 ms /   129 tokens
0.00.928.711 I ggml_metal_free: deallocating

real	0m0.944s
user	0m0.079s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.906 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.484 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.486 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.491 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.494 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.495 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.275 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.276 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.277 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.277 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.277 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.278 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.278 I llama_model_loader: - type  f32:  194 tensors
0.00.024.279 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.279 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.280 I print_info: file format = GGUF V3 (latest)
0.00.024.281 I print_info: file type   = Q5_1
0.00.024.282 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.438 I load: special tokens cache size = 25
0.00.038.504 I load: token to piece cache size = 0.2984 MB
0.00.038.508 I print_info: arch             = gptneox
0.00.038.509 I print_info: vocab_only       = 0
0.00.038.509 I print_info: n_ctx_train      = 2048
0.00.038.509 I print_info: n_embd           = 2048
0.00.038.509 I print_info: n_layer          = 24
0.00.038.513 I print_info: n_head           = 16
0.00.038.514 I print_info: n_head_kv        = 16
0.00.038.514 I print_info: n_rot            = 32
0.00.038.514 I print_info: n_swa            = 0
0.00.038.515 I print_info: n_embd_head_k    = 128
0.00.038.515 I print_info: n_embd_head_v    = 128
0.00.038.518 I print_info: n_gqa            = 1
0.00.038.519 I print_info: n_embd_k_gqa     = 2048
0.00.038.520 I print_info: n_embd_v_gqa     = 2048
0.00.038.520 I print_info: f_norm_eps       = 1.0e-05
0.00.038.521 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.521 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.521 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.521 I print_info: f_logit_scale    = 0.0e+00
0.00.038.522 I print_info: n_ff             = 8192
0.00.038.522 I print_info: n_expert         = 0
0.00.038.522 I print_info: n_expert_used    = 0
0.00.038.523 I print_info: causal attn      = 1
0.00.038.523 I print_info: pooling type     = 0
0.00.038.523 I print_info: rope type        = 2
0.00.038.523 I print_info: rope scaling     = linear
0.00.038.524 I print_info: freq_base_train  = 10000.0
0.00.038.524 I print_info: freq_scale_train = 1
0.00.038.524 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.524 I print_info: rope_finetuned   = unknown
0.00.038.524 I print_info: ssm_d_conv       = 0
0.00.038.525 I print_info: ssm_d_inner      = 0
0.00.038.525 I print_info: ssm_d_state      = 0
0.00.038.525 I print_info: ssm_dt_rank      = 0
0.00.038.525 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.525 I print_info: model type       = 1.4B
0.00.038.526 I print_info: model params     = 1.41 B
0.00.038.526 I print_info: general.name     = 1.4B
0.00.038.527 I print_info: vocab type       = BPE
0.00.038.528 I print_info: n_vocab          = 50304
0.00.038.528 I print_info: n_merges         = 50009
0.00.038.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.529 I print_info: LF token         = 187 'Ċ'
0.00.038.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.530 I print_info: max token length = 1024
0.00.038.530 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.518 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.532 I load_tensors: offloading output layer to GPU
0.00.621.533 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.568 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.621.573 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.623.023 I llama_context: n_seq_max     = 1
0.00.623.027 I llama_context: n_ctx         = 128
0.00.623.027 I llama_context: n_ctx_per_seq = 128
0.00.623.028 I llama_context: n_batch       = 128
0.00.623.029 I llama_context: n_ubatch      = 128
0.00.623.029 I llama_context: flash_attn    = 0
0.00.623.031 I llama_context: freq_base     = 10000.0
0.00.623.031 I llama_context: freq_scale    = 1
0.00.623.032 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.623.035 I ggml_metal_init: allocating
0.00.623.107 I ggml_metal_init: found device: Apple M4
0.00.623.122 I ggml_metal_init: picking default device: Apple M4
0.00.624.993 I ggml_metal_init: using embedded metal library
0.00.631.811 I ggml_metal_init: GPU name:   Apple M4
0.00.631.818 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.818 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.819 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.823 I ggml_metal_init: simdgroup reduction   = true
0.00.631.823 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.823 I ggml_metal_init: has residency sets    = true
0.00.631.824 I ggml_metal_init: has bfloat            = true
0.00.631.824 I ggml_metal_init: use bfloat            = true
0.00.631.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.707 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.953 I init:      Metal KV buffer size =    24.00 MiB
0.00.652.956 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.652.980 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.656.059 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.656.061 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.656.061 I llama_context: graph nodes  = 967
0.00.656.061 I llama_context: graph splits = 2
0.00.656.065 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.656.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.813 I 
0.00.685.870 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.886 I perplexity: tokenizing the input ..
0.00.692.258 I perplexity: tokenization took 6.37 ms
0.00.692.273 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.280 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.838.629 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.838.643 I llama_perf_context_print:        load time =     676.90 ms
0.00.838.644 I llama_perf_context_print: prompt eval time =     144.56 ms /   128 tokens (    1.13 ms per token,   885.46 tokens per second)
0.00.838.645 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.645 I llama_perf_context_print:       total time =     152.83 ms /   129 tokens
0.00.839.281 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.080s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.463 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.782 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.808 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.704 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.705 I llama_model_loader: - type  f32:  194 tensors
0.00.025.705 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.705 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.706 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.706 I print_info: file format = GGUF V3 (latest)
0.00.025.707 I print_info: file type   = Q2_K - Medium
0.00.025.708 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.701 I load: special tokens cache size = 25
0.00.039.635 I load: token to piece cache size = 0.2984 MB
0.00.039.638 I print_info: arch             = gptneox
0.00.039.638 I print_info: vocab_only       = 0
0.00.039.639 I print_info: n_ctx_train      = 2048
0.00.039.639 I print_info: n_embd           = 2048
0.00.039.639 I print_info: n_layer          = 24
0.00.039.643 I print_info: n_head           = 16
0.00.039.643 I print_info: n_head_kv        = 16
0.00.039.643 I print_info: n_rot            = 32
0.00.039.644 I print_info: n_swa            = 0
0.00.039.644 I print_info: n_embd_head_k    = 128
0.00.039.644 I print_info: n_embd_head_v    = 128
0.00.039.645 I print_info: n_gqa            = 1
0.00.039.645 I print_info: n_embd_k_gqa     = 2048
0.00.039.646 I print_info: n_embd_v_gqa     = 2048
0.00.039.647 I print_info: f_norm_eps       = 1.0e-05
0.00.039.647 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.647 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.647 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.650 I print_info: f_logit_scale    = 0.0e+00
0.00.039.651 I print_info: n_ff             = 8192
0.00.039.651 I print_info: n_expert         = 0
0.00.039.651 I print_info: n_expert_used    = 0
0.00.039.652 I print_info: causal attn      = 1
0.00.039.652 I print_info: pooling type     = 0
0.00.039.652 I print_info: rope type        = 2
0.00.039.652 I print_info: rope scaling     = linear
0.00.039.653 I print_info: freq_base_train  = 10000.0
0.00.039.653 I print_info: freq_scale_train = 1
0.00.039.653 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.653 I print_info: rope_finetuned   = unknown
0.00.039.653 I print_info: ssm_d_conv       = 0
0.00.039.655 I print_info: ssm_d_inner      = 0
0.00.039.655 I print_info: ssm_d_state      = 0
0.00.039.655 I print_info: ssm_dt_rank      = 0
0.00.039.655 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.655 I print_info: model type       = 1.4B
0.00.039.656 I print_info: model params     = 1.41 B
0.00.039.656 I print_info: general.name     = 1.4B
0.00.039.657 I print_info: vocab type       = BPE
0.00.039.657 I print_info: n_vocab          = 50304
0.00.039.657 I print_info: n_merges         = 50009
0.00.039.657 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.657 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.658 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.658 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.658 I print_info: LF token         = 187 'Ċ'
0.00.039.662 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.662 I print_info: max token length = 1024
0.00.039.663 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.352.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.352.385 I load_tensors: offloading output layer to GPU
0.00.352.385 I load_tensors: offloaded 25/25 layers to GPU
0.00.352.417 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.352.418 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.353.768 I llama_context: n_seq_max     = 1
0.00.353.773 I llama_context: n_ctx         = 128
0.00.353.773 I llama_context: n_ctx_per_seq = 128
0.00.353.774 I llama_context: n_batch       = 128
0.00.353.774 I llama_context: n_ubatch      = 128
0.00.353.774 I llama_context: flash_attn    = 0
0.00.353.778 I llama_context: freq_base     = 10000.0
0.00.353.778 I llama_context: freq_scale    = 1
0.00.353.779 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.353.789 I ggml_metal_init: allocating
0.00.353.865 I ggml_metal_init: found device: Apple M4
0.00.353.880 I ggml_metal_init: picking default device: Apple M4
0.00.355.757 I ggml_metal_init: using embedded metal library
0.00.361.544 I ggml_metal_init: GPU name:   Apple M4
0.00.361.567 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.361.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.361.570 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.361.571 I ggml_metal_init: simdgroup reduction   = true
0.00.361.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.361.571 I ggml_metal_init: has residency sets    = true
0.00.361.572 I ggml_metal_init: has bfloat            = true
0.00.361.572 I ggml_metal_init: use bfloat            = true
0.00.361.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.361.579 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.657 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.387.191 I init:      Metal KV buffer size =    24.00 MiB
0.00.387.196 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.387.231 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.390.546 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.390.548 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.390.549 I llama_context: graph nodes  = 967
0.00.390.549 I llama_context: graph splits = 2
0.00.390.554 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.390.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.418.375 I 
0.00.418.431 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.418.448 I perplexity: tokenizing the input ..
0.00.424.998 I perplexity: tokenization took 6.547 ms
0.00.425.019 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.558.834 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.560.136 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.560.147 I llama_perf_context_print:        load time =     407.90 ms
0.00.560.148 I llama_perf_context_print: prompt eval time =     132.80 ms /   128 tokens (    1.04 ms per token,   963.84 tokens per second)
0.00.560.149 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.560.149 I llama_perf_context_print:       total time =     141.78 ms /   129 tokens
0.00.560.735 I ggml_metal_free: deallocating

real	0m0.575s
user	0m0.082s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.290 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.411 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.430 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.430 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.168 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.980 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.980 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.981 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.981 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.981 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.982 I llama_model_loader: - type  f32:  194 tensors
0.00.024.983 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.983 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.983 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.983 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.984 I print_info: file format = GGUF V3 (latest)
0.00.024.984 I print_info: file type   = Q3_K - Medium
0.00.024.985 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.046 I load: special tokens cache size = 25
0.00.039.121 I load: token to piece cache size = 0.2984 MB
0.00.039.125 I print_info: arch             = gptneox
0.00.039.125 I print_info: vocab_only       = 0
0.00.039.126 I print_info: n_ctx_train      = 2048
0.00.039.126 I print_info: n_embd           = 2048
0.00.039.126 I print_info: n_layer          = 24
0.00.039.130 I print_info: n_head           = 16
0.00.039.131 I print_info: n_head_kv        = 16
0.00.039.131 I print_info: n_rot            = 32
0.00.039.131 I print_info: n_swa            = 0
0.00.039.131 I print_info: n_embd_head_k    = 128
0.00.039.134 I print_info: n_embd_head_v    = 128
0.00.039.135 I print_info: n_gqa            = 1
0.00.039.135 I print_info: n_embd_k_gqa     = 2048
0.00.039.136 I print_info: n_embd_v_gqa     = 2048
0.00.039.137 I print_info: f_norm_eps       = 1.0e-05
0.00.039.137 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.137 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.137 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.139 I print_info: f_logit_scale    = 0.0e+00
0.00.039.140 I print_info: n_ff             = 8192
0.00.039.140 I print_info: n_expert         = 0
0.00.039.141 I print_info: n_expert_used    = 0
0.00.039.142 I print_info: causal attn      = 1
0.00.039.142 I print_info: pooling type     = 0
0.00.039.142 I print_info: rope type        = 2
0.00.039.142 I print_info: rope scaling     = linear
0.00.039.143 I print_info: freq_base_train  = 10000.0
0.00.039.143 I print_info: freq_scale_train = 1
0.00.039.143 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.144 I print_info: rope_finetuned   = unknown
0.00.039.144 I print_info: ssm_d_conv       = 0
0.00.039.144 I print_info: ssm_d_inner      = 0
0.00.039.144 I print_info: ssm_d_state      = 0
0.00.039.144 I print_info: ssm_dt_rank      = 0
0.00.039.146 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.146 I print_info: model type       = 1.4B
0.00.039.146 I print_info: model params     = 1.41 B
0.00.039.146 I print_info: general.name     = 1.4B
0.00.039.147 I print_info: vocab type       = BPE
0.00.039.147 I print_info: n_vocab          = 50304
0.00.039.147 I print_info: n_merges         = 50009
0.00.039.147 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.147 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.148 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.148 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.148 I print_info: LF token         = 187 'Ċ'
0.00.039.148 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.151 I print_info: max token length = 1024
0.00.039.152 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.454.978 I load_tensors: offloading 24 repeating layers to GPU
0.00.454.991 I load_tensors: offloading output layer to GPU
0.00.454.992 I load_tensors: offloaded 25/25 layers to GPU
0.00.455.028 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.455.029 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.456.407 I llama_context: n_seq_max     = 1
0.00.456.411 I llama_context: n_ctx         = 128
0.00.456.412 I llama_context: n_ctx_per_seq = 128
0.00.456.412 I llama_context: n_batch       = 128
0.00.456.413 I llama_context: n_ubatch      = 128
0.00.456.413 I llama_context: flash_attn    = 0
0.00.456.415 I llama_context: freq_base     = 10000.0
0.00.456.415 I llama_context: freq_scale    = 1
0.00.456.416 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.456.418 I ggml_metal_init: allocating
0.00.456.479 I ggml_metal_init: found device: Apple M4
0.00.456.494 I ggml_metal_init: picking default device: Apple M4
0.00.458.322 I ggml_metal_init: using embedded metal library
0.00.464.078 I ggml_metal_init: GPU name:   Apple M4
0.00.464.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.464.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.464.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.464.104 I ggml_metal_init: simdgroup reduction   = true
0.00.464.104 I ggml_metal_init: simdgroup matrix mul. = true
0.00.464.105 I ggml_metal_init: has residency sets    = true
0.00.464.105 I ggml_metal_init: has bfloat            = true
0.00.464.105 I ggml_metal_init: use bfloat            = true
0.00.464.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.464.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.485.612 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.489.041 I init:      Metal KV buffer size =    24.00 MiB
0.00.489.047 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.489.091 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.492.496 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.492.498 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.492.498 I llama_context: graph nodes  = 967
0.00.492.499 I llama_context: graph splits = 2
0.00.492.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.492.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.853 I 
0.00.520.920 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.936 I perplexity: tokenizing the input ..
0.00.527.692 I perplexity: tokenization took 6.751 ms
0.00.527.713 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.670.121 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.671.467 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.671.482 I llama_perf_context_print:        load time =     511.56 ms
0.00.671.483 I llama_perf_context_print: prompt eval time =     141.42 ms /   128 tokens (    1.10 ms per token,   905.13 tokens per second)
0.00.671.484 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.671.484 I llama_perf_context_print:       total time =     150.63 ms /   129 tokens
0.00.672.034 I ggml_metal_free: deallocating

real	0m0.685s
user	0m0.082s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.334 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.588 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.594 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.596 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.597 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.597 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.597 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.598 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.599 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.599 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.602 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.603 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.603 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.605 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.605 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.361 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.348 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.121 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.123 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.123 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.123 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.124 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.124 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.125 I llama_model_loader: - type  f32:  194 tensors
0.00.025.125 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.125 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.125 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.126 I print_info: file format = GGUF V3 (latest)
0.00.025.131 I print_info: file type   = Q4_K - Medium
0.00.025.132 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.124 I load: special tokens cache size = 25
0.00.039.059 I load: token to piece cache size = 0.2984 MB
0.00.039.064 I print_info: arch             = gptneox
0.00.039.065 I print_info: vocab_only       = 0
0.00.039.065 I print_info: n_ctx_train      = 2048
0.00.039.065 I print_info: n_embd           = 2048
0.00.039.065 I print_info: n_layer          = 24
0.00.039.070 I print_info: n_head           = 16
0.00.039.071 I print_info: n_head_kv        = 16
0.00.039.071 I print_info: n_rot            = 32
0.00.039.071 I print_info: n_swa            = 0
0.00.039.072 I print_info: n_embd_head_k    = 128
0.00.039.072 I print_info: n_embd_head_v    = 128
0.00.039.072 I print_info: n_gqa            = 1
0.00.039.073 I print_info: n_embd_k_gqa     = 2048
0.00.039.074 I print_info: n_embd_v_gqa     = 2048
0.00.039.075 I print_info: f_norm_eps       = 1.0e-05
0.00.039.075 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.075 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.075 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.075 I print_info: f_logit_scale    = 0.0e+00
0.00.039.076 I print_info: n_ff             = 8192
0.00.039.076 I print_info: n_expert         = 0
0.00.039.076 I print_info: n_expert_used    = 0
0.00.039.076 I print_info: causal attn      = 1
0.00.039.077 I print_info: pooling type     = 0
0.00.039.077 I print_info: rope type        = 2
0.00.039.077 I print_info: rope scaling     = linear
0.00.039.077 I print_info: freq_base_train  = 10000.0
0.00.039.078 I print_info: freq_scale_train = 1
0.00.039.078 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.078 I print_info: rope_finetuned   = unknown
0.00.039.080 I print_info: ssm_d_conv       = 0
0.00.039.080 I print_info: ssm_d_inner      = 0
0.00.039.080 I print_info: ssm_d_state      = 0
0.00.039.081 I print_info: ssm_dt_rank      = 0
0.00.039.081 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.081 I print_info: model type       = 1.4B
0.00.039.081 I print_info: model params     = 1.41 B
0.00.039.081 I print_info: general.name     = 1.4B
0.00.039.082 I print_info: vocab type       = BPE
0.00.039.082 I print_info: n_vocab          = 50304
0.00.039.082 I print_info: n_merges         = 50009
0.00.039.082 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.083 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.084 I print_info: LF token         = 187 'Ċ'
0.00.039.084 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.084 I print_info: max token length = 1024
0.00.039.084 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.533.351 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.370 I load_tensors: offloading output layer to GPU
0.00.533.371 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.413 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.414 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.534.825 I llama_context: n_seq_max     = 1
0.00.534.829 I llama_context: n_ctx         = 128
0.00.534.829 I llama_context: n_ctx_per_seq = 128
0.00.534.830 I llama_context: n_batch       = 128
0.00.534.830 I llama_context: n_ubatch      = 128
0.00.534.831 I llama_context: flash_attn    = 0
0.00.534.833 I llama_context: freq_base     = 10000.0
0.00.534.834 I llama_context: freq_scale    = 1
0.00.534.834 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.534.837 I ggml_metal_init: allocating
0.00.534.932 I ggml_metal_init: found device: Apple M4
0.00.534.947 I ggml_metal_init: picking default device: Apple M4
0.00.536.883 I ggml_metal_init: using embedded metal library
0.00.543.314 I ggml_metal_init: GPU name:   Apple M4
0.00.543.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.320 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.321 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.322 I ggml_metal_init: simdgroup reduction   = true
0.00.543.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.323 I ggml_metal_init: has residency sets    = true
0.00.543.323 I ggml_metal_init: has bfloat            = true
0.00.543.324 I ggml_metal_init: use bfloat            = true
0.00.543.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.562 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.565.216 I init:      Metal KV buffer size =    24.00 MiB
0.00.565.220 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.565.268 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.568.437 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.568.439 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.568.439 I llama_context: graph nodes  = 967
0.00.568.440 I llama_context: graph splits = 2
0.00.568.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.568.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.723 I 
0.00.598.785 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.802 I perplexity: tokenizing the input ..
0.00.605.576 I perplexity: tokenization took 6.771 ms
0.00.605.594 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.662 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.754.996 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.755.010 I llama_perf_context_print:        load time =     589.38 ms
0.00.755.011 I llama_perf_context_print: prompt eval time =     147.43 ms /   128 tokens (    1.15 ms per token,   868.19 tokens per second)
0.00.755.011 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.755.012 I llama_perf_context_print:       total time =     156.29 ms /   129 tokens
0.00.755.662 I ggml_metal_free: deallocating

real	0m0.770s
user	0m0.079s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.401 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.583 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.585 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.585 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.586 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.591 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.592 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.593 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.593 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.356 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.358 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.358 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.359 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.360 I llama_model_loader: - type  f32:  194 tensors
0.00.026.360 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.360 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.361 I print_info: file format = GGUF V3 (latest)
0.00.026.365 I print_info: file type   = Q5_K - Medium
0.00.026.367 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.718 I load: special tokens cache size = 25
0.00.040.617 I load: token to piece cache size = 0.2984 MB
0.00.040.620 I print_info: arch             = gptneox
0.00.040.620 I print_info: vocab_only       = 0
0.00.040.620 I print_info: n_ctx_train      = 2048
0.00.040.621 I print_info: n_embd           = 2048
0.00.040.621 I print_info: n_layer          = 24
0.00.040.625 I print_info: n_head           = 16
0.00.040.625 I print_info: n_head_kv        = 16
0.00.040.626 I print_info: n_rot            = 32
0.00.040.626 I print_info: n_swa            = 0
0.00.040.626 I print_info: n_embd_head_k    = 128
0.00.040.626 I print_info: n_embd_head_v    = 128
0.00.040.627 I print_info: n_gqa            = 1
0.00.040.627 I print_info: n_embd_k_gqa     = 2048
0.00.040.628 I print_info: n_embd_v_gqa     = 2048
0.00.040.629 I print_info: f_norm_eps       = 1.0e-05
0.00.040.629 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.630 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.630 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.630 I print_info: f_logit_scale    = 0.0e+00
0.00.040.632 I print_info: n_ff             = 8192
0.00.040.632 I print_info: n_expert         = 0
0.00.040.633 I print_info: n_expert_used    = 0
0.00.040.633 I print_info: causal attn      = 1
0.00.040.633 I print_info: pooling type     = 0
0.00.040.633 I print_info: rope type        = 2
0.00.040.633 I print_info: rope scaling     = linear
0.00.040.634 I print_info: freq_base_train  = 10000.0
0.00.040.634 I print_info: freq_scale_train = 1
0.00.040.634 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.634 I print_info: rope_finetuned   = unknown
0.00.040.635 I print_info: ssm_d_conv       = 0
0.00.040.635 I print_info: ssm_d_inner      = 0
0.00.040.635 I print_info: ssm_d_state      = 0
0.00.040.635 I print_info: ssm_dt_rank      = 0
0.00.040.635 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.635 I print_info: model type       = 1.4B
0.00.040.636 I print_info: model params     = 1.41 B
0.00.040.636 I print_info: general.name     = 1.4B
0.00.040.636 I print_info: vocab type       = BPE
0.00.040.637 I print_info: n_vocab          = 50304
0.00.040.637 I print_info: n_merges         = 50009
0.00.040.637 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.637 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.637 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.638 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.638 I print_info: LF token         = 187 'Ċ'
0.00.040.638 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.639 I print_info: max token length = 1024
0.00.040.639 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.216 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.234 I load_tensors: offloading output layer to GPU
0.00.621.235 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.270 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.621.271 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.622.671 I llama_context: n_seq_max     = 1
0.00.622.675 I llama_context: n_ctx         = 128
0.00.622.676 I llama_context: n_ctx_per_seq = 128
0.00.622.676 I llama_context: n_batch       = 128
0.00.622.676 I llama_context: n_ubatch      = 128
0.00.622.677 I llama_context: flash_attn    = 0
0.00.622.679 I llama_context: freq_base     = 10000.0
0.00.622.680 I llama_context: freq_scale    = 1
0.00.622.680 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.683 I ggml_metal_init: allocating
0.00.622.763 I ggml_metal_init: found device: Apple M4
0.00.622.777 I ggml_metal_init: picking default device: Apple M4
0.00.624.686 I ggml_metal_init: using embedded metal library
0.00.631.242 I ggml_metal_init: GPU name:   Apple M4
0.00.631.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.249 I ggml_metal_init: simdgroup reduction   = true
0.00.631.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.249 I ggml_metal_init: has residency sets    = true
0.00.631.249 I ggml_metal_init: has bfloat            = true
0.00.631.250 I ggml_metal_init: use bfloat            = true
0.00.631.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.717 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.123 I init:      Metal KV buffer size =    24.00 MiB
0.00.652.129 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.652.159 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.655.329 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.655.331 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.655.331 I llama_context: graph nodes  = 967
0.00.655.332 I llama_context: graph splits = 2
0.00.655.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.655.336 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.749 I 
0.00.686.808 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.827 I perplexity: tokenizing the input ..
0.00.693.365 I perplexity: tokenization took 6.536 ms
0.00.693.385 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.251 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.835.555 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.835.572 I llama_perf_context_print:        load time =     676.34 ms
0.00.835.573 I llama_perf_context_print: prompt eval time =     139.88 ms /   128 tokens (    1.09 ms per token,   915.09 tokens per second)
0.00.835.576 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.576 I llama_perf_context_print:       total time =     148.83 ms /   129 tokens
0.00.836.132 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.080s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.322 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.352 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.138 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.892 I llama_model_loader: - type  f32:  194 tensors
0.00.024.892 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.893 I print_info: file format = GGUF V3 (latest)
0.00.024.893 I print_info: file type   = Q6_K
0.00.024.894 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.910 I load: special tokens cache size = 25
0.00.039.037 I load: token to piece cache size = 0.2984 MB
0.00.039.041 I print_info: arch             = gptneox
0.00.039.041 I print_info: vocab_only       = 0
0.00.039.041 I print_info: n_ctx_train      = 2048
0.00.039.041 I print_info: n_embd           = 2048
0.00.039.041 I print_info: n_layer          = 24
0.00.039.046 I print_info: n_head           = 16
0.00.039.046 I print_info: n_head_kv        = 16
0.00.039.047 I print_info: n_rot            = 32
0.00.039.047 I print_info: n_swa            = 0
0.00.039.047 I print_info: n_embd_head_k    = 128
0.00.039.047 I print_info: n_embd_head_v    = 128
0.00.039.048 I print_info: n_gqa            = 1
0.00.039.051 I print_info: n_embd_k_gqa     = 2048
0.00.039.052 I print_info: n_embd_v_gqa     = 2048
0.00.039.053 I print_info: f_norm_eps       = 1.0e-05
0.00.039.053 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.054 I print_info: f_logit_scale    = 0.0e+00
0.00.039.056 I print_info: n_ff             = 8192
0.00.039.056 I print_info: n_expert         = 0
0.00.039.056 I print_info: n_expert_used    = 0
0.00.039.056 I print_info: causal attn      = 1
0.00.039.056 I print_info: pooling type     = 0
0.00.039.056 I print_info: rope type        = 2
0.00.039.057 I print_info: rope scaling     = linear
0.00.039.057 I print_info: freq_base_train  = 10000.0
0.00.039.057 I print_info: freq_scale_train = 1
0.00.039.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.058 I print_info: rope_finetuned   = unknown
0.00.039.058 I print_info: ssm_d_conv       = 0
0.00.039.058 I print_info: ssm_d_inner      = 0
0.00.039.058 I print_info: ssm_d_state      = 0
0.00.039.058 I print_info: ssm_dt_rank      = 0
0.00.039.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.059 I print_info: model type       = 1.4B
0.00.039.063 I print_info: model params     = 1.41 B
0.00.039.063 I print_info: general.name     = 1.4B
0.00.039.064 I print_info: vocab type       = BPE
0.00.039.064 I print_info: n_vocab          = 50304
0.00.039.064 I print_info: n_merges         = 50009
0.00.039.064 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.064 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.064 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.065 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.065 I print_info: LF token         = 187 'Ċ'
0.00.039.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.065 I print_info: max token length = 1024
0.00.039.066 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.623.290 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.295 I load_tensors: offloading output layer to GPU
0.00.623.297 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.331 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.623.335 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.624.629 I llama_context: n_seq_max     = 1
0.00.624.631 I llama_context: n_ctx         = 128
0.00.624.632 I llama_context: n_ctx_per_seq = 128
0.00.624.632 I llama_context: n_batch       = 128
0.00.624.632 I llama_context: n_ubatch      = 128
0.00.624.633 I llama_context: flash_attn    = 0
0.00.624.634 I llama_context: freq_base     = 10000.0
0.00.624.634 I llama_context: freq_scale    = 1
0.00.624.635 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.624.636 I ggml_metal_init: allocating
0.00.624.652 I ggml_metal_init: found device: Apple M4
0.00.624.662 I ggml_metal_init: picking default device: Apple M4
0.00.626.043 I ggml_metal_init: using embedded metal library
0.00.632.235 I ggml_metal_init: GPU name:   Apple M4
0.00.632.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.240 I ggml_metal_init: simdgroup reduction   = true
0.00.632.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.241 I ggml_metal_init: has residency sets    = true
0.00.632.241 I ggml_metal_init: has bfloat            = true
0.00.632.241 I ggml_metal_init: use bfloat            = true
0.00.632.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.246 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.436 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.653.016 I init:      Metal KV buffer size =    24.00 MiB
0.00.653.022 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.653.052 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.656.243 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.656.244 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.656.245 I llama_context: graph nodes  = 967
0.00.656.245 I llama_context: graph splits = 2
0.00.656.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.656.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.410 I 
0.00.688.511 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.532 I perplexity: tokenizing the input ..
0.00.695.477 I perplexity: tokenization took 6.94 ms
0.00.695.498 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.478 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.838.783 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.838.804 I llama_perf_context_print:        load time =     679.08 ms
0.00.838.805 I llama_perf_context_print: prompt eval time =     140.97 ms /   128 tokens (    1.10 ms per token,   908.00 tokens per second)
0.00.838.806 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.807 I llama_perf_context_print:       total time =     150.40 ms /   129 tokens
0.00.839.436 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.079s
sys	0m0.156s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.409 I build: 4711 (972f91c7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.352 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.454 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.475 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.475 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.476 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.477 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.479 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.481 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.269 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.271 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.271 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.272 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.273 I llama_model_loader: - type  f32:  194 tensors
0.00.057.273 I llama_model_loader: - type  f16:   98 tensors
0.00.057.274 I print_info: file format = GGUF V3 (latest)
0.00.057.275 I print_info: file type   = all F32 (guessed)
0.00.057.277 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.326 I load: special tokens cache size = 25
0.00.078.663 I load: token to piece cache size = 0.2984 MB
0.00.078.667 I print_info: arch             = gptneox
0.00.078.667 I print_info: vocab_only       = 0
0.00.078.667 I print_info: n_ctx_train      = 2048
0.00.078.667 I print_info: n_embd           = 2048
0.00.078.667 I print_info: n_layer          = 24
0.00.078.671 I print_info: n_head           = 16
0.00.078.672 I print_info: n_head_kv        = 16
0.00.078.673 I print_info: n_rot            = 32
0.00.078.673 I print_info: n_swa            = 0
0.00.078.673 I print_info: n_embd_head_k    = 128
0.00.078.673 I print_info: n_embd_head_v    = 128
0.00.078.676 I print_info: n_gqa            = 1
0.00.078.677 I print_info: n_embd_k_gqa     = 2048
0.00.078.677 I print_info: n_embd_v_gqa     = 2048
0.00.078.678 I print_info: f_norm_eps       = 1.0e-05
0.00.078.678 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.679 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.679 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.679 I print_info: f_logit_scale    = 0.0e+00
0.00.078.680 I print_info: n_ff             = 8192
0.00.078.681 I print_info: n_expert         = 0
0.00.078.682 I print_info: n_expert_used    = 0
0.00.078.682 I print_info: causal attn      = 1
0.00.078.682 I print_info: pooling type     = 0
0.00.078.682 I print_info: rope type        = 2
0.00.078.682 I print_info: rope scaling     = linear
0.00.078.683 I print_info: freq_base_train  = 10000.0
0.00.078.683 I print_info: freq_scale_train = 1
0.00.078.683 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.684 I print_info: rope_finetuned   = unknown
0.00.078.684 I print_info: ssm_d_conv       = 0
0.00.078.684 I print_info: ssm_d_inner      = 0
0.00.078.684 I print_info: ssm_d_state      = 0
0.00.078.684 I print_info: ssm_dt_rank      = 0
0.00.078.684 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.685 I print_info: model type       = 1.4B
0.00.078.685 I print_info: model params     = 1.41 B
0.00.078.685 I print_info: general.name     = 1.4B
0.00.078.686 I print_info: vocab type       = BPE
0.00.078.686 I print_info: n_vocab          = 50304
0.00.078.686 I print_info: n_merges         = 50009
0.00.078.687 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.687 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.687 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.687 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.688 I print_info: LF token         = 187 'Ċ'
0.00.078.688 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.688 I print_info: max token length = 1024
0.00.078.689 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.491.619 I load_tensors: offloading 24 repeating layers to GPU
0.01.491.626 I load_tensors: offloading output layer to GPU
0.01.491.626 I load_tensors: offloaded 25/25 layers to GPU
0.01.491.654 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.491.657 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.492.574 I llama_context: n_seq_max     = 1
0.01.492.575 I llama_context: n_ctx         = 128
0.01.492.575 I llama_context: n_ctx_per_seq = 128
0.01.492.576 I llama_context: n_batch       = 128
0.01.492.576 I llama_context: n_ubatch      = 128
0.01.492.576 I llama_context: flash_attn    = 0
0.01.492.577 I llama_context: freq_base     = 10000.0
0.01.492.577 I llama_context: freq_scale    = 1
0.01.492.578 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.492.584 I ggml_metal_init: allocating
0.01.492.670 I ggml_metal_init: found device: Apple M4
0.01.492.680 I ggml_metal_init: picking default device: Apple M4
0.01.493.917 I ggml_metal_init: using embedded metal library
0.01.502.318 I ggml_metal_init: GPU name:   Apple M4
0.01.502.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.502.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.502.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.502.323 I ggml_metal_init: simdgroup reduction   = true
0.01.502.323 I ggml_metal_init: simdgroup matrix mul. = true
0.01.502.323 I ggml_metal_init: has residency sets    = true
0.01.502.324 I ggml_metal_init: has bfloat            = true
0.01.502.324 I ggml_metal_init: use bfloat            = true
0.01.502.324 I ggml_metal_init: hasUnifiedMemory      = true
0.01.502.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.515.441 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.517.410 I init:      Metal KV buffer size =    24.00 MiB
0.01.517.413 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.517.428 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.519.332 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.519.333 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.519.334 I llama_context: graph nodes  = 967
0.01.519.334 I llama_context: graph splits = 2
0.01.519.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.519.336 I 
0.01.519.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.519.375 I compute_imatrix: tokenizing the input ..
0.01.523.929 I compute_imatrix: tokenization took 4.553 ms
0.01.523.931 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.785.719 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.788.289 I llama_perf_context_print:        load time =    1759.36 ms
0.01.788.290 I llama_perf_context_print: prompt eval time =     260.48 ms /   128 tokens (    2.04 ms per token,   491.40 tokens per second)
0.01.788.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.788.291 I llama_perf_context_print:       total time =    1761.93 ms /   129 tokens
0.01.788.986 I ggml_metal_free: deallocating

real	0m1.979s
user	0m0.133s
sys	0m0.289s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4711 (972f91c7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154208c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1542090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154209540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1542099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154209e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15420a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15420a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15420ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15420afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15420b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15420b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15420bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15420ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15420d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15420da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15420e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15420e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15420efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15420f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15420fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1542105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154210cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1542113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154211c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1542123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154212670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154212930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154212da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1542134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154213930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154213ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154214400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154214870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154214b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154214fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154215410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154215880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154215cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154216160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1542165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154216a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154216eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154217320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154217790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154217c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154218070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1542184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154218950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1542190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154219550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1542199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154219e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15421a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15421a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15421ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15421b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15421b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15421ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15421be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15421c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15421c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15421cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15421d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15421d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15421db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15421e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15421e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15421ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15421ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15421f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15421f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15421fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154220300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154220800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x154220db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154221360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154221910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154221ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154222470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154222a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x154222fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154223580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154223b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1542240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154224690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154224c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1542251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1542257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154225d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154226300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1542268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154226e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154227410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1542279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154227f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154228520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154228ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154218c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154229230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1542296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154229b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15422a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15422a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15422ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15422b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15422b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15422bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15422c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15422c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15422ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15422d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15422d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15422df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15422e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15422ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15422ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15422f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15422f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15422fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154230300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154230800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154230d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154231200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154231700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154231c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154232100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154232600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154232b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154233000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154233500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154233a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154233f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154234400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154234900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154234e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154235300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154235800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154235d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154236200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154236700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154236c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154237100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154237600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154237b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154238000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154238500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154238a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154238f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154239400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154239900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154239e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15423a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15423a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15423ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15423b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15423b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15423bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15423c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15423c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15423cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15423d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15423d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15423da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15423df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15423e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15423e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15423ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15423f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15423f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15423fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154240200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154240700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154240c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154241100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154241600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154241b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154242000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154242500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154242a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154242f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154243400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154243900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154243e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154244300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154244800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154244d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154245200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154245700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154245c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154246100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154246600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154246b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154247000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154247500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154247ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154248060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154248610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154248bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1542491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1542497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154249df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15424a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15424aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15424ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15424b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15424b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15424c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15424c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15424ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15424cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15424d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15424dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15424e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15424e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15424ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15424f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15424f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15424fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154250160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1542506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154250c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154251150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1542516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154251bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154252140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154252690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154252be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154253130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154253680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154253bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154254120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154254670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154254bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154255110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154255660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154255bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154256100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154256650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154256ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1542570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154257640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154257b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1542580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154258630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154258b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1542590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154259620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154259b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15425a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15425a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15425ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15425b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15425b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15425bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15425c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15425c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15425cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15425d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15425d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15425db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15425e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15425e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15425eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15425f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15425f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15425fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154260060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154260500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1542609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154260e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1542612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154261780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154261c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1542620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154262560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154262a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154262ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154263340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1542637e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154263c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154264120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1542645c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154264b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154265230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154265950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154266070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154266790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154266a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154267240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154267500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154267b10 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
0.00.752.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.752.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138704ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138704f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1387053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138705830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138705ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138706110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138706580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1387069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138706e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1387073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138707850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138707ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1387089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1387091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1387099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13870a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13870a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13870af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13870b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13870be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13870c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13870cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13870d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13870da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13870e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13870e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13870e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13870eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13870f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13870f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13870f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13870fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138710280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138710540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1387109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138710e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138711290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138711700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138711b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138711fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138712450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1387128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138712d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1387131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138713610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138713a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138713ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1387147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138714c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1387150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138715520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138715990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138715e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138716270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1387166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138716c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138717150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1387175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138717a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138717ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138718310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138718780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138719060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1387194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138719db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13871a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13871a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13871ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13871af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13871b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13871b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13871bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13871c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13871c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13871ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13871ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13871d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13871d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13871dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13871e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13871e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13871e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13871ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13871f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13871f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13871fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13871ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1387203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138720830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138720ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138721110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138721580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1387219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138721e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1387222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138722740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138722bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138723020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138723490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138723900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138723d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1387241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138724650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138724ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138724f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1387253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138725810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138725c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1387260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138726560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1387269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138726e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1387272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138727720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138727b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138728000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138728470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1387288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138728d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1387291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138729630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138729aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138729f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13872a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13872a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13872ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13872b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13872b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13872b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13872be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13872c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13872c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13872cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13872cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13872d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13872d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13872dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13872e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13872e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13872ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13872eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13872f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13872f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13872fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1387300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138730520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138730990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138730e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138731270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1387316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138731b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138731fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138732430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1387328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138732d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138733180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1387335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138733a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138733ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138734340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1387347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138734c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138735090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138735cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138735f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138736240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1387366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138736b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138736f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138737400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138737870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138737ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138738150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1387385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138738a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138738ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138739310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138739780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13873a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13873a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13873a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13873adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13873b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13873b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13873bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13873bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13873c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13873c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13873ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13873d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13873d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13873da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13873de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13873e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13873e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13873ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13873f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13873f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13873fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13873ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138740390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138740800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138740c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1387410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138741600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138741b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138742680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138742940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138742f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1387434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138743a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138744040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138744600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138744bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138745180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138745740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138745d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1387462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138746880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138746e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138747400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1387479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138747f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138748540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138748b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1387490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138749680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138749c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13874a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13874a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13874ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13874b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13874b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13874bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13874c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13874ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13874d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13874d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13874db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13874e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13874e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13874ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13874f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13874f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13874fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1387503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138750980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138750f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138751500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138751ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138752080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138752640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138752c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1387531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138753780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138753d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138754300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1387548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138754e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138755440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138755a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138755fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138756580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138756b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138757040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138757540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138757f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138758440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138758940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138758e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138759340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138759840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138759d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13875a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13875a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13875ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13875b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13875b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13875c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13875c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13875ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13875d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13875d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13875e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13875e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13875e930 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154221070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1542265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154220ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1542287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154226010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15422d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15422d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15422cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154228230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154222ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15422aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154247d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154227c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154222730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154225a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1542243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15422a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1542477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15422c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1542276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154222180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1542254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154223df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15422a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15422bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154227120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154221bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154224f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154229dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15422ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154226b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154224950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15422b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1542677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154248e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154249aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15424b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154211930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15420bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15421c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154266d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15421ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154228d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15424bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15424a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154213060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154267f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154268230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1542684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1542687b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x154268a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154268d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x154268ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1542692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x154269570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154269830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x154269af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x154269db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15426a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15426a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15426a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15426a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15426ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15426ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15426b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15426b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15426b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15426b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15426bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15426beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15426c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15426c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15426c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15426c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15426cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15426cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15426d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15426d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15426d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15426da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15426dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15426dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15426e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15426e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15426e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15426eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15426ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15426f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15426f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15426f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15426f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15426fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15426fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1542700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154270370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154270630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1542708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154270bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154270e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154271130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1542713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1542716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154271970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154271c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154271ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1542721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x154272470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154272730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1542729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154272cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154272f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154273230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1542734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1542737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154273a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154273d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154273ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1542742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154274570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154274830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154274af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154274db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154275070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154275330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1542755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1542758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154275b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154275e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1542760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1542763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154276670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154276930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154276bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154276eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154277170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154277430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1542776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1542779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154277c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154277f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1542781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1542784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154278770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154278a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154278cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154278fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154279270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154279530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1542797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154279ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154279d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15427a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15427a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15427a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15427a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15427ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15427adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15427b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15427b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15427b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15427b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15427bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15427be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15427c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15427c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15427c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15427c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15427cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15427cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15427d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15427d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15427d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15427d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15427dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15427df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15427e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15427e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15427e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15427ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15427ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15427eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15427f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15427f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15427f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15427faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15427fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154280070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154280330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1542805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1542808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154280b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154280e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1542810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1542813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154281670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154281930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154281bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154281eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154282170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154282430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1542826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1542829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154282c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154282f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1542831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1542834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154283770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x154283a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154283cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154283fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154284580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x154284840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154284b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154284dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154285080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154285340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154285600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1542858c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154285b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154285e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154286390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1542868e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154286e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154287380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1542878d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154287e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154288370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1542888c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154288e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154289360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1542898b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154289e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15428a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15428a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15428adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15428b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15428b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15428bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15428c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15428c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15428cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15428d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15428d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15428ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15428e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15428e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15428edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15428f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15428f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15428fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1542902f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154290840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154290d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1542912e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154291830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x154291d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1542922d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154292820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154292d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1542932c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154293810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154293d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1542942b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154294800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154294d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1542952a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1542957f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154295c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154296130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1542965d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154296a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154296f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1542973b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154297850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154297cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154298190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154298630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154298ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154298f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154299410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1542998b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154299d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15429a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15429a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15429b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15429b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15429bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15429c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15429c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15429cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15429d2a0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.809s
user	0m0.281s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4711 (972f91c7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d00ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d00b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d00b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d00be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d00c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d00c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d00cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d00d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d00db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d00e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d00e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d00ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d00f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d00fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d0104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d010c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d011320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d011a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d012160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d012930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d013050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d013770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d013e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d014730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d014e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d015110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d015720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d016390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d0168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d016b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d017030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d0172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d017b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d0180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d018380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d018820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d018cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d019160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d019600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d019aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d019f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d01a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d01a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d01ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d01afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d01b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d01bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d01c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d01cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d01d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d01d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d01dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d01e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d01e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d01f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d01f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d01fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d01fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d020380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d020b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d020e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d0212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d021770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d0220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d022550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d0229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d022e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d023330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d0237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d023c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d024110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d0245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d024b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d025050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d0255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d025af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d026590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d026ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d027030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d027580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d027ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d028020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d028570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d028ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d029010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d029560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d029ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d02a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d02a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d02aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d02aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d02b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d02ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d02bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d02c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d01c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d02c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d02d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d02d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d02dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d02e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d02e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d02ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d02f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d02f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d02fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d030120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d030670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d030bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d031110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d031660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d031b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d031fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d032440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d0328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d032d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d033220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d0336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d033b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d034000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d0344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d034940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d034de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d035280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d035720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d035bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d036060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d036500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d0369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d036e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d0372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d037780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d037c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d0380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d038560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d038a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d038ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d0397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d039c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d03a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d03a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d03aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d03af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d03b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d03b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d03bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d03c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d03c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d03cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d03cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d03d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d03d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d03dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d03e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d03e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d03eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d03efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d03f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d03f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d03fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d040240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d0406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d040b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d041020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d0414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d041960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d041e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d0422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d042740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d042be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d043080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d043520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d0439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d043e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d044300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d0447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d044c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d0450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d045580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d045a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d045ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d046360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d046800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d046ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d047140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d0475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d047a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d047f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d0483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d048860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d048db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d049300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d049850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d049da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d04a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d04a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d04ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d04b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d04ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d04bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d04c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d04c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d04ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d04d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d04da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d04df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d04e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d04eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d04f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d04f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d04fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d0500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d050610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d050b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d0510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d051600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d051b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d0520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d0525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d052b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d053090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d0535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d053b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d054080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d0545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d054b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d055070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d0555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d055b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d056060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d0565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d056b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d057050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d0575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d057af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d058040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d058590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d058ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d059030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d059580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d059ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d05a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d05a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d05aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d05b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d05b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d05bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d05c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d05c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d05caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d05cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d05d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d05da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d05dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d05e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d05ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d05efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d05f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d05fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d05ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d060510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d060a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d060fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d061500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d0619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d061e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d0622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d062780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d062c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d0630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d063560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d063a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d063ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d064340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d0647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d064c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d065120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d0655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d065a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d065fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d0666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d066df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d067510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d067c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d067ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d0686e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d0689a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d068fb0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
0.00.095.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14be05ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14be06360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14be067d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14be06c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14be070b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14be07520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14be07990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14be07e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14be08270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14be086e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14be08b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14be09210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14be09d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14be0a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14be0acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14be0b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14be0bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14be0c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14be0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14be0d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14be0d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14be0df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14be0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14be0edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14be0f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14be0f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14be0fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14be0fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14be10340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14be107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14be10cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14be111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14be11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14be118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14be11d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14be121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14be12730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14be12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14be13130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14be13630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14be13b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14be14030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14be14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14be14a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14be14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14be153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14be15810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14be15c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14be160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14be16560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14be169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14be16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14be172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14be17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14be17b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14be18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14be18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14be18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14be190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14be198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14be19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14be1a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14be1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14be1ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bf09980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bf09df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bf0a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bf0aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bf0af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bf0b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bf0b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bf0bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bf0c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bf0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bf0cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bf0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bf0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bf0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bf0e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bf0e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bf0ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bf0f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bf0f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bf0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bf10140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bf10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bf10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bf11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bf11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bf11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bf12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bf12670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bf12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bf13110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bf13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bf13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bf14100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bf14650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bf14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bf150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bf15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bf15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bf160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bf16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bf16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bf170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bf17620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bf17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bf180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bf18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bf18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bf190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bf19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bf19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bf19f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bf1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bf1a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bf1ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bf1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bf1b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bf1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bf1bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bf1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bf1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bf1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bf1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bf1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bf1db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bf1e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bf1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bf1e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bf1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bf1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bf1f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bf1fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bf20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bf20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bf209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bf20e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bf212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bf21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bf21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bf220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bf22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bf22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bf22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bf23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bf237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bf23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bf24120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bf245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bf24a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bf24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bf253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bf25840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bf25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bf26180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bf26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bf26ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bf26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bf27400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bf278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bf27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bf281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bf28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bf28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bf29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bf294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bf299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bf29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bf2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bf2a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bf2adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bf2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bf2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bf2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bf2c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bf2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bf2cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bf2d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bf2d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bf2daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bf2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bf2e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bf2e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bf2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bf2f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bf2f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bf2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bf302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bf307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bf30cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bf311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bf316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bf31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bf32250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bf32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bf32db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bf333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bf339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bf33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bf347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bf34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bf34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bf35540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bf35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bf36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bf367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bf36c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bf37120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bf378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bf37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bf38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bf388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bf38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bf39360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bf398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf39e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf3a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf3a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf3adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf3c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf3d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf3ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf3e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf40840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf40d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf41830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf41d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf42820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf42d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf43810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf43d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf44800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf44d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf45d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf46d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf47280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf47d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf48d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf49260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf49d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf4a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf4ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf4b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf4b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf4be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bf4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bf4c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bf4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bf4d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf4d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bf4de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf4e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bf4ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bf4f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bf4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bf50260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bf50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bf50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bf51430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bf516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf51d00 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d068c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d04a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d04a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d04af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d01e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d01da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d020030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d04cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d0153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d01bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d01c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d01cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d01b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d01d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d0143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d020640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d02cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d0681b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d0175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d017870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d04d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d04b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d0159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d015ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d015f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d069410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d0696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d069990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d069c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d069f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d06a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d06a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d06a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d06aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d06acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d06af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d06b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d06b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d06b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d06ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d06bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d06c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d06c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d06c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d06c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d06cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d06cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d06d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d06d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d06d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d06d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d06db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d06de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d06e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d06e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d06e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d06e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d06ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d06eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d06f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d06f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d06f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d06f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d06fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d06ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d070210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d0704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d070790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d070a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d070d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d070fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d071290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d071550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d071810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d071ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d071d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d072050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d072310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d0725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d072890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d072b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d072e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d0730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d073390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d073650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d073910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d073bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d073e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d074150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d074410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d0746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d074990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d074c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d074f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d0751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d075490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d075750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d075a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d075cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d075f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d076250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d076510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d0767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d076a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d076d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d077010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d0772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d077590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d077850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d077b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d077dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d078090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d078350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d078610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d0788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d078b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d078e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d079110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d0793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d079690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d079950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d079c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d079ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d07a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d07a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d07a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d07a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d07ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d07af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d07b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d07b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d07b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d07ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d07bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d07bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d07c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d07c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d07c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d07cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d07cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d07d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d07d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d07d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d07d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d07db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d07de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d07e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d07e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d07e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d07e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d07ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d07ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d07f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d07f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d07f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d07f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d07fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d07ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d0801d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d080490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d080750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d080a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d080cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d080f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d081250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d081510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d0817d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d081a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d081d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d082010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d0822d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d082590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d082850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d082b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d082dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d083090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d083350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d083610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d0838d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d083b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d083e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d084110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d0843d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d084690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d084950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d084c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d084ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d085190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d085450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d085710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d0859d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d085c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d085f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d086210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d0864d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d086790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d086a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d086d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d086fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d087290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d087550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d087810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d087ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d087d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d088050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d088310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d0885d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d088890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d088c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d089130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d0898e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d089ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d089e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d08a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d08a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d08abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d08b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d08b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d08b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d08bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d08c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d08c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d08cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d08cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d08d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d08d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d08dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d08e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d08e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d08e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d08ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d08f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d08f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d08fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d090000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d090470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d0908e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d090d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d0911c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d091630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d091aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d091f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d092380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d0927f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d092c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d0930d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d093540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d0939b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d093e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d094290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d094700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d094b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d094fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d095450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d0958c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d095d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d0961a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d096610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d096a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d096ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d097360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d0977d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d097c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d0980b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d098520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d098990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d098e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d099270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d0996e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d099b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d099fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d09a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d09a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d09ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d09b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d09b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d09ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d09bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d09c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d09c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d09cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d09d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d09d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d09df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d09e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d09edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d09f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d09f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d09ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d0a0240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d0a0850 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.948s
user	0m0.230s
sys	0m0.184s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
