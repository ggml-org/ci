+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DGGML_CUDA=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- OpenMP found
-- Using llamafile
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- CUDA found
-- Using CUDA architectures: 52;61;70;75
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (4.8s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m5.040s
user	0m3.764s
sys	0m1.276s
+ make -j
[  0%] Generating build details from Git
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o
[  4%] Built target sha1
[  4%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o
[  5%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o
[  5%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o
[  6%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o
[  6%] Built target sha256
[  6%] Built target xxhash
[  6%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o
[  7%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/conv-transpose-1d.cu.o
[  7%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o
[  8%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o
[  8%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  8%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o
[  9%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o
[  9%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn-tile-f16.cu.o
[  9%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn-tile-f32.cu.o
[ 10%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o
[ 10%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o
[ 11%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o
[ 11%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o
[ 12%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o
[ 12%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o
[ 12%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o
[ 13%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o
[ 13%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o
[ 13%] Built target build_info
[ 13%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o
[ 14%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o
[ 15%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o
[ 15%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o
[ 15%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o
[ 16%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o
[ 17%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o
[ 17%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.cu.o
[ 17%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda.cu.o
[ 18%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.cu.o
[ 18%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.cu.o
[ 19%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.cu.o
[ 20%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu.o
[ 20%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.cu.o
[ 20%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu.o
[ 21%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q2_k.cu.o
[ 21%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q3_k.cu.o
[ 21%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_0.cu.o
[ 22%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_1.cu.o
[ 22%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_k.cu.o
[ 23%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_0.cu.o
[ 23%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_1.cu.o
[ 24%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_k.cu.o
[ 24%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q8_0.cu.o
[ 24%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q6_k.cu.o
[ 25%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o
[ 25%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o
[ 26%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o
[ 26%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o
[ 27%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o
[ 27%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o
[ 28%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o
[ 28%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o
[ 28%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o
[ 29%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o
[ 29%] Building CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o
[ 30%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o
[ 30%] Linking CXX shared library libggml.so
[ 30%] Built target ggml
[ 31%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 31%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 31%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 31%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 32%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 32%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-gguf
[ 34%] Linking CXX executable ../../bin/llama-gguf-hash
[ 34%] Built target llama-gguf
[ 34%] Built target llama-gguf-hash
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘void llama_sample_softmax(llama_context*, llama_token_data_array*)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:21:14: error: ‘sort’ is not a member of ‘std’; did you mean ‘sqrt’?
   21 |         std::sort(candidates->data, candidates->data + candidates->size, [](const llama_token_data & a, const llama_token_data & b) {
      |              ^~~~
      |              sqrt
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘void llama_sample_top_k(llama_context*, llama_token_data_array*, int32_t, size_t)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:64:18: error: ‘partial_sort’ is not a member of ‘std’; did you mean ‘partial_sum’?
   64 |             std::partial_sort(candidates->data, candidates->data + k, candidates->data + candidates->size, comp);
      |                  ^~~~~~~~~~~~
      |                  partial_sum
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:106:22: error: ‘sort’ is not a member of ‘std’; did you mean ‘sqrt’?
  106 |                 std::sort(ptr, ptr + histo[j], comp);
      |                      ^~~~
      |                      sqrt
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:110:18: error: ‘partial_sort’ is not a member of ‘std’; did you mean ‘partial_sum’?
  110 |             std::partial_sort(ptr, ptr + k - ndone, ptr + histo[ib], comp);
      |                  ^~~~~~~~~~~~
      |                  partial_sum
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:112:18: error: ‘memcpy’ is not a member of ‘std’; did you mean ‘wmemcpy’?
  112 |             std::memcpy(candidates->data, tmp_tokens.data(), k*sizeof(llama_token_data));
      |                  ^~~~~~
      |                  wmemcpy
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘void llama_sample_min_p(llama_context*, llama_token_data_array*, float, size_t)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:183:13: error: ‘memcpy’ was not declared in this scope
  183 |             memcpy(candidates->data, filtered_tokens.data(), filtered_tokens.size()*sizeof(llama_token_data));
      |             ^~~~~~
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:5:1: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
    4 | #include <cfloat>
  +++ |+#include <cstring>
    5 | 
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:193:18: error: ‘sort’ is not a member of ‘std’; did you mean ‘sqrt’?
  193 |             std::sort(candidates->data, candidates->data + candidates->size, [](const llama_token_data & a, const llama_token_data & b) {
      |                  ^~~~
      |                  sqrt
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘void llama_sample_typical(llama_context*, llama_token_data_array*, float, size_t)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:304:10: error: ‘sort’ is not a member of ‘std’; did you mean ‘sqrt’?
  304 |     std::sort(indices.begin(), indices.end(), [&](size_t a, size_t b) {
      |          ^~~~
      |          sqrt
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘void llama_sample_repetition_penalties(llama_context*, llama_token_data_array*, const llama_token*, size_t, float, float, float)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:434:10: error: ‘unordered_map’ is not a member of ‘std’
  434 |     std::unordered_map<llama_token, int> token_count;
      |          ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:5:1: note: ‘std::unordered_map’ is defined in header ‘<unordered_map>’; did you forget to ‘#include <unordered_map>’?
    4 | #include <cfloat>
  +++ |+#include <unordered_map>
    5 | 
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:434:35: error: expected primary-expression before ‘,’ token
  434 |     std::unordered_map<llama_token, int> token_count;
      |                                   ^
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:434:37: error: expected primary-expression before ‘int’
  434 |     std::unordered_map<llama_token, int> token_count;
      |                                     ^~~
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:436:9: error: ‘token_count’ was not declared in this scope
  436 |         token_count[last_tokens[i]]++;
      |         ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:441:33: error: ‘token_count’ was not declared in this scope
  441 |         const auto token_iter = token_count.find(candidates->data[i].id);
      |                                 ^~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘void llama_log_softmax(float*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:467:25: error: ‘max_element’ is not a member of ‘std’; did you mean ‘tuple_element’?
  467 |     float max_l = *std::max_element(array, array + size);
      |                         ^~~~~~~~~~~
      |                         tuple_element
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘llama_token llama_sample_token_mirostat(llama_context*, llama_token_data_array*, float, float, int32_t, float*)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:535:57: error: ‘find_if’ is not a member of ‘std’
  535 |     size_t X_idx = std::distance(candidates->data, std::find_if(candidates->data, candidates->data + candidates->size, [&](const llama_token_data & candidate) {
      |                                                         ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘llama_token llama_sample_token_mirostat_v2(llama_context*, llama_token_data_array*, float, float, float*)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:555:61: error: ‘find_if’ is not a member of ‘std’
  555 |     candidates->size = std::distance(candidates->data, std::find_if(candidates->data, candidates->data + candidates->size, [&](const llama_token_data & candidate) {
      |                                                             ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:575:57: error: ‘find_if’ is not a member of ‘std’
  575 |     size_t X_idx = std::distance(candidates->data, std::find_if(candidates->data, candidates->data + candidates->size, [&](const llama_token_data & candidate) {
      |                                                         ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-sampling.cpp: In function ‘llama_token llama_sample_token_greedy(llama_context*, llama_token_data_array*)’:
/home/ggml/work/llama.cpp/src/llama-sampling.cpp:594:28: error: ‘max_element’ is not a member of ‘std’; did you mean ‘tuple_element’?
  594 |     auto * max_iter = std::max_element(candidates->data, candidates->data + candidates->size, [](const llama_token_data & a, const llama_token_data & b) {
      |                            ^~~~~~~~~~~
      |                            tuple_element
make[2]: *** [src/CMakeFiles/llama.dir/build.make:90: src/CMakeFiles/llama.dir/llama-sampling.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1663: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m15.176s
user	0m14.332s
sys	0m2.137s
