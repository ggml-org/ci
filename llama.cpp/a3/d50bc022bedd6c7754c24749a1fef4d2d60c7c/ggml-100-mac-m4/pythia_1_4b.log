Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.525s
user	0m0.860s
sys	0m1.215s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Built target llava
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX static library libcommon.a
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-sampling
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-sampling
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-log
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Built target test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Built target test-backend-ops
[ 60%] Built target test-chat-template
[ 60%] Built target test-model-load-cancel
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Linking CXX executable ../bin/test-rope
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Built target test-quantize-fns
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target test-rope
[ 70%] Built target test-quantize-perf
[ 70%] Built target llama-batched-bench
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-embedding
[ 70%] Built target llama-batched
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-imatrix
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookahead
[ 79%] Built target llama-infill
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-cli
[ 81%] Built target llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Built target llama-passkey
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-perplexity
[ 89%] Built target llama-retrieval
[ 89%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-run
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-speculative
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative-simple
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-tokenize
[ 94%] Linking CXX executable ../../bin/llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-tts
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-gen-docs
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.058s
user	0m5.964s
sys	0m9.458s

main: quantize time =  2567.63 ms
main:    total time =  2567.63 ms

main: quantize time =  1323.62 ms
main:    total time =  1323.62 ms

main: quantize time =  1503.44 ms
main:    total time =  1503.44 ms

main: quantize time =  1417.73 ms
main:    total time =  1417.73 ms

main: quantize time =  1756.81 ms
main:    total time =  1756.81 ms

main: quantize time =  4971.89 ms
main:    total time =  4971.89 ms

main: quantize time =  5652.44 ms
main:    total time =  5652.44 ms

main: quantize time =  7039.38 ms
main:    total time =  7039.38 ms

main: quantize time =  6105.75 ms
main:    total time =  6105.75 ms

main: quantize time =  4589.95 ms
main:    total time =  4589.95 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.194 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.371 I main: llama backend init
0.00.000.380 I main: load the model and apply lora adapter, if any
0.00.073.012 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.084.294 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.084.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.084.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.084.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.084.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.084.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.084.317 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.084.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.084.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.084.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.084.326 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.084.326 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.084.327 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.084.328 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.084.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.084.332 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.084.332 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.091.485 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.093.758 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.100.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.100.946 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.100.947 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.100.948 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.100.949 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.100.950 I llama_model_loader: - type  f32:  194 tensors
0.00.100.951 I llama_model_loader: - type  f16:   98 tensors
0.00.140.071 I llm_load_vocab: special tokens cache size = 25
0.00.147.654 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.147.658 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.147.659 I llm_load_print_meta: arch             = gptneox
0.00.147.659 I llm_load_print_meta: vocab type       = BPE
0.00.147.659 I llm_load_print_meta: n_vocab          = 50304
0.00.147.659 I llm_load_print_meta: n_merges         = 50009
0.00.147.660 I llm_load_print_meta: vocab_only       = 0
0.00.147.660 I llm_load_print_meta: n_ctx_train      = 2048
0.00.147.662 I llm_load_print_meta: n_embd           = 2048
0.00.147.663 I llm_load_print_meta: n_layer          = 24
0.00.147.666 I llm_load_print_meta: n_head           = 16
0.00.147.667 I llm_load_print_meta: n_head_kv        = 16
0.00.147.667 I llm_load_print_meta: n_rot            = 32
0.00.147.668 I llm_load_print_meta: n_swa            = 0
0.00.147.668 I llm_load_print_meta: n_embd_head_k    = 128
0.00.147.668 I llm_load_print_meta: n_embd_head_v    = 128
0.00.147.669 I llm_load_print_meta: n_gqa            = 1
0.00.147.669 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.147.670 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.147.671 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.147.671 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.147.671 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.147.671 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.147.671 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.147.672 I llm_load_print_meta: n_ff             = 8192
0.00.147.672 I llm_load_print_meta: n_expert         = 0
0.00.147.672 I llm_load_print_meta: n_expert_used    = 0
0.00.147.673 I llm_load_print_meta: causal attn      = 1
0.00.147.673 I llm_load_print_meta: pooling type     = 0
0.00.147.673 I llm_load_print_meta: rope type        = 2
0.00.147.673 I llm_load_print_meta: rope scaling     = linear
0.00.147.674 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.147.674 I llm_load_print_meta: freq_scale_train = 1
0.00.147.674 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.147.674 I llm_load_print_meta: rope_finetuned   = unknown
0.00.147.675 I llm_load_print_meta: ssm_d_conv       = 0
0.00.147.675 I llm_load_print_meta: ssm_d_inner      = 0
0.00.147.675 I llm_load_print_meta: ssm_d_state      = 0
0.00.147.675 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.147.675 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.147.675 I llm_load_print_meta: model type       = 1.4B
0.00.147.676 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.147.676 I llm_load_print_meta: model params     = 1.41 B
0.00.147.677 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.147.678 I llm_load_print_meta: general.name     = 1.4B
0.00.147.679 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.147.679 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.147.679 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.147.679 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.147.680 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.147.680 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.147.680 I llm_load_print_meta: max token length = 1024
0.00.150.494 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.150.494 I llm_load_tensors: offloading output layer to GPU
0.00.150.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.150.513 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.150.514 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.151.524 I llama_new_context_with_model: n_seq_max     = 1
0.00.151.525 I llama_new_context_with_model: n_ctx         = 2048
0.00.151.525 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.151.526 I llama_new_context_with_model: n_batch       = 2048
0.00.151.526 I llama_new_context_with_model: n_ubatch      = 512
0.00.151.526 I llama_new_context_with_model: flash_attn    = 0
0.00.151.527 I llama_new_context_with_model: freq_base     = 10000.0
0.00.151.527 I llama_new_context_with_model: freq_scale    = 1
0.00.151.527 I ggml_metal_init: allocating
0.00.151.530 I ggml_metal_init: found device: Apple M4
0.00.151.533 I ggml_metal_init: picking default device: Apple M4
0.00.152.242 I ggml_metal_init: using embedded metal library
0.00.289.259 I ggml_metal_init: GPU name:   Apple M4
0.00.289.280 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.289.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.289.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.289.283 I ggml_metal_init: simdgroup reduction   = true
0.00.289.283 I ggml_metal_init: simdgroup matrix mul. = true
0.00.289.284 I ggml_metal_init: has bfloat            = true
0.00.289.284 I ggml_metal_init: use bfloat            = true
0.00.289.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.289.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.322.704 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.348.180 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.348.187 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.348.208 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.349.127 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.349.129 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.349.130 I llama_new_context_with_model: graph nodes  = 967
0.00.349.130 I llama_new_context_with_model: graph splits = 2
0.00.349.133 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.349.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.349.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.073 I main: llama threadpool init, n_threads = 4
0.00.432.116 I 
0.00.432.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.432.145 I 
0.00.432.366 I sampler seed: 1234
0.00.432.372 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.432.416 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.432.418 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.432.418 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.263.923 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.02.263.924 I llama_perf_context_print:        load time =     359.05 ms
0.02.263.925 I llama_perf_context_print: prompt eval time =      44.13 ms /     7 tokens (    6.30 ms per token,   158.63 tokens per second)
0.02.263.926 I llama_perf_context_print:        eval time =    1784.38 ms /    63 runs   (   28.32 ms per token,    35.31 tokens per second)
0.02.263.926 I llama_perf_context_print:       total time =    1831.85 ms /    70 tokens
0.02.264.146 I ggml_metal_free: deallocating

real	0m2.568s
user	0m0.166s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.980 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.811 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.816 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.744 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.692 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.694 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.695 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.695 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.695 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.696 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.696 I llama_model_loader: - type  f32:  194 tensors
0.00.033.696 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.102 I llm_load_vocab: special tokens cache size = 25
0.00.062.218 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.223 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.224 I llm_load_print_meta: arch             = gptneox
0.00.062.224 I llm_load_print_meta: vocab type       = BPE
0.00.062.225 I llm_load_print_meta: n_vocab          = 50304
0.00.062.225 I llm_load_print_meta: n_merges         = 50009
0.00.062.225 I llm_load_print_meta: vocab_only       = 0
0.00.062.225 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.225 I llm_load_print_meta: n_embd           = 2048
0.00.062.225 I llm_load_print_meta: n_layer          = 24
0.00.062.230 I llm_load_print_meta: n_head           = 16
0.00.062.231 I llm_load_print_meta: n_head_kv        = 16
0.00.062.231 I llm_load_print_meta: n_rot            = 32
0.00.062.231 I llm_load_print_meta: n_swa            = 0
0.00.062.232 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.232 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.236 I llm_load_print_meta: n_gqa            = 1
0.00.062.236 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.237 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.238 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.238 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.238 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.239 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.239 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.240 I llm_load_print_meta: n_ff             = 8192
0.00.062.240 I llm_load_print_meta: n_expert         = 0
0.00.062.240 I llm_load_print_meta: n_expert_used    = 0
0.00.062.240 I llm_load_print_meta: causal attn      = 1
0.00.062.241 I llm_load_print_meta: pooling type     = 0
0.00.062.241 I llm_load_print_meta: rope type        = 2
0.00.062.241 I llm_load_print_meta: rope scaling     = linear
0.00.062.242 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.242 I llm_load_print_meta: freq_scale_train = 1
0.00.062.242 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.242 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.244 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.244 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.244 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.244 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.245 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.245 I llm_load_print_meta: model type       = 1.4B
0.00.062.246 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.246 I llm_load_print_meta: model params     = 1.41 B
0.00.062.247 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.247 I llm_load_print_meta: general.name     = 1.4B
0.00.062.247 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.247 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.248 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.248 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.248 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.249 I llm_load_print_meta: max token length = 1024
0.00.064.681 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.681 I llm_load_tensors: offloading output layer to GPU
0.00.064.681 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.692 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.693 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.665 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.665 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.666 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.666 I llama_new_context_with_model: n_batch       = 2048
0.00.065.666 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.666 I llama_new_context_with_model: flash_attn    = 0
0.00.065.667 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.667 I llama_new_context_with_model: freq_scale    = 1
0.00.065.668 I ggml_metal_init: allocating
0.00.065.671 I ggml_metal_init: found device: Apple M4
0.00.065.673 I ggml_metal_init: picking default device: Apple M4
0.00.066.379 I ggml_metal_init: using embedded metal library
0.00.068.981 I ggml_metal_init: GPU name:   Apple M4
0.00.068.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.984 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.984 I ggml_metal_init: simdgroup reduction   = true
0.00.068.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.985 I ggml_metal_init: has bfloat            = true
0.00.068.985 I ggml_metal_init: use bfloat            = true
0.00.068.985 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.468 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.572 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.580 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.602 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.849 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.850 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.851 I llama_new_context_with_model: graph nodes  = 967
0.00.105.851 I llama_new_context_with_model: graph splits = 2
0.00.105.854 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.983 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.983 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.339.032 I main: llama threadpool init, n_threads = 4
0.01.339.065 I 
0.01.339.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.339.100 I 
0.01.339.349 I sampler seed: 1234
0.01.339.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.339.387 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.339.398 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.339.400 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.428.871 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.02.428.872 I llama_perf_context_print:        load time =    1329.05 ms
0.02.428.873 I llama_perf_context_print: prompt eval time =      39.55 ms /     7 tokens (    5.65 ms per token,   176.98 tokens per second)
0.02.428.874 I llama_perf_context_print:        eval time =    1047.15 ms /    63 runs   (   16.62 ms per token,    60.16 tokens per second)
0.02.428.874 I llama_perf_context_print:       total time =    1089.84 ms /    70 tokens
0.02.429.123 I ggml_metal_free: deallocating

real	0m2.449s
user	0m0.114s
sys	0m0.230s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.017.296 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.061 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.063 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.064 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.067 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.044 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.044 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.045 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.045 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.045 I llama_model_loader: - type  f32:  194 tensors
0.00.039.046 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.046 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.237 I llm_load_vocab: special tokens cache size = 25
0.00.071.502 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.505 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.505 I llm_load_print_meta: arch             = gptneox
0.00.071.506 I llm_load_print_meta: vocab type       = BPE
0.00.071.506 I llm_load_print_meta: n_vocab          = 50304
0.00.071.506 I llm_load_print_meta: n_merges         = 50009
0.00.071.507 I llm_load_print_meta: vocab_only       = 0
0.00.071.507 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.507 I llm_load_print_meta: n_embd           = 2048
0.00.071.509 I llm_load_print_meta: n_layer          = 24
0.00.071.513 I llm_load_print_meta: n_head           = 16
0.00.071.514 I llm_load_print_meta: n_head_kv        = 16
0.00.071.514 I llm_load_print_meta: n_rot            = 32
0.00.071.515 I llm_load_print_meta: n_swa            = 0
0.00.071.515 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.515 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.516 I llm_load_print_meta: n_gqa            = 1
0.00.071.516 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.517 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.517 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.518 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.518 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.518 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.519 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.519 I llm_load_print_meta: n_ff             = 8192
0.00.071.520 I llm_load_print_meta: n_expert         = 0
0.00.071.520 I llm_load_print_meta: n_expert_used    = 0
0.00.071.520 I llm_load_print_meta: causal attn      = 1
0.00.071.520 I llm_load_print_meta: pooling type     = 0
0.00.071.521 I llm_load_print_meta: rope type        = 2
0.00.071.521 I llm_load_print_meta: rope scaling     = linear
0.00.071.521 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.522 I llm_load_print_meta: freq_scale_train = 1
0.00.071.522 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.522 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.525 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.525 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.525 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.525 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.525 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.526 I llm_load_print_meta: model type       = 1.4B
0.00.071.526 I llm_load_print_meta: model ftype      = Q4_0
0.00.071.526 I llm_load_print_meta: model params     = 1.41 B
0.00.071.526 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.071.527 I llm_load_print_meta: general.name     = 1.4B
0.00.071.528 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.528 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.528 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.528 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.529 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.071.529 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.529 I llm_load_print_meta: max token length = 1024
0.00.074.134 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.134 I llm_load_tensors: offloading output layer to GPU
0.00.074.135 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.147 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.074.148 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.075.339 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.340 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.341 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.341 I llama_new_context_with_model: n_batch       = 2048
0.00.075.341 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.341 I llama_new_context_with_model: flash_attn    = 0
0.00.075.342 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.342 I llama_new_context_with_model: freq_scale    = 1
0.00.075.343 I ggml_metal_init: allocating
0.00.075.350 I ggml_metal_init: found device: Apple M4
0.00.075.352 I ggml_metal_init: picking default device: Apple M4
0.00.076.184 I ggml_metal_init: using embedded metal library
0.00.079.305 I ggml_metal_init: GPU name:   Apple M4
0.00.079.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.308 I ggml_metal_init: simdgroup reduction   = true
0.00.079.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.308 I ggml_metal_init: has bfloat            = true
0.00.079.309 I ggml_metal_init: use bfloat            = true
0.00.079.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.139 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.118.117 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.118.124 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.118.150 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.232 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.119.234 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.119.234 I llama_new_context_with_model: graph nodes  = 967
0.00.119.235 I llama_new_context_with_model: graph splits = 2
0.00.119.239 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.119.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.119.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.321 I main: llama threadpool init, n_threads = 4
0.00.746.370 I 
0.00.746.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.388 I 
0.00.746.617 I sampler seed: 1234
0.00.746.622 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.638 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.638 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.638 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.421.556 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.421.557 I llama_perf_context_print:        load time =     729.02 ms
0.01.421.557 I llama_perf_context_print: prompt eval time =      39.78 ms /     7 tokens (    5.68 ms per token,   175.99 tokens per second)
0.01.421.558 I llama_perf_context_print:        eval time =     632.23 ms /    63 runs   (   10.04 ms per token,    99.65 tokens per second)
0.01.421.558 I llama_perf_context_print:       total time =     675.24 ms /    70 tokens
0.01.421.808 I ggml_metal_free: deallocating

real	0m1.440s
user	0m0.120s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.628 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.288 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.299 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.300 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.306 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.307 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.307 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.205 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.286 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.135 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.136 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.136 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.137 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.137 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.137 I llama_model_loader: - type  f32:  194 tensors
0.00.032.138 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.138 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.469 I llm_load_vocab: special tokens cache size = 25
0.00.060.337 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.340 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.340 I llm_load_print_meta: arch             = gptneox
0.00.060.340 I llm_load_print_meta: vocab type       = BPE
0.00.060.341 I llm_load_print_meta: n_vocab          = 50304
0.00.060.341 I llm_load_print_meta: n_merges         = 50009
0.00.060.341 I llm_load_print_meta: vocab_only       = 0
0.00.060.341 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.341 I llm_load_print_meta: n_embd           = 2048
0.00.060.341 I llm_load_print_meta: n_layer          = 24
0.00.060.344 I llm_load_print_meta: n_head           = 16
0.00.060.345 I llm_load_print_meta: n_head_kv        = 16
0.00.060.345 I llm_load_print_meta: n_rot            = 32
0.00.060.345 I llm_load_print_meta: n_swa            = 0
0.00.060.345 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.346 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.346 I llm_load_print_meta: n_gqa            = 1
0.00.060.347 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.348 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.348 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.350 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.351 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.351 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.352 I llm_load_print_meta: n_ff             = 8192
0.00.060.352 I llm_load_print_meta: n_expert         = 0
0.00.060.352 I llm_load_print_meta: n_expert_used    = 0
0.00.060.353 I llm_load_print_meta: causal attn      = 1
0.00.060.354 I llm_load_print_meta: pooling type     = 0
0.00.060.354 I llm_load_print_meta: rope type        = 2
0.00.060.356 I llm_load_print_meta: rope scaling     = linear
0.00.060.357 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.357 I llm_load_print_meta: freq_scale_train = 1
0.00.060.357 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.358 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.358 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.358 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.358 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.358 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.358 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.358 I llm_load_print_meta: model type       = 1.4B
0.00.060.359 I llm_load_print_meta: model ftype      = Q4_1
0.00.060.359 I llm_load_print_meta: model params     = 1.41 B
0.00.060.360 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.060.360 I llm_load_print_meta: general.name     = 1.4B
0.00.060.360 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.360 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.361 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.361 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.361 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.364 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.364 I llm_load_print_meta: max token length = 1024
0.00.062.309 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.309 I llm_load_tensors: offloading output layer to GPU
0.00.062.310 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.320 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.062.321 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.063.204 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.205 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.205 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.206 I llama_new_context_with_model: n_batch       = 2048
0.00.063.206 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.206 I llama_new_context_with_model: flash_attn    = 0
0.00.063.207 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.207 I llama_new_context_with_model: freq_scale    = 1
0.00.063.208 I ggml_metal_init: allocating
0.00.063.215 I ggml_metal_init: found device: Apple M4
0.00.063.218 I ggml_metal_init: picking default device: Apple M4
0.00.063.855 I ggml_metal_init: using embedded metal library
0.00.066.324 I ggml_metal_init: GPU name:   Apple M4
0.00.066.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.327 I ggml_metal_init: simdgroup reduction   = true
0.00.066.327 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.327 I ggml_metal_init: has bfloat            = true
0.00.066.328 I ggml_metal_init: use bfloat            = true
0.00.066.329 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.330 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.346 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.587 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.594 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.613 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.632 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.633 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.633 I llama_new_context_with_model: graph nodes  = 967
0.00.098.634 I llama_new_context_with_model: graph splits = 2
0.00.098.636 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.158 I main: llama threadpool init, n_threads = 4
0.00.774.198 I 
0.00.774.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.224 I 
0.00.774.461 I sampler seed: 1234
0.00.774.468 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.494 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.494 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.490.239 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64253.39 tokens per second)
0.01.490.240 I llama_perf_context_print:        load time =     765.52 ms
0.01.490.241 I llama_perf_context_print: prompt eval time =      39.53 ms /     7 tokens (    5.65 ms per token,   177.07 tokens per second)
0.01.490.241 I llama_perf_context_print:        eval time =     673.42 ms /    63 runs   (   10.69 ms per token,    93.55 tokens per second)
0.01.490.242 I llama_perf_context_print:       total time =     716.08 ms /    70 tokens
0.01.490.480 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.112s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.340 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.807 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.811 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.814 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.820 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.821 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.572 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.349 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.351 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.352 I llama_model_loader: - type  f32:  194 tensors
0.00.025.352 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.353 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.409 I llm_load_vocab: special tokens cache size = 25
0.00.051.471 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.474 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.474 I llm_load_print_meta: arch             = gptneox
0.00.051.475 I llm_load_print_meta: vocab type       = BPE
0.00.051.475 I llm_load_print_meta: n_vocab          = 50304
0.00.051.475 I llm_load_print_meta: n_merges         = 50009
0.00.051.475 I llm_load_print_meta: vocab_only       = 0
0.00.051.476 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.476 I llm_load_print_meta: n_embd           = 2048
0.00.051.476 I llm_load_print_meta: n_layer          = 24
0.00.051.479 I llm_load_print_meta: n_head           = 16
0.00.051.479 I llm_load_print_meta: n_head_kv        = 16
0.00.051.480 I llm_load_print_meta: n_rot            = 32
0.00.051.480 I llm_load_print_meta: n_swa            = 0
0.00.051.480 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.480 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.481 I llm_load_print_meta: n_gqa            = 1
0.00.051.482 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.482 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.483 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.483 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.483 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.483 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.484 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.484 I llm_load_print_meta: n_ff             = 8192
0.00.051.484 I llm_load_print_meta: n_expert         = 0
0.00.051.485 I llm_load_print_meta: n_expert_used    = 0
0.00.051.486 I llm_load_print_meta: causal attn      = 1
0.00.051.488 I llm_load_print_meta: pooling type     = 0
0.00.051.488 I llm_load_print_meta: rope type        = 2
0.00.051.488 I llm_load_print_meta: rope scaling     = linear
0.00.051.488 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.489 I llm_load_print_meta: freq_scale_train = 1
0.00.051.489 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.489 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.489 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.489 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.490 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.490 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.490 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.491 I llm_load_print_meta: model type       = 1.4B
0.00.051.491 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.492 I llm_load_print_meta: model params     = 1.41 B
0.00.051.493 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.493 I llm_load_print_meta: general.name     = 1.4B
0.00.051.493 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.493 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.493 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.495 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.496 I llm_load_print_meta: max token length = 1024
0.00.053.549 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.549 I llm_load_tensors: offloading output layer to GPU
0.00.053.549 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.560 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.561 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.511 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.512 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.512 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.512 I llama_new_context_with_model: n_batch       = 2048
0.00.054.512 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.513 I llama_new_context_with_model: flash_attn    = 0
0.00.054.513 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.513 I llama_new_context_with_model: freq_scale    = 1
0.00.054.514 I ggml_metal_init: allocating
0.00.054.517 I ggml_metal_init: found device: Apple M4
0.00.054.519 I ggml_metal_init: picking default device: Apple M4
0.00.055.257 I ggml_metal_init: using embedded metal library
0.00.057.559 I ggml_metal_init: GPU name:   Apple M4
0.00.057.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.562 I ggml_metal_init: simdgroup reduction   = true
0.00.057.562 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.562 I ggml_metal_init: has bfloat            = true
0.00.057.562 I ggml_metal_init: use bfloat            = true
0.00.057.563 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.178 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.212 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.217 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.232 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.289 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.291 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.291 I llama_new_context_with_model: graph nodes  = 967
0.00.087.291 I llama_new_context_with_model: graph splits = 2
0.00.087.294 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.311 I main: llama threadpool init, n_threads = 4
0.00.777.357 I 
0.00.777.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.387 I 
0.00.777.615 I sampler seed: 1234
0.00.777.619 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.661 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.673 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.673 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.570.068 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.570.069 I llama_perf_context_print:        load time =     766.97 ms
0.01.570.069 I llama_perf_context_print: prompt eval time =      47.05 ms /     7 tokens (    6.72 ms per token,   148.77 tokens per second)
0.01.570.070 I llama_perf_context_print:        eval time =     742.31 ms /    63 runs   (   11.78 ms per token,    84.87 tokens per second)
0.01.570.070 I llama_perf_context_print:       total time =     792.76 ms /    70 tokens
0.01.570.300 I ggml_metal_free: deallocating

real	0m1.589s
user	0m0.109s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.776 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.988 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.992 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.994 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.994 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.995 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.996 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.996 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.996 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.997 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.997 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.998 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.004 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.807 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.576 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.578 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.578 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.579 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.579 I llama_model_loader: - type  f32:  194 tensors
0.00.023.579 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.580 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.566 I llm_load_vocab: special tokens cache size = 25
0.00.049.527 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.530 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.530 I llm_load_print_meta: arch             = gptneox
0.00.049.530 I llm_load_print_meta: vocab type       = BPE
0.00.049.530 I llm_load_print_meta: n_vocab          = 50304
0.00.049.531 I llm_load_print_meta: n_merges         = 50009
0.00.049.531 I llm_load_print_meta: vocab_only       = 0
0.00.049.531 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.531 I llm_load_print_meta: n_embd           = 2048
0.00.049.531 I llm_load_print_meta: n_layer          = 24
0.00.049.535 I llm_load_print_meta: n_head           = 16
0.00.049.535 I llm_load_print_meta: n_head_kv        = 16
0.00.049.538 I llm_load_print_meta: n_rot            = 32
0.00.049.538 I llm_load_print_meta: n_swa            = 0
0.00.049.538 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.538 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.539 I llm_load_print_meta: n_gqa            = 1
0.00.049.540 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.540 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.541 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.541 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.541 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.542 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.542 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.542 I llm_load_print_meta: n_ff             = 8192
0.00.049.543 I llm_load_print_meta: n_expert         = 0
0.00.049.543 I llm_load_print_meta: n_expert_used    = 0
0.00.049.544 I llm_load_print_meta: causal attn      = 1
0.00.049.546 I llm_load_print_meta: pooling type     = 0
0.00.049.546 I llm_load_print_meta: rope type        = 2
0.00.049.546 I llm_load_print_meta: rope scaling     = linear
0.00.049.547 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.547 I llm_load_print_meta: freq_scale_train = 1
0.00.049.547 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.547 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.547 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.548 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.548 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.548 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.548 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.552 I llm_load_print_meta: model type       = 1.4B
0.00.049.552 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.553 I llm_load_print_meta: model params     = 1.41 B
0.00.049.553 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.553 I llm_load_print_meta: general.name     = 1.4B
0.00.049.554 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.555 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.555 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.555 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.555 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.556 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.556 I llm_load_print_meta: max token length = 1024
0.00.051.573 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.574 I llm_load_tensors: offloading output layer to GPU
0.00.051.574 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.584 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.585 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.471 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.471 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.472 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.472 I llama_new_context_with_model: n_batch       = 2048
0.00.052.472 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.472 I llama_new_context_with_model: flash_attn    = 0
0.00.052.473 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.473 I llama_new_context_with_model: freq_scale    = 1
0.00.052.473 I ggml_metal_init: allocating
0.00.052.480 I ggml_metal_init: found device: Apple M4
0.00.052.483 I ggml_metal_init: picking default device: Apple M4
0.00.053.054 I ggml_metal_init: using embedded metal library
0.00.055.412 I ggml_metal_init: GPU name:   Apple M4
0.00.055.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.414 I ggml_metal_init: simdgroup reduction   = true
0.00.055.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.416 I ggml_metal_init: has bfloat            = true
0.00.055.416 I ggml_metal_init: use bfloat            = true
0.00.055.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.421 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.821 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.278 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.285 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.303 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.181 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.183 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.183 I llama_new_context_with_model: graph nodes  = 967
0.00.084.183 I llama_new_context_with_model: graph splits = 2
0.00.084.186 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.314 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.314 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.700 I main: llama threadpool init, n_threads = 4
0.00.693.743 I 
0.00.693.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.785 I 
0.00.694.017 I sampler seed: 1234
0.00.694.021 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.060 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.064 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.064 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.532.347 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.532.348 I llama_perf_context_print:        load time =     684.92 ms
0.01.532.348 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.73 tokens per second)
0.01.532.349 I llama_perf_context_print:        eval time =     793.15 ms /    63 runs   (   12.59 ms per token,    79.43 tokens per second)
0.01.532.349 I llama_perf_context_print:       total time =     838.65 ms /    70 tokens
0.01.532.541 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.108s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.300 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.610 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.618 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.619 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.620 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.622 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.310 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.075 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.076 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.077 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.077 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.077 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.078 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.078 I llama_model_loader: - type  f32:  194 tensors
0.00.023.078 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.079 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.079 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.925 I llm_load_vocab: special tokens cache size = 25
0.00.049.027 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.030 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.030 I llm_load_print_meta: arch             = gptneox
0.00.049.031 I llm_load_print_meta: vocab type       = BPE
0.00.049.031 I llm_load_print_meta: n_vocab          = 50304
0.00.049.031 I llm_load_print_meta: n_merges         = 50009
0.00.049.031 I llm_load_print_meta: vocab_only       = 0
0.00.049.032 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.032 I llm_load_print_meta: n_embd           = 2048
0.00.049.032 I llm_load_print_meta: n_layer          = 24
0.00.049.035 I llm_load_print_meta: n_head           = 16
0.00.049.036 I llm_load_print_meta: n_head_kv        = 16
0.00.049.036 I llm_load_print_meta: n_rot            = 32
0.00.049.036 I llm_load_print_meta: n_swa            = 0
0.00.049.036 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.037 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.038 I llm_load_print_meta: n_gqa            = 1
0.00.049.039 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.039 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.040 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.040 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.040 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.040 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.040 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.041 I llm_load_print_meta: n_ff             = 8192
0.00.049.041 I llm_load_print_meta: n_expert         = 0
0.00.049.042 I llm_load_print_meta: n_expert_used    = 0
0.00.049.042 I llm_load_print_meta: causal attn      = 1
0.00.049.042 I llm_load_print_meta: pooling type     = 0
0.00.049.043 I llm_load_print_meta: rope type        = 2
0.00.049.043 I llm_load_print_meta: rope scaling     = linear
0.00.049.043 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.043 I llm_load_print_meta: freq_scale_train = 1
0.00.049.044 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.044 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.044 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.044 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.044 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.045 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.045 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.045 I llm_load_print_meta: model type       = 1.4B
0.00.049.045 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.046 I llm_load_print_meta: model params     = 1.41 B
0.00.049.046 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.046 I llm_load_print_meta: general.name     = 1.4B
0.00.049.049 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.049 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.049 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.050 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.050 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.050 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.050 I llm_load_print_meta: max token length = 1024
0.00.050.863 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.864 I llm_load_tensors: offloading output layer to GPU
0.00.050.864 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.875 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.876 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.737 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.738 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.738 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.739 I llama_new_context_with_model: n_batch       = 2048
0.00.051.739 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.739 I llama_new_context_with_model: flash_attn    = 0
0.00.051.739 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.739 I llama_new_context_with_model: freq_scale    = 1
0.00.051.740 I ggml_metal_init: allocating
0.00.051.743 I ggml_metal_init: found device: Apple M4
0.00.051.745 I ggml_metal_init: picking default device: Apple M4
0.00.052.349 I ggml_metal_init: using embedded metal library
0.00.054.640 I ggml_metal_init: GPU name:   Apple M4
0.00.054.642 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.643 I ggml_metal_init: simdgroup reduction   = true
0.00.054.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.643 I ggml_metal_init: has bfloat            = true
0.00.054.643 I ggml_metal_init: use bfloat            = true
0.00.054.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.247 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.614 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.619 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.641 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.696 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.698 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.698 I llama_new_context_with_model: graph nodes  = 967
0.00.083.698 I llama_new_context_with_model: graph splits = 2
0.00.083.701 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.849 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.850 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.516 I main: llama threadpool init, n_threads = 4
0.00.434.557 I 
0.00.434.587 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.587 I 
0.00.434.824 I sampler seed: 1234
0.00.434.828 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.434.873 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.434.877 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.434.877 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.113.089 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.01.113.090 I llama_perf_context_print:        load time =     425.21 ms
0.01.113.090 I llama_perf_context_print: prompt eval time =      35.94 ms /     7 tokens (    5.13 ms per token,   194.77 tokens per second)
0.01.113.091 I llama_perf_context_print:        eval time =     639.26 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.113.091 I llama_perf_context_print:       total time =     678.58 ms /    70 tokens
0.01.113.309 I ggml_metal_free: deallocating

real	0m1.132s
user	0m0.108s
sys	0m0.106s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.301 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.628 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.637 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.638 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.640 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.640 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.642 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.643 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.643 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.645 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.645 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.677 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.459 I llama_model_loader: - type  f32:  194 tensors
0.00.024.459 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.460 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.460 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.460 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.359 I llm_load_vocab: special tokens cache size = 25
0.00.051.524 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.527 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.527 I llm_load_print_meta: arch             = gptneox
0.00.051.528 I llm_load_print_meta: vocab type       = BPE
0.00.051.528 I llm_load_print_meta: n_vocab          = 50304
0.00.051.528 I llm_load_print_meta: n_merges         = 50009
0.00.051.528 I llm_load_print_meta: vocab_only       = 0
0.00.051.529 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.529 I llm_load_print_meta: n_embd           = 2048
0.00.051.529 I llm_load_print_meta: n_layer          = 24
0.00.051.532 I llm_load_print_meta: n_head           = 16
0.00.051.533 I llm_load_print_meta: n_head_kv        = 16
0.00.051.533 I llm_load_print_meta: n_rot            = 32
0.00.051.533 I llm_load_print_meta: n_swa            = 0
0.00.051.533 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.533 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.534 I llm_load_print_meta: n_gqa            = 1
0.00.051.535 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.536 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.536 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.536 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.536 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.537 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.537 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.537 I llm_load_print_meta: n_ff             = 8192
0.00.051.538 I llm_load_print_meta: n_expert         = 0
0.00.051.538 I llm_load_print_meta: n_expert_used    = 0
0.00.051.538 I llm_load_print_meta: causal attn      = 1
0.00.051.538 I llm_load_print_meta: pooling type     = 0
0.00.051.538 I llm_load_print_meta: rope type        = 2
0.00.051.539 I llm_load_print_meta: rope scaling     = linear
0.00.051.541 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.542 I llm_load_print_meta: freq_scale_train = 1
0.00.051.542 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.542 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.542 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.542 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.542 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.542 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.543 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.543 I llm_load_print_meta: model type       = 1.4B
0.00.051.543 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.544 I llm_load_print_meta: model params     = 1.41 B
0.00.051.544 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.544 I llm_load_print_meta: general.name     = 1.4B
0.00.051.545 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.545 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.545 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.545 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.546 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.549 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.550 I llm_load_print_meta: max token length = 1024
0.00.053.572 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.572 I llm_load_tensors: offloading output layer to GPU
0.00.053.573 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.584 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.585 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.459 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.459 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.460 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.460 I llama_new_context_with_model: n_batch       = 2048
0.00.054.460 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.460 I llama_new_context_with_model: flash_attn    = 0
0.00.054.461 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.461 I llama_new_context_with_model: freq_scale    = 1
0.00.054.462 I ggml_metal_init: allocating
0.00.054.465 I ggml_metal_init: found device: Apple M4
0.00.054.467 I ggml_metal_init: picking default device: Apple M4
0.00.055.078 I ggml_metal_init: using embedded metal library
0.00.057.393 I ggml_metal_init: GPU name:   Apple M4
0.00.057.395 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.395 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.396 I ggml_metal_init: simdgroup reduction   = true
0.00.057.396 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.396 I ggml_metal_init: has bfloat            = true
0.00.057.396 I ggml_metal_init: use bfloat            = true
0.00.057.396 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.264 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.443 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.451 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.473 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.452 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.453 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.453 I llama_new_context_with_model: graph nodes  = 967
0.00.087.454 I llama_new_context_with_model: graph splits = 2
0.00.087.456 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.639 I main: llama threadpool init, n_threads = 4
0.00.540.675 I 
0.00.540.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.698 I 
0.00.540.926 I sampler seed: 1234
0.00.540.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.946 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.948 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.948 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.284.520 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.284.520 I llama_perf_context_print:        load time =     531.33 ms
0.01.284.521 I llama_perf_context_print: prompt eval time =      44.35 ms /     7 tokens (    6.34 ms per token,   157.84 tokens per second)
0.01.284.521 I llama_perf_context_print:        eval time =     696.27 ms /    63 runs   (   11.05 ms per token,    90.48 tokens per second)
0.01.284.522 I llama_perf_context_print:       total time =     743.88 ms /    70 tokens
0.01.284.712 I ggml_metal_free: deallocating

real	0m1.300s
user	0m0.111s
sys	0m0.127s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.635 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.448 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.449 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.451 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.452 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.452 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.453 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.453 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.453 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.454 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.456 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.444 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.334 I llama_model_loader: - type  f32:  194 tensors
0.00.024.334 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.334 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.334 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.081 I llm_load_vocab: special tokens cache size = 25
0.00.051.117 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.120 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.120 I llm_load_print_meta: arch             = gptneox
0.00.051.120 I llm_load_print_meta: vocab type       = BPE
0.00.051.121 I llm_load_print_meta: n_vocab          = 50304
0.00.051.121 I llm_load_print_meta: n_merges         = 50009
0.00.051.121 I llm_load_print_meta: vocab_only       = 0
0.00.051.121 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.121 I llm_load_print_meta: n_embd           = 2048
0.00.051.122 I llm_load_print_meta: n_layer          = 24
0.00.051.124 I llm_load_print_meta: n_head           = 16
0.00.051.125 I llm_load_print_meta: n_head_kv        = 16
0.00.051.125 I llm_load_print_meta: n_rot            = 32
0.00.051.125 I llm_load_print_meta: n_swa            = 0
0.00.051.125 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.125 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.126 I llm_load_print_meta: n_gqa            = 1
0.00.051.127 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.128 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.128 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.128 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.129 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.129 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.129 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.129 I llm_load_print_meta: n_ff             = 8192
0.00.051.130 I llm_load_print_meta: n_expert         = 0
0.00.051.130 I llm_load_print_meta: n_expert_used    = 0
0.00.051.130 I llm_load_print_meta: causal attn      = 1
0.00.051.130 I llm_load_print_meta: pooling type     = 0
0.00.051.130 I llm_load_print_meta: rope type        = 2
0.00.051.130 I llm_load_print_meta: rope scaling     = linear
0.00.051.131 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.131 I llm_load_print_meta: freq_scale_train = 1
0.00.051.131 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.132 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.132 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.132 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.132 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.134 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.134 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.135 I llm_load_print_meta: model type       = 1.4B
0.00.051.135 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.136 I llm_load_print_meta: model params     = 1.41 B
0.00.051.136 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.136 I llm_load_print_meta: general.name     = 1.4B
0.00.051.137 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.137 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.138 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.138 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.139 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.139 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.139 I llm_load_print_meta: max token length = 1024
0.00.053.220 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.220 I llm_load_tensors: offloading output layer to GPU
0.00.053.220 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.231 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.232 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.126 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.127 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.127 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.128 I llama_new_context_with_model: n_batch       = 2048
0.00.054.128 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.128 I llama_new_context_with_model: flash_attn    = 0
0.00.054.129 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.129 I llama_new_context_with_model: freq_scale    = 1
0.00.054.129 I ggml_metal_init: allocating
0.00.054.135 I ggml_metal_init: found device: Apple M4
0.00.054.138 I ggml_metal_init: picking default device: Apple M4
0.00.054.733 I ggml_metal_init: using embedded metal library
0.00.057.256 I ggml_metal_init: GPU name:   Apple M4
0.00.057.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.258 I ggml_metal_init: simdgroup reduction   = true
0.00.057.258 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.259 I ggml_metal_init: has bfloat            = true
0.00.057.259 I ggml_metal_init: use bfloat            = true
0.00.057.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.664 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.900 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.907 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.926 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.874 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.875 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.876 I llama_new_context_with_model: graph nodes  = 967
0.00.087.876 I llama_new_context_with_model: graph splits = 2
0.00.087.879 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.900 I main: llama threadpool init, n_threads = 4
0.00.617.939 I 
0.00.617.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.987 I 
0.00.618.207 I sampler seed: 1234
0.00.618.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.618.247 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.618.249 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.618.249 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.654 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.01.376.654 I llama_perf_context_print:        load time =     609.26 ms
0.01.376.656 I llama_perf_context_print: prompt eval time =      50.70 ms /     7 tokens (    7.24 ms per token,   138.05 tokens per second)
0.01.376.656 I llama_perf_context_print:        eval time =     704.92 ms /    63 runs   (   11.19 ms per token,    89.37 tokens per second)
0.01.376.657 I llama_perf_context_print:       total time =     758.76 ms /    70 tokens
0.01.376.888 I ggml_metal_free: deallocating

real	0m1.394s
user	0m0.110s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.846 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.306 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.316 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.317 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.319 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.319 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.320 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.320 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.321 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.321 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.321 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.322 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.322 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.324 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.324 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.241 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.152 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.153 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.153 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.154 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.154 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.154 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.155 I llama_model_loader: - type  f32:  194 tensors
0.00.025.155 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.156 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.068 I llm_load_vocab: special tokens cache size = 25
0.00.051.992 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.995 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.996 I llm_load_print_meta: arch             = gptneox
0.00.051.996 I llm_load_print_meta: vocab type       = BPE
0.00.051.996 I llm_load_print_meta: n_vocab          = 50304
0.00.051.996 I llm_load_print_meta: n_merges         = 50009
0.00.051.997 I llm_load_print_meta: vocab_only       = 0
0.00.051.997 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.997 I llm_load_print_meta: n_embd           = 2048
0.00.051.997 I llm_load_print_meta: n_layer          = 24
0.00.052.000 I llm_load_print_meta: n_head           = 16
0.00.052.002 I llm_load_print_meta: n_head_kv        = 16
0.00.052.002 I llm_load_print_meta: n_rot            = 32
0.00.052.002 I llm_load_print_meta: n_swa            = 0
0.00.052.003 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.003 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.003 I llm_load_print_meta: n_gqa            = 1
0.00.052.008 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.009 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.010 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.010 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.010 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.011 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.011 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.011 I llm_load_print_meta: n_ff             = 8192
0.00.052.012 I llm_load_print_meta: n_expert         = 0
0.00.052.012 I llm_load_print_meta: n_expert_used    = 0
0.00.052.012 I llm_load_print_meta: causal attn      = 1
0.00.052.012 I llm_load_print_meta: pooling type     = 0
0.00.052.012 I llm_load_print_meta: rope type        = 2
0.00.052.016 I llm_load_print_meta: rope scaling     = linear
0.00.052.016 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.016 I llm_load_print_meta: freq_scale_train = 1
0.00.052.017 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.017 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.017 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.017 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.018 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.018 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.018 I llm_load_print_meta: model type       = 1.4B
0.00.052.018 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.019 I llm_load_print_meta: model params     = 1.41 B
0.00.052.019 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.019 I llm_load_print_meta: general.name     = 1.4B
0.00.052.020 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.020 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.020 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.020 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.021 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.021 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.021 I llm_load_print_meta: max token length = 1024
0.00.054.124 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.124 I llm_load_tensors: offloading output layer to GPU
0.00.054.124 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.135 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.136 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.099 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.100 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.100 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.100 I llama_new_context_with_model: n_batch       = 2048
0.00.055.100 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.100 I llama_new_context_with_model: flash_attn    = 0
0.00.055.101 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.101 I llama_new_context_with_model: freq_scale    = 1
0.00.055.102 I ggml_metal_init: allocating
0.00.055.105 I ggml_metal_init: found device: Apple M4
0.00.055.107 I ggml_metal_init: picking default device: Apple M4
0.00.055.706 I ggml_metal_init: using embedded metal library
0.00.058.058 I ggml_metal_init: GPU name:   Apple M4
0.00.058.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.060 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.061 I ggml_metal_init: simdgroup reduction   = true
0.00.058.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.061 I ggml_metal_init: has bfloat            = true
0.00.058.061 I ggml_metal_init: use bfloat            = true
0.00.058.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.000 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.961 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.968 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.987 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.020 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.021 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.021 I llama_new_context_with_model: graph nodes  = 967
0.00.089.022 I llama_new_context_with_model: graph splits = 2
0.00.089.024 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.359 I main: llama threadpool init, n_threads = 4
0.00.686.401 I 
0.00.686.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.422 I 
0.00.686.638 I sampler seed: 1234
0.00.686.642 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.676 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.677 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.678 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.535.010 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.535.011 I llama_perf_context_print:        load time =     676.51 ms
0.01.535.012 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.80 tokens per second)
0.01.535.013 I llama_perf_context_print:        eval time =     793.78 ms /    63 runs   (   12.60 ms per token,    79.37 tokens per second)
0.01.535.013 I llama_perf_context_print:       total time =     848.65 ms /    70 tokens
0.01.535.237 I ggml_metal_free: deallocating

real	0m1.552s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.695 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.261 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.267 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.268 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.270 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.271 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.276 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.276 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.277 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.963 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.964 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.965 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.965 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.965 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.966 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.966 I llama_model_loader: - type  f32:  194 tensors
0.00.023.966 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.929 I llm_load_vocab: special tokens cache size = 25
0.00.049.729 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.731 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.732 I llm_load_print_meta: arch             = gptneox
0.00.049.732 I llm_load_print_meta: vocab type       = BPE
0.00.049.733 I llm_load_print_meta: n_vocab          = 50304
0.00.049.733 I llm_load_print_meta: n_merges         = 50009
0.00.049.733 I llm_load_print_meta: vocab_only       = 0
0.00.049.733 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.733 I llm_load_print_meta: n_embd           = 2048
0.00.049.733 I llm_load_print_meta: n_layer          = 24
0.00.049.736 I llm_load_print_meta: n_head           = 16
0.00.049.737 I llm_load_print_meta: n_head_kv        = 16
0.00.049.740 I llm_load_print_meta: n_rot            = 32
0.00.049.741 I llm_load_print_meta: n_swa            = 0
0.00.049.741 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.741 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.742 I llm_load_print_meta: n_gqa            = 1
0.00.049.742 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.743 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.744 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.744 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.744 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.744 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.744 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.745 I llm_load_print_meta: n_ff             = 8192
0.00.049.745 I llm_load_print_meta: n_expert         = 0
0.00.049.745 I llm_load_print_meta: n_expert_used    = 0
0.00.049.746 I llm_load_print_meta: causal attn      = 1
0.00.049.747 I llm_load_print_meta: pooling type     = 0
0.00.049.749 I llm_load_print_meta: rope type        = 2
0.00.049.749 I llm_load_print_meta: rope scaling     = linear
0.00.049.749 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.750 I llm_load_print_meta: freq_scale_train = 1
0.00.049.750 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.750 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.750 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.751 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.752 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.753 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.753 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.753 I llm_load_print_meta: model type       = 1.4B
0.00.049.754 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.754 I llm_load_print_meta: model params     = 1.41 B
0.00.049.758 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.758 I llm_load_print_meta: general.name     = 1.4B
0.00.049.758 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.758 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.758 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.759 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.760 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.760 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.760 I llm_load_print_meta: max token length = 1024
0.00.051.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.826 I llm_load_tensors: offloading output layer to GPU
0.00.051.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.837 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.838 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.747 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.748 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.748 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.748 I llama_new_context_with_model: n_batch       = 2048
0.00.052.748 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.749 I llama_new_context_with_model: flash_attn    = 0
0.00.052.749 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.749 I llama_new_context_with_model: freq_scale    = 1
0.00.052.750 I ggml_metal_init: allocating
0.00.052.753 I ggml_metal_init: found device: Apple M4
0.00.052.755 I ggml_metal_init: picking default device: Apple M4
0.00.053.334 I ggml_metal_init: using embedded metal library
0.00.055.633 I ggml_metal_init: GPU name:   Apple M4
0.00.055.634 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.635 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.635 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.635 I ggml_metal_init: simdgroup reduction   = true
0.00.055.637 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.637 I ggml_metal_init: has bfloat            = true
0.00.055.637 I ggml_metal_init: use bfloat            = true
0.00.055.638 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.561 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.092 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.102 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.132 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.239 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.240 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.241 I llama_new_context_with_model: graph nodes  = 967
0.00.087.241 I llama_new_context_with_model: graph splits = 2
0.00.087.244 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.397 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.253 I main: llama threadpool init, n_threads = 4
0.00.746.290 I 
0.00.746.312 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.312 I 
0.00.746.473 I sampler seed: 1234
0.00.746.477 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.491 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.491 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.627.144 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.627.145 I llama_perf_context_print:        load time =     737.56 ms
0.01.627.146 I llama_perf_context_print: prompt eval time =      54.33 ms /     7 tokens (    7.76 ms per token,   128.85 tokens per second)
0.01.627.147 I llama_perf_context_print:        eval time =     823.67 ms /    63 runs   (   13.07 ms per token,    76.49 tokens per second)
0.01.627.147 I llama_perf_context_print:       total time =     880.89 ms /    70 tokens
0.01.627.384 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.003.032 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.030.510 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.994 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.008 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.009 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.010 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.010 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.012 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.012 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.013 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.020 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.205 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.209 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.216 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.217 I llama_model_loader: - type  f32:  194 tensors
0.00.058.217 I llama_model_loader: - type  f16:   98 tensors
0.00.080.513 I llm_load_vocab: special tokens cache size = 25
0.00.086.437 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.441 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.441 I llm_load_print_meta: arch             = gptneox
0.00.086.442 I llm_load_print_meta: vocab type       = BPE
0.00.086.442 I llm_load_print_meta: n_vocab          = 50304
0.00.086.442 I llm_load_print_meta: n_merges         = 50009
0.00.086.442 I llm_load_print_meta: vocab_only       = 0
0.00.086.444 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.444 I llm_load_print_meta: n_embd           = 2048
0.00.086.444 I llm_load_print_meta: n_layer          = 24
0.00.086.448 I llm_load_print_meta: n_head           = 16
0.00.086.449 I llm_load_print_meta: n_head_kv        = 16
0.00.086.449 I llm_load_print_meta: n_rot            = 32
0.00.086.449 I llm_load_print_meta: n_swa            = 0
0.00.086.453 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.454 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.454 I llm_load_print_meta: n_gqa            = 1
0.00.086.455 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.455 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.456 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.457 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.457 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.457 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.457 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.458 I llm_load_print_meta: n_ff             = 8192
0.00.086.458 I llm_load_print_meta: n_expert         = 0
0.00.086.458 I llm_load_print_meta: n_expert_used    = 0
0.00.086.458 I llm_load_print_meta: causal attn      = 1
0.00.086.458 I llm_load_print_meta: pooling type     = 0
0.00.086.459 I llm_load_print_meta: rope type        = 2
0.00.086.460 I llm_load_print_meta: rope scaling     = linear
0.00.086.460 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.460 I llm_load_print_meta: freq_scale_train = 1
0.00.086.460 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.461 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.461 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.461 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.461 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.462 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.462 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.462 I llm_load_print_meta: model type       = 1.4B
0.00.086.462 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.463 I llm_load_print_meta: model params     = 1.41 B
0.00.086.463 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.463 I llm_load_print_meta: general.name     = 1.4B
0.00.086.464 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.464 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.466 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.466 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.466 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.466 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.466 I llm_load_print_meta: max token length = 1024
0.00.088.339 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.340 I llm_load_tensors: offloading output layer to GPU
0.00.088.340 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.346 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.347 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.330 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.331 I llama_new_context_with_model: n_ctx         = 128
0.00.089.331 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.331 I llama_new_context_with_model: n_batch       = 128
0.00.089.331 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.331 I llama_new_context_with_model: flash_attn    = 0
0.00.089.332 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.332 I llama_new_context_with_model: freq_scale    = 1
0.00.089.332 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.333 I ggml_metal_init: allocating
0.00.089.336 I ggml_metal_init: found device: Apple M4
0.00.089.338 I ggml_metal_init: picking default device: Apple M4
0.00.090.083 I ggml_metal_init: using embedded metal library
0.00.092.664 I ggml_metal_init: GPU name:   Apple M4
0.00.092.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.666 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.667 I ggml_metal_init: simdgroup reduction   = true
0.00.092.667 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.667 I ggml_metal_init: has bfloat            = true
0.00.092.667 I ggml_metal_init: use bfloat            = true
0.00.092.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.767 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.021 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.024 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.039 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.903 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.904 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.904 I llama_new_context_with_model: graph nodes  = 967
0.00.103.905 I llama_new_context_with_model: graph splits = 2
0.00.103.906 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.911.252 I 
0.01.911.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.911.350 I perplexity: tokenizing the input ..
0.01.925.626 I perplexity: tokenization took 14.274 ms
0.01.925.632 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.02.063.838 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.02.065.449 I Final estimate: PPL = 10.1498 +/- 3.22650

0.02.065.468 I llama_perf_context_print:        load time =    1880.74 ms
0.02.065.469 I llama_perf_context_print: prompt eval time =     137.19 ms /   128 tokens (    1.07 ms per token,   933.01 tokens per second)
0.02.065.471 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.02.065.471 I llama_perf_context_print:       total time =     154.22 ms /   129 tokens
0.02.066.224 I ggml_metal_free: deallocating

real	0m2.264s
user	0m0.110s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.298 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.224 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.138 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.032.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.156 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.156 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.157 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.158 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.158 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.158 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.159 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.159 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.160 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.160 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.101 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.101 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.101 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.102 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.102 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.046.103 I llama_model_loader: - type  f32:  194 tensors
0.00.046.103 I llama_model_loader: - type q8_0:   98 tensors
0.00.079.175 I llm_load_vocab: special tokens cache size = 25
0.00.086.644 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.647 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.647 I llm_load_print_meta: arch             = gptneox
0.00.086.648 I llm_load_print_meta: vocab type       = BPE
0.00.086.648 I llm_load_print_meta: n_vocab          = 50304
0.00.086.648 I llm_load_print_meta: n_merges         = 50009
0.00.086.648 I llm_load_print_meta: vocab_only       = 0
0.00.086.648 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.649 I llm_load_print_meta: n_embd           = 2048
0.00.086.649 I llm_load_print_meta: n_layer          = 24
0.00.086.653 I llm_load_print_meta: n_head           = 16
0.00.086.654 I llm_load_print_meta: n_head_kv        = 16
0.00.086.654 I llm_load_print_meta: n_rot            = 32
0.00.086.657 I llm_load_print_meta: n_swa            = 0
0.00.086.657 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.657 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.658 I llm_load_print_meta: n_gqa            = 1
0.00.086.658 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.659 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.660 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.660 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.660 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.660 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.660 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.661 I llm_load_print_meta: n_ff             = 8192
0.00.086.661 I llm_load_print_meta: n_expert         = 0
0.00.086.661 I llm_load_print_meta: n_expert_used    = 0
0.00.086.662 I llm_load_print_meta: causal attn      = 1
0.00.086.662 I llm_load_print_meta: pooling type     = 0
0.00.086.662 I llm_load_print_meta: rope type        = 2
0.00.086.662 I llm_load_print_meta: rope scaling     = linear
0.00.086.663 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.663 I llm_load_print_meta: freq_scale_train = 1
0.00.086.664 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.664 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.664 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.664 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.664 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.665 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.665 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.665 I llm_load_print_meta: model type       = 1.4B
0.00.086.665 I llm_load_print_meta: model ftype      = Q8_0
0.00.086.666 I llm_load_print_meta: model params     = 1.41 B
0.00.086.668 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.086.668 I llm_load_print_meta: general.name     = 1.4B
0.00.086.668 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.668 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.668 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.670 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.671 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.671 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.671 I llm_load_print_meta: max token length = 1024
0.00.088.984 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.985 I llm_load_tensors: offloading output layer to GPU
0.00.088.985 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.991 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.088.992 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.090.113 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.114 I llama_new_context_with_model: n_ctx         = 128
0.00.090.114 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.114 I llama_new_context_with_model: n_batch       = 128
0.00.090.115 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.115 I llama_new_context_with_model: flash_attn    = 0
0.00.090.115 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.116 I llama_new_context_with_model: freq_scale    = 1
0.00.090.116 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.117 I ggml_metal_init: allocating
0.00.090.123 I ggml_metal_init: found device: Apple M4
0.00.090.126 I ggml_metal_init: picking default device: Apple M4
0.00.090.858 I ggml_metal_init: using embedded metal library
0.00.093.869 I ggml_metal_init: GPU name:   Apple M4
0.00.093.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.872 I ggml_metal_init: simdgroup reduction   = true
0.00.093.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.873 I ggml_metal_init: has bfloat            = true
0.00.093.873 I ggml_metal_init: use bfloat            = true
0.00.093.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.603 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.107 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.112 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.126 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.028 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.029 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.030 I llama_new_context_with_model: graph nodes  = 967
0.00.109.030 I llama_new_context_with_model: graph splits = 2
0.00.109.032 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.081.846 I 
0.01.081.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.081.894 I perplexity: tokenizing the input ..
0.01.092.908 I perplexity: tokenization took 11.011 ms
0.01.092.912 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.223.899 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.225.050 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.225.076 I llama_perf_context_print:        load time =    1064.61 ms
0.01.225.077 I llama_perf_context_print: prompt eval time =     130.73 ms /   128 tokens (    1.02 ms per token,   979.10 tokens per second)
0.01.225.078 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.225.079 I llama_perf_context_print:       total time =     143.23 ms /   129 tokens
0.01.225.648 I ggml_metal_free: deallocating

real	0m1.244s
user	0m0.106s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.246 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.830 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.590 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.594 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.595 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.595 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.599 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.600 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.601 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.601 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.602 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.602 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.604 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.604 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.604 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.258 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.057 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.057 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.058 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.058 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.058 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.059 I llama_model_loader: - type  f32:  194 tensors
0.00.029.059 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.059 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.086 I llm_load_vocab: special tokens cache size = 25
0.00.054.994 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.997 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.998 I llm_load_print_meta: arch             = gptneox
0.00.054.998 I llm_load_print_meta: vocab type       = BPE
0.00.054.998 I llm_load_print_meta: n_vocab          = 50304
0.00.054.998 I llm_load_print_meta: n_merges         = 50009
0.00.054.999 I llm_load_print_meta: vocab_only       = 0
0.00.054.999 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.999 I llm_load_print_meta: n_embd           = 2048
0.00.054.999 I llm_load_print_meta: n_layer          = 24
0.00.055.002 I llm_load_print_meta: n_head           = 16
0.00.055.002 I llm_load_print_meta: n_head_kv        = 16
0.00.055.003 I llm_load_print_meta: n_rot            = 32
0.00.055.003 I llm_load_print_meta: n_swa            = 0
0.00.055.003 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.003 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.004 I llm_load_print_meta: n_gqa            = 1
0.00.055.004 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.005 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.006 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.006 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.007 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.007 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.009 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.010 I llm_load_print_meta: n_ff             = 8192
0.00.055.010 I llm_load_print_meta: n_expert         = 0
0.00.055.010 I llm_load_print_meta: n_expert_used    = 0
0.00.055.010 I llm_load_print_meta: causal attn      = 1
0.00.055.010 I llm_load_print_meta: pooling type     = 0
0.00.055.010 I llm_load_print_meta: rope type        = 2
0.00.055.012 I llm_load_print_meta: rope scaling     = linear
0.00.055.012 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.012 I llm_load_print_meta: freq_scale_train = 1
0.00.055.012 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.013 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.016 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.016 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.016 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.016 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.016 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.016 I llm_load_print_meta: model type       = 1.4B
0.00.055.017 I llm_load_print_meta: model ftype      = Q4_0
0.00.055.017 I llm_load_print_meta: model params     = 1.41 B
0.00.055.018 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.055.018 I llm_load_print_meta: general.name     = 1.4B
0.00.055.018 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.018 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.018 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.018 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.019 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.019 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.019 I llm_load_print_meta: max token length = 1024
0.00.056.605 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.605 I llm_load_tensors: offloading output layer to GPU
0.00.056.606 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.616 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.617 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.470 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.471 I llama_new_context_with_model: n_ctx         = 128
0.00.057.471 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.057.471 I llama_new_context_with_model: n_batch       = 128
0.00.057.472 I llama_new_context_with_model: n_ubatch      = 128
0.00.057.472 I llama_new_context_with_model: flash_attn    = 0
0.00.057.472 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.473 I llama_new_context_with_model: freq_scale    = 1
0.00.057.473 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.473 I ggml_metal_init: allocating
0.00.057.479 I ggml_metal_init: found device: Apple M4
0.00.057.483 I ggml_metal_init: picking default device: Apple M4
0.00.058.026 I ggml_metal_init: using embedded metal library
0.00.060.364 I ggml_metal_init: GPU name:   Apple M4
0.00.060.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.366 I ggml_metal_init: simdgroup reduction   = true
0.00.060.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.367 I ggml_metal_init: has bfloat            = true
0.00.060.367 I ggml_metal_init: use bfloat            = true
0.00.060.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.879 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.139 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.142 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.159 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.028 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.030 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.030 I llama_new_context_with_model: graph nodes  = 967
0.00.072.030 I llama_new_context_with_model: graph splits = 2
0.00.072.031 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.085 I 
0.00.639.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.135 I perplexity: tokenizing the input ..
0.00.647.269 I perplexity: tokenization took 8.131 ms
0.00.647.272 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.140 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.771.304 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.771.327 I llama_perf_context_print:        load time =     626.25 ms
0.00.771.328 I llama_perf_context_print: prompt eval time =     122.63 ms /   128 tokens (    0.96 ms per token,  1043.81 tokens per second)
0.00.771.329 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.329 I llama_perf_context_print:       total time =     132.25 ms /   129 tokens
0.00.771.843 I ggml_metal_free: deallocating

real	0m0.787s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.873 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.232 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.236 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.239 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.240 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.241 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.241 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.241 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.242 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.242 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.244 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.969 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.029 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.786 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.786 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.787 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.787 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.788 I llama_model_loader: - type  f32:  194 tensors
0.00.022.788 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.788 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.660 I llm_load_vocab: special tokens cache size = 25
0.00.048.625 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.628 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.628 I llm_load_print_meta: arch             = gptneox
0.00.048.629 I llm_load_print_meta: vocab type       = BPE
0.00.048.629 I llm_load_print_meta: n_vocab          = 50304
0.00.048.629 I llm_load_print_meta: n_merges         = 50009
0.00.048.629 I llm_load_print_meta: vocab_only       = 0
0.00.048.629 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.630 I llm_load_print_meta: n_embd           = 2048
0.00.048.630 I llm_load_print_meta: n_layer          = 24
0.00.048.633 I llm_load_print_meta: n_head           = 16
0.00.048.634 I llm_load_print_meta: n_head_kv        = 16
0.00.048.634 I llm_load_print_meta: n_rot            = 32
0.00.048.634 I llm_load_print_meta: n_swa            = 0
0.00.048.635 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.635 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.635 I llm_load_print_meta: n_gqa            = 1
0.00.048.636 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.637 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.638 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.638 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.638 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.638 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.638 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.639 I llm_load_print_meta: n_ff             = 8192
0.00.048.639 I llm_load_print_meta: n_expert         = 0
0.00.048.639 I llm_load_print_meta: n_expert_used    = 0
0.00.048.639 I llm_load_print_meta: causal attn      = 1
0.00.048.640 I llm_load_print_meta: pooling type     = 0
0.00.048.640 I llm_load_print_meta: rope type        = 2
0.00.048.640 I llm_load_print_meta: rope scaling     = linear
0.00.048.640 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.642 I llm_load_print_meta: freq_scale_train = 1
0.00.048.642 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.643 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.643 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.643 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.643 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.643 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.644 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.645 I llm_load_print_meta: model type       = 1.4B
0.00.048.645 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.645 I llm_load_print_meta: model params     = 1.41 B
0.00.048.646 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.646 I llm_load_print_meta: general.name     = 1.4B
0.00.048.646 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.647 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.647 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.651 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.652 I llm_load_print_meta: max token length = 1024
0.00.050.617 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.617 I llm_load_tensors: offloading output layer to GPU
0.00.050.618 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.628 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.629 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.523 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.524 I llama_new_context_with_model: n_ctx         = 128
0.00.051.524 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.524 I llama_new_context_with_model: n_batch       = 128
0.00.051.524 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.524 I llama_new_context_with_model: flash_attn    = 0
0.00.051.525 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.525 I llama_new_context_with_model: freq_scale    = 1
0.00.051.525 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.526 I ggml_metal_init: allocating
0.00.051.531 I ggml_metal_init: found device: Apple M4
0.00.051.533 I ggml_metal_init: picking default device: Apple M4
0.00.052.107 I ggml_metal_init: using embedded metal library
0.00.054.420 I ggml_metal_init: GPU name:   Apple M4
0.00.054.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.422 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.423 I ggml_metal_init: simdgroup reduction   = true
0.00.054.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.423 I ggml_metal_init: has bfloat            = true
0.00.054.423 I ggml_metal_init: use bfloat            = true
0.00.054.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.424 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.744 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.029 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.037 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.060 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.906 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.907 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.907 I llama_new_context_with_model: graph nodes  = 967
0.00.065.908 I llama_new_context_with_model: graph splits = 2
0.00.065.909 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.909 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.454 I 
0.00.642.496 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.513 I perplexity: tokenizing the input ..
0.00.650.329 I perplexity: tokenization took 7.815 ms
0.00.650.333 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.773.244 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.774.482 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.774.496 I llama_perf_context_print:        load time =     633.58 ms
0.00.774.498 I llama_perf_context_print: prompt eval time =     122.69 ms /   128 tokens (    0.96 ms per token,  1043.32 tokens per second)
0.00.774.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.774.499 I llama_perf_context_print:       total time =     132.04 ms /   129 tokens
0.00.774.987 I ggml_metal_free: deallocating

real	0m0.788s
user	0m0.076s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.825 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.529 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.531 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.266 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.206 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.207 I llama_model_loader: - type  f32:  194 tensors
0.00.024.208 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.208 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.222 I llm_load_vocab: special tokens cache size = 25
0.00.050.974 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.980 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.980 I llm_load_print_meta: arch             = gptneox
0.00.050.981 I llm_load_print_meta: vocab type       = BPE
0.00.050.981 I llm_load_print_meta: n_vocab          = 50304
0.00.050.981 I llm_load_print_meta: n_merges         = 50009
0.00.050.981 I llm_load_print_meta: vocab_only       = 0
0.00.050.981 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.982 I llm_load_print_meta: n_embd           = 2048
0.00.050.984 I llm_load_print_meta: n_layer          = 24
0.00.050.986 I llm_load_print_meta: n_head           = 16
0.00.050.987 I llm_load_print_meta: n_head_kv        = 16
0.00.050.987 I llm_load_print_meta: n_rot            = 32
0.00.050.988 I llm_load_print_meta: n_swa            = 0
0.00.050.988 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.988 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.989 I llm_load_print_meta: n_gqa            = 1
0.00.050.994 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.995 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.995 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.995 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.996 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.996 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.996 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.997 I llm_load_print_meta: n_ff             = 8192
0.00.050.997 I llm_load_print_meta: n_expert         = 0
0.00.050.997 I llm_load_print_meta: n_expert_used    = 0
0.00.050.997 I llm_load_print_meta: causal attn      = 1
0.00.050.997 I llm_load_print_meta: pooling type     = 0
0.00.050.997 I llm_load_print_meta: rope type        = 2
0.00.050.998 I llm_load_print_meta: rope scaling     = linear
0.00.050.998 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.998 I llm_load_print_meta: freq_scale_train = 1
0.00.050.998 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.999 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.999 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.999 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.000 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.000 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.000 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.000 I llm_load_print_meta: model type       = 1.4B
0.00.051.002 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.002 I llm_load_print_meta: model params     = 1.41 B
0.00.051.002 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.003 I llm_load_print_meta: general.name     = 1.4B
0.00.051.003 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.003 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.003 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.003 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.003 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.004 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.004 I llm_load_print_meta: max token length = 1024
0.00.053.045 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.045 I llm_load_tensors: offloading output layer to GPU
0.00.053.046 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.056 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.057 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.982 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.982 I llama_new_context_with_model: n_ctx         = 128
0.00.053.983 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.983 I llama_new_context_with_model: n_batch       = 128
0.00.053.983 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.983 I llama_new_context_with_model: flash_attn    = 0
0.00.053.983 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.984 I llama_new_context_with_model: freq_scale    = 1
0.00.053.984 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.985 I ggml_metal_init: allocating
0.00.053.988 I ggml_metal_init: found device: Apple M4
0.00.053.990 I ggml_metal_init: picking default device: Apple M4
0.00.054.571 I ggml_metal_init: using embedded metal library
0.00.056.892 I ggml_metal_init: GPU name:   Apple M4
0.00.056.893 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.893 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.894 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.894 I ggml_metal_init: simdgroup reduction   = true
0.00.056.894 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.894 I ggml_metal_init: has bfloat            = true
0.00.056.894 I ggml_metal_init: use bfloat            = true
0.00.056.895 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.462 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.675 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.678 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.692 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.599 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.600 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.600 I llama_new_context_with_model: graph nodes  = 967
0.00.068.600 I llama_new_context_with_model: graph splits = 2
0.00.068.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.433 I 
0.00.707.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.473 I perplexity: tokenizing the input ..
0.00.715.293 I perplexity: tokenization took 7.818 ms
0.00.715.298 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.260 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.851.419 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.851.436 I llama_perf_context_print:        load time =     697.60 ms
0.00.851.437 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   950.01 tokens per second)
0.00.851.438 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.438 I llama_perf_context_print:       total time =     144.00 ms /   129 tokens
0.00.851.906 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.078s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.939 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.370 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.374 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.376 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.378 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.378 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.378 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.379 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.379 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.381 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.381 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.077 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.822 I llama_model_loader: - type  f32:  194 tensors
0.00.022.822 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.823 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.782 I llm_load_vocab: special tokens cache size = 25
0.00.048.746 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.751 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.751 I llm_load_print_meta: arch             = gptneox
0.00.048.754 I llm_load_print_meta: vocab type       = BPE
0.00.048.754 I llm_load_print_meta: n_vocab          = 50304
0.00.048.754 I llm_load_print_meta: n_merges         = 50009
0.00.048.755 I llm_load_print_meta: vocab_only       = 0
0.00.048.755 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.755 I llm_load_print_meta: n_embd           = 2048
0.00.048.755 I llm_load_print_meta: n_layer          = 24
0.00.048.758 I llm_load_print_meta: n_head           = 16
0.00.048.759 I llm_load_print_meta: n_head_kv        = 16
0.00.048.759 I llm_load_print_meta: n_rot            = 32
0.00.048.759 I llm_load_print_meta: n_swa            = 0
0.00.048.759 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.760 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.762 I llm_load_print_meta: n_gqa            = 1
0.00.048.762 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.763 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.764 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.764 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.766 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.766 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.766 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.767 I llm_load_print_meta: n_ff             = 8192
0.00.048.767 I llm_load_print_meta: n_expert         = 0
0.00.048.767 I llm_load_print_meta: n_expert_used    = 0
0.00.048.767 I llm_load_print_meta: causal attn      = 1
0.00.048.767 I llm_load_print_meta: pooling type     = 0
0.00.048.767 I llm_load_print_meta: rope type        = 2
0.00.048.768 I llm_load_print_meta: rope scaling     = linear
0.00.048.768 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.768 I llm_load_print_meta: freq_scale_train = 1
0.00.048.768 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.769 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.769 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.769 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.769 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.769 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.769 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.773 I llm_load_print_meta: model type       = 1.4B
0.00.048.773 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.774 I llm_load_print_meta: model params     = 1.41 B
0.00.048.775 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.775 I llm_load_print_meta: general.name     = 1.4B
0.00.048.775 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.775 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.775 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.776 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.776 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.776 I llm_load_print_meta: max token length = 1024
0.00.050.801 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.801 I llm_load_tensors: offloading output layer to GPU
0.00.050.801 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.812 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.813 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.760 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.761 I llama_new_context_with_model: n_ctx         = 128
0.00.051.761 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.761 I llama_new_context_with_model: n_batch       = 128
0.00.051.761 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.761 I llama_new_context_with_model: flash_attn    = 0
0.00.051.762 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.762 I llama_new_context_with_model: freq_scale    = 1
0.00.051.763 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.763 I ggml_metal_init: allocating
0.00.051.771 I ggml_metal_init: found device: Apple M4
0.00.051.773 I ggml_metal_init: picking default device: Apple M4
0.00.052.361 I ggml_metal_init: using embedded metal library
0.00.054.725 I ggml_metal_init: GPU name:   Apple M4
0.00.054.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.726 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.727 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.727 I ggml_metal_init: simdgroup reduction   = true
0.00.054.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.727 I ggml_metal_init: has bfloat            = true
0.00.054.727 I ggml_metal_init: use bfloat            = true
0.00.054.728 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.156 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.652 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.657 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.671 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.582 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.583 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.584 I llama_new_context_with_model: graph nodes  = 967
0.00.066.584 I llama_new_context_with_model: graph splits = 2
0.00.066.585 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.585 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.232 I 
0.00.766.260 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.276 I perplexity: tokenizing the input ..
0.00.773.931 I perplexity: tokenization took 7.654 ms
0.00.773.935 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.908.747 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.909.904 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.909.917 I llama_perf_context_print:        load time =     757.29 ms
0.00.909.918 I llama_perf_context_print: prompt eval time =     134.57 ms /   128 tokens (    1.05 ms per token,   951.16 tokens per second)
0.00.909.919 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.909.919 I llama_perf_context_print:       total time =     143.69 ms /   129 tokens
0.00.910.502 I ggml_metal_free: deallocating

real	0m0.925s
user	0m0.077s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.670 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.017 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.030 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.031 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.033 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.554 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.555 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.556 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.556 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.556 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.557 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.557 I llama_model_loader: - type  f32:  194 tensors
0.00.023.557 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.557 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.557 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.226 I llm_load_vocab: special tokens cache size = 25
0.00.049.150 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.152 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.153 I llm_load_print_meta: arch             = gptneox
0.00.049.153 I llm_load_print_meta: vocab type       = BPE
0.00.049.154 I llm_load_print_meta: n_vocab          = 50304
0.00.049.154 I llm_load_print_meta: n_merges         = 50009
0.00.049.154 I llm_load_print_meta: vocab_only       = 0
0.00.049.154 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.154 I llm_load_print_meta: n_embd           = 2048
0.00.049.154 I llm_load_print_meta: n_layer          = 24
0.00.049.157 I llm_load_print_meta: n_head           = 16
0.00.049.157 I llm_load_print_meta: n_head_kv        = 16
0.00.049.158 I llm_load_print_meta: n_rot            = 32
0.00.049.158 I llm_load_print_meta: n_swa            = 0
0.00.049.158 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.158 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.159 I llm_load_print_meta: n_gqa            = 1
0.00.049.159 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.160 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.161 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.161 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.161 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.161 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.161 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.164 I llm_load_print_meta: n_ff             = 8192
0.00.049.164 I llm_load_print_meta: n_expert         = 0
0.00.049.164 I llm_load_print_meta: n_expert_used    = 0
0.00.049.164 I llm_load_print_meta: causal attn      = 1
0.00.049.164 I llm_load_print_meta: pooling type     = 0
0.00.049.165 I llm_load_print_meta: rope type        = 2
0.00.049.165 I llm_load_print_meta: rope scaling     = linear
0.00.049.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.166 I llm_load_print_meta: freq_scale_train = 1
0.00.049.166 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.166 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.166 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.168 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.168 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.168 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.168 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.169 I llm_load_print_meta: model type       = 1.4B
0.00.049.169 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.169 I llm_load_print_meta: model params     = 1.41 B
0.00.049.170 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.170 I llm_load_print_meta: general.name     = 1.4B
0.00.049.170 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.171 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.171 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.171 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.171 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.171 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.172 I llm_load_print_meta: max token length = 1024
0.00.051.020 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.020 I llm_load_tensors: offloading output layer to GPU
0.00.051.020 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.031 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.032 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.908 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.909 I llama_new_context_with_model: n_ctx         = 128
0.00.051.909 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.909 I llama_new_context_with_model: n_batch       = 128
0.00.051.909 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.910 I llama_new_context_with_model: flash_attn    = 0
0.00.051.910 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.910 I llama_new_context_with_model: freq_scale    = 1
0.00.051.911 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.911 I ggml_metal_init: allocating
0.00.051.918 I ggml_metal_init: found device: Apple M4
0.00.051.920 I ggml_metal_init: picking default device: Apple M4
0.00.052.483 I ggml_metal_init: using embedded metal library
0.00.054.810 I ggml_metal_init: GPU name:   Apple M4
0.00.054.812 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.813 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.813 I ggml_metal_init: simdgroup reduction   = true
0.00.054.813 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.813 I ggml_metal_init: has bfloat            = true
0.00.054.813 I ggml_metal_init: use bfloat            = true
0.00.054.814 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.281 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.536 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.539 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.553 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.407 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.408 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.409 I llama_new_context_with_model: graph nodes  = 967
0.00.066.409 I llama_new_context_with_model: graph splits = 2
0.00.066.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.410 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.413.471 I 
0.00.413.511 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.413.524 I perplexity: tokenizing the input ..
0.00.421.844 I perplexity: tokenization took 8.319 ms
0.00.421.848 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.554.163 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.555.434 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.555.449 I llama_perf_context_print:        load time =     403.79 ms
0.00.555.450 I llama_perf_context_print: prompt eval time =     132.09 ms /   128 tokens (    1.03 ms per token,   969.04 tokens per second)
0.00.555.451 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.555.451 I llama_perf_context_print:       total time =     141.98 ms /   129 tokens
0.00.555.914 I ggml_metal_free: deallocating

real	0m0.571s
user	0m0.077s
sys	0m0.071s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.858 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.647 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.653 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.654 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.654 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.654 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.655 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.656 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.656 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.665 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.409 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.409 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.410 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.410 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.410 I llama_model_loader: - type  f32:  194 tensors
0.00.023.411 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.411 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.411 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.411 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.927 I llm_load_vocab: special tokens cache size = 25
0.00.049.897 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.900 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.900 I llm_load_print_meta: arch             = gptneox
0.00.049.900 I llm_load_print_meta: vocab type       = BPE
0.00.049.900 I llm_load_print_meta: n_vocab          = 50304
0.00.049.901 I llm_load_print_meta: n_merges         = 50009
0.00.049.901 I llm_load_print_meta: vocab_only       = 0
0.00.049.901 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.901 I llm_load_print_meta: n_embd           = 2048
0.00.049.901 I llm_load_print_meta: n_layer          = 24
0.00.049.904 I llm_load_print_meta: n_head           = 16
0.00.049.905 I llm_load_print_meta: n_head_kv        = 16
0.00.049.905 I llm_load_print_meta: n_rot            = 32
0.00.049.905 I llm_load_print_meta: n_swa            = 0
0.00.049.905 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.905 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.908 I llm_load_print_meta: n_gqa            = 1
0.00.049.909 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.911 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.912 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.912 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.912 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.913 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.913 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.913 I llm_load_print_meta: n_ff             = 8192
0.00.049.914 I llm_load_print_meta: n_expert         = 0
0.00.049.914 I llm_load_print_meta: n_expert_used    = 0
0.00.049.914 I llm_load_print_meta: causal attn      = 1
0.00.049.914 I llm_load_print_meta: pooling type     = 0
0.00.049.914 I llm_load_print_meta: rope type        = 2
0.00.049.916 I llm_load_print_meta: rope scaling     = linear
0.00.049.916 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.916 I llm_load_print_meta: freq_scale_train = 1
0.00.049.916 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.917 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.917 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.917 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.917 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.917 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.918 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.918 I llm_load_print_meta: model type       = 1.4B
0.00.049.919 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.919 I llm_load_print_meta: model params     = 1.41 B
0.00.049.919 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.921 I llm_load_print_meta: general.name     = 1.4B
0.00.049.921 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.921 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.921 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.922 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.922 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.922 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.923 I llm_load_print_meta: max token length = 1024
0.00.051.887 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.887 I llm_load_tensors: offloading output layer to GPU
0.00.051.888 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.898 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.899 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.782 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.782 I llama_new_context_with_model: n_ctx         = 128
0.00.052.782 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.783 I llama_new_context_with_model: n_batch       = 128
0.00.052.783 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.783 I llama_new_context_with_model: flash_attn    = 0
0.00.052.783 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.784 I llama_new_context_with_model: freq_scale    = 1
0.00.052.784 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.784 I ggml_metal_init: allocating
0.00.052.790 I ggml_metal_init: found device: Apple M4
0.00.052.792 I ggml_metal_init: picking default device: Apple M4
0.00.053.342 I ggml_metal_init: using embedded metal library
0.00.055.677 I ggml_metal_init: GPU name:   Apple M4
0.00.055.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.679 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.680 I ggml_metal_init: simdgroup reduction   = true
0.00.055.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.680 I ggml_metal_init: has bfloat            = true
0.00.055.680 I ggml_metal_init: use bfloat            = true
0.00.055.681 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.206 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.504 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.508 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.521 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.463 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.464 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.465 I llama_new_context_with_model: graph nodes  = 967
0.00.067.465 I llama_new_context_with_model: graph splits = 2
0.00.067.466 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.583 I 
0.00.505.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.633 I perplexity: tokenizing the input ..
0.00.513.844 I perplexity: tokenization took 8.21 ms
0.00.513.848 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.242 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.385 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.401 I llama_perf_context_print:        load time =     496.72 ms
0.00.647.401 I llama_perf_context_print: prompt eval time =     132.15 ms /   128 tokens (    1.03 ms per token,   968.62 tokens per second)
0.00.647.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.403 I llama_perf_context_print:       total time =     141.82 ms /   129 tokens
0.00.648.006 I ggml_metal_free: deallocating

real	0m0.661s
user	0m0.078s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.791 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.710 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.712 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.713 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.717 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.717 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.718 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.719 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.571 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.344 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.345 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.345 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.346 I llama_model_loader: - type  f32:  194 tensors
0.00.023.346 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.347 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.347 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.862 I llm_load_vocab: special tokens cache size = 25
0.00.049.906 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.909 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.909 I llm_load_print_meta: arch             = gptneox
0.00.049.910 I llm_load_print_meta: vocab type       = BPE
0.00.049.910 I llm_load_print_meta: n_vocab          = 50304
0.00.049.910 I llm_load_print_meta: n_merges         = 50009
0.00.049.910 I llm_load_print_meta: vocab_only       = 0
0.00.049.910 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.911 I llm_load_print_meta: n_embd           = 2048
0.00.049.911 I llm_load_print_meta: n_layer          = 24
0.00.049.913 I llm_load_print_meta: n_head           = 16
0.00.049.914 I llm_load_print_meta: n_head_kv        = 16
0.00.049.914 I llm_load_print_meta: n_rot            = 32
0.00.049.914 I llm_load_print_meta: n_swa            = 0
0.00.049.915 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.915 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.916 I llm_load_print_meta: n_gqa            = 1
0.00.049.916 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.917 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.917 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.919 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.919 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.920 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.920 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.920 I llm_load_print_meta: n_ff             = 8192
0.00.049.921 I llm_load_print_meta: n_expert         = 0
0.00.049.921 I llm_load_print_meta: n_expert_used    = 0
0.00.049.921 I llm_load_print_meta: causal attn      = 1
0.00.049.921 I llm_load_print_meta: pooling type     = 0
0.00.049.921 I llm_load_print_meta: rope type        = 2
0.00.049.922 I llm_load_print_meta: rope scaling     = linear
0.00.049.922 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.922 I llm_load_print_meta: freq_scale_train = 1
0.00.049.923 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.923 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.923 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.923 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.925 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.925 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.925 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.925 I llm_load_print_meta: model type       = 1.4B
0.00.049.926 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.926 I llm_load_print_meta: model params     = 1.41 B
0.00.049.927 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.927 I llm_load_print_meta: general.name     = 1.4B
0.00.049.927 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.927 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.928 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.928 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.928 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.928 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.929 I llm_load_print_meta: max token length = 1024
0.00.051.881 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.881 I llm_load_tensors: offloading output layer to GPU
0.00.051.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.892 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.893 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.763 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.764 I llama_new_context_with_model: n_ctx         = 128
0.00.052.764 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.764 I llama_new_context_with_model: n_batch       = 128
0.00.052.764 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.765 I llama_new_context_with_model: flash_attn    = 0
0.00.052.765 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.765 I llama_new_context_with_model: freq_scale    = 1
0.00.052.766 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.766 I ggml_metal_init: allocating
0.00.052.773 I ggml_metal_init: found device: Apple M4
0.00.052.775 I ggml_metal_init: picking default device: Apple M4
0.00.053.358 I ggml_metal_init: using embedded metal library
0.00.055.680 I ggml_metal_init: GPU name:   Apple M4
0.00.055.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.683 I ggml_metal_init: simdgroup reduction   = true
0.00.055.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.683 I ggml_metal_init: has bfloat            = true
0.00.055.683 I ggml_metal_init: use bfloat            = true
0.00.055.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.953 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.239 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.242 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.258 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.125 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.126 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.127 I llama_new_context_with_model: graph nodes  = 967
0.00.067.127 I llama_new_context_with_model: graph splits = 2
0.00.067.128 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.902 I 
0.00.545.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.956 I perplexity: tokenizing the input ..
0.00.553.791 I perplexity: tokenization took 7.834 ms
0.00.553.795 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.687.974 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.689.144 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.689.159 I llama_perf_context_print:        load time =     537.10 ms
0.00.689.160 I llama_perf_context_print: prompt eval time =     133.95 ms /   128 tokens (    1.05 ms per token,   955.59 tokens per second)
0.00.689.161 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.161 I llama_perf_context_print:       total time =     143.26 ms /   129 tokens
0.00.689.620 I ggml_metal_free: deallocating

real	0m0.704s
user	0m0.078s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.199 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.781 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.785 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.792 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.800 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.800 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.801 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.591 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.450 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.451 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.452 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.453 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.453 I llama_model_loader: - type  f32:  194 tensors
0.00.024.453 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.454 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.064 I llm_load_vocab: special tokens cache size = 25
0.00.051.208 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.211 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.211 I llm_load_print_meta: arch             = gptneox
0.00.051.211 I llm_load_print_meta: vocab type       = BPE
0.00.051.212 I llm_load_print_meta: n_vocab          = 50304
0.00.051.212 I llm_load_print_meta: n_merges         = 50009
0.00.051.212 I llm_load_print_meta: vocab_only       = 0
0.00.051.212 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.212 I llm_load_print_meta: n_embd           = 2048
0.00.051.213 I llm_load_print_meta: n_layer          = 24
0.00.051.215 I llm_load_print_meta: n_head           = 16
0.00.051.216 I llm_load_print_meta: n_head_kv        = 16
0.00.051.216 I llm_load_print_meta: n_rot            = 32
0.00.051.216 I llm_load_print_meta: n_swa            = 0
0.00.051.216 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.217 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.217 I llm_load_print_meta: n_gqa            = 1
0.00.051.218 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.219 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.219 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.220 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.220 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.220 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.220 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.221 I llm_load_print_meta: n_ff             = 8192
0.00.051.221 I llm_load_print_meta: n_expert         = 0
0.00.051.221 I llm_load_print_meta: n_expert_used    = 0
0.00.051.221 I llm_load_print_meta: causal attn      = 1
0.00.051.221 I llm_load_print_meta: pooling type     = 0
0.00.051.221 I llm_load_print_meta: rope type        = 2
0.00.051.222 I llm_load_print_meta: rope scaling     = linear
0.00.051.223 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.224 I llm_load_print_meta: freq_scale_train = 1
0.00.051.224 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.224 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.224 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.224 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.225 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.225 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.225 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.225 I llm_load_print_meta: model type       = 1.4B
0.00.051.226 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.226 I llm_load_print_meta: model params     = 1.41 B
0.00.051.228 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.228 I llm_load_print_meta: general.name     = 1.4B
0.00.051.229 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.229 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.229 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.229 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.230 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: max token length = 1024
0.00.053.219 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.219 I llm_load_tensors: offloading output layer to GPU
0.00.053.219 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.230 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.231 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.171 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.171 I llama_new_context_with_model: n_ctx         = 128
0.00.054.171 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.172 I llama_new_context_with_model: n_batch       = 128
0.00.054.172 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.172 I llama_new_context_with_model: flash_attn    = 0
0.00.054.172 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.173 I llama_new_context_with_model: freq_scale    = 1
0.00.054.173 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.174 I ggml_metal_init: allocating
0.00.054.178 I ggml_metal_init: found device: Apple M4
0.00.054.181 I ggml_metal_init: picking default device: Apple M4
0.00.054.751 I ggml_metal_init: using embedded metal library
0.00.057.093 I ggml_metal_init: GPU name:   Apple M4
0.00.057.095 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.095 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.096 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.096 I ggml_metal_init: simdgroup reduction   = true
0.00.057.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.096 I ggml_metal_init: has bfloat            = true
0.00.057.097 I ggml_metal_init: use bfloat            = true
0.00.057.097 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.098 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.721 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.000 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.003 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.018 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.938 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.939 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.939 I llama_new_context_with_model: graph nodes  = 967
0.00.068.940 I llama_new_context_with_model: graph splits = 2
0.00.068.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.768 I 
0.00.641.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.813 I perplexity: tokenizing the input ..
0.00.648.600 I perplexity: tokenization took 6.786 ms
0.00.648.604 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.110 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.790.323 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.790.342 I llama_perf_context_print:        load time =     631.57 ms
0.00.790.343 I llama_perf_context_print: prompt eval time =     140.25 ms /   128 tokens (    1.10 ms per token,   912.67 tokens per second)
0.00.790.344 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.344 I llama_perf_context_print:       total time =     148.57 ms /   129 tokens
0.00.790.846 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.078s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.138 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.873 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.877 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.881 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.881 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.881 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.883 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.883 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.884 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.884 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.884 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.617 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.325 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.326 I llama_model_loader: - type  f32:  194 tensors
0.00.023.327 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.051 I llm_load_vocab: special tokens cache size = 25
0.00.048.999 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.001 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.002 I llm_load_print_meta: arch             = gptneox
0.00.049.002 I llm_load_print_meta: vocab type       = BPE
0.00.049.002 I llm_load_print_meta: n_vocab          = 50304
0.00.049.003 I llm_load_print_meta: n_merges         = 50009
0.00.049.003 I llm_load_print_meta: vocab_only       = 0
0.00.049.003 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.003 I llm_load_print_meta: n_embd           = 2048
0.00.049.003 I llm_load_print_meta: n_layer          = 24
0.00.049.006 I llm_load_print_meta: n_head           = 16
0.00.049.007 I llm_load_print_meta: n_head_kv        = 16
0.00.049.007 I llm_load_print_meta: n_rot            = 32
0.00.049.008 I llm_load_print_meta: n_swa            = 0
0.00.049.008 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.008 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.009 I llm_load_print_meta: n_gqa            = 1
0.00.049.009 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.010 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.010 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.011 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.011 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.011 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.011 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.012 I llm_load_print_meta: n_ff             = 8192
0.00.049.013 I llm_load_print_meta: n_expert         = 0
0.00.049.014 I llm_load_print_meta: n_expert_used    = 0
0.00.049.014 I llm_load_print_meta: causal attn      = 1
0.00.049.014 I llm_load_print_meta: pooling type     = 0
0.00.049.014 I llm_load_print_meta: rope type        = 2
0.00.049.014 I llm_load_print_meta: rope scaling     = linear
0.00.049.015 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.015 I llm_load_print_meta: freq_scale_train = 1
0.00.049.015 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.017 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.017 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.018 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.018 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.018 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.018 I llm_load_print_meta: model type       = 1.4B
0.00.049.019 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.019 I llm_load_print_meta: model params     = 1.41 B
0.00.049.019 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.020 I llm_load_print_meta: general.name     = 1.4B
0.00.049.021 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.021 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.022 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.022 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.023 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.023 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.023 I llm_load_print_meta: max token length = 1024
0.00.050.655 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.655 I llm_load_tensors: offloading output layer to GPU
0.00.050.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.665 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.667 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.615 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.615 I llama_new_context_with_model: n_ctx         = 128
0.00.051.616 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.616 I llama_new_context_with_model: n_batch       = 128
0.00.051.616 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.616 I llama_new_context_with_model: flash_attn    = 0
0.00.051.617 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.617 I llama_new_context_with_model: freq_scale    = 1
0.00.051.617 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.618 I ggml_metal_init: allocating
0.00.051.621 I ggml_metal_init: found device: Apple M4
0.00.051.623 I ggml_metal_init: picking default device: Apple M4
0.00.052.215 I ggml_metal_init: using embedded metal library
0.00.054.539 I ggml_metal_init: GPU name:   Apple M4
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.542 I ggml_metal_init: simdgroup reduction   = true
0.00.054.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.542 I ggml_metal_init: has bfloat            = true
0.00.054.542 I ggml_metal_init: use bfloat            = true
0.00.054.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.999 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.351 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.354 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.368 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.282 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.283 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.284 I llama_new_context_with_model: graph nodes  = 967
0.00.066.284 I llama_new_context_with_model: graph splits = 2
0.00.066.285 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.004 I 
0.00.680.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.039 I perplexity: tokenizing the input ..
0.00.687.762 I perplexity: tokenization took 7.722 ms
0.00.687.766 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.828 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.828.958 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.828.973 I llama_perf_context_print:        load time =     670.86 ms
0.00.828.974 I llama_perf_context_print: prompt eval time =     139.81 ms /   128 tokens (    1.09 ms per token,   915.55 tokens per second)
0.00.828.975 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.975 I llama_perf_context_print:       total time =     148.97 ms /   129 tokens
0.00.829.402 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.078s
sys	0m0.129s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.335 I build: 4434 (a3d50bc0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.575 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.569 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.573 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.575 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.575 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.576 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.576 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.580 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.581 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.582 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.582 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.582 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.583 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.552 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.032 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.033 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.034 I llama_model_loader: - type  f32:  194 tensors
0.00.051.035 I llama_model_loader: - type  f16:   98 tensors
0.00.079.533 I llm_load_vocab: special tokens cache size = 25
0.00.086.137 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.141 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.141 I llm_load_print_meta: arch             = gptneox
0.00.086.142 I llm_load_print_meta: vocab type       = BPE
0.00.086.142 I llm_load_print_meta: n_vocab          = 50304
0.00.086.142 I llm_load_print_meta: n_merges         = 50009
0.00.086.142 I llm_load_print_meta: vocab_only       = 0
0.00.086.142 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.145 I llm_load_print_meta: n_embd           = 2048
0.00.086.145 I llm_load_print_meta: n_layer          = 24
0.00.086.148 I llm_load_print_meta: n_head           = 16
0.00.086.149 I llm_load_print_meta: n_head_kv        = 16
0.00.086.149 I llm_load_print_meta: n_rot            = 32
0.00.086.150 I llm_load_print_meta: n_swa            = 0
0.00.086.150 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.150 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.151 I llm_load_print_meta: n_gqa            = 1
0.00.086.151 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.152 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.152 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.153 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.153 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.153 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.153 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.154 I llm_load_print_meta: n_ff             = 8192
0.00.086.154 I llm_load_print_meta: n_expert         = 0
0.00.086.155 I llm_load_print_meta: n_expert_used    = 0
0.00.086.155 I llm_load_print_meta: causal attn      = 1
0.00.086.155 I llm_load_print_meta: pooling type     = 0
0.00.086.162 I llm_load_print_meta: rope type        = 2
0.00.086.163 I llm_load_print_meta: rope scaling     = linear
0.00.086.164 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.165 I llm_load_print_meta: freq_scale_train = 1
0.00.086.165 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.165 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.166 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.166 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.166 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.166 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.166 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.167 I llm_load_print_meta: model type       = 1.4B
0.00.086.167 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.168 I llm_load_print_meta: model params     = 1.41 B
0.00.086.168 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.169 I llm_load_print_meta: general.name     = 1.4B
0.00.086.169 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.169 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.169 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.170 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.170 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.172 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.172 I llm_load_print_meta: max token length = 1024
0.00.088.153 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.153 I llm_load_tensors: offloading output layer to GPU
0.00.088.153 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.164 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.165 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.086 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.087 I llama_new_context_with_model: n_ctx         = 128
0.00.089.087 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.087 I llama_new_context_with_model: n_batch       = 128
0.00.089.087 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.087 I llama_new_context_with_model: flash_attn    = 0
0.00.089.088 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.088 I llama_new_context_with_model: freq_scale    = 1
0.00.089.088 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.089 I ggml_metal_init: allocating
0.00.089.092 I ggml_metal_init: found device: Apple M4
0.00.089.094 I ggml_metal_init: picking default device: Apple M4
0.00.089.750 I ggml_metal_init: using embedded metal library
0.00.092.441 I ggml_metal_init: GPU name:   Apple M4
0.00.092.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.443 I ggml_metal_init: simdgroup reduction   = true
0.00.092.444 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.444 I ggml_metal_init: has bfloat            = true
0.00.092.444 I ggml_metal_init: use bfloat            = true
0.00.092.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.450 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.797 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.800 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.815 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.606 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.607 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.608 I llama_new_context_with_model: graph nodes  = 967
0.00.103.608 I llama_new_context_with_model: graph splits = 2
0.00.103.609 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.609 I 
0.00.103.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.637 I compute_imatrix: tokenizing the input ..
0.00.110.300 I compute_imatrix: tokenization took 6.663 ms
0.00.110.302 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.617.132 I compute_imatrix: 1.51 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.620.252 I llama_perf_context_print:        load time =    1595.56 ms
0.01.620.254 I llama_perf_context_print: prompt eval time =    1506.23 ms /   128 tokens (   11.77 ms per token,    84.98 tokens per second)
0.01.620.255 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.620.261 I llama_perf_context_print:       total time =    1598.67 ms /   129 tokens
0.01.621.219 I ggml_metal_free: deallocating

real	0m1.816s
user	0m0.180s
sys	0m0.249s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4434 (a3d50bc0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12da0a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12da0ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12da0b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12da0b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12da0be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12da0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12da0c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12da0cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12da0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12da0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12da0df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12da0e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12da0ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12da0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12da0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12da10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12da10d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12da11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12da11b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12da12360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12da12a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12da131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12da138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12da14160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12da14880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12da14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12da15150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12da15dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12da16300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12da165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12da16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12da16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12da175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12da17af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12da17db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12da18250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12da186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12da18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12da19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12da194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12da19970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12da19e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12da1a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12da1a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12da1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12da1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12da1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12da1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12da1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12da1cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12da1d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12da1d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12da1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12da1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12da1eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12da1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12da1f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12da1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12da1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12da205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12da20860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12da20d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12da211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12da21640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12da21ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12da21f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12da22420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12da228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12da22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12da23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12da236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12da23b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12da23fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12da24530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12da24a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12da24fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12da25520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12da25a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12da25fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12da26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12da26a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12da26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12da27500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12da27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12da27fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12da284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12da28a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12da28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12da294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12da29a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12da29f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12da2a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12da2aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12da2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12da2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12da2ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12da2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12da1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12da2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12da2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12da2d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12da2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12da2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12da2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12da2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12da2eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12da2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12da2f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12da2fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12da300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12da305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12da30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12da31090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12da31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12da319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12da31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12da32310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12da327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12da32c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12da330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12da33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12da33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12da33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12da34370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12da34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12da34cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12da35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12da355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12da35a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12da35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12da363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12da36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12da36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12da371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12da37650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12da37af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12da37f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12da38430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12da388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12da38d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12da39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12da396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12da39b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12da39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12da3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12da3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12da3add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12da3b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12da3b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12da3bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12da3c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12da3c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12da3c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12da3ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12da3d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12da3d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12da3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12da3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12da3e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12da3e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12da3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12da3f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12da3f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12da3fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12da40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12da405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12da40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12da40ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12da41390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12da41830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12da41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12da42170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12da42610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12da42ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12da42f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12da433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12da43890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12da43d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12da441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12da44670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12da44b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12da44fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12da45450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12da458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12da45d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12da46230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12da466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12da46b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12da47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12da474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12da47950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12da47df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12da48290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12da487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12da48d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12da49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12da497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12da49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12da4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12da4a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12da4acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12da4b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12da4b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12da4bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12da4c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12da4c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12da4d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12da4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12da4d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12da4de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12da4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12da4eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12da4f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12da4f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12da4faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12da50040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12da50590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12da50ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12da51030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12da51580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12da51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12da52020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12da52570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12da52ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12da53010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12da53560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12da53ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12da54000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12da54550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12da54aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12da54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12da55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12da55a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12da55fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12da56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12da56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12da56fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12da57520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12da57a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12da57fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12da58510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12da58a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12da58fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12da59500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12da59a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12da59fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12da5a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12da5aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12da5af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12da5b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12da5ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12da5bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12da5c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12da5ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12da5cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12da5d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12da5da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12da5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12da5e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12da5ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12da5ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12da5f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12da5f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12da5ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12da60490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12da609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12da60f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12da613d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12da61870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12da61d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12da621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12da62650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12da62af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12da62f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12da63430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12da638d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12da63d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12da64210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12da646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12da64b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12da64ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12da65490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12da659e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12da66100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12da66820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12da66f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12da67660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12da67920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12da68110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12da683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12da689e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.128.499 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.128.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12da68690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12da4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12da49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12da4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12da1da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12da1d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12da1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12da4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12da14e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12da1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12da1c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12da1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12da1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12da1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12da13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12da20070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12da2c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12da67be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12da16fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12da172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12da4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12da4af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12da15410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12da156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12da15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12da68e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12da69100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12da693c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12da69680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12da69940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12da69c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12da69ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12da6a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12da6a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12da6a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12da6a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12da6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12da6af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12da6b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12da6b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12da6b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12da6ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12da6bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12da6bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12da6c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12da6c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12da6c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12da6cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12da6cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12da6d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12da6d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12da6d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12da6d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12da6db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12da6de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12da6e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12da6e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12da6e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12da6e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12da6ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12da6ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12da6f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12da6f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12da6f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12da6f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12da6fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12da6ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12da701c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12da70480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12da70740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12da70a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12da70cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12da70f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12da71240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12da71500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12da717c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12da71a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12da71d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12da72000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12da722c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12da72580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12da72840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12da72b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12da72dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12da73080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12da73340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12da73600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12da738c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12da73b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12da73e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12da74100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12da743c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12da74680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12da74940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12da74c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12da74ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12da75180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12da75440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12da75700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12da759c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12da75c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12da75f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12da76200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12da764c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12da76780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12da76a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12da76d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12da76fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12da77280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12da77540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12da77800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12da77ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12da77d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12da78040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12da78300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12da785c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12da78880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12da78b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12da78e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12da790c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12da79380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12da79640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12da79900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12da79bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12da79e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12da7a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12da7a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12da7a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12da7a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12da7ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12da7af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12da7b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12da7b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12da7b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12da7ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12da7bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12da7bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12da7c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12da7c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12da7c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12da7ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12da7cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12da7d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12da7d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12da7d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12da7d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12da7db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12da7ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12da7e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12da7e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12da7e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12da7e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12da7eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12da7ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12da7f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12da7f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12da7f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12da7f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12da7fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12da7fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12da80180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12da80440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12da80700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12da809c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12da80c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12da80f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12da81200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12da814c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12da81780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12da81a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12da81d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12da81fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12da82280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12da82540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12da82800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12da82ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12da82d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12da83040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12da83300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12da835c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12da83880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12da83b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12da83e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12da840c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12da84380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12da84640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12da84900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12da84bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12da84e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12da85140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12da85400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12da856c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12da85980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12da85c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12da85f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12da861c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12da86480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12da86740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12da86a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12da86cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12da86f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12da87240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12da87500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12da877c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12da87a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12da87d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12da88000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12da882c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12da88580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12da88840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12da88ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12da892b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12da89570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12da899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12da89e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12da8a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12da8a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12da8aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12da8b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12da8b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12da8b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12da8bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12da8c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12da8c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12da8cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12da8cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12da8d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12da8d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12da8dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12da8e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12da8e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12da8e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12da8ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12da8f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12da8f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12da8fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12da8fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12da90460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12da908d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12da90d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12da911b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12da91620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12da91a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12da91f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12da92370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12da927e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12da92c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12da930c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12da93530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12da939a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12da93e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12da94280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12da946f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12da94b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12da94fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12da95440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12da958b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12da95d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12da96190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12da96600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12da96a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12da96ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12da97350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12da977c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12da97c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12da980a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12da98510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12da98980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12da98df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12da99260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12da996d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12da99b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12da99fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12da9a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12da9a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12da9ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12da9b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12da9b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12da9ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12da9bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12da9c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12da9c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12da9cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12da9d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12da9dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12da9e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12da9ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12da9eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12da9f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12da9f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12da9ff60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1427044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1427056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1427063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142706cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142707140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142707860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142708380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142708b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142709340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142709a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14270a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14270a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14270afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14270b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14270be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14270c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14270cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14270d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14270da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14270dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14270e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14270e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14270e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14270ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14270f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14270f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14270fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14270fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1427102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142710710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142710b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142710ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142711460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1427118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142711d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1427121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142712620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142712a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142712f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142713370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1427137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142713c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1427140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142714530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1427149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142715280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1427156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142715b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142715fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142716540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142716a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142716eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142717320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142717790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142717c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142718070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1427184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142718950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142718dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142719230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1427196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142719b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142719f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14271a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14271a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14271acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14271b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14271b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14271ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14271be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14271c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14271c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14271cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14271d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14271d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14271d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14271dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14271e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14271e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14271eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14271ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14271f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14271f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14271fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142720120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142720590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142720a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142720e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1427212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142721750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142722030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1427224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142722910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142722d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1427231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142723a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142723d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1427241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142724620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142724a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142724f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142725370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1427257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142725c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1427260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142726530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1427269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142726e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142727280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1427276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142727fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142728440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1427288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142728d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142729190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142729600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142729a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142729ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14272a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14272a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14272ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14272b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14272b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14272b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14272bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14272c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14272c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14272cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14272cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14272d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14272d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14272dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14272e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14272e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14272ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14272eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14272f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14272f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14272fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142730080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1427304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142730960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142730dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142731240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1427316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142731b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142732400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142732870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1427335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142733a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142733ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142734310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142734780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142734bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142735060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1427354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142735940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142735db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142736220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142736690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142736b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142736f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1427373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142737850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142737cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142738130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1427385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142738a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142738e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1427392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142739760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142739bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14273a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14273a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14273a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14273ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14273b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14273b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14273bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14273bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14273c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14273c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14273cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14273d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14273d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14273d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14273de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14273e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14273e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14273ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14273f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14273f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14273f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14273fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1427401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142740650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142740ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142740f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142741ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142741d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142742030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1427424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142742910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142742d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1427431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142743660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142743ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142743f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1427443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142744820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142744c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142745100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142745570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1427459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142745e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1427462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142746730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142746ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142747010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142747480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1427478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142747d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1427481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142748640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142748ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142748f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142749800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142749c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14274a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14274a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14274a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14274ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14274b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14274b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14274bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14274bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14274c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14274c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14274cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14274d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14274d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14274da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14274df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14274e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14274e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14274ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14274f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14274f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14274f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14274fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142750280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1427506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142750b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142750fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142751440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1427518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142751d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142752190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142752600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142752a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142752ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142753350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1427537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142753c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1427540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142754510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142754980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142754df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142755260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1427556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142756140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142756860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142756f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1427576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142757960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142757dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1427583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1427589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.787s
user	0m0.283s
sys	0m0.277s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4434 (a3d50bc0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134e0b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134e0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134e0bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134e0c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134e0c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134e0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134e0d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134e0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134e0df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134e0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134e0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134e0ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134e0f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134e10160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134e10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134e11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134e117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134e11ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134e125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134e12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134e134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134e13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134e14320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134e14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134e152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134e155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134e15bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134e16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134e16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134e17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134e174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134e17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134e18010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134e18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134e18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134e18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134e19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134e195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134e19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134e19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134e1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134e1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134e1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134e1b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134e1b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134e1ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134e1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134e1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134e1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134e1d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134e1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134e1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134e1e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134e1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134e1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134e1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134e1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134e20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134e20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134e21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134e212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134e21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134e21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134e220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134e22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134e229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134e22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134e23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134e237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134e23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134e24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134e245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134e24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134e24f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134e254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134e25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134e25f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134e264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134e26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134e26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134e274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134e27a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134e27f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134e284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134e28a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134e28f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134e294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134e299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134e29f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134e2a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134e2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134e2af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134e2b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134e2b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134e2bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134e2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134e2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134e1c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134e2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134e2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134e2db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134e2e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134e2e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134e2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134e2f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134e2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134e2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134e30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134e305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134e30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134e31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134e315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134e31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134e31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134e32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134e328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134e32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134e33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134e336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134e33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134e33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134e34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134e34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134e34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134e35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134e35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134e36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134e364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134e36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134e36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134e372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134e37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134e37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134e380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134e38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134e389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134e38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134e39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134e397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134e39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134e3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134e3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134e3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134e3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134e3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134e3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134e3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134e3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134e3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134e3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134e3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134e3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134e3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134e3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134e3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134e3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134e3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134e3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134e3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134e3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134e3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134e40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134e406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134e40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134e414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134e41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134e42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134e42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134e42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134e43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134e43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134e439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134e43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134e442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134e44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134e44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134e450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134e45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134e45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134e45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134e46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134e467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134e46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134e47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134e475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134e47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134e47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134e483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134e48850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134e48cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134e49240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134e49790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134e49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134e4a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134e4a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134e4ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134e4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134e4b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134e4bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134e4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134e4c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134e4cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134e4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134e4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134e4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134e4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134e4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134e4f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134e4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134e4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134e50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134e50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134e50aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134e50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134e51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134e51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134e51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134e52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134e52a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134e52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134e53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134e53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134e53fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134e54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134e54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134e54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134e55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134e55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134e55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134e564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134e56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134e56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134e574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134e57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134e57f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134e584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134e58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134e58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134e594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134e59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134e59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134e5a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134e5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134e5af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134e5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134e5b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134e5bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134e5c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134e5c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134e5cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134e5d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134e5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134e5df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134e5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134e5e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134e5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134e5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134e5f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134e5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134e60450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134e609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134e60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134e61440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134e61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134e61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134e622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134e62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134e62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134e630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134e63550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134e639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134e63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134e64330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134e647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134e64c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134e65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134e655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134e65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134e65ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134e66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134e66b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134e67280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134e679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134e680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134e68380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134e68b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134e68e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134e69440 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.093.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127104dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127105240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1271056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127105b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127105f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127106400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127106870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127106ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127107150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1271075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127107a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127108120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127108c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1271093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127109c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12710a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12710aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12710b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12710b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12710bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12710c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12710cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12710d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12710dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12710e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12710e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12710e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12710ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12710f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12710f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12710fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12710ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127110430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1271106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127110b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127110fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127111440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1271118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127111d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127112190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127112600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127112a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127112ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127113350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1271137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127113c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1271140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127114510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127114980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127114df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127115260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1271156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127115b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127115fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127116420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127116890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127116e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127117300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127117770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127117be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127118050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1271184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127118930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127118da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127119210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127119680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127119af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127119f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12711a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12711a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12711acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12711b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12711b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12711ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12711be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12711c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12711c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12711cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12711d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12711d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12711d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12711dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12711e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12711e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12711ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12711ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12711f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12711f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12711fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127120100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127120570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1271209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127120e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1271212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127121730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127121ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127122010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127122480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1271228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127122d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1271231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127123640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127123ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127123f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127124390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127124800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127124c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1271250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127125550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1271259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127125e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1271262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127126710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127126b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127126ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127127460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1271278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127127d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1271281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127128620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127128a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127128f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127129370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1271297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127129c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12712a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12712a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12712a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12712ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12712b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12712b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12712bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12712bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12712c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12712c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12712cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12712d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12712d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12712da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12712dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12712e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12712e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12712ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12712f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12712f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12712f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12712fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127130260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1271306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127130b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127130fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127131420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127131890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127131d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127132170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1271325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127132a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127132ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127133330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1271337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127133c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127134080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1271344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127134960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127134dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127135240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127135e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127136130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1271363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127136860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127136cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127137140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1271375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127137a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127137e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127138300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127138770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127138be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127139050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1271394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127139930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127139da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12713a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12713a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12713aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12713af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12713b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12713b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12713bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12713c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12713c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12713ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12713ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12713d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12713d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12713dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12713e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12713e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12713e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12713ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12713f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12713f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12713fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1271400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127140540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1271409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127140e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127141290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1271417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127141cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127142830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127142af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1271430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127143670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127143c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1271441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1271447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127144d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127145330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1271458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127145eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127146470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127146a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127146ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1271475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127147b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127148130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1271486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127148cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127149270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127149830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127149df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12714a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12714a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12714af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12714b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12714bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12714c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12714c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12714cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12714d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12714d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12714dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12714e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12714e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12714ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12714f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12714f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12714ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127150570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127150b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1271510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1271516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127151c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127152230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1271527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127152db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127153370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127153930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127153ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1271544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127154a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127155030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1271555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127155bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127156170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127156730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127156cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1271571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1271576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127157bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1271580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1271585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127158af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127158ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1271594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1271599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127159ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12715a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12715a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12715adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12715b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12715b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12715c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12715c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12715d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12715d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12715da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12715e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12715e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12715eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134e690f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134e4c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134e4a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134e4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134e1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134e1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134e204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134e4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134e15860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134e1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134e1cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134e1d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134e1b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134e1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134e14860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134e20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134e2d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134e68640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134e17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134e17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134e4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134e4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134e15e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134e16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134e163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134e698a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134e69b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134e69e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134e6a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134e6a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134e6a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134e6a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134e6abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134e6aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134e6b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134e6b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134e6b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134e6b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134e6bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134e6bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134e6c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134e6c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134e6c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134e6ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134e6cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134e6cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134e6d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134e6d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134e6d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134e6daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134e6dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134e6e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134e6e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134e6e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134e6e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134e6eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134e6ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134e6f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134e6f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134e6f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134e6f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134e6fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134e6fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134e70120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134e703e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134e706a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134e70960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134e70c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134e70ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134e711a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134e71460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134e71720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134e719e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134e71ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134e71f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134e72220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134e724e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134e727a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134e72a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134e72d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134e72fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134e732a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134e73560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134e73820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134e73ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134e73da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134e74060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134e74320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134e745e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134e748a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134e74b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134e74e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134e750e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134e753a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134e75660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134e75920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134e75be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134e75ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134e76160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134e76420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134e766e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134e769a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134e76c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134e76f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134e771e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134e774a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134e77760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134e77a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134e77ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134e77fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134e78260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134e78520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134e787e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134e78aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134e78d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134e79020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134e792e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134e795a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134e79860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134e79b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134e79de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134e7a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134e7a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134e7a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134e7a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134e7aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134e7ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134e7b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134e7b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134e7b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134e7b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134e7bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134e7bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134e7c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134e7c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134e7c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134e7c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134e7cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134e7cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134e7d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134e7d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134e7d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134e7da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134e7dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134e7dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134e7e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134e7e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134e7e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134e7eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134e7eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134e7f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134e7f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134e7f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134e7f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134e7fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134e7fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134e800e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134e803a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134e80660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134e80920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134e80be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134e80ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134e81160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134e81420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134e816e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134e819a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134e81c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134e81f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134e821e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134e824a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134e82760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134e82a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134e82ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134e82fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134e83260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134e83520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134e837e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134e83aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134e83d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134e84020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134e842e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134e845a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134e84860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134e84b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134e84de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134e850a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134e85360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134e85620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134e858e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134e85ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134e85e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134e86120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134e863e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134e866a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134e86960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134e86c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134e86ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134e871a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134e87460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134e87720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134e879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134e87ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134e87f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134e88220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134e884e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134e887a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134e88a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134e88d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134e88fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134e892a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134e89870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134e89b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134e8a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134e8a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134e8ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134e8b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134e8b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134e8bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134e8c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134e8c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134e8cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134e8d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134e8d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134e8daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134e8e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134e8e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134e8eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134e8f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134e8f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134e8fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134e90020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134e90570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134e90ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134e91010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134e91560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134e91ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134e92000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134e92550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134e92aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134e92ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134e93540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134e93a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134e93fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134e94530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134e94a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134e94fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134e95520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134e95a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134e95fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134e96510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134e96a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134e96fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134e97500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134e97a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134e97fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134e984f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134e98a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134e98f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134e994e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134e99a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134e99f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134e9a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134e9aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134e9af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134e9b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134e9ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134e9bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134e9c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134e9c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134e9c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134e9cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134e9d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134e9d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134e9d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134e9ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134e9e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134e9e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134e9eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134e9ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134e9f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134e9f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134e9fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134ea0150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134ea05c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134ea12b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134ea19d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134ea20f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134ea23b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134ea2820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134ea2e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134ea3430 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.915s
user	0m0.242s
sys	0m0.137s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
